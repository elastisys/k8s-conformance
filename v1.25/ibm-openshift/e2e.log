I0627 16:22:05.181380      23 e2e.go:116] Starting e2e run "05315eb1-c48b-405c-954b-4593116b0046" on Ginkgo node 1
Jun 27 16:22:05.198: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1687882925 - will randomize all specs

Will run 360 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Jun 27 16:22:05.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 16:22:05.370: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Jun 27 16:22:05.411: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Jun 27 16:22:05.480: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Jun 27 16:22:05.480: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
Jun 27 16:22:05.480: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Jun 27 16:22:05.493: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
Jun 27 16:22:05.493: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
Jun 27 16:22:05.493: INFO: e2e test version: v1.25.10
Jun 27 16:22:05.496: INFO: kube-apiserver version: v1.25.10+3fe2906
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Jun 27 16:22:05.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 16:22:05.511: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.144 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jun 27 16:22:05.368: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 16:22:05.370: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Jun 27 16:22:05.411: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Jun 27 16:22:05.480: INFO: 14 / 14 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Jun 27 16:22:05.480: INFO: expected 5 pod replicas in namespace 'kube-system', 5 are Running and Ready.
    Jun 27 16:22:05.480: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Jun 27 16:22:05.493: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibm-keepalived-watcher' (0 seconds elapsed)
    Jun 27 16:22:05.493: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'ibmcloud-block-storage-driver' (0 seconds elapsed)
    Jun 27 16:22:05.493: INFO: e2e test version: v1.25.10
    Jun 27 16:22:05.496: INFO: kube-apiserver version: v1.25.10+3fe2906
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Jun 27 16:22:05.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 16:22:05.511: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:22:05.536
Jun 27 16:22:05.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 16:22:05.537
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:22:05.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:22:05.582
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
Jun 27 16:22:05.609: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-3672af80-571f-4cd9-b283-7018f5c00d44 06/27/23 16:22:05.609
STEP: Creating configMap with name cm-test-opt-upd-c861f2f6-32ed-4513-9566-53464a3f04a4 06/27/23 16:22:05.623
STEP: Creating the pod 06/27/23 16:22:05.637
Jun 27 16:22:05.685: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f" in namespace "configmap-2237" to be "running and ready"
Jun 27 16:22:05.706: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.27513ms
Jun 27 16:22:05.706: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:22:07.718: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032541945s
Jun 27 16:22:07.718: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:22:09.720: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034037788s
Jun 27 16:22:09.720: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:22:11.717: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030961798s
Jun 27 16:22:11.717: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:22:13.720: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034174637s
Jun 27 16:22:13.720: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:22:15.716: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Running", Reason="", readiness=true. Elapsed: 10.030432764s
Jun 27 16:22:15.716: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Running (Ready = true)
Jun 27 16:22:15.716: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-3672af80-571f-4cd9-b283-7018f5c00d44 06/27/23 16:22:15.882
STEP: Updating configmap cm-test-opt-upd-c861f2f6-32ed-4513-9566-53464a3f04a4 06/27/23 16:22:15.9
STEP: Creating configMap with name cm-test-opt-create-58065eee-3a77-4bee-9767-31a070ad124a 06/27/23 16:22:15.913
STEP: waiting to observe update in volume 06/27/23 16:22:15.932
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 16:23:29.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2237" for this suite. 06/27/23 16:23:29.43
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":1,"skipped":5,"failed":0}
------------------------------
â€¢ [SLOW TEST] [83.911 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:22:05.536
    Jun 27 16:22:05.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 16:22:05.537
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:22:05.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:22:05.582
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    Jun 27 16:22:05.609: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-3672af80-571f-4cd9-b283-7018f5c00d44 06/27/23 16:22:05.609
    STEP: Creating configMap with name cm-test-opt-upd-c861f2f6-32ed-4513-9566-53464a3f04a4 06/27/23 16:22:05.623
    STEP: Creating the pod 06/27/23 16:22:05.637
    Jun 27 16:22:05.685: INFO: Waiting up to 5m0s for pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f" in namespace "configmap-2237" to be "running and ready"
    Jun 27 16:22:05.706: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.27513ms
    Jun 27 16:22:05.706: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:22:07.718: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032541945s
    Jun 27 16:22:07.718: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:22:09.720: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034037788s
    Jun 27 16:22:09.720: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:22:11.717: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.030961798s
    Jun 27 16:22:11.717: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:22:13.720: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.034174637s
    Jun 27 16:22:13.720: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:22:15.716: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f": Phase="Running", Reason="", readiness=true. Elapsed: 10.030432764s
    Jun 27 16:22:15.716: INFO: The phase of Pod pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f is Running (Ready = true)
    Jun 27 16:22:15.716: INFO: Pod "pod-configmaps-1ca6f727-6707-4250-a161-3bfda90b992f" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-3672af80-571f-4cd9-b283-7018f5c00d44 06/27/23 16:22:15.882
    STEP: Updating configmap cm-test-opt-upd-c861f2f6-32ed-4513-9566-53464a3f04a4 06/27/23 16:22:15.9
    STEP: Creating configMap with name cm-test-opt-create-58065eee-3a77-4bee-9767-31a070ad124a 06/27/23 16:22:15.913
    STEP: waiting to observe update in volume 06/27/23 16:22:15.932
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 16:23:29.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2237" for this suite. 06/27/23 16:23:29.43
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:23:29.452
Jun 27 16:23:29.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 16:23:29.453
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:29.493
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:29.5
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 06/27/23 16:23:29.507
Jun 27 16:23:29.558: INFO: Waiting up to 5m0s for pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736" in namespace "emptydir-8507" to be "Succeeded or Failed"
Jun 27 16:23:29.571: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 12.409677ms
Jun 27 16:23:31.582: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023239781s
Jun 27 16:23:33.581: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022419359s
Jun 27 16:23:35.584: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02591236s
Jun 27 16:23:37.582: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023468866s
Jun 27 16:23:39.582: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.023459103s
STEP: Saw pod success 06/27/23 16:23:39.582
Jun 27 16:23:39.583: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736" satisfied condition "Succeeded or Failed"
Jun 27 16:23:39.592: INFO: Trying to get logs from node 10.113.180.96 pod pod-368d56d9-9b64-4f10-a75d-b5afa0e78736 container test-container: <nil>
STEP: delete the pod 06/27/23 16:23:39.659
Jun 27 16:23:39.686: INFO: Waiting for pod pod-368d56d9-9b64-4f10-a75d-b5afa0e78736 to disappear
Jun 27 16:23:39.695: INFO: Pod pod-368d56d9-9b64-4f10-a75d-b5afa0e78736 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 16:23:39.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8507" for this suite. 06/27/23 16:23:39.709
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":2,"skipped":76,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.293 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:23:29.452
    Jun 27 16:23:29.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 16:23:29.453
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:29.493
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:29.5
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/27/23 16:23:29.507
    Jun 27 16:23:29.558: INFO: Waiting up to 5m0s for pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736" in namespace "emptydir-8507" to be "Succeeded or Failed"
    Jun 27 16:23:29.571: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 12.409677ms
    Jun 27 16:23:31.582: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023239781s
    Jun 27 16:23:33.581: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022419359s
    Jun 27 16:23:35.584: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 6.02591236s
    Jun 27 16:23:37.582: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Pending", Reason="", readiness=false. Elapsed: 8.023468866s
    Jun 27 16:23:39.582: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.023459103s
    STEP: Saw pod success 06/27/23 16:23:39.582
    Jun 27 16:23:39.583: INFO: Pod "pod-368d56d9-9b64-4f10-a75d-b5afa0e78736" satisfied condition "Succeeded or Failed"
    Jun 27 16:23:39.592: INFO: Trying to get logs from node 10.113.180.96 pod pod-368d56d9-9b64-4f10-a75d-b5afa0e78736 container test-container: <nil>
    STEP: delete the pod 06/27/23 16:23:39.659
    Jun 27 16:23:39.686: INFO: Waiting for pod pod-368d56d9-9b64-4f10-a75d-b5afa0e78736 to disappear
    Jun 27 16:23:39.695: INFO: Pod pod-368d56d9-9b64-4f10-a75d-b5afa0e78736 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 16:23:39.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8507" for this suite. 06/27/23 16:23:39.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:23:39.748
Jun 27 16:23:39.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename limitrange 06/27/23 16:23:39.749
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:39.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:39.79
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 06/27/23 16:23:39.799
STEP: Setting up watch 06/27/23 16:23:39.799
STEP: Submitting a LimitRange 06/27/23 16:23:39.917
STEP: Verifying LimitRange creation was observed 06/27/23 16:23:39.934
STEP: Fetching the LimitRange to ensure it has proper values 06/27/23 16:23:39.934
Jun 27 16:23:39.946: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 27 16:23:39.947: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 06/27/23 16:23:39.947
STEP: Ensuring Pod has resource requirements applied from LimitRange 06/27/23 16:23:39.978
Jun 27 16:23:39.988: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Jun 27 16:23:39.988: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 06/27/23 16:23:39.988
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/27/23 16:23:40.018
Jun 27 16:23:40.028: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Jun 27 16:23:40.028: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 06/27/23 16:23:40.028
STEP: Failing to create a Pod with more than max resources 06/27/23 16:23:40.043
STEP: Updating a LimitRange 06/27/23 16:23:40.055
STEP: Verifying LimitRange updating is effective 06/27/23 16:23:40.073
STEP: Creating a Pod with less than former min resources 06/27/23 16:23:42.089
STEP: Failing to create a Pod with more than max resources 06/27/23 16:23:42.116
STEP: Deleting a LimitRange 06/27/23 16:23:42.129
STEP: Verifying the LimitRange was deleted 06/27/23 16:23:42.159
Jun 27 16:23:47.176: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 06/27/23 16:23:47.177
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Jun 27 16:23:47.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-3393" for this suite. 06/27/23 16:23:47.239
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":3,"skipped":97,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.533 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:23:39.748
    Jun 27 16:23:39.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename limitrange 06/27/23 16:23:39.749
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:39.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:39.79
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 06/27/23 16:23:39.799
    STEP: Setting up watch 06/27/23 16:23:39.799
    STEP: Submitting a LimitRange 06/27/23 16:23:39.917
    STEP: Verifying LimitRange creation was observed 06/27/23 16:23:39.934
    STEP: Fetching the LimitRange to ensure it has proper values 06/27/23 16:23:39.934
    Jun 27 16:23:39.946: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun 27 16:23:39.947: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 06/27/23 16:23:39.947
    STEP: Ensuring Pod has resource requirements applied from LimitRange 06/27/23 16:23:39.978
    Jun 27 16:23:39.988: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Jun 27 16:23:39.988: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 06/27/23 16:23:39.988
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 06/27/23 16:23:40.018
    Jun 27 16:23:40.028: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Jun 27 16:23:40.028: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 06/27/23 16:23:40.028
    STEP: Failing to create a Pod with more than max resources 06/27/23 16:23:40.043
    STEP: Updating a LimitRange 06/27/23 16:23:40.055
    STEP: Verifying LimitRange updating is effective 06/27/23 16:23:40.073
    STEP: Creating a Pod with less than former min resources 06/27/23 16:23:42.089
    STEP: Failing to create a Pod with more than max resources 06/27/23 16:23:42.116
    STEP: Deleting a LimitRange 06/27/23 16:23:42.129
    STEP: Verifying the LimitRange was deleted 06/27/23 16:23:42.159
    Jun 27 16:23:47.176: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 06/27/23 16:23:47.177
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Jun 27 16:23:47.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-3393" for this suite. 06/27/23 16:23:47.239
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:23:47.292
Jun 27 16:23:47.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:23:47.293
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:47.327
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:47.337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 06/27/23 16:23:47.35
Jun 27 16:23:47.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9611 create -f -'
Jun 27 16:23:48.259: INFO: stderr: ""
Jun 27 16:23:48.259: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/27/23 16:23:48.259
Jun 27 16:23:49.270: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 16:23:49.270: INFO: Found 0 / 1
Jun 27 16:23:50.272: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 16:23:50.272: INFO: Found 0 / 1
Jun 27 16:23:51.269: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 16:23:51.269: INFO: Found 1 / 1
Jun 27 16:23:51.269: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 06/27/23 16:23:51.269
Jun 27 16:23:51.278: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 16:23:51.278: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 27 16:23:51.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9611 patch pod agnhost-primary-v87p7 -p {"metadata":{"annotations":{"x":"y"}}}'
Jun 27 16:23:51.496: INFO: stderr: ""
Jun 27 16:23:51.496: INFO: stdout: "pod/agnhost-primary-v87p7 patched\n"
STEP: checking annotations 06/27/23 16:23:51.496
Jun 27 16:23:51.506: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 16:23:51.506: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:23:51.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9611" for this suite. 06/27/23 16:23:51.524
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":4,"skipped":115,"failed":0}
------------------------------
â€¢ [4.250 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:23:47.292
    Jun 27 16:23:47.292: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:23:47.293
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:47.327
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:47.337
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 06/27/23 16:23:47.35
    Jun 27 16:23:47.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9611 create -f -'
    Jun 27 16:23:48.259: INFO: stderr: ""
    Jun 27 16:23:48.259: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/27/23 16:23:48.259
    Jun 27 16:23:49.270: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 16:23:49.270: INFO: Found 0 / 1
    Jun 27 16:23:50.272: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 16:23:50.272: INFO: Found 0 / 1
    Jun 27 16:23:51.269: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 16:23:51.269: INFO: Found 1 / 1
    Jun 27 16:23:51.269: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 06/27/23 16:23:51.269
    Jun 27 16:23:51.278: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 16:23:51.278: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 27 16:23:51.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9611 patch pod agnhost-primary-v87p7 -p {"metadata":{"annotations":{"x":"y"}}}'
    Jun 27 16:23:51.496: INFO: stderr: ""
    Jun 27 16:23:51.496: INFO: stdout: "pod/agnhost-primary-v87p7 patched\n"
    STEP: checking annotations 06/27/23 16:23:51.496
    Jun 27 16:23:51.506: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 16:23:51.506: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:23:51.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9611" for this suite. 06/27/23 16:23:51.524
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:23:51.547
Jun 27 16:23:51.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 16:23:51.55
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:51.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:51.594
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 06/27/23 16:23:51.602
Jun 27 16:23:51.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d" in namespace "downward-api-2908" to be "Succeeded or Failed"
Jun 27 16:23:51.652: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.421408ms
Jun 27 16:23:53.663: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019506035s
Jun 27 16:23:55.662: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017972727s
Jun 27 16:23:57.662: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018584648s
STEP: Saw pod success 06/27/23 16:23:57.662
Jun 27 16:23:57.663: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d" satisfied condition "Succeeded or Failed"
Jun 27 16:23:57.672: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d container client-container: <nil>
STEP: delete the pod 06/27/23 16:23:57.697
Jun 27 16:23:57.728: INFO: Waiting for pod downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d to disappear
Jun 27 16:23:57.736: INFO: Pod downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 16:23:57.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2908" for this suite. 06/27/23 16:23:57.753
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":5,"skipped":127,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.224 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:23:51.547
    Jun 27 16:23:51.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 16:23:51.55
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:51.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:51.594
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 06/27/23 16:23:51.602
    Jun 27 16:23:51.644: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d" in namespace "downward-api-2908" to be "Succeeded or Failed"
    Jun 27 16:23:51.652: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d": Phase="Pending", Reason="", readiness=false. Elapsed: 8.421408ms
    Jun 27 16:23:53.663: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019506035s
    Jun 27 16:23:55.662: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.017972727s
    Jun 27 16:23:57.662: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.018584648s
    STEP: Saw pod success 06/27/23 16:23:57.662
    Jun 27 16:23:57.663: INFO: Pod "downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d" satisfied condition "Succeeded or Failed"
    Jun 27 16:23:57.672: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d container client-container: <nil>
    STEP: delete the pod 06/27/23 16:23:57.697
    Jun 27 16:23:57.728: INFO: Waiting for pod downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d to disappear
    Jun 27 16:23:57.736: INFO: Pod downwardapi-volume-b1cd760c-c0cc-4957-8fad-7591d5ac113d no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 16:23:57.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2908" for this suite. 06/27/23 16:23:57.753
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:23:57.783
Jun 27 16:23:57.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 16:23:57.785
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:57.812
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:57.823
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Jun 27 16:23:57.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/27/23 16:24:07.405
Jun 27 16:24:07.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 --namespace=crd-publish-openapi-9996 create -f -'
Jun 27 16:24:09.487: INFO: stderr: ""
Jun 27 16:24:09.487: INFO: stdout: "e2e-test-crd-publish-openapi-9445-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 27 16:24:09.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 --namespace=crd-publish-openapi-9996 delete e2e-test-crd-publish-openapi-9445-crds test-cr'
Jun 27 16:24:09.720: INFO: stderr: ""
Jun 27 16:24:09.720: INFO: stdout: "e2e-test-crd-publish-openapi-9445-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Jun 27 16:24:09.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 --namespace=crd-publish-openapi-9996 apply -f -'
Jun 27 16:24:11.724: INFO: stderr: ""
Jun 27 16:24:11.724: INFO: stdout: "e2e-test-crd-publish-openapi-9445-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Jun 27 16:24:11.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 --namespace=crd-publish-openapi-9996 delete e2e-test-crd-publish-openapi-9445-crds test-cr'
Jun 27 16:24:11.900: INFO: stderr: ""
Jun 27 16:24:11.900: INFO: stdout: "e2e-test-crd-publish-openapi-9445-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 06/27/23 16:24:11.9
Jun 27 16:24:11.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 explain e2e-test-crd-publish-openapi-9445-crds'
Jun 27 16:24:13.499: INFO: stderr: ""
Jun 27 16:24:13.499: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9445-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:24:22.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-9996" for this suite. 06/27/23 16:24:22.475
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":6,"skipped":157,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.721 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:23:57.783
    Jun 27 16:23:57.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 16:23:57.785
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:23:57.812
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:23:57.823
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Jun 27 16:23:57.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/27/23 16:24:07.405
    Jun 27 16:24:07.406: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 --namespace=crd-publish-openapi-9996 create -f -'
    Jun 27 16:24:09.487: INFO: stderr: ""
    Jun 27 16:24:09.487: INFO: stdout: "e2e-test-crd-publish-openapi-9445-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun 27 16:24:09.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 --namespace=crd-publish-openapi-9996 delete e2e-test-crd-publish-openapi-9445-crds test-cr'
    Jun 27 16:24:09.720: INFO: stderr: ""
    Jun 27 16:24:09.720: INFO: stdout: "e2e-test-crd-publish-openapi-9445-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Jun 27 16:24:09.720: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 --namespace=crd-publish-openapi-9996 apply -f -'
    Jun 27 16:24:11.724: INFO: stderr: ""
    Jun 27 16:24:11.724: INFO: stdout: "e2e-test-crd-publish-openapi-9445-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Jun 27 16:24:11.724: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 --namespace=crd-publish-openapi-9996 delete e2e-test-crd-publish-openapi-9445-crds test-cr'
    Jun 27 16:24:11.900: INFO: stderr: ""
    Jun 27 16:24:11.900: INFO: stdout: "e2e-test-crd-publish-openapi-9445-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 06/27/23 16:24:11.9
    Jun 27 16:24:11.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-9996 explain e2e-test-crd-publish-openapi-9445-crds'
    Jun 27 16:24:13.499: INFO: stderr: ""
    Jun 27 16:24:13.499: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-9445-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:24:22.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-9996" for this suite. 06/27/23 16:24:22.475
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:24:22.505
Jun 27 16:24:22.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 16:24:22.507
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:22.571
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:22.59
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 16:24:22.747
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:24:23.048
STEP: Deploying the webhook pod 06/27/23 16:24:23.092
STEP: Wait for the deployment to be ready 06/27/23 16:24:23.149
Jun 27 16:24:23.188: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 27 16:24:25.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 16:24:27.278
STEP: Verifying the service has paired with the endpoint 06/27/23 16:24:27.31
Jun 27 16:24:28.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/27/23 16:24:28.334
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/27/23 16:24:28.399
STEP: Creating a dummy validating-webhook-configuration object 06/27/23 16:24:28.464
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/27/23 16:24:28.504
STEP: Creating a dummy mutating-webhook-configuration object 06/27/23 16:24:28.522
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/27/23 16:24:28.563
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:24:28.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1085" for this suite. 06/27/23 16:24:28.656
STEP: Destroying namespace "webhook-1085-markers" for this suite. 06/27/23 16:24:28.683
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":7,"skipped":158,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.367 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:24:22.505
    Jun 27 16:24:22.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 16:24:22.507
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:22.571
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:22.59
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 16:24:22.747
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:24:23.048
    STEP: Deploying the webhook pod 06/27/23 16:24:23.092
    STEP: Wait for the deployment to be ready 06/27/23 16:24:23.149
    Jun 27 16:24:23.188: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 27 16:24:25.258: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 16:24:27.278
    STEP: Verifying the service has paired with the endpoint 06/27/23 16:24:27.31
    Jun 27 16:24:28.312: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/27/23 16:24:28.334
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 06/27/23 16:24:28.399
    STEP: Creating a dummy validating-webhook-configuration object 06/27/23 16:24:28.464
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 06/27/23 16:24:28.504
    STEP: Creating a dummy mutating-webhook-configuration object 06/27/23 16:24:28.522
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 06/27/23 16:24:28.563
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:24:28.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1085" for this suite. 06/27/23 16:24:28.656
    STEP: Destroying namespace "webhook-1085-markers" for this suite. 06/27/23 16:24:28.683
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:24:28.873
Jun 27 16:24:28.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 16:24:28.875
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:28.928
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:28.948
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 06/27/23 16:24:28.963
Jun 27 16:24:29.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63" in namespace "downward-api-1345" to be "Succeeded or Failed"
Jun 27 16:24:29.045: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63": Phase="Pending", Reason="", readiness=false. Elapsed: 21.139945ms
Jun 27 16:24:31.064: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040171195s
Jun 27 16:24:33.063: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03997166s
Jun 27 16:24:35.065: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041613706s
STEP: Saw pod success 06/27/23 16:24:35.065
Jun 27 16:24:35.066: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63" satisfied condition "Succeeded or Failed"
Jun 27 16:24:35.085: INFO: Trying to get logs from node 10.113.180.96 pod downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63 container client-container: <nil>
STEP: delete the pod 06/27/23 16:24:35.179
Jun 27 16:24:35.231: INFO: Waiting for pod downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63 to disappear
Jun 27 16:24:35.252: INFO: Pod downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 16:24:35.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1345" for this suite. 06/27/23 16:24:35.279
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":8,"skipped":170,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.437 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:24:28.873
    Jun 27 16:24:28.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 16:24:28.875
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:28.928
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:28.948
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 06/27/23 16:24:28.963
    Jun 27 16:24:29.023: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63" in namespace "downward-api-1345" to be "Succeeded or Failed"
    Jun 27 16:24:29.045: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63": Phase="Pending", Reason="", readiness=false. Elapsed: 21.139945ms
    Jun 27 16:24:31.064: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040171195s
    Jun 27 16:24:33.063: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03997166s
    Jun 27 16:24:35.065: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041613706s
    STEP: Saw pod success 06/27/23 16:24:35.065
    Jun 27 16:24:35.066: INFO: Pod "downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63" satisfied condition "Succeeded or Failed"
    Jun 27 16:24:35.085: INFO: Trying to get logs from node 10.113.180.96 pod downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63 container client-container: <nil>
    STEP: delete the pod 06/27/23 16:24:35.179
    Jun 27 16:24:35.231: INFO: Waiting for pod downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63 to disappear
    Jun 27 16:24:35.252: INFO: Pod downwardapi-volume-9c62c602-2707-4ee2-bdb4-3fd5843ffb63 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 16:24:35.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1345" for this suite. 06/27/23 16:24:35.279
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:24:35.318
Jun 27 16:24:35.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:24:35.32
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:35.39
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:35.406
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-535a2473-71fe-4dc9-a6c3-34d9d5d84eb7 06/27/23 16:24:35.423
STEP: Creating secret with name secret-projected-all-test-volume-61dc2f30-6707-4d3a-ae4a-936e630a75c5 06/27/23 16:24:35.449
STEP: Creating a pod to test Check all projections for projected volume plugin 06/27/23 16:24:35.471
Jun 27 16:24:35.585: INFO: Waiting up to 5m0s for pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678" in namespace "projected-8179" to be "Succeeded or Failed"
Jun 27 16:24:35.601: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678": Phase="Pending", Reason="", readiness=false. Elapsed: 15.7957ms
Jun 27 16:24:37.623: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037041735s
Jun 27 16:24:39.622: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036627791s
Jun 27 16:24:41.622: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036084297s
STEP: Saw pod success 06/27/23 16:24:41.622
Jun 27 16:24:41.622: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678" satisfied condition "Succeeded or Failed"
Jun 27 16:24:41.644: INFO: Trying to get logs from node 10.113.180.90 pod projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678 container projected-all-volume-test: <nil>
STEP: delete the pod 06/27/23 16:24:41.731
Jun 27 16:24:41.789: INFO: Waiting for pod projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678 to disappear
Jun 27 16:24:41.806: INFO: Pod projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678 no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Jun 27 16:24:41.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8179" for this suite. 06/27/23 16:24:41.845
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":9,"skipped":172,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.560 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:24:35.318
    Jun 27 16:24:35.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:24:35.32
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:35.39
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:35.406
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-535a2473-71fe-4dc9-a6c3-34d9d5d84eb7 06/27/23 16:24:35.423
    STEP: Creating secret with name secret-projected-all-test-volume-61dc2f30-6707-4d3a-ae4a-936e630a75c5 06/27/23 16:24:35.449
    STEP: Creating a pod to test Check all projections for projected volume plugin 06/27/23 16:24:35.471
    Jun 27 16:24:35.585: INFO: Waiting up to 5m0s for pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678" in namespace "projected-8179" to be "Succeeded or Failed"
    Jun 27 16:24:35.601: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678": Phase="Pending", Reason="", readiness=false. Elapsed: 15.7957ms
    Jun 27 16:24:37.623: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037041735s
    Jun 27 16:24:39.622: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036627791s
    Jun 27 16:24:41.622: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036084297s
    STEP: Saw pod success 06/27/23 16:24:41.622
    Jun 27 16:24:41.622: INFO: Pod "projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678" satisfied condition "Succeeded or Failed"
    Jun 27 16:24:41.644: INFO: Trying to get logs from node 10.113.180.90 pod projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678 container projected-all-volume-test: <nil>
    STEP: delete the pod 06/27/23 16:24:41.731
    Jun 27 16:24:41.789: INFO: Waiting for pod projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678 to disappear
    Jun 27 16:24:41.806: INFO: Pod projected-volume-4a64a7b0-dec1-4c14-ac96-5aab598c5678 no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Jun 27 16:24:41.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8179" for this suite. 06/27/23 16:24:41.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:24:41.88
Jun 27 16:24:41.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 16:24:41.883
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:41.937
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:41.954
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 16:24:42.054
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:24:43.075
STEP: Deploying the webhook pod 06/27/23 16:24:43.124
STEP: Wait for the deployment to be ready 06/27/23 16:24:43.168
Jun 27 16:24:43.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun 27 16:24:45.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 16:24:47.257
STEP: Verifying the service has paired with the endpoint 06/27/23 16:24:47.29
Jun 27 16:24:48.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 06/27/23 16:24:48.306
STEP: create a pod 06/27/23 16:24:48.389
Jun 27 16:24:48.477: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7110" to be "running"
Jun 27 16:24:48.494: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.35848ms
Jun 27 16:24:50.514: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.037848056s
Jun 27 16:24:50.514: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 06/27/23 16:24:50.515
Jun 27 16:24:50.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=webhook-7110 attach --namespace=webhook-7110 to-be-attached-pod -i -c=container1'
Jun 27 16:24:50.760: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:24:50.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7110" for this suite. 06/27/23 16:24:50.805
STEP: Destroying namespace "webhook-7110-markers" for this suite. 06/27/23 16:24:50.831
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":10,"skipped":187,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.107 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:24:41.88
    Jun 27 16:24:41.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 16:24:41.883
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:41.937
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:41.954
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 16:24:42.054
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:24:43.075
    STEP: Deploying the webhook pod 06/27/23 16:24:43.124
    STEP: Wait for the deployment to be ready 06/27/23 16:24:43.168
    Jun 27 16:24:43.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jun 27 16:24:45.257: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 24, 43, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 16:24:47.257
    STEP: Verifying the service has paired with the endpoint 06/27/23 16:24:47.29
    Jun 27 16:24:48.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 06/27/23 16:24:48.306
    STEP: create a pod 06/27/23 16:24:48.389
    Jun 27 16:24:48.477: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-7110" to be "running"
    Jun 27 16:24:48.494: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.35848ms
    Jun 27 16:24:50.514: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.037848056s
    Jun 27 16:24:50.514: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 06/27/23 16:24:50.515
    Jun 27 16:24:50.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=webhook-7110 attach --namespace=webhook-7110 to-be-attached-pod -i -c=container1'
    Jun 27 16:24:50.760: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:24:50.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7110" for this suite. 06/27/23 16:24:50.805
    STEP: Destroying namespace "webhook-7110-markers" for this suite. 06/27/23 16:24:50.831
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:24:51.001
Jun 27 16:24:51.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 16:24:51.003
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:51.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:51.065
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Jun 27 16:24:51.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:24:52.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4590" for this suite. 06/27/23 16:24:52.212
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":11,"skipped":241,"failed":0}
------------------------------
â€¢ [1.233 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:24:51.001
    Jun 27 16:24:51.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 16:24:51.003
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:51.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:51.065
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Jun 27 16:24:51.080: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:24:52.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4590" for this suite. 06/27/23 16:24:52.212
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:24:52.238
Jun 27 16:24:52.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 16:24:52.24
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:52.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:52.295
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 06/27/23 16:24:52.314
Jun 27 16:24:52.422: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896" in namespace "emptydir-2029" to be "running"
Jun 27 16:24:52.441: INFO: Pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896": Phase="Pending", Reason="", readiness=false. Elapsed: 19.560842ms
Jun 27 16:24:54.462: INFO: Pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040000736s
Jun 27 16:24:56.480: INFO: Pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896": Phase="Running", Reason="", readiness=false. Elapsed: 4.058070269s
Jun 27 16:24:56.480: INFO: Pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896" satisfied condition "running"
STEP: Reading file content from the nginx-container 06/27/23 16:24:56.48
Jun 27 16:24:56.480: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2029 PodName:pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 16:24:56.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 16:24:56.482: INFO: ExecWithOptions: Clientset creation
Jun 27 16:24:56.482: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-2029/pods/pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Jun 27 16:24:56.858: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 16:24:56.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2029" for this suite. 06/27/23 16:24:56.9
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":12,"skipped":257,"failed":0}
------------------------------
â€¢ [4.688 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:24:52.238
    Jun 27 16:24:52.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 16:24:52.24
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:52.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:52.295
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 06/27/23 16:24:52.314
    Jun 27 16:24:52.422: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896" in namespace "emptydir-2029" to be "running"
    Jun 27 16:24:52.441: INFO: Pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896": Phase="Pending", Reason="", readiness=false. Elapsed: 19.560842ms
    Jun 27 16:24:54.462: INFO: Pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040000736s
    Jun 27 16:24:56.480: INFO: Pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896": Phase="Running", Reason="", readiness=false. Elapsed: 4.058070269s
    Jun 27 16:24:56.480: INFO: Pod "pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896" satisfied condition "running"
    STEP: Reading file content from the nginx-container 06/27/23 16:24:56.48
    Jun 27 16:24:56.480: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-2029 PodName:pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 16:24:56.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 16:24:56.482: INFO: ExecWithOptions: Clientset creation
    Jun 27 16:24:56.482: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/emptydir-2029/pods/pod-sharedvolume-94f5db64-6a33-4a8f-af76-ed591871b896/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Jun 27 16:24:56.858: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 16:24:56.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2029" for this suite. 06/27/23 16:24:56.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:24:56.94
Jun 27 16:24:56.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:24:56.942
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:57.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:57.04
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/27/23 16:24:57.056
Jun 27 16:24:57.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4893 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 27 16:24:57.244: INFO: stderr: ""
Jun 27 16:24:57.244: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 06/27/23 16:24:57.244
STEP: verifying the pod e2e-test-httpd-pod was created 06/27/23 16:25:07.295
Jun 27 16:25:07.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4893 get pod e2e-test-httpd-pod -o json'
Jun 27 16:25:07.447: INFO: stderr: ""
Jun 27 16:25:07.447: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"7135195ace46ad793092da308a2a4a175a8f0c3d2429ac60243bc8097caa811d\",\n            \"cni.projectcalico.org/podIP\": \"172.30.60.123/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.60.123/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.60.123\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.60.123\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-06-27T16:24:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4893\",\n        \"resourceVersion\": \"70463\",\n        \"uid\": \"0e9afcfa-e8c5-486b-bcec-5aca21ab6b02\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-52bdb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-hhjtx\"\n            }\n        ],\n        \"nodeName\": \"10.113.180.96\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c32,c4\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-52bdb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-27T16:24:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-27T16:25:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-27T16:25:04Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-27T16:24:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://d9c8f49a3b6ba3ca2cb40fcb168fe34ffdd5ffc9c0ad9d321441e826926c490b\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-27T16:25:03Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.113.180.96\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.60.123\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.60.123\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-27T16:24:57Z\"\n    }\n}\n"
STEP: replace the image in the pod 06/27/23 16:25:07.448
Jun 27 16:25:07.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4893 replace -f -'
Jun 27 16:25:09.974: INFO: stderr: ""
Jun 27 16:25:09.974: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 06/27/23 16:25:09.974
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Jun 27 16:25:09.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4893 delete pods e2e-test-httpd-pod'
Jun 27 16:25:22.558: INFO: stderr: ""
Jun 27 16:25:22.558: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:25:22.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4893" for this suite. 06/27/23 16:25:22.587
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":13,"skipped":370,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.675 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:24:56.94
    Jun 27 16:24:56.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:24:56.942
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:24:57.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:24:57.04
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/27/23 16:24:57.056
    Jun 27 16:24:57.057: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4893 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun 27 16:24:57.244: INFO: stderr: ""
    Jun 27 16:24:57.244: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 06/27/23 16:24:57.244
    STEP: verifying the pod e2e-test-httpd-pod was created 06/27/23 16:25:07.295
    Jun 27 16:25:07.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4893 get pod e2e-test-httpd-pod -o json'
    Jun 27 16:25:07.447: INFO: stderr: ""
    Jun 27 16:25:07.447: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"7135195ace46ad793092da308a2a4a175a8f0c3d2429ac60243bc8097caa811d\",\n            \"cni.projectcalico.org/podIP\": \"172.30.60.123/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.30.60.123/32\",\n            \"k8s.v1.cni.cncf.io/network-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.60.123\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"k8s.v1.cni.cncf.io/networks-status\": \"[{\\n    \\\"name\\\": \\\"k8s-pod-network\\\",\\n    \\\"ips\\\": [\\n        \\\"172.30.60.123\\\"\\n    ],\\n    \\\"default\\\": true,\\n    \\\"dns\\\": {}\\n}]\",\n            \"openshift.io/scc\": \"anyuid\"\n        },\n        \"creationTimestamp\": \"2023-06-27T16:24:57Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-4893\",\n        \"resourceVersion\": \"70463\",\n        \"uid\": \"0e9afcfa-e8c5-486b-bcec-5aca21ab6b02\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"securityContext\": {\n                    \"capabilities\": {\n                        \"drop\": [\n                            \"MKNOD\"\n                        ]\n                    }\n                },\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-52bdb\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"imagePullSecrets\": [\n            {\n                \"name\": \"default-dockercfg-hhjtx\"\n            }\n        ],\n        \"nodeName\": \"10.113.180.96\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {\n            \"seLinuxOptions\": {\n                \"level\": \"s0:c32,c4\"\n            }\n        },\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-52bdb\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"service-ca.crt\",\n                                        \"path\": \"service-ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"openshift-service-ca.crt\"\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-27T16:24:57Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-27T16:25:04Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-27T16:25:04Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-06-27T16:24:57Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://d9c8f49a3b6ba3ca2cb40fcb168fe34ffdd5ffc9c0ad9d321441e826926c490b\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-06-27T16:25:03Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.113.180.96\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.30.60.123\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.30.60.123\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-06-27T16:24:57Z\"\n    }\n}\n"
    STEP: replace the image in the pod 06/27/23 16:25:07.448
    Jun 27 16:25:07.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4893 replace -f -'
    Jun 27 16:25:09.974: INFO: stderr: ""
    Jun 27 16:25:09.974: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 06/27/23 16:25:09.974
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Jun 27 16:25:09.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4893 delete pods e2e-test-httpd-pod'
    Jun 27 16:25:22.558: INFO: stderr: ""
    Jun 27 16:25:22.558: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:25:22.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4893" for this suite. 06/27/23 16:25:22.587
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:25:22.615
Jun 27 16:25:22.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 16:25:22.618
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:22.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:22.685
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 16:25:22.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1578" for this suite. 06/27/23 16:25:22.746
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":14,"skipped":373,"failed":0}
------------------------------
â€¢ [0.161 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:25:22.615
    Jun 27 16:25:22.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 16:25:22.618
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:22.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:22.685
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 16:25:22.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1578" for this suite. 06/27/23 16:25:22.746
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:25:22.779
Jun 27 16:25:22.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 16:25:22.782
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:22.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:22.859
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 06/27/23 16:25:22.873
STEP: Creating a ResourceQuota 06/27/23 16:25:27.887
STEP: Ensuring resource quota status is calculated 06/27/23 16:25:27.905
STEP: Creating a Pod that fits quota 06/27/23 16:25:29.92
STEP: Ensuring ResourceQuota status captures the pod usage 06/27/23 16:25:29.995
STEP: Not allowing a pod to be created that exceeds remaining quota 06/27/23 16:25:32.012
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/27/23 16:25:32.064
STEP: Ensuring a pod cannot update its resource requirements 06/27/23 16:25:32.101
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/27/23 16:25:32.134
STEP: Deleting the pod 06/27/23 16:25:34.148
STEP: Ensuring resource quota status released the pod usage 06/27/23 16:25:34.206
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 16:25:36.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8021" for this suite. 06/27/23 16:25:36.248
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":15,"skipped":382,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.496 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:25:22.779
    Jun 27 16:25:22.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 16:25:22.782
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:22.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:22.859
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 06/27/23 16:25:22.873
    STEP: Creating a ResourceQuota 06/27/23 16:25:27.887
    STEP: Ensuring resource quota status is calculated 06/27/23 16:25:27.905
    STEP: Creating a Pod that fits quota 06/27/23 16:25:29.92
    STEP: Ensuring ResourceQuota status captures the pod usage 06/27/23 16:25:29.995
    STEP: Not allowing a pod to be created that exceeds remaining quota 06/27/23 16:25:32.012
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 06/27/23 16:25:32.064
    STEP: Ensuring a pod cannot update its resource requirements 06/27/23 16:25:32.101
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 06/27/23 16:25:32.134
    STEP: Deleting the pod 06/27/23 16:25:34.148
    STEP: Ensuring resource quota status released the pod usage 06/27/23 16:25:34.206
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 16:25:36.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8021" for this suite. 06/27/23 16:25:36.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:25:36.278
Jun 27 16:25:36.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:25:36.28
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:36.333
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:36.346
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/27/23 16:25:36.36
Jun 27 16:25:36.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-3007 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Jun 27 16:25:36.543: INFO: stderr: ""
Jun 27 16:25:36.543: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 06/27/23 16:25:36.543
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Jun 27 16:25:36.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-3007 delete pods e2e-test-httpd-pod'
Jun 27 16:25:44.622: INFO: stderr: ""
Jun 27 16:25:44.622: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:25:44.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3007" for this suite. 06/27/23 16:25:44.667
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":16,"skipped":392,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.419 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:25:36.278
    Jun 27 16:25:36.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:25:36.28
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:36.333
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:36.346
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/27/23 16:25:36.36
    Jun 27 16:25:36.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-3007 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Jun 27 16:25:36.543: INFO: stderr: ""
    Jun 27 16:25:36.543: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 06/27/23 16:25:36.543
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Jun 27 16:25:36.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-3007 delete pods e2e-test-httpd-pod'
    Jun 27 16:25:44.622: INFO: stderr: ""
    Jun 27 16:25:44.622: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:25:44.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3007" for this suite. 06/27/23 16:25:44.667
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:25:44.7
Jun 27 16:25:44.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename runtimeclass 06/27/23 16:25:44.701
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:44.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:44.768
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 06/27/23 16:25:44.781
STEP: getting /apis/node.k8s.io 06/27/23 16:25:44.796
STEP: getting /apis/node.k8s.io/v1 06/27/23 16:25:44.802
STEP: creating 06/27/23 16:25:44.809
STEP: watching 06/27/23 16:25:44.886
Jun 27 16:25:44.886: INFO: starting watch
STEP: getting 06/27/23 16:25:44.915
STEP: listing 06/27/23 16:25:44.937
STEP: patching 06/27/23 16:25:44.957
STEP: updating 06/27/23 16:25:44.982
Jun 27 16:25:45.005: INFO: waiting for watch events with expected annotations
STEP: deleting 06/27/23 16:25:45.006
STEP: deleting a collection 06/27/23 16:25:45.072
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 27 16:25:45.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3975" for this suite. 06/27/23 16:25:45.192
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":17,"skipped":406,"failed":0}
------------------------------
â€¢ [0.516 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:25:44.7
    Jun 27 16:25:44.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename runtimeclass 06/27/23 16:25:44.701
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:44.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:44.768
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 06/27/23 16:25:44.781
    STEP: getting /apis/node.k8s.io 06/27/23 16:25:44.796
    STEP: getting /apis/node.k8s.io/v1 06/27/23 16:25:44.802
    STEP: creating 06/27/23 16:25:44.809
    STEP: watching 06/27/23 16:25:44.886
    Jun 27 16:25:44.886: INFO: starting watch
    STEP: getting 06/27/23 16:25:44.915
    STEP: listing 06/27/23 16:25:44.937
    STEP: patching 06/27/23 16:25:44.957
    STEP: updating 06/27/23 16:25:44.982
    Jun 27 16:25:45.005: INFO: waiting for watch events with expected annotations
    STEP: deleting 06/27/23 16:25:45.006
    STEP: deleting a collection 06/27/23 16:25:45.072
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 27 16:25:45.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3975" for this suite. 06/27/23 16:25:45.192
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:25:45.218
Jun 27 16:25:45.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename job 06/27/23 16:25:45.221
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:45.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:45.286
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 06/27/23 16:25:45.317
STEP: Patching the Job 06/27/23 16:25:45.338
STEP: Watching for Job to be patched 06/27/23 16:25:45.385
Jun 27 16:25:45.393: INFO: Event ADDED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun 27 16:25:45.393: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking:]
Jun 27 16:25:45.393: INFO: Event MODIFIED found for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 06/27/23 16:25:45.393
STEP: Watching for Job to be updated 06/27/23 16:25:45.43
Jun 27 16:25:45.438: INFO: Event MODIFIED found for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:45.438: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 06/27/23 16:25:45.438
Jun 27 16:25:45.453: INFO: Job: e2e-4g59p as labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p]
STEP: Waiting for job to complete 06/27/23 16:25:45.453
STEP: Delete a job collection with a labelselector 06/27/23 16:25:57.469
STEP: Watching for Job to be deleted 06/27/23 16:25:57.499
Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:57.512: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:57.513: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:57.513: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Jun 27 16:25:57.513: INFO: Event DELETED found for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 06/27/23 16:25:57.513
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 27 16:25:57.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1671" for this suite. 06/27/23 16:25:57.561
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":18,"skipped":422,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.388 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:25:45.218
    Jun 27 16:25:45.218: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename job 06/27/23 16:25:45.221
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:45.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:45.286
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 06/27/23 16:25:45.317
    STEP: Patching the Job 06/27/23 16:25:45.338
    STEP: Watching for Job to be patched 06/27/23 16:25:45.385
    Jun 27 16:25:45.393: INFO: Event ADDED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun 27 16:25:45.393: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking:]
    Jun 27 16:25:45.393: INFO: Event MODIFIED found for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 06/27/23 16:25:45.393
    STEP: Watching for Job to be updated 06/27/23 16:25:45.43
    Jun 27 16:25:45.438: INFO: Event MODIFIED found for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:45.438: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 06/27/23 16:25:45.438
    Jun 27 16:25:45.453: INFO: Job: e2e-4g59p as labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p]
    STEP: Waiting for job to complete 06/27/23 16:25:45.453
    STEP: Delete a job collection with a labelselector 06/27/23 16:25:57.469
    STEP: Watching for Job to be deleted 06/27/23 16:25:57.499
    Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:57.508: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:57.512: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:57.513: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:57.513: INFO: Event MODIFIED observed for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Jun 27 16:25:57.513: INFO: Event DELETED found for Job e2e-4g59p in namespace job-1671 with labels: map[e2e-4g59p:patched e2e-job-label:e2e-4g59p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 06/27/23 16:25:57.513
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 27 16:25:57.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1671" for this suite. 06/27/23 16:25:57.561
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:25:57.609
Jun 27 16:25:57.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:25:57.611
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:57.655
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:57.675
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 06/27/23 16:25:57.692
Jun 27 16:25:57.789: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6" in namespace "projected-104" to be "Succeeded or Failed"
Jun 27 16:25:57.812: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.39852ms
Jun 27 16:25:59.839: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049856745s
Jun 27 16:26:01.831: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041655905s
Jun 27 16:26:03.831: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041557613s
STEP: Saw pod success 06/27/23 16:26:03.831
Jun 27 16:26:03.831: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6" satisfied condition "Succeeded or Failed"
Jun 27 16:26:03.848: INFO: Trying to get logs from node 10.113.180.96 pod downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6 container client-container: <nil>
STEP: delete the pod 06/27/23 16:26:03.93
Jun 27 16:26:04.008: INFO: Waiting for pod downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6 to disappear
Jun 27 16:26:04.028: INFO: Pod downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 16:26:04.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-104" for this suite. 06/27/23 16:26:04.097
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":19,"skipped":439,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.516 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:25:57.609
    Jun 27 16:25:57.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:25:57.611
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:25:57.655
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:25:57.675
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 06/27/23 16:25:57.692
    Jun 27 16:25:57.789: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6" in namespace "projected-104" to be "Succeeded or Failed"
    Jun 27 16:25:57.812: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6": Phase="Pending", Reason="", readiness=false. Elapsed: 22.39852ms
    Jun 27 16:25:59.839: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049856745s
    Jun 27 16:26:01.831: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041655905s
    Jun 27 16:26:03.831: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.041557613s
    STEP: Saw pod success 06/27/23 16:26:03.831
    Jun 27 16:26:03.831: INFO: Pod "downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6" satisfied condition "Succeeded or Failed"
    Jun 27 16:26:03.848: INFO: Trying to get logs from node 10.113.180.96 pod downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6 container client-container: <nil>
    STEP: delete the pod 06/27/23 16:26:03.93
    Jun 27 16:26:04.008: INFO: Waiting for pod downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6 to disappear
    Jun 27 16:26:04.028: INFO: Pod downwardapi-volume-f802ad86-da37-4b2c-800c-6b5dc54317d6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 16:26:04.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-104" for this suite. 06/27/23 16:26:04.097
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:26:04.15
Jun 27 16:26:04.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 16:26:04.151
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:04.202
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:04.218
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Jun 27 16:26:04.303: INFO: Waiting up to 5m0s for pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb" in namespace "pods-3104" to be "running and ready"
Jun 27 16:26:04.320: INFO: Pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.750668ms
Jun 27 16:26:04.320: INFO: The phase of Pod server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:26:06.343: INFO: Pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040417919s
Jun 27 16:26:06.344: INFO: The phase of Pod server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:26:08.341: INFO: Pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb": Phase="Running", Reason="", readiness=true. Elapsed: 4.037602784s
Jun 27 16:26:08.341: INFO: The phase of Pod server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb is Running (Ready = true)
Jun 27 16:26:08.341: INFO: Pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb" satisfied condition "running and ready"
Jun 27 16:26:08.446: INFO: Waiting up to 5m0s for pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f" in namespace "pods-3104" to be "Succeeded or Failed"
Jun 27 16:26:08.468: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.409323ms
Jun 27 16:26:10.488: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041976741s
Jun 27 16:26:12.490: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043341179s
Jun 27 16:26:14.496: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050016827s
STEP: Saw pod success 06/27/23 16:26:14.496
Jun 27 16:26:14.497: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f" satisfied condition "Succeeded or Failed"
Jun 27 16:26:14.521: INFO: Trying to get logs from node 10.113.180.90 pod client-envvars-309127c1-c58c-4962-9df3-a5571236a72f container env3cont: <nil>
STEP: delete the pod 06/27/23 16:26:14.6
Jun 27 16:26:14.660: INFO: Waiting for pod client-envvars-309127c1-c58c-4962-9df3-a5571236a72f to disappear
Jun 27 16:26:14.680: INFO: Pod client-envvars-309127c1-c58c-4962-9df3-a5571236a72f no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 16:26:14.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3104" for this suite. 06/27/23 16:26:14.734
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":20,"skipped":487,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.609 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:26:04.15
    Jun 27 16:26:04.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 16:26:04.151
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:04.202
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:04.218
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Jun 27 16:26:04.303: INFO: Waiting up to 5m0s for pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb" in namespace "pods-3104" to be "running and ready"
    Jun 27 16:26:04.320: INFO: Pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 16.750668ms
    Jun 27 16:26:04.320: INFO: The phase of Pod server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:26:06.343: INFO: Pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040417919s
    Jun 27 16:26:06.344: INFO: The phase of Pod server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:26:08.341: INFO: Pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb": Phase="Running", Reason="", readiness=true. Elapsed: 4.037602784s
    Jun 27 16:26:08.341: INFO: The phase of Pod server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb is Running (Ready = true)
    Jun 27 16:26:08.341: INFO: Pod "server-envvars-c971914b-df95-46ec-ab1e-7237b4fd4dbb" satisfied condition "running and ready"
    Jun 27 16:26:08.446: INFO: Waiting up to 5m0s for pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f" in namespace "pods-3104" to be "Succeeded or Failed"
    Jun 27 16:26:08.468: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f": Phase="Pending", Reason="", readiness=false. Elapsed: 21.409323ms
    Jun 27 16:26:10.488: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041976741s
    Jun 27 16:26:12.490: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043341179s
    Jun 27 16:26:14.496: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050016827s
    STEP: Saw pod success 06/27/23 16:26:14.496
    Jun 27 16:26:14.497: INFO: Pod "client-envvars-309127c1-c58c-4962-9df3-a5571236a72f" satisfied condition "Succeeded or Failed"
    Jun 27 16:26:14.521: INFO: Trying to get logs from node 10.113.180.90 pod client-envvars-309127c1-c58c-4962-9df3-a5571236a72f container env3cont: <nil>
    STEP: delete the pod 06/27/23 16:26:14.6
    Jun 27 16:26:14.660: INFO: Waiting for pod client-envvars-309127c1-c58c-4962-9df3-a5571236a72f to disappear
    Jun 27 16:26:14.680: INFO: Pod client-envvars-309127c1-c58c-4962-9df3-a5571236a72f no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 16:26:14.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3104" for this suite. 06/27/23 16:26:14.734
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:26:14.76
Jun 27 16:26:14.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename init-container 06/27/23 16:26:14.761
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:14.817
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:14.833
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 06/27/23 16:26:14.85
Jun 27 16:26:14.850: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 27 16:26:20.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9367" for this suite. 06/27/23 16:26:20.031
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":21,"skipped":512,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.298 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:26:14.76
    Jun 27 16:26:14.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename init-container 06/27/23 16:26:14.761
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:14.817
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:14.833
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 06/27/23 16:26:14.85
    Jun 27 16:26:14.850: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 27 16:26:20.001: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9367" for this suite. 06/27/23 16:26:20.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:26:20.059
Jun 27 16:26:20.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 16:26:20.061
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:20.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:20.133
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Jun 27 16:26:20.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: creating the pod 06/27/23 16:26:20.149
STEP: submitting the pod to kubernetes 06/27/23 16:26:20.149
Jun 27 16:26:20.214: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264" in namespace "pods-5535" to be "running and ready"
Jun 27 16:26:20.238: INFO: Pod "pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264": Phase="Pending", Reason="", readiness=false. Elapsed: 24.427402ms
Jun 27 16:26:20.238: INFO: The phase of Pod pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:26:22.258: INFO: Pod "pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264": Phase="Running", Reason="", readiness=true. Elapsed: 2.044528455s
Jun 27 16:26:22.258: INFO: The phase of Pod pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264 is Running (Ready = true)
Jun 27 16:26:22.258: INFO: Pod "pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 16:26:22.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-5535" for this suite. 06/27/23 16:26:22.392
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":22,"skipped":519,"failed":0}
------------------------------
â€¢ [2.374 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:26:20.059
    Jun 27 16:26:20.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 16:26:20.061
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:20.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:20.133
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Jun 27 16:26:20.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: creating the pod 06/27/23 16:26:20.149
    STEP: submitting the pod to kubernetes 06/27/23 16:26:20.149
    Jun 27 16:26:20.214: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264" in namespace "pods-5535" to be "running and ready"
    Jun 27 16:26:20.238: INFO: Pod "pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264": Phase="Pending", Reason="", readiness=false. Elapsed: 24.427402ms
    Jun 27 16:26:20.238: INFO: The phase of Pod pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:26:22.258: INFO: Pod "pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264": Phase="Running", Reason="", readiness=true. Elapsed: 2.044528455s
    Jun 27 16:26:22.258: INFO: The phase of Pod pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264 is Running (Ready = true)
    Jun 27 16:26:22.258: INFO: Pod "pod-logs-websocket-03c326e8-56b3-440a-a260-b54f2185d264" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 16:26:22.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-5535" for this suite. 06/27/23 16:26:22.392
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:26:22.439
Jun 27 16:26:22.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-webhook 06/27/23 16:26:22.441
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:22.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:22.51
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/27/23 16:26:22.524
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/27/23 16:26:23.027
STEP: Deploying the custom resource conversion webhook pod 06/27/23 16:26:23.073
STEP: Wait for the deployment to be ready 06/27/23 16:26:23.118
Jun 27 16:26:23.160: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
Jun 27 16:26:25.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 26, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 26, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 26, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 26, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 16:26:27.254
STEP: Verifying the service has paired with the endpoint 06/27/23 16:26:27.288
Jun 27 16:26:28.289: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Jun 27 16:26:28.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Creating a v1 custom resource 06/27/23 16:26:31.121
STEP: Create a v2 custom resource 06/27/23 16:26:31.191
STEP: List CRs in v1 06/27/23 16:26:31.325
STEP: List CRs in v2 06/27/23 16:26:31.349
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:26:31.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5180" for this suite. 06/27/23 16:26:31.963
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":23,"skipped":534,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.706 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:26:22.439
    Jun 27 16:26:22.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-webhook 06/27/23 16:26:22.441
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:22.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:22.51
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/27/23 16:26:22.524
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/27/23 16:26:23.027
    STEP: Deploying the custom resource conversion webhook pod 06/27/23 16:26:23.073
    STEP: Wait for the deployment to be ready 06/27/23 16:26:23.118
    Jun 27 16:26:23.160: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    Jun 27 16:26:25.235: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 26, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 26, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 26, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 26, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 16:26:27.254
    STEP: Verifying the service has paired with the endpoint 06/27/23 16:26:27.288
    Jun 27 16:26:28.289: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Jun 27 16:26:28.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Creating a v1 custom resource 06/27/23 16:26:31.121
    STEP: Create a v2 custom resource 06/27/23 16:26:31.191
    STEP: List CRs in v1 06/27/23 16:26:31.325
    STEP: List CRs in v2 06/27/23 16:26:31.349
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:26:31.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5180" for this suite. 06/27/23 16:26:31.963
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:26:32.146
Jun 27 16:26:32.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename svcaccounts 06/27/23 16:26:32.147
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:32.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:32.237
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Jun 27 16:26:32.392: INFO: created pod pod-service-account-defaultsa
Jun 27 16:26:32.392: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Jun 27 16:26:32.439: INFO: created pod pod-service-account-mountsa
Jun 27 16:26:32.439: INFO: pod pod-service-account-mountsa service account token volume mount: true
Jun 27 16:26:32.481: INFO: created pod pod-service-account-nomountsa
Jun 27 16:26:32.481: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Jun 27 16:26:32.555: INFO: created pod pod-service-account-defaultsa-mountspec
Jun 27 16:26:32.555: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Jun 27 16:26:32.614: INFO: created pod pod-service-account-mountsa-mountspec
Jun 27 16:26:32.614: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Jun 27 16:26:32.680: INFO: created pod pod-service-account-nomountsa-mountspec
Jun 27 16:26:32.680: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Jun 27 16:26:32.756: INFO: created pod pod-service-account-defaultsa-nomountspec
Jun 27 16:26:32.756: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Jun 27 16:26:32.830: INFO: created pod pod-service-account-mountsa-nomountspec
Jun 27 16:26:32.830: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Jun 27 16:26:32.954: INFO: created pod pod-service-account-nomountsa-nomountspec
Jun 27 16:26:32.954: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 27 16:26:32.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7440" for this suite. 06/27/23 16:26:33.015
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":24,"skipped":560,"failed":0}
------------------------------
â€¢ [0.913 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:26:32.146
    Jun 27 16:26:32.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename svcaccounts 06/27/23 16:26:32.147
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:32.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:32.237
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Jun 27 16:26:32.392: INFO: created pod pod-service-account-defaultsa
    Jun 27 16:26:32.392: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Jun 27 16:26:32.439: INFO: created pod pod-service-account-mountsa
    Jun 27 16:26:32.439: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Jun 27 16:26:32.481: INFO: created pod pod-service-account-nomountsa
    Jun 27 16:26:32.481: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Jun 27 16:26:32.555: INFO: created pod pod-service-account-defaultsa-mountspec
    Jun 27 16:26:32.555: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Jun 27 16:26:32.614: INFO: created pod pod-service-account-mountsa-mountspec
    Jun 27 16:26:32.614: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Jun 27 16:26:32.680: INFO: created pod pod-service-account-nomountsa-mountspec
    Jun 27 16:26:32.680: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Jun 27 16:26:32.756: INFO: created pod pod-service-account-defaultsa-nomountspec
    Jun 27 16:26:32.756: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Jun 27 16:26:32.830: INFO: created pod pod-service-account-mountsa-nomountspec
    Jun 27 16:26:32.830: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Jun 27 16:26:32.954: INFO: created pod pod-service-account-nomountsa-nomountspec
    Jun 27 16:26:32.954: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 27 16:26:32.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7440" for this suite. 06/27/23 16:26:33.015
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:26:33.06
Jun 27 16:26:33.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename endpointslice 06/27/23 16:26:33.061
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:33.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:33.131
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 27 16:26:33.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-7895" for this suite. 06/27/23 16:26:33.556
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":25,"skipped":583,"failed":0}
------------------------------
â€¢ [0.531 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:26:33.06
    Jun 27 16:26:33.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename endpointslice 06/27/23 16:26:33.061
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:33.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:33.131
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 27 16:26:33.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-7895" for this suite. 06/27/23 16:26:33.556
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:26:33.593
Jun 27 16:26:33.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-preemption 06/27/23 16:26:33.596
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:33.669
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:33.706
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 27 16:26:33.798: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 27 16:27:34.097: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 06/27/23 16:27:34.129
Jun 27 16:27:34.251: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 27 16:27:34.302: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 27 16:27:34.388: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 27 16:27:34.440: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 27 16:27:34.537: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 27 16:27:34.585: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/27/23 16:27:34.585
Jun 27 16:27:34.586: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7718" to be "running"
Jun 27 16:27:34.608: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.846868ms
Jun 27 16:27:36.631: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045125699s
Jun 27 16:27:38.634: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048050773s
Jun 27 16:27:40.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044458643s
Jun 27 16:27:42.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044751911s
Jun 27 16:27:44.633: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047852329s
Jun 27 16:27:46.628: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.042100723s
Jun 27 16:27:46.628: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun 27 16:27:46.628: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
Jun 27 16:27:46.646: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.380935ms
Jun 27 16:27:46.646: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 27 16:27:46.646: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
Jun 27 16:27:46.665: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.99433ms
Jun 27 16:27:46.665: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 27 16:27:46.665: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
Jun 27 16:27:46.685: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.023507ms
Jun 27 16:27:46.686: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 27 16:27:46.686: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
Jun 27 16:27:46.703: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.885931ms
Jun 27 16:27:46.704: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 27 16:27:46.704: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
Jun 27 16:27:46.723: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.074474ms
Jun 27 16:27:46.723: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 06/27/23 16:27:46.723
Jun 27 16:27:46.769: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Jun 27 16:27:46.791: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.722065ms
Jun 27 16:27:48.816: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04728249s
Jun 27 16:27:50.811: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041788281s
Jun 27 16:27:52.809: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.040247865s
Jun 27 16:27:52.809: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 27 16:27:53.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-7718" for this suite. 06/27/23 16:27:53.052
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":26,"skipped":586,"failed":0}
------------------------------
â€¢ [SLOW TEST] [79.653 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:26:33.593
    Jun 27 16:26:33.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-preemption 06/27/23 16:26:33.596
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:26:33.669
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:26:33.706
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 27 16:26:33.798: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 27 16:27:34.097: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 06/27/23 16:27:34.129
    Jun 27 16:27:34.251: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun 27 16:27:34.302: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun 27 16:27:34.388: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun 27 16:27:34.440: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun 27 16:27:34.537: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun 27 16:27:34.585: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/27/23 16:27:34.585
    Jun 27 16:27:34.586: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-7718" to be "running"
    Jun 27 16:27:34.608: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 22.846868ms
    Jun 27 16:27:36.631: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.045125699s
    Jun 27 16:27:38.634: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.048050773s
    Jun 27 16:27:40.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044458643s
    Jun 27 16:27:42.630: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044751911s
    Jun 27 16:27:44.633: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.047852329s
    Jun 27 16:27:46.628: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.042100723s
    Jun 27 16:27:46.628: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun 27 16:27:46.628: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
    Jun 27 16:27:46.646: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.380935ms
    Jun 27 16:27:46.646: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 27 16:27:46.646: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
    Jun 27 16:27:46.665: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 18.99433ms
    Jun 27 16:27:46.665: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 27 16:27:46.665: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
    Jun 27 16:27:46.685: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.023507ms
    Jun 27 16:27:46.686: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 27 16:27:46.686: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
    Jun 27 16:27:46.703: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.885931ms
    Jun 27 16:27:46.704: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 27 16:27:46.704: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-7718" to be "running"
    Jun 27 16:27:46.723: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 19.074474ms
    Jun 27 16:27:46.723: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 06/27/23 16:27:46.723
    Jun 27 16:27:46.769: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Jun 27 16:27:46.791: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 21.722065ms
    Jun 27 16:27:48.816: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04728249s
    Jun 27 16:27:50.811: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041788281s
    Jun 27 16:27:52.809: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 6.040247865s
    Jun 27 16:27:52.809: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 16:27:53.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-7718" for this suite. 06/27/23 16:27:53.052
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:27:53.251
Jun 27 16:27:53.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename svc-latency 06/27/23 16:27:53.255
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:27:53.308
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:27:53.322
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Jun 27 16:27:53.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: creating replication controller svc-latency-rc in namespace svc-latency-8313 06/27/23 16:27:53.339
I0627 16:27:53.380253      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8313, replica count: 1
I0627 16:27:54.431564      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0627 16:27:55.432669      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0627 16:27:56.433174      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 16:27:56.576: INFO: Created: latency-svc-7k2b8
Jun 27 16:27:56.637: INFO: Got endpoints: latency-svc-7k2b8 [103.502387ms]
Jun 27 16:27:56.673: INFO: Created: latency-svc-dxk82
Jun 27 16:27:56.694: INFO: Got endpoints: latency-svc-dxk82 [55.389839ms]
Jun 27 16:27:56.695: INFO: Created: latency-svc-9sj7x
Jun 27 16:27:56.705: INFO: Got endpoints: latency-svc-9sj7x [67.529393ms]
Jun 27 16:27:56.706: INFO: Created: latency-svc-25rcl
Jun 27 16:27:56.714: INFO: Got endpoints: latency-svc-25rcl [74.927657ms]
Jun 27 16:27:56.717: INFO: Created: latency-svc-x6x9p
Jun 27 16:27:56.726: INFO: Got endpoints: latency-svc-x6x9p [86.758361ms]
Jun 27 16:27:56.740: INFO: Created: latency-svc-vrdnv
Jun 27 16:27:56.745: INFO: Got endpoints: latency-svc-vrdnv [104.95403ms]
Jun 27 16:27:56.749: INFO: Created: latency-svc-b2w7c
Jun 27 16:27:56.759: INFO: Got endpoints: latency-svc-b2w7c [119.42356ms]
Jun 27 16:27:56.764: INFO: Created: latency-svc-kcfmq
Jun 27 16:27:56.775: INFO: Created: latency-svc-vw5b6
Jun 27 16:27:56.777: INFO: Got endpoints: latency-svc-kcfmq [138.434245ms]
Jun 27 16:27:56.788: INFO: Got endpoints: latency-svc-vw5b6 [148.362238ms]
Jun 27 16:27:56.793: INFO: Created: latency-svc-pksv5
Jun 27 16:27:56.804: INFO: Got endpoints: latency-svc-pksv5 [165.407544ms]
Jun 27 16:27:56.805: INFO: Created: latency-svc-fwbhq
Jun 27 16:27:56.813: INFO: Got endpoints: latency-svc-fwbhq [173.94747ms]
Jun 27 16:27:56.825: INFO: Created: latency-svc-dj88w
Jun 27 16:27:56.829: INFO: Got endpoints: latency-svc-dj88w [188.922475ms]
Jun 27 16:27:56.829: INFO: Created: latency-svc-twzbw
Jun 27 16:27:56.840: INFO: Got endpoints: latency-svc-twzbw [199.412472ms]
Jun 27 16:27:56.842: INFO: Created: latency-svc-mtgb4
Jun 27 16:27:56.852: INFO: Got endpoints: latency-svc-mtgb4 [211.683965ms]
Jun 27 16:27:56.856: INFO: Created: latency-svc-m5rt7
Jun 27 16:27:56.864: INFO: Created: latency-svc-bwb42
Jun 27 16:27:56.866: INFO: Got endpoints: latency-svc-m5rt7 [227.900309ms]
Jun 27 16:27:56.879: INFO: Got endpoints: latency-svc-bwb42 [239.842923ms]
Jun 27 16:27:56.883: INFO: Created: latency-svc-knkl7
Jun 27 16:27:56.893: INFO: Got endpoints: latency-svc-knkl7 [199.12881ms]
Jun 27 16:27:56.894: INFO: Created: latency-svc-5h2qs
Jun 27 16:27:56.902: INFO: Got endpoints: latency-svc-5h2qs [196.913225ms]
Jun 27 16:27:56.903: INFO: Created: latency-svc-4pbcv
Jun 27 16:27:56.909: INFO: Created: latency-svc-whffs
Jun 27 16:27:56.912: INFO: Got endpoints: latency-svc-4pbcv [198.036005ms]
Jun 27 16:27:56.924: INFO: Created: latency-svc-rgvjg
Jun 27 16:27:56.925: INFO: Got endpoints: latency-svc-whffs [198.506454ms]
Jun 27 16:27:56.936: INFO: Got endpoints: latency-svc-rgvjg [189.604467ms]
Jun 27 16:27:56.936: INFO: Created: latency-svc-mxkrz
Jun 27 16:27:56.946: INFO: Got endpoints: latency-svc-mxkrz [187.533613ms]
Jun 27 16:27:57.099: INFO: Created: latency-svc-5klxz
Jun 27 16:27:57.105: INFO: Created: latency-svc-rzntt
Jun 27 16:27:57.105: INFO: Created: latency-svc-hl9z8
Jun 27 16:27:57.105: INFO: Created: latency-svc-85sg8
Jun 27 16:27:57.105: INFO: Created: latency-svc-gwjwg
Jun 27 16:27:57.107: INFO: Created: latency-svc-q2mzq
Jun 27 16:27:57.107: INFO: Created: latency-svc-bv82r
Jun 27 16:27:57.107: INFO: Created: latency-svc-bfr6g
Jun 27 16:27:57.107: INFO: Created: latency-svc-pcqws
Jun 27 16:27:57.108: INFO: Created: latency-svc-p5wrj
Jun 27 16:27:57.108: INFO: Created: latency-svc-j4425
Jun 27 16:27:57.108: INFO: Created: latency-svc-628l8
Jun 27 16:27:57.109: INFO: Created: latency-svc-49b2p
Jun 27 16:27:57.109: INFO: Created: latency-svc-l6p7s
Jun 27 16:27:57.109: INFO: Created: latency-svc-d92mf
Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-5klxz [265.963074ms]
Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-gwjwg [205.715664ms]
Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-rzntt [193.157918ms]
Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-d92mf [305.21445ms]
Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-pcqws [215.710126ms]
Jun 27 16:27:57.127: INFO: Got endpoints: latency-svc-85sg8 [349.551941ms]
Jun 27 16:27:57.133: INFO: Got endpoints: latency-svc-l6p7s [253.325439ms]
Jun 27 16:27:57.133: INFO: Got endpoints: latency-svc-hl9z8 [197.446662ms]
Jun 27 16:27:57.133: INFO: Got endpoints: latency-svc-p5wrj [186.909953ms]
Jun 27 16:27:57.134: INFO: Got endpoints: latency-svc-j4425 [240.454411ms]
Jun 27 16:27:57.137: INFO: Got endpoints: latency-svc-49b2p [308.673822ms]
Jun 27 16:27:57.146: INFO: Got endpoints: latency-svc-628l8 [341.573047ms]
Jun 27 16:27:57.146: INFO: Got endpoints: latency-svc-q2mzq [280.633535ms]
Jun 27 16:27:57.146: INFO: Got endpoints: latency-svc-bfr6g [358.391194ms]
Jun 27 16:27:57.147: INFO: Got endpoints: latency-svc-bv82r [307.340564ms]
Jun 27 16:27:57.152: INFO: Created: latency-svc-8958n
Jun 27 16:27:57.163: INFO: Got endpoints: latency-svc-8958n [45.00075ms]
Jun 27 16:27:57.163: INFO: Created: latency-svc-g4vlv
Jun 27 16:27:57.171: INFO: Got endpoints: latency-svc-g4vlv [53.448979ms]
Jun 27 16:27:57.176: INFO: Created: latency-svc-9kj9v
Jun 27 16:27:57.188: INFO: Got endpoints: latency-svc-9kj9v [69.987258ms]
Jun 27 16:27:57.191: INFO: Created: latency-svc-bpjln
Jun 27 16:27:57.202: INFO: Got endpoints: latency-svc-bpjln [83.257775ms]
Jun 27 16:27:57.202: INFO: Created: latency-svc-t7phs
Jun 27 16:27:57.213: INFO: Got endpoints: latency-svc-t7phs [94.40645ms]
Jun 27 16:27:57.218: INFO: Created: latency-svc-82zm8
Jun 27 16:27:57.228: INFO: Got endpoints: latency-svc-82zm8 [100.718534ms]
Jun 27 16:27:57.230: INFO: Created: latency-svc-mqzm2
Jun 27 16:27:57.237: INFO: Created: latency-svc-wk96n
Jun 27 16:27:57.237: INFO: Got endpoints: latency-svc-mqzm2 [104.318347ms]
Jun 27 16:27:57.246: INFO: Got endpoints: latency-svc-wk96n [112.928064ms]
Jun 27 16:27:57.247: INFO: Created: latency-svc-nwc4c
Jun 27 16:27:57.257: INFO: Got endpoints: latency-svc-nwc4c [123.94136ms]
Jun 27 16:27:57.260: INFO: Created: latency-svc-d2kjk
Jun 27 16:27:57.269: INFO: Created: latency-svc-pnb96
Jun 27 16:27:57.274: INFO: Got endpoints: latency-svc-d2kjk [139.816492ms]
Jun 27 16:27:57.282: INFO: Created: latency-svc-jcczd
Jun 27 16:27:57.284: INFO: Got endpoints: latency-svc-pnb96 [146.946867ms]
Jun 27 16:27:57.292: INFO: Created: latency-svc-bv7h8
Jun 27 16:27:57.300: INFO: Got endpoints: latency-svc-jcczd [153.486333ms]
Jun 27 16:27:57.302: INFO: Got endpoints: latency-svc-bv7h8 [155.728617ms]
Jun 27 16:27:57.311: INFO: Created: latency-svc-8c2t8
Jun 27 16:27:57.320: INFO: Got endpoints: latency-svc-8c2t8 [173.207863ms]
Jun 27 16:27:57.326: INFO: Created: latency-svc-hpr4v
Jun 27 16:27:57.338: INFO: Got endpoints: latency-svc-hpr4v [190.696287ms]
Jun 27 16:27:57.510: INFO: Created: latency-svc-lvkjg
Jun 27 16:27:57.511: INFO: Created: latency-svc-22ppm
Jun 27 16:27:57.511: INFO: Created: latency-svc-4mx5w
Jun 27 16:27:57.512: INFO: Created: latency-svc-92sb9
Jun 27 16:27:57.512: INFO: Created: latency-svc-7kwtn
Jun 27 16:27:57.512: INFO: Created: latency-svc-7fd85
Jun 27 16:27:57.512: INFO: Created: latency-svc-c4bxf
Jun 27 16:27:57.512: INFO: Created: latency-svc-q4dpp
Jun 27 16:27:57.513: INFO: Created: latency-svc-clpwp
Jun 27 16:27:57.513: INFO: Created: latency-svc-54d98
Jun 27 16:27:57.513: INFO: Created: latency-svc-l5l55
Jun 27 16:27:57.513: INFO: Created: latency-svc-cdg5m
Jun 27 16:27:57.513: INFO: Created: latency-svc-j89kw
Jun 27 16:27:57.513: INFO: Created: latency-svc-4sngk
Jun 27 16:27:57.514: INFO: Created: latency-svc-69992
Jun 27 16:27:57.534: INFO: Got endpoints: latency-svc-4mx5w [362.595379ms]
Jun 27 16:27:57.535: INFO: Got endpoints: latency-svc-lvkjg [306.735344ms]
Jun 27 16:27:57.535: INFO: Got endpoints: latency-svc-7fd85 [277.293088ms]
Jun 27 16:27:57.535: INFO: Got endpoints: latency-svc-q4dpp [235.388513ms]
Jun 27 16:27:57.536: INFO: Got endpoints: latency-svc-92sb9 [233.766809ms]
Jun 27 16:27:57.538: INFO: Got endpoints: latency-svc-54d98 [218.620684ms]
Jun 27 16:27:57.548: INFO: Got endpoints: latency-svc-j89kw [274.451228ms]
Jun 27 16:27:57.548: INFO: Got endpoints: latency-svc-l5l55 [302.228365ms]
Jun 27 16:27:57.549: INFO: Got endpoints: latency-svc-cdg5m [335.954575ms]
Jun 27 16:27:57.549: INFO: Got endpoints: latency-svc-22ppm [264.307518ms]
Jun 27 16:27:57.551: INFO: Got endpoints: latency-svc-c4bxf [362.623777ms]
Jun 27 16:27:57.554: INFO: Got endpoints: latency-svc-clpwp [215.874084ms]
Jun 27 16:27:57.554: INFO: Got endpoints: latency-svc-4sngk [316.759945ms]
Jun 27 16:27:57.554: INFO: Got endpoints: latency-svc-7kwtn [391.194454ms]
Jun 27 16:27:57.555: INFO: Got endpoints: latency-svc-69992 [353.180294ms]
Jun 27 16:27:57.607: INFO: Created: latency-svc-btrdx
Jun 27 16:27:57.607: INFO: Got endpoints: latency-svc-btrdx [71.738153ms]
Jun 27 16:27:57.608: INFO: Created: latency-svc-r75hz
Jun 27 16:27:57.608: INFO: Created: latency-svc-dtshf
Jun 27 16:27:57.608: INFO: Got endpoints: latency-svc-dtshf [73.708849ms]
Jun 27 16:27:57.609: INFO: Created: latency-svc-ffdmc
Jun 27 16:27:57.609: INFO: Got endpoints: latency-svc-ffdmc [74.804744ms]
Jun 27 16:27:57.617: INFO: Created: latency-svc-qx5gk
Jun 27 16:27:57.617: INFO: Got endpoints: latency-svc-r75hz [81.562045ms]
Jun 27 16:27:57.628: INFO: Got endpoints: latency-svc-qx5gk [92.923959ms]
Jun 27 16:27:57.629: INFO: Created: latency-svc-fcjnn
Jun 27 16:27:57.640: INFO: Created: latency-svc-srg4r
Jun 27 16:27:57.641: INFO: Got endpoints: latency-svc-fcjnn [101.154132ms]
Jun 27 16:27:57.648: INFO: Got endpoints: latency-svc-srg4r [99.620439ms]
Jun 27 16:27:57.649: INFO: Created: latency-svc-q6vtm
Jun 27 16:27:57.663: INFO: Got endpoints: latency-svc-q6vtm [114.395945ms]
Jun 27 16:27:57.663: INFO: Created: latency-svc-zwj8t
Jun 27 16:27:57.677: INFO: Got endpoints: latency-svc-zwj8t [128.388078ms]
Jun 27 16:27:57.678: INFO: Created: latency-svc-fzk2w
Jun 27 16:27:57.685: INFO: Got endpoints: latency-svc-fzk2w [133.910412ms]
Jun 27 16:27:57.685: INFO: Created: latency-svc-jpdpx
Jun 27 16:27:57.691: INFO: Got endpoints: latency-svc-jpdpx [142.217931ms]
Jun 27 16:27:57.692: INFO: Created: latency-svc-9rxk2
Jun 27 16:27:57.704: INFO: Got endpoints: latency-svc-9rxk2 [148.597295ms]
Jun 27 16:27:57.704: INFO: Created: latency-svc-d6kq4
Jun 27 16:27:57.713: INFO: Created: latency-svc-n826p
Jun 27 16:27:57.715: INFO: Got endpoints: latency-svc-d6kq4 [161.017606ms]
Jun 27 16:27:57.723: INFO: Created: latency-svc-n86px
Jun 27 16:27:57.724: INFO: Got endpoints: latency-svc-n826p [170.187373ms]
Jun 27 16:27:57.742: INFO: Got endpoints: latency-svc-n86px [188.001364ms]
Jun 27 16:27:57.756: INFO: Created: latency-svc-94gbz
Jun 27 16:27:57.756: INFO: Got endpoints: latency-svc-94gbz [148.843212ms]
Jun 27 16:27:57.757: INFO: Created: latency-svc-zqggz
Jun 27 16:27:57.765: INFO: Created: latency-svc-gl7gd
Jun 27 16:27:57.777: INFO: Created: latency-svc-lsh5g
Jun 27 16:27:57.778: INFO: Got endpoints: latency-svc-gl7gd [169.757291ms]
Jun 27 16:27:57.780: INFO: Got endpoints: latency-svc-zqggz [171.330628ms]
Jun 27 16:27:57.787: INFO: Got endpoints: latency-svc-lsh5g [170.437635ms]
Jun 27 16:27:57.790: INFO: Created: latency-svc-8mnlg
Jun 27 16:27:57.802: INFO: Got endpoints: latency-svc-8mnlg [174.083682ms]
Jun 27 16:27:57.807: INFO: Created: latency-svc-8pj6p
Jun 27 16:27:57.825: INFO: Got endpoints: latency-svc-8pj6p [184.791048ms]
Jun 27 16:27:57.826: INFO: Created: latency-svc-zp8f9
Jun 27 16:27:57.830: INFO: Got endpoints: latency-svc-zp8f9 [181.504514ms]
Jun 27 16:27:57.834: INFO: Created: latency-svc-8grbm
Jun 27 16:27:57.843: INFO: Created: latency-svc-r6n77
Jun 27 16:27:57.865: INFO: Created: latency-svc-jxc47
Jun 27 16:27:57.866: INFO: Got endpoints: latency-svc-8grbm [202.261264ms]
Jun 27 16:27:57.866: INFO: Got endpoints: latency-svc-r6n77 [189.169604ms]
Jun 27 16:27:57.880: INFO: Got endpoints: latency-svc-jxc47 [194.708892ms]
Jun 27 16:27:57.880: INFO: Created: latency-svc-bsm4j
Jun 27 16:27:57.892: INFO: Got endpoints: latency-svc-bsm4j [200.331606ms]
Jun 27 16:27:57.892: INFO: Created: latency-svc-8tcb2
Jun 27 16:27:57.905: INFO: Created: latency-svc-c4ftm
Jun 27 16:27:57.907: INFO: Got endpoints: latency-svc-8tcb2 [203.640988ms]
Jun 27 16:27:57.915: INFO: Got endpoints: latency-svc-c4ftm [200.598998ms]
Jun 27 16:27:57.920: INFO: Created: latency-svc-n5pfz
Jun 27 16:27:57.928: INFO: Got endpoints: latency-svc-n5pfz [203.962541ms]
Jun 27 16:27:57.932: INFO: Created: latency-svc-n8r6f
Jun 27 16:27:57.942: INFO: Created: latency-svc-fml42
Jun 27 16:27:57.943: INFO: Got endpoints: latency-svc-n8r6f [200.22691ms]
Jun 27 16:27:57.952: INFO: Got endpoints: latency-svc-fml42 [195.632656ms]
Jun 27 16:27:57.956: INFO: Created: latency-svc-xx6ch
Jun 27 16:27:57.974: INFO: Got endpoints: latency-svc-xx6ch [193.245476ms]
Jun 27 16:27:57.975: INFO: Created: latency-svc-mn5w9
Jun 27 16:27:57.982: INFO: Created: latency-svc-85458
Jun 27 16:27:57.982: INFO: Got endpoints: latency-svc-mn5w9 [204.276899ms]
Jun 27 16:27:57.988: INFO: Got endpoints: latency-svc-85458 [201.046033ms]
Jun 27 16:27:57.990: INFO: Created: latency-svc-fpvcm
Jun 27 16:27:58.004: INFO: Got endpoints: latency-svc-fpvcm [201.784585ms]
Jun 27 16:27:58.009: INFO: Created: latency-svc-pbz5x
Jun 27 16:27:58.020: INFO: Got endpoints: latency-svc-pbz5x [194.119153ms]
Jun 27 16:27:58.022: INFO: Created: latency-svc-ppbw2
Jun 27 16:27:58.030: INFO: Got endpoints: latency-svc-ppbw2 [199.806787ms]
Jun 27 16:27:58.036: INFO: Created: latency-svc-8565l
Jun 27 16:27:58.059: INFO: Got endpoints: latency-svc-8565l [193.333828ms]
Jun 27 16:27:58.064: INFO: Created: latency-svc-tt4bg
Jun 27 16:27:58.068: INFO: Created: latency-svc-jq4bs
Jun 27 16:27:58.071: INFO: Got endpoints: latency-svc-tt4bg [205.039471ms]
Jun 27 16:27:58.075: INFO: Created: latency-svc-7r7hj
Jun 27 16:27:58.093: INFO: Got endpoints: latency-svc-7r7hj [201.373473ms]
Jun 27 16:27:58.094: INFO: Got endpoints: latency-svc-jq4bs [213.760944ms]
Jun 27 16:27:58.106: INFO: Created: latency-svc-ctvz7
Jun 27 16:27:58.128: INFO: Got endpoints: latency-svc-ctvz7 [220.246972ms]
Jun 27 16:27:58.274: INFO: Created: latency-svc-85h96
Jun 27 16:27:58.284: INFO: Created: latency-svc-vswx2
Jun 27 16:27:58.285: INFO: Created: latency-svc-m9w4m
Jun 27 16:27:58.285: INFO: Created: latency-svc-bvwg6
Jun 27 16:27:58.286: INFO: Created: latency-svc-pd6dp
Jun 27 16:27:58.286: INFO: Created: latency-svc-wmxrt
Jun 27 16:27:58.287: INFO: Created: latency-svc-m2jb7
Jun 27 16:27:58.287: INFO: Created: latency-svc-9bp7f
Jun 27 16:27:58.287: INFO: Created: latency-svc-xnvj2
Jun 27 16:27:58.288: INFO: Created: latency-svc-548ff
Jun 27 16:27:58.289: INFO: Created: latency-svc-2fk7j
Jun 27 16:27:58.290: INFO: Created: latency-svc-7cxmr
Jun 27 16:27:58.291: INFO: Created: latency-svc-kmz8q
Jun 27 16:27:58.288: INFO: Created: latency-svc-llntf
Jun 27 16:27:58.292: INFO: Created: latency-svc-dkk7b
Jun 27 16:27:58.301: INFO: Got endpoints: latency-svc-85h96 [230.110892ms]
Jun 27 16:27:58.306: INFO: Got endpoints: latency-svc-vswx2 [246.530835ms]
Jun 27 16:27:58.306: INFO: Got endpoints: latency-svc-wmxrt [212.661801ms]
Jun 27 16:27:58.308: INFO: Got endpoints: latency-svc-m9w4m [392.250443ms]
Jun 27 16:27:58.308: INFO: Got endpoints: latency-svc-bvwg6 [214.033819ms]
Jun 27 16:27:58.308: INFO: Got endpoints: latency-svc-xnvj2 [179.73058ms]
Jun 27 16:27:58.308: INFO: Got endpoints: latency-svc-2fk7j [365.609052ms]
Jun 27 16:27:58.318: INFO: Got endpoints: latency-svc-kmz8q [335.317616ms]
Jun 27 16:27:58.318: INFO: Got endpoints: latency-svc-548ff [366.064881ms]
Jun 27 16:27:58.318: INFO: Got endpoints: latency-svc-9bp7f [389.990034ms]
Jun 27 16:27:58.323: INFO: Got endpoints: latency-svc-7cxmr [292.813765ms]
Jun 27 16:27:58.323: INFO: Got endpoints: latency-svc-llntf [303.239895ms]
Jun 27 16:27:58.323: INFO: Got endpoints: latency-svc-pd6dp [319.472895ms]
Jun 27 16:27:58.324: INFO: Got endpoints: latency-svc-dkk7b [349.98413ms]
Jun 27 16:27:58.324: INFO: Got endpoints: latency-svc-m2jb7 [335.408636ms]
Jun 27 16:27:58.339: INFO: Created: latency-svc-zm2tr
Jun 27 16:27:58.351: INFO: Created: latency-svc-wwvqh
Jun 27 16:27:58.358: INFO: Got endpoints: latency-svc-zm2tr [56.435278ms]
Jun 27 16:27:58.361: INFO: Got endpoints: latency-svc-wwvqh [54.719254ms]
Jun 27 16:27:58.361: INFO: Created: latency-svc-zfx79
Jun 27 16:27:58.371: INFO: Created: latency-svc-28wnd
Jun 27 16:27:58.373: INFO: Got endpoints: latency-svc-zfx79 [66.357528ms]
Jun 27 16:27:58.381: INFO: Created: latency-svc-zkb74
Jun 27 16:27:58.381: INFO: Got endpoints: latency-svc-28wnd [72.501581ms]
Jun 27 16:27:58.418: INFO: Got endpoints: latency-svc-zkb74 [110.660227ms]
Jun 27 16:27:58.591: INFO: Created: latency-svc-kmghx
Jun 27 16:27:58.592: INFO: Created: latency-svc-ppbvz
Jun 27 16:27:58.592: INFO: Created: latency-svc-z5gn8
Jun 27 16:27:58.594: INFO: Created: latency-svc-dp6rb
Jun 27 16:27:58.594: INFO: Got endpoints: latency-svc-kmghx [285.719164ms]
Jun 27 16:27:58.594: INFO: Created: latency-svc-v44nn
Jun 27 16:27:58.594: INFO: Created: latency-svc-jnl8g
Jun 27 16:27:58.594: INFO: Created: latency-svc-qx92r
Jun 27 16:27:58.595: INFO: Created: latency-svc-f77b5
Jun 27 16:27:58.595: INFO: Created: latency-svc-2tclc
Jun 27 16:27:58.595: INFO: Created: latency-svc-r7djx
Jun 27 16:27:58.595: INFO: Created: latency-svc-zxlv6
Jun 27 16:27:58.595: INFO: Created: latency-svc-jgvtk
Jun 27 16:27:58.595: INFO: Created: latency-svc-wwf9q
Jun 27 16:27:58.595: INFO: Created: latency-svc-n2kbw
Jun 27 16:27:58.595: INFO: Created: latency-svc-nhr76
Jun 27 16:27:58.596: INFO: Got endpoints: latency-svc-ppbvz [223.666872ms]
Jun 27 16:27:58.601: INFO: Got endpoints: latency-svc-v44nn [277.409961ms]
Jun 27 16:27:58.602: INFO: Got endpoints: latency-svc-dp6rb [293.581323ms]
Jun 27 16:27:58.602: INFO: Got endpoints: latency-svc-2tclc [243.612122ms]
Jun 27 16:27:58.602: INFO: Got endpoints: latency-svc-z5gn8 [278.677357ms]
Jun 27 16:27:58.606: INFO: Got endpoints: latency-svc-nhr76 [224.862078ms]
Jun 27 16:27:58.610: INFO: Got endpoints: latency-svc-jgvtk [291.634636ms]
Jun 27 16:27:58.612: INFO: Got endpoints: latency-svc-qx92r [288.365166ms]
Jun 27 16:27:58.613: INFO: Got endpoints: latency-svc-jnl8g [251.808286ms]
Jun 27 16:27:58.617: INFO: Got endpoints: latency-svc-f77b5 [298.911681ms]
Jun 27 16:27:58.621: INFO: Got endpoints: latency-svc-wwf9q [302.489815ms]
Jun 27 16:27:58.629: INFO: Got endpoints: latency-svc-zxlv6 [306.266903ms]
Jun 27 16:27:58.629: INFO: Got endpoints: latency-svc-n2kbw [210.702174ms]
Jun 27 16:27:58.629: INFO: Got endpoints: latency-svc-r7djx [305.93618ms]
Jun 27 16:27:58.638: INFO: Created: latency-svc-4s9kn
Jun 27 16:27:58.646: INFO: Got endpoints: latency-svc-4s9kn [52.413425ms]
Jun 27 16:27:58.649: INFO: Created: latency-svc-6554w
Jun 27 16:27:58.666: INFO: Created: latency-svc-hx4mw
Jun 27 16:27:58.666: INFO: Got endpoints: latency-svc-6554w [70.189083ms]
Jun 27 16:27:58.676: INFO: Got endpoints: latency-svc-hx4mw [74.072056ms]
Jun 27 16:27:58.676: INFO: Created: latency-svc-2q8d6
Jun 27 16:27:58.683: INFO: Got endpoints: latency-svc-2q8d6 [81.271681ms]
Jun 27 16:27:58.684: INFO: Created: latency-svc-8j2qx
Jun 27 16:27:58.694: INFO: Got endpoints: latency-svc-8j2qx [92.417652ms]
Jun 27 16:27:58.700: INFO: Created: latency-svc-xk2hx
Jun 27 16:27:58.715: INFO: Got endpoints: latency-svc-xk2hx [113.977649ms]
Jun 27 16:27:58.716: INFO: Created: latency-svc-scdlf
Jun 27 16:27:58.738: INFO: Got endpoints: latency-svc-scdlf [131.807361ms]
Jun 27 16:27:58.933: INFO: Created: latency-svc-697tr
Jun 27 16:27:58.945: INFO: Created: latency-svc-v957h
Jun 27 16:27:58.946: INFO: Created: latency-svc-mmmlw
Jun 27 16:27:58.946: INFO: Created: latency-svc-tgcgb
Jun 27 16:27:58.946: INFO: Created: latency-svc-jstgh
Jun 27 16:27:58.946: INFO: Created: latency-svc-jb262
Jun 27 16:27:58.946: INFO: Created: latency-svc-bql6v
Jun 27 16:27:58.947: INFO: Created: latency-svc-dbx2r
Jun 27 16:27:58.947: INFO: Created: latency-svc-ljpts
Jun 27 16:27:58.947: INFO: Created: latency-svc-bkm4z
Jun 27 16:27:58.948: INFO: Created: latency-svc-2ctg7
Jun 27 16:27:58.948: INFO: Created: latency-svc-bmx8b
Jun 27 16:27:58.948: INFO: Created: latency-svc-q5lb7
Jun 27 16:27:58.951: INFO: Got endpoints: latency-svc-697tr [338.18936ms]
Jun 27 16:27:58.951: INFO: Created: latency-svc-llfgk
Jun 27 16:27:58.951: INFO: Got endpoints: latency-svc-ljpts [341.576266ms]
Jun 27 16:27:58.952: INFO: Created: latency-svc-85f22
Jun 27 16:27:58.956: INFO: Got endpoints: latency-svc-llfgk [326.299457ms]
Jun 27 16:27:58.956: INFO: Got endpoints: latency-svc-jstgh [326.588886ms]
Jun 27 16:27:58.956: INFO: Got endpoints: latency-svc-q5lb7 [273.21609ms]
Jun 27 16:27:58.964: INFO: Got endpoints: latency-svc-bkm4z [248.941618ms]
Jun 27 16:27:58.973: INFO: Got endpoints: latency-svc-v957h [278.267145ms]
Jun 27 16:27:58.973: INFO: Got endpoints: latency-svc-tgcgb [234.853466ms]
Jun 27 16:27:58.973: INFO: Got endpoints: latency-svc-mmmlw [326.356932ms]
Jun 27 16:27:58.974: INFO: Got endpoints: latency-svc-bql6v [352.712155ms]
Jun 27 16:27:58.973: INFO: Got endpoints: latency-svc-85f22 [296.625729ms]
Jun 27 16:27:58.993: INFO: Got endpoints: latency-svc-2ctg7 [326.845333ms]
Jun 27 16:27:58.994: INFO: Created: latency-svc-j8wp7
Jun 27 16:27:58.994: INFO: Got endpoints: latency-svc-bmx8b [381.500429ms]
Jun 27 16:27:58.994: INFO: Got endpoints: latency-svc-jb262 [364.944177ms]
Jun 27 16:27:58.994: INFO: Got endpoints: latency-svc-dbx2r [377.602338ms]
Jun 27 16:27:59.008: INFO: Created: latency-svc-qrtrn
Jun 27 16:27:59.025: INFO: Created: latency-svc-krl9d
Jun 27 16:27:59.025: INFO: Got endpoints: latency-svc-krl9d [69.642226ms]
Jun 27 16:27:59.026: INFO: Got endpoints: latency-svc-j8wp7 [74.802008ms]
Jun 27 16:27:59.026: INFO: Got endpoints: latency-svc-qrtrn [74.363595ms]
Jun 27 16:27:59.028: INFO: Created: latency-svc-8cmwq
Jun 27 16:27:59.037: INFO: Created: latency-svc-ds6bf
Jun 27 16:27:59.046: INFO: Got endpoints: latency-svc-8cmwq [89.232563ms]
Jun 27 16:27:59.046: INFO: Got endpoints: latency-svc-ds6bf [90.021892ms]
Jun 27 16:27:59.050: INFO: Created: latency-svc-9lkh6
Jun 27 16:27:59.058: INFO: Got endpoints: latency-svc-9lkh6 [93.76838ms]
Jun 27 16:27:59.068: INFO: Created: latency-svc-kmhkb
Jun 27 16:27:59.078: INFO: Got endpoints: latency-svc-kmhkb [105.635074ms]
Jun 27 16:27:59.079: INFO: Created: latency-svc-dqvc7
Jun 27 16:27:59.090: INFO: Got endpoints: latency-svc-dqvc7 [117.611456ms]
Jun 27 16:27:59.295: INFO: Created: latency-svc-l6n87
Jun 27 16:27:59.296: INFO: Created: latency-svc-g2lbt
Jun 27 16:27:59.297: INFO: Created: latency-svc-8qpw7
Jun 27 16:27:59.300: INFO: Created: latency-svc-r94k8
Jun 27 16:27:59.301: INFO: Created: latency-svc-297mj
Jun 27 16:27:59.302: INFO: Created: latency-svc-chnks
Jun 27 16:27:59.302: INFO: Created: latency-svc-cp7nb
Jun 27 16:27:59.302: INFO: Created: latency-svc-whdbn
Jun 27 16:27:59.308: INFO: Created: latency-svc-kx6tr
Jun 27 16:27:59.309: INFO: Created: latency-svc-xthzt
Jun 27 16:27:59.310: INFO: Got endpoints: latency-svc-r94k8 [315.343154ms]
Jun 27 16:27:59.310: INFO: Got endpoints: latency-svc-8qpw7 [219.215023ms]
Jun 27 16:27:59.310: INFO: Created: latency-svc-j4lf5
Jun 27 16:27:59.310: INFO: Created: latency-svc-tw6ft
Jun 27 16:27:59.310: INFO: Created: latency-svc-gtkx5
Jun 27 16:27:59.310: INFO: Got endpoints: latency-svc-chnks [284.606867ms]
Jun 27 16:27:59.311: INFO: Created: latency-svc-6c4rk
Jun 27 16:27:59.311: INFO: Got endpoints: latency-svc-cp7nb [316.585406ms]
Jun 27 16:27:59.311: INFO: Got endpoints: latency-svc-g2lbt [336.524792ms]
Jun 27 16:27:59.312: INFO: Created: latency-svc-s56wt
Jun 27 16:27:59.329: INFO: Got endpoints: latency-svc-whdbn [334.835929ms]
Jun 27 16:27:59.329: INFO: Got endpoints: latency-svc-l6n87 [303.847568ms]
Jun 27 16:27:59.331: INFO: Got endpoints: latency-svc-kx6tr [337.408377ms]
Jun 27 16:27:59.331: INFO: Got endpoints: latency-svc-s56wt [252.207652ms]
Jun 27 16:27:59.331: INFO: Got endpoints: latency-svc-297mj [357.010473ms]
Jun 27 16:27:59.335: INFO: Got endpoints: latency-svc-gtkx5 [361.40955ms]
Jun 27 16:27:59.338: INFO: Got endpoints: latency-svc-xthzt [280.154193ms]
Jun 27 16:27:59.344: INFO: Got endpoints: latency-svc-j4lf5 [297.525113ms]
Jun 27 16:27:59.344: INFO: Got endpoints: latency-svc-6c4rk [318.522129ms]
Jun 27 16:27:59.345: INFO: Got endpoints: latency-svc-tw6ft [298.715312ms]
Jun 27 16:27:59.350: INFO: Created: latency-svc-phnsc
Jun 27 16:27:59.371: INFO: Got endpoints: latency-svc-phnsc [61.850534ms]
Jun 27 16:27:59.474: INFO: Created: latency-svc-tmpth
Jun 27 16:27:59.480: INFO: Created: latency-svc-ts97m
Jun 27 16:27:59.480: INFO: Created: latency-svc-vxnx2
Jun 27 16:27:59.481: INFO: Created: latency-svc-4zzrc
Jun 27 16:27:59.481: INFO: Created: latency-svc-qmqp6
Jun 27 16:27:59.482: INFO: Created: latency-svc-fms52
Jun 27 16:27:59.483: INFO: Created: latency-svc-rkbgj
Jun 27 16:27:59.483: INFO: Created: latency-svc-jbtss
Jun 27 16:27:59.483: INFO: Created: latency-svc-jpvd7
Jun 27 16:27:59.513: INFO: Created: latency-svc-bvrw8
Jun 27 16:27:59.514: INFO: Created: latency-svc-szvkg
Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-fms52 [184.283677ms]
Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-tmpth [184.335872ms]
Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-vxnx2 [205.450101ms]
Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-ts97m [184.132042ms]
Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-4zzrc [204.960356ms]
Jun 27 16:27:59.516: INFO: Got endpoints: latency-svc-qmqp6 [180.782885ms]
Jun 27 16:27:59.516: INFO: Got endpoints: latency-svc-jbtss [185.530221ms]
Jun 27 16:27:59.516: INFO: Got endpoints: latency-svc-jpvd7 [185.384368ms]
Jun 27 16:27:59.516: INFO: Got endpoints: latency-svc-szvkg [205.745427ms]
Jun 27 16:27:59.517: INFO: Got endpoints: latency-svc-bvrw8 [205.995022ms]
Jun 27 16:27:59.517: INFO: Got endpoints: latency-svc-rkbgj [178.44243ms]
Jun 27 16:27:59.517: INFO: Latencies: [45.00075ms 52.413425ms 53.448979ms 54.719254ms 55.389839ms 56.435278ms 61.850534ms 66.357528ms 67.529393ms 69.642226ms 69.987258ms 70.189083ms 71.738153ms 72.501581ms 73.708849ms 74.072056ms 74.363595ms 74.802008ms 74.804744ms 74.927657ms 81.271681ms 81.562045ms 83.257775ms 86.758361ms 89.232563ms 90.021892ms 92.417652ms 92.923959ms 93.76838ms 94.40645ms 99.620439ms 100.718534ms 101.154132ms 104.318347ms 104.95403ms 105.635074ms 110.660227ms 112.928064ms 113.977649ms 114.395945ms 117.611456ms 119.42356ms 123.94136ms 128.388078ms 131.807361ms 133.910412ms 138.434245ms 139.816492ms 142.217931ms 146.946867ms 148.362238ms 148.597295ms 148.843212ms 153.486333ms 155.728617ms 161.017606ms 165.407544ms 169.757291ms 170.187373ms 170.437635ms 171.330628ms 173.207863ms 173.94747ms 174.083682ms 178.44243ms 179.73058ms 180.782885ms 181.504514ms 184.132042ms 184.283677ms 184.335872ms 184.791048ms 185.384368ms 185.530221ms 186.909953ms 187.533613ms 188.001364ms 188.922475ms 189.169604ms 189.604467ms 190.696287ms 193.157918ms 193.245476ms 193.333828ms 194.119153ms 194.708892ms 195.632656ms 196.913225ms 197.446662ms 198.036005ms 198.506454ms 199.12881ms 199.412472ms 199.806787ms 200.22691ms 200.331606ms 200.598998ms 201.046033ms 201.373473ms 201.784585ms 202.261264ms 203.640988ms 203.962541ms 204.276899ms 204.960356ms 205.039471ms 205.450101ms 205.715664ms 205.745427ms 205.995022ms 210.702174ms 211.683965ms 212.661801ms 213.760944ms 214.033819ms 215.710126ms 215.874084ms 218.620684ms 219.215023ms 220.246972ms 223.666872ms 224.862078ms 227.900309ms 230.110892ms 233.766809ms 234.853466ms 235.388513ms 239.842923ms 240.454411ms 243.612122ms 246.530835ms 248.941618ms 251.808286ms 252.207652ms 253.325439ms 264.307518ms 265.963074ms 273.21609ms 274.451228ms 277.293088ms 277.409961ms 278.267145ms 278.677357ms 280.154193ms 280.633535ms 284.606867ms 285.719164ms 288.365166ms 291.634636ms 292.813765ms 293.581323ms 296.625729ms 297.525113ms 298.715312ms 298.911681ms 302.228365ms 302.489815ms 303.239895ms 303.847568ms 305.21445ms 305.93618ms 306.266903ms 306.735344ms 307.340564ms 308.673822ms 315.343154ms 316.585406ms 316.759945ms 318.522129ms 319.472895ms 326.299457ms 326.356932ms 326.588886ms 326.845333ms 334.835929ms 335.317616ms 335.408636ms 335.954575ms 336.524792ms 337.408377ms 338.18936ms 341.573047ms 341.576266ms 349.551941ms 349.98413ms 352.712155ms 353.180294ms 357.010473ms 358.391194ms 361.40955ms 362.595379ms 362.623777ms 364.944177ms 365.609052ms 366.064881ms 377.602338ms 381.500429ms 389.990034ms 391.194454ms 392.250443ms]
Jun 27 16:27:59.517: INFO: 50 %ile: 202.261264ms
Jun 27 16:27:59.517: INFO: 90 %ile: 338.18936ms
Jun 27 16:27:59.517: INFO: 99 %ile: 391.194454ms
Jun 27 16:27:59.517: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Jun 27 16:27:59.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-8313" for this suite. 06/27/23 16:27:59.546
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":27,"skipped":588,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.326 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:27:53.251
    Jun 27 16:27:53.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename svc-latency 06/27/23 16:27:53.255
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:27:53.308
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:27:53.322
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Jun 27 16:27:53.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-8313 06/27/23 16:27:53.339
    I0627 16:27:53.380253      23 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-8313, replica count: 1
    I0627 16:27:54.431564      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0627 16:27:55.432669      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0627 16:27:56.433174      23 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 27 16:27:56.576: INFO: Created: latency-svc-7k2b8
    Jun 27 16:27:56.637: INFO: Got endpoints: latency-svc-7k2b8 [103.502387ms]
    Jun 27 16:27:56.673: INFO: Created: latency-svc-dxk82
    Jun 27 16:27:56.694: INFO: Got endpoints: latency-svc-dxk82 [55.389839ms]
    Jun 27 16:27:56.695: INFO: Created: latency-svc-9sj7x
    Jun 27 16:27:56.705: INFO: Got endpoints: latency-svc-9sj7x [67.529393ms]
    Jun 27 16:27:56.706: INFO: Created: latency-svc-25rcl
    Jun 27 16:27:56.714: INFO: Got endpoints: latency-svc-25rcl [74.927657ms]
    Jun 27 16:27:56.717: INFO: Created: latency-svc-x6x9p
    Jun 27 16:27:56.726: INFO: Got endpoints: latency-svc-x6x9p [86.758361ms]
    Jun 27 16:27:56.740: INFO: Created: latency-svc-vrdnv
    Jun 27 16:27:56.745: INFO: Got endpoints: latency-svc-vrdnv [104.95403ms]
    Jun 27 16:27:56.749: INFO: Created: latency-svc-b2w7c
    Jun 27 16:27:56.759: INFO: Got endpoints: latency-svc-b2w7c [119.42356ms]
    Jun 27 16:27:56.764: INFO: Created: latency-svc-kcfmq
    Jun 27 16:27:56.775: INFO: Created: latency-svc-vw5b6
    Jun 27 16:27:56.777: INFO: Got endpoints: latency-svc-kcfmq [138.434245ms]
    Jun 27 16:27:56.788: INFO: Got endpoints: latency-svc-vw5b6 [148.362238ms]
    Jun 27 16:27:56.793: INFO: Created: latency-svc-pksv5
    Jun 27 16:27:56.804: INFO: Got endpoints: latency-svc-pksv5 [165.407544ms]
    Jun 27 16:27:56.805: INFO: Created: latency-svc-fwbhq
    Jun 27 16:27:56.813: INFO: Got endpoints: latency-svc-fwbhq [173.94747ms]
    Jun 27 16:27:56.825: INFO: Created: latency-svc-dj88w
    Jun 27 16:27:56.829: INFO: Got endpoints: latency-svc-dj88w [188.922475ms]
    Jun 27 16:27:56.829: INFO: Created: latency-svc-twzbw
    Jun 27 16:27:56.840: INFO: Got endpoints: latency-svc-twzbw [199.412472ms]
    Jun 27 16:27:56.842: INFO: Created: latency-svc-mtgb4
    Jun 27 16:27:56.852: INFO: Got endpoints: latency-svc-mtgb4 [211.683965ms]
    Jun 27 16:27:56.856: INFO: Created: latency-svc-m5rt7
    Jun 27 16:27:56.864: INFO: Created: latency-svc-bwb42
    Jun 27 16:27:56.866: INFO: Got endpoints: latency-svc-m5rt7 [227.900309ms]
    Jun 27 16:27:56.879: INFO: Got endpoints: latency-svc-bwb42 [239.842923ms]
    Jun 27 16:27:56.883: INFO: Created: latency-svc-knkl7
    Jun 27 16:27:56.893: INFO: Got endpoints: latency-svc-knkl7 [199.12881ms]
    Jun 27 16:27:56.894: INFO: Created: latency-svc-5h2qs
    Jun 27 16:27:56.902: INFO: Got endpoints: latency-svc-5h2qs [196.913225ms]
    Jun 27 16:27:56.903: INFO: Created: latency-svc-4pbcv
    Jun 27 16:27:56.909: INFO: Created: latency-svc-whffs
    Jun 27 16:27:56.912: INFO: Got endpoints: latency-svc-4pbcv [198.036005ms]
    Jun 27 16:27:56.924: INFO: Created: latency-svc-rgvjg
    Jun 27 16:27:56.925: INFO: Got endpoints: latency-svc-whffs [198.506454ms]
    Jun 27 16:27:56.936: INFO: Got endpoints: latency-svc-rgvjg [189.604467ms]
    Jun 27 16:27:56.936: INFO: Created: latency-svc-mxkrz
    Jun 27 16:27:56.946: INFO: Got endpoints: latency-svc-mxkrz [187.533613ms]
    Jun 27 16:27:57.099: INFO: Created: latency-svc-5klxz
    Jun 27 16:27:57.105: INFO: Created: latency-svc-rzntt
    Jun 27 16:27:57.105: INFO: Created: latency-svc-hl9z8
    Jun 27 16:27:57.105: INFO: Created: latency-svc-85sg8
    Jun 27 16:27:57.105: INFO: Created: latency-svc-gwjwg
    Jun 27 16:27:57.107: INFO: Created: latency-svc-q2mzq
    Jun 27 16:27:57.107: INFO: Created: latency-svc-bv82r
    Jun 27 16:27:57.107: INFO: Created: latency-svc-bfr6g
    Jun 27 16:27:57.107: INFO: Created: latency-svc-pcqws
    Jun 27 16:27:57.108: INFO: Created: latency-svc-p5wrj
    Jun 27 16:27:57.108: INFO: Created: latency-svc-j4425
    Jun 27 16:27:57.108: INFO: Created: latency-svc-628l8
    Jun 27 16:27:57.109: INFO: Created: latency-svc-49b2p
    Jun 27 16:27:57.109: INFO: Created: latency-svc-l6p7s
    Jun 27 16:27:57.109: INFO: Created: latency-svc-d92mf
    Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-5klxz [265.963074ms]
    Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-gwjwg [205.715664ms]
    Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-rzntt [193.157918ms]
    Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-d92mf [305.21445ms]
    Jun 27 16:27:57.118: INFO: Got endpoints: latency-svc-pcqws [215.710126ms]
    Jun 27 16:27:57.127: INFO: Got endpoints: latency-svc-85sg8 [349.551941ms]
    Jun 27 16:27:57.133: INFO: Got endpoints: latency-svc-l6p7s [253.325439ms]
    Jun 27 16:27:57.133: INFO: Got endpoints: latency-svc-hl9z8 [197.446662ms]
    Jun 27 16:27:57.133: INFO: Got endpoints: latency-svc-p5wrj [186.909953ms]
    Jun 27 16:27:57.134: INFO: Got endpoints: latency-svc-j4425 [240.454411ms]
    Jun 27 16:27:57.137: INFO: Got endpoints: latency-svc-49b2p [308.673822ms]
    Jun 27 16:27:57.146: INFO: Got endpoints: latency-svc-628l8 [341.573047ms]
    Jun 27 16:27:57.146: INFO: Got endpoints: latency-svc-q2mzq [280.633535ms]
    Jun 27 16:27:57.146: INFO: Got endpoints: latency-svc-bfr6g [358.391194ms]
    Jun 27 16:27:57.147: INFO: Got endpoints: latency-svc-bv82r [307.340564ms]
    Jun 27 16:27:57.152: INFO: Created: latency-svc-8958n
    Jun 27 16:27:57.163: INFO: Got endpoints: latency-svc-8958n [45.00075ms]
    Jun 27 16:27:57.163: INFO: Created: latency-svc-g4vlv
    Jun 27 16:27:57.171: INFO: Got endpoints: latency-svc-g4vlv [53.448979ms]
    Jun 27 16:27:57.176: INFO: Created: latency-svc-9kj9v
    Jun 27 16:27:57.188: INFO: Got endpoints: latency-svc-9kj9v [69.987258ms]
    Jun 27 16:27:57.191: INFO: Created: latency-svc-bpjln
    Jun 27 16:27:57.202: INFO: Got endpoints: latency-svc-bpjln [83.257775ms]
    Jun 27 16:27:57.202: INFO: Created: latency-svc-t7phs
    Jun 27 16:27:57.213: INFO: Got endpoints: latency-svc-t7phs [94.40645ms]
    Jun 27 16:27:57.218: INFO: Created: latency-svc-82zm8
    Jun 27 16:27:57.228: INFO: Got endpoints: latency-svc-82zm8 [100.718534ms]
    Jun 27 16:27:57.230: INFO: Created: latency-svc-mqzm2
    Jun 27 16:27:57.237: INFO: Created: latency-svc-wk96n
    Jun 27 16:27:57.237: INFO: Got endpoints: latency-svc-mqzm2 [104.318347ms]
    Jun 27 16:27:57.246: INFO: Got endpoints: latency-svc-wk96n [112.928064ms]
    Jun 27 16:27:57.247: INFO: Created: latency-svc-nwc4c
    Jun 27 16:27:57.257: INFO: Got endpoints: latency-svc-nwc4c [123.94136ms]
    Jun 27 16:27:57.260: INFO: Created: latency-svc-d2kjk
    Jun 27 16:27:57.269: INFO: Created: latency-svc-pnb96
    Jun 27 16:27:57.274: INFO: Got endpoints: latency-svc-d2kjk [139.816492ms]
    Jun 27 16:27:57.282: INFO: Created: latency-svc-jcczd
    Jun 27 16:27:57.284: INFO: Got endpoints: latency-svc-pnb96 [146.946867ms]
    Jun 27 16:27:57.292: INFO: Created: latency-svc-bv7h8
    Jun 27 16:27:57.300: INFO: Got endpoints: latency-svc-jcczd [153.486333ms]
    Jun 27 16:27:57.302: INFO: Got endpoints: latency-svc-bv7h8 [155.728617ms]
    Jun 27 16:27:57.311: INFO: Created: latency-svc-8c2t8
    Jun 27 16:27:57.320: INFO: Got endpoints: latency-svc-8c2t8 [173.207863ms]
    Jun 27 16:27:57.326: INFO: Created: latency-svc-hpr4v
    Jun 27 16:27:57.338: INFO: Got endpoints: latency-svc-hpr4v [190.696287ms]
    Jun 27 16:27:57.510: INFO: Created: latency-svc-lvkjg
    Jun 27 16:27:57.511: INFO: Created: latency-svc-22ppm
    Jun 27 16:27:57.511: INFO: Created: latency-svc-4mx5w
    Jun 27 16:27:57.512: INFO: Created: latency-svc-92sb9
    Jun 27 16:27:57.512: INFO: Created: latency-svc-7kwtn
    Jun 27 16:27:57.512: INFO: Created: latency-svc-7fd85
    Jun 27 16:27:57.512: INFO: Created: latency-svc-c4bxf
    Jun 27 16:27:57.512: INFO: Created: latency-svc-q4dpp
    Jun 27 16:27:57.513: INFO: Created: latency-svc-clpwp
    Jun 27 16:27:57.513: INFO: Created: latency-svc-54d98
    Jun 27 16:27:57.513: INFO: Created: latency-svc-l5l55
    Jun 27 16:27:57.513: INFO: Created: latency-svc-cdg5m
    Jun 27 16:27:57.513: INFO: Created: latency-svc-j89kw
    Jun 27 16:27:57.513: INFO: Created: latency-svc-4sngk
    Jun 27 16:27:57.514: INFO: Created: latency-svc-69992
    Jun 27 16:27:57.534: INFO: Got endpoints: latency-svc-4mx5w [362.595379ms]
    Jun 27 16:27:57.535: INFO: Got endpoints: latency-svc-lvkjg [306.735344ms]
    Jun 27 16:27:57.535: INFO: Got endpoints: latency-svc-7fd85 [277.293088ms]
    Jun 27 16:27:57.535: INFO: Got endpoints: latency-svc-q4dpp [235.388513ms]
    Jun 27 16:27:57.536: INFO: Got endpoints: latency-svc-92sb9 [233.766809ms]
    Jun 27 16:27:57.538: INFO: Got endpoints: latency-svc-54d98 [218.620684ms]
    Jun 27 16:27:57.548: INFO: Got endpoints: latency-svc-j89kw [274.451228ms]
    Jun 27 16:27:57.548: INFO: Got endpoints: latency-svc-l5l55 [302.228365ms]
    Jun 27 16:27:57.549: INFO: Got endpoints: latency-svc-cdg5m [335.954575ms]
    Jun 27 16:27:57.549: INFO: Got endpoints: latency-svc-22ppm [264.307518ms]
    Jun 27 16:27:57.551: INFO: Got endpoints: latency-svc-c4bxf [362.623777ms]
    Jun 27 16:27:57.554: INFO: Got endpoints: latency-svc-clpwp [215.874084ms]
    Jun 27 16:27:57.554: INFO: Got endpoints: latency-svc-4sngk [316.759945ms]
    Jun 27 16:27:57.554: INFO: Got endpoints: latency-svc-7kwtn [391.194454ms]
    Jun 27 16:27:57.555: INFO: Got endpoints: latency-svc-69992 [353.180294ms]
    Jun 27 16:27:57.607: INFO: Created: latency-svc-btrdx
    Jun 27 16:27:57.607: INFO: Got endpoints: latency-svc-btrdx [71.738153ms]
    Jun 27 16:27:57.608: INFO: Created: latency-svc-r75hz
    Jun 27 16:27:57.608: INFO: Created: latency-svc-dtshf
    Jun 27 16:27:57.608: INFO: Got endpoints: latency-svc-dtshf [73.708849ms]
    Jun 27 16:27:57.609: INFO: Created: latency-svc-ffdmc
    Jun 27 16:27:57.609: INFO: Got endpoints: latency-svc-ffdmc [74.804744ms]
    Jun 27 16:27:57.617: INFO: Created: latency-svc-qx5gk
    Jun 27 16:27:57.617: INFO: Got endpoints: latency-svc-r75hz [81.562045ms]
    Jun 27 16:27:57.628: INFO: Got endpoints: latency-svc-qx5gk [92.923959ms]
    Jun 27 16:27:57.629: INFO: Created: latency-svc-fcjnn
    Jun 27 16:27:57.640: INFO: Created: latency-svc-srg4r
    Jun 27 16:27:57.641: INFO: Got endpoints: latency-svc-fcjnn [101.154132ms]
    Jun 27 16:27:57.648: INFO: Got endpoints: latency-svc-srg4r [99.620439ms]
    Jun 27 16:27:57.649: INFO: Created: latency-svc-q6vtm
    Jun 27 16:27:57.663: INFO: Got endpoints: latency-svc-q6vtm [114.395945ms]
    Jun 27 16:27:57.663: INFO: Created: latency-svc-zwj8t
    Jun 27 16:27:57.677: INFO: Got endpoints: latency-svc-zwj8t [128.388078ms]
    Jun 27 16:27:57.678: INFO: Created: latency-svc-fzk2w
    Jun 27 16:27:57.685: INFO: Got endpoints: latency-svc-fzk2w [133.910412ms]
    Jun 27 16:27:57.685: INFO: Created: latency-svc-jpdpx
    Jun 27 16:27:57.691: INFO: Got endpoints: latency-svc-jpdpx [142.217931ms]
    Jun 27 16:27:57.692: INFO: Created: latency-svc-9rxk2
    Jun 27 16:27:57.704: INFO: Got endpoints: latency-svc-9rxk2 [148.597295ms]
    Jun 27 16:27:57.704: INFO: Created: latency-svc-d6kq4
    Jun 27 16:27:57.713: INFO: Created: latency-svc-n826p
    Jun 27 16:27:57.715: INFO: Got endpoints: latency-svc-d6kq4 [161.017606ms]
    Jun 27 16:27:57.723: INFO: Created: latency-svc-n86px
    Jun 27 16:27:57.724: INFO: Got endpoints: latency-svc-n826p [170.187373ms]
    Jun 27 16:27:57.742: INFO: Got endpoints: latency-svc-n86px [188.001364ms]
    Jun 27 16:27:57.756: INFO: Created: latency-svc-94gbz
    Jun 27 16:27:57.756: INFO: Got endpoints: latency-svc-94gbz [148.843212ms]
    Jun 27 16:27:57.757: INFO: Created: latency-svc-zqggz
    Jun 27 16:27:57.765: INFO: Created: latency-svc-gl7gd
    Jun 27 16:27:57.777: INFO: Created: latency-svc-lsh5g
    Jun 27 16:27:57.778: INFO: Got endpoints: latency-svc-gl7gd [169.757291ms]
    Jun 27 16:27:57.780: INFO: Got endpoints: latency-svc-zqggz [171.330628ms]
    Jun 27 16:27:57.787: INFO: Got endpoints: latency-svc-lsh5g [170.437635ms]
    Jun 27 16:27:57.790: INFO: Created: latency-svc-8mnlg
    Jun 27 16:27:57.802: INFO: Got endpoints: latency-svc-8mnlg [174.083682ms]
    Jun 27 16:27:57.807: INFO: Created: latency-svc-8pj6p
    Jun 27 16:27:57.825: INFO: Got endpoints: latency-svc-8pj6p [184.791048ms]
    Jun 27 16:27:57.826: INFO: Created: latency-svc-zp8f9
    Jun 27 16:27:57.830: INFO: Got endpoints: latency-svc-zp8f9 [181.504514ms]
    Jun 27 16:27:57.834: INFO: Created: latency-svc-8grbm
    Jun 27 16:27:57.843: INFO: Created: latency-svc-r6n77
    Jun 27 16:27:57.865: INFO: Created: latency-svc-jxc47
    Jun 27 16:27:57.866: INFO: Got endpoints: latency-svc-8grbm [202.261264ms]
    Jun 27 16:27:57.866: INFO: Got endpoints: latency-svc-r6n77 [189.169604ms]
    Jun 27 16:27:57.880: INFO: Got endpoints: latency-svc-jxc47 [194.708892ms]
    Jun 27 16:27:57.880: INFO: Created: latency-svc-bsm4j
    Jun 27 16:27:57.892: INFO: Got endpoints: latency-svc-bsm4j [200.331606ms]
    Jun 27 16:27:57.892: INFO: Created: latency-svc-8tcb2
    Jun 27 16:27:57.905: INFO: Created: latency-svc-c4ftm
    Jun 27 16:27:57.907: INFO: Got endpoints: latency-svc-8tcb2 [203.640988ms]
    Jun 27 16:27:57.915: INFO: Got endpoints: latency-svc-c4ftm [200.598998ms]
    Jun 27 16:27:57.920: INFO: Created: latency-svc-n5pfz
    Jun 27 16:27:57.928: INFO: Got endpoints: latency-svc-n5pfz [203.962541ms]
    Jun 27 16:27:57.932: INFO: Created: latency-svc-n8r6f
    Jun 27 16:27:57.942: INFO: Created: latency-svc-fml42
    Jun 27 16:27:57.943: INFO: Got endpoints: latency-svc-n8r6f [200.22691ms]
    Jun 27 16:27:57.952: INFO: Got endpoints: latency-svc-fml42 [195.632656ms]
    Jun 27 16:27:57.956: INFO: Created: latency-svc-xx6ch
    Jun 27 16:27:57.974: INFO: Got endpoints: latency-svc-xx6ch [193.245476ms]
    Jun 27 16:27:57.975: INFO: Created: latency-svc-mn5w9
    Jun 27 16:27:57.982: INFO: Created: latency-svc-85458
    Jun 27 16:27:57.982: INFO: Got endpoints: latency-svc-mn5w9 [204.276899ms]
    Jun 27 16:27:57.988: INFO: Got endpoints: latency-svc-85458 [201.046033ms]
    Jun 27 16:27:57.990: INFO: Created: latency-svc-fpvcm
    Jun 27 16:27:58.004: INFO: Got endpoints: latency-svc-fpvcm [201.784585ms]
    Jun 27 16:27:58.009: INFO: Created: latency-svc-pbz5x
    Jun 27 16:27:58.020: INFO: Got endpoints: latency-svc-pbz5x [194.119153ms]
    Jun 27 16:27:58.022: INFO: Created: latency-svc-ppbw2
    Jun 27 16:27:58.030: INFO: Got endpoints: latency-svc-ppbw2 [199.806787ms]
    Jun 27 16:27:58.036: INFO: Created: latency-svc-8565l
    Jun 27 16:27:58.059: INFO: Got endpoints: latency-svc-8565l [193.333828ms]
    Jun 27 16:27:58.064: INFO: Created: latency-svc-tt4bg
    Jun 27 16:27:58.068: INFO: Created: latency-svc-jq4bs
    Jun 27 16:27:58.071: INFO: Got endpoints: latency-svc-tt4bg [205.039471ms]
    Jun 27 16:27:58.075: INFO: Created: latency-svc-7r7hj
    Jun 27 16:27:58.093: INFO: Got endpoints: latency-svc-7r7hj [201.373473ms]
    Jun 27 16:27:58.094: INFO: Got endpoints: latency-svc-jq4bs [213.760944ms]
    Jun 27 16:27:58.106: INFO: Created: latency-svc-ctvz7
    Jun 27 16:27:58.128: INFO: Got endpoints: latency-svc-ctvz7 [220.246972ms]
    Jun 27 16:27:58.274: INFO: Created: latency-svc-85h96
    Jun 27 16:27:58.284: INFO: Created: latency-svc-vswx2
    Jun 27 16:27:58.285: INFO: Created: latency-svc-m9w4m
    Jun 27 16:27:58.285: INFO: Created: latency-svc-bvwg6
    Jun 27 16:27:58.286: INFO: Created: latency-svc-pd6dp
    Jun 27 16:27:58.286: INFO: Created: latency-svc-wmxrt
    Jun 27 16:27:58.287: INFO: Created: latency-svc-m2jb7
    Jun 27 16:27:58.287: INFO: Created: latency-svc-9bp7f
    Jun 27 16:27:58.287: INFO: Created: latency-svc-xnvj2
    Jun 27 16:27:58.288: INFO: Created: latency-svc-548ff
    Jun 27 16:27:58.289: INFO: Created: latency-svc-2fk7j
    Jun 27 16:27:58.290: INFO: Created: latency-svc-7cxmr
    Jun 27 16:27:58.291: INFO: Created: latency-svc-kmz8q
    Jun 27 16:27:58.288: INFO: Created: latency-svc-llntf
    Jun 27 16:27:58.292: INFO: Created: latency-svc-dkk7b
    Jun 27 16:27:58.301: INFO: Got endpoints: latency-svc-85h96 [230.110892ms]
    Jun 27 16:27:58.306: INFO: Got endpoints: latency-svc-vswx2 [246.530835ms]
    Jun 27 16:27:58.306: INFO: Got endpoints: latency-svc-wmxrt [212.661801ms]
    Jun 27 16:27:58.308: INFO: Got endpoints: latency-svc-m9w4m [392.250443ms]
    Jun 27 16:27:58.308: INFO: Got endpoints: latency-svc-bvwg6 [214.033819ms]
    Jun 27 16:27:58.308: INFO: Got endpoints: latency-svc-xnvj2 [179.73058ms]
    Jun 27 16:27:58.308: INFO: Got endpoints: latency-svc-2fk7j [365.609052ms]
    Jun 27 16:27:58.318: INFO: Got endpoints: latency-svc-kmz8q [335.317616ms]
    Jun 27 16:27:58.318: INFO: Got endpoints: latency-svc-548ff [366.064881ms]
    Jun 27 16:27:58.318: INFO: Got endpoints: latency-svc-9bp7f [389.990034ms]
    Jun 27 16:27:58.323: INFO: Got endpoints: latency-svc-7cxmr [292.813765ms]
    Jun 27 16:27:58.323: INFO: Got endpoints: latency-svc-llntf [303.239895ms]
    Jun 27 16:27:58.323: INFO: Got endpoints: latency-svc-pd6dp [319.472895ms]
    Jun 27 16:27:58.324: INFO: Got endpoints: latency-svc-dkk7b [349.98413ms]
    Jun 27 16:27:58.324: INFO: Got endpoints: latency-svc-m2jb7 [335.408636ms]
    Jun 27 16:27:58.339: INFO: Created: latency-svc-zm2tr
    Jun 27 16:27:58.351: INFO: Created: latency-svc-wwvqh
    Jun 27 16:27:58.358: INFO: Got endpoints: latency-svc-zm2tr [56.435278ms]
    Jun 27 16:27:58.361: INFO: Got endpoints: latency-svc-wwvqh [54.719254ms]
    Jun 27 16:27:58.361: INFO: Created: latency-svc-zfx79
    Jun 27 16:27:58.371: INFO: Created: latency-svc-28wnd
    Jun 27 16:27:58.373: INFO: Got endpoints: latency-svc-zfx79 [66.357528ms]
    Jun 27 16:27:58.381: INFO: Created: latency-svc-zkb74
    Jun 27 16:27:58.381: INFO: Got endpoints: latency-svc-28wnd [72.501581ms]
    Jun 27 16:27:58.418: INFO: Got endpoints: latency-svc-zkb74 [110.660227ms]
    Jun 27 16:27:58.591: INFO: Created: latency-svc-kmghx
    Jun 27 16:27:58.592: INFO: Created: latency-svc-ppbvz
    Jun 27 16:27:58.592: INFO: Created: latency-svc-z5gn8
    Jun 27 16:27:58.594: INFO: Created: latency-svc-dp6rb
    Jun 27 16:27:58.594: INFO: Got endpoints: latency-svc-kmghx [285.719164ms]
    Jun 27 16:27:58.594: INFO: Created: latency-svc-v44nn
    Jun 27 16:27:58.594: INFO: Created: latency-svc-jnl8g
    Jun 27 16:27:58.594: INFO: Created: latency-svc-qx92r
    Jun 27 16:27:58.595: INFO: Created: latency-svc-f77b5
    Jun 27 16:27:58.595: INFO: Created: latency-svc-2tclc
    Jun 27 16:27:58.595: INFO: Created: latency-svc-r7djx
    Jun 27 16:27:58.595: INFO: Created: latency-svc-zxlv6
    Jun 27 16:27:58.595: INFO: Created: latency-svc-jgvtk
    Jun 27 16:27:58.595: INFO: Created: latency-svc-wwf9q
    Jun 27 16:27:58.595: INFO: Created: latency-svc-n2kbw
    Jun 27 16:27:58.595: INFO: Created: latency-svc-nhr76
    Jun 27 16:27:58.596: INFO: Got endpoints: latency-svc-ppbvz [223.666872ms]
    Jun 27 16:27:58.601: INFO: Got endpoints: latency-svc-v44nn [277.409961ms]
    Jun 27 16:27:58.602: INFO: Got endpoints: latency-svc-dp6rb [293.581323ms]
    Jun 27 16:27:58.602: INFO: Got endpoints: latency-svc-2tclc [243.612122ms]
    Jun 27 16:27:58.602: INFO: Got endpoints: latency-svc-z5gn8 [278.677357ms]
    Jun 27 16:27:58.606: INFO: Got endpoints: latency-svc-nhr76 [224.862078ms]
    Jun 27 16:27:58.610: INFO: Got endpoints: latency-svc-jgvtk [291.634636ms]
    Jun 27 16:27:58.612: INFO: Got endpoints: latency-svc-qx92r [288.365166ms]
    Jun 27 16:27:58.613: INFO: Got endpoints: latency-svc-jnl8g [251.808286ms]
    Jun 27 16:27:58.617: INFO: Got endpoints: latency-svc-f77b5 [298.911681ms]
    Jun 27 16:27:58.621: INFO: Got endpoints: latency-svc-wwf9q [302.489815ms]
    Jun 27 16:27:58.629: INFO: Got endpoints: latency-svc-zxlv6 [306.266903ms]
    Jun 27 16:27:58.629: INFO: Got endpoints: latency-svc-n2kbw [210.702174ms]
    Jun 27 16:27:58.629: INFO: Got endpoints: latency-svc-r7djx [305.93618ms]
    Jun 27 16:27:58.638: INFO: Created: latency-svc-4s9kn
    Jun 27 16:27:58.646: INFO: Got endpoints: latency-svc-4s9kn [52.413425ms]
    Jun 27 16:27:58.649: INFO: Created: latency-svc-6554w
    Jun 27 16:27:58.666: INFO: Created: latency-svc-hx4mw
    Jun 27 16:27:58.666: INFO: Got endpoints: latency-svc-6554w [70.189083ms]
    Jun 27 16:27:58.676: INFO: Got endpoints: latency-svc-hx4mw [74.072056ms]
    Jun 27 16:27:58.676: INFO: Created: latency-svc-2q8d6
    Jun 27 16:27:58.683: INFO: Got endpoints: latency-svc-2q8d6 [81.271681ms]
    Jun 27 16:27:58.684: INFO: Created: latency-svc-8j2qx
    Jun 27 16:27:58.694: INFO: Got endpoints: latency-svc-8j2qx [92.417652ms]
    Jun 27 16:27:58.700: INFO: Created: latency-svc-xk2hx
    Jun 27 16:27:58.715: INFO: Got endpoints: latency-svc-xk2hx [113.977649ms]
    Jun 27 16:27:58.716: INFO: Created: latency-svc-scdlf
    Jun 27 16:27:58.738: INFO: Got endpoints: latency-svc-scdlf [131.807361ms]
    Jun 27 16:27:58.933: INFO: Created: latency-svc-697tr
    Jun 27 16:27:58.945: INFO: Created: latency-svc-v957h
    Jun 27 16:27:58.946: INFO: Created: latency-svc-mmmlw
    Jun 27 16:27:58.946: INFO: Created: latency-svc-tgcgb
    Jun 27 16:27:58.946: INFO: Created: latency-svc-jstgh
    Jun 27 16:27:58.946: INFO: Created: latency-svc-jb262
    Jun 27 16:27:58.946: INFO: Created: latency-svc-bql6v
    Jun 27 16:27:58.947: INFO: Created: latency-svc-dbx2r
    Jun 27 16:27:58.947: INFO: Created: latency-svc-ljpts
    Jun 27 16:27:58.947: INFO: Created: latency-svc-bkm4z
    Jun 27 16:27:58.948: INFO: Created: latency-svc-2ctg7
    Jun 27 16:27:58.948: INFO: Created: latency-svc-bmx8b
    Jun 27 16:27:58.948: INFO: Created: latency-svc-q5lb7
    Jun 27 16:27:58.951: INFO: Got endpoints: latency-svc-697tr [338.18936ms]
    Jun 27 16:27:58.951: INFO: Created: latency-svc-llfgk
    Jun 27 16:27:58.951: INFO: Got endpoints: latency-svc-ljpts [341.576266ms]
    Jun 27 16:27:58.952: INFO: Created: latency-svc-85f22
    Jun 27 16:27:58.956: INFO: Got endpoints: latency-svc-llfgk [326.299457ms]
    Jun 27 16:27:58.956: INFO: Got endpoints: latency-svc-jstgh [326.588886ms]
    Jun 27 16:27:58.956: INFO: Got endpoints: latency-svc-q5lb7 [273.21609ms]
    Jun 27 16:27:58.964: INFO: Got endpoints: latency-svc-bkm4z [248.941618ms]
    Jun 27 16:27:58.973: INFO: Got endpoints: latency-svc-v957h [278.267145ms]
    Jun 27 16:27:58.973: INFO: Got endpoints: latency-svc-tgcgb [234.853466ms]
    Jun 27 16:27:58.973: INFO: Got endpoints: latency-svc-mmmlw [326.356932ms]
    Jun 27 16:27:58.974: INFO: Got endpoints: latency-svc-bql6v [352.712155ms]
    Jun 27 16:27:58.973: INFO: Got endpoints: latency-svc-85f22 [296.625729ms]
    Jun 27 16:27:58.993: INFO: Got endpoints: latency-svc-2ctg7 [326.845333ms]
    Jun 27 16:27:58.994: INFO: Created: latency-svc-j8wp7
    Jun 27 16:27:58.994: INFO: Got endpoints: latency-svc-bmx8b [381.500429ms]
    Jun 27 16:27:58.994: INFO: Got endpoints: latency-svc-jb262 [364.944177ms]
    Jun 27 16:27:58.994: INFO: Got endpoints: latency-svc-dbx2r [377.602338ms]
    Jun 27 16:27:59.008: INFO: Created: latency-svc-qrtrn
    Jun 27 16:27:59.025: INFO: Created: latency-svc-krl9d
    Jun 27 16:27:59.025: INFO: Got endpoints: latency-svc-krl9d [69.642226ms]
    Jun 27 16:27:59.026: INFO: Got endpoints: latency-svc-j8wp7 [74.802008ms]
    Jun 27 16:27:59.026: INFO: Got endpoints: latency-svc-qrtrn [74.363595ms]
    Jun 27 16:27:59.028: INFO: Created: latency-svc-8cmwq
    Jun 27 16:27:59.037: INFO: Created: latency-svc-ds6bf
    Jun 27 16:27:59.046: INFO: Got endpoints: latency-svc-8cmwq [89.232563ms]
    Jun 27 16:27:59.046: INFO: Got endpoints: latency-svc-ds6bf [90.021892ms]
    Jun 27 16:27:59.050: INFO: Created: latency-svc-9lkh6
    Jun 27 16:27:59.058: INFO: Got endpoints: latency-svc-9lkh6 [93.76838ms]
    Jun 27 16:27:59.068: INFO: Created: latency-svc-kmhkb
    Jun 27 16:27:59.078: INFO: Got endpoints: latency-svc-kmhkb [105.635074ms]
    Jun 27 16:27:59.079: INFO: Created: latency-svc-dqvc7
    Jun 27 16:27:59.090: INFO: Got endpoints: latency-svc-dqvc7 [117.611456ms]
    Jun 27 16:27:59.295: INFO: Created: latency-svc-l6n87
    Jun 27 16:27:59.296: INFO: Created: latency-svc-g2lbt
    Jun 27 16:27:59.297: INFO: Created: latency-svc-8qpw7
    Jun 27 16:27:59.300: INFO: Created: latency-svc-r94k8
    Jun 27 16:27:59.301: INFO: Created: latency-svc-297mj
    Jun 27 16:27:59.302: INFO: Created: latency-svc-chnks
    Jun 27 16:27:59.302: INFO: Created: latency-svc-cp7nb
    Jun 27 16:27:59.302: INFO: Created: latency-svc-whdbn
    Jun 27 16:27:59.308: INFO: Created: latency-svc-kx6tr
    Jun 27 16:27:59.309: INFO: Created: latency-svc-xthzt
    Jun 27 16:27:59.310: INFO: Got endpoints: latency-svc-r94k8 [315.343154ms]
    Jun 27 16:27:59.310: INFO: Got endpoints: latency-svc-8qpw7 [219.215023ms]
    Jun 27 16:27:59.310: INFO: Created: latency-svc-j4lf5
    Jun 27 16:27:59.310: INFO: Created: latency-svc-tw6ft
    Jun 27 16:27:59.310: INFO: Created: latency-svc-gtkx5
    Jun 27 16:27:59.310: INFO: Got endpoints: latency-svc-chnks [284.606867ms]
    Jun 27 16:27:59.311: INFO: Created: latency-svc-6c4rk
    Jun 27 16:27:59.311: INFO: Got endpoints: latency-svc-cp7nb [316.585406ms]
    Jun 27 16:27:59.311: INFO: Got endpoints: latency-svc-g2lbt [336.524792ms]
    Jun 27 16:27:59.312: INFO: Created: latency-svc-s56wt
    Jun 27 16:27:59.329: INFO: Got endpoints: latency-svc-whdbn [334.835929ms]
    Jun 27 16:27:59.329: INFO: Got endpoints: latency-svc-l6n87 [303.847568ms]
    Jun 27 16:27:59.331: INFO: Got endpoints: latency-svc-kx6tr [337.408377ms]
    Jun 27 16:27:59.331: INFO: Got endpoints: latency-svc-s56wt [252.207652ms]
    Jun 27 16:27:59.331: INFO: Got endpoints: latency-svc-297mj [357.010473ms]
    Jun 27 16:27:59.335: INFO: Got endpoints: latency-svc-gtkx5 [361.40955ms]
    Jun 27 16:27:59.338: INFO: Got endpoints: latency-svc-xthzt [280.154193ms]
    Jun 27 16:27:59.344: INFO: Got endpoints: latency-svc-j4lf5 [297.525113ms]
    Jun 27 16:27:59.344: INFO: Got endpoints: latency-svc-6c4rk [318.522129ms]
    Jun 27 16:27:59.345: INFO: Got endpoints: latency-svc-tw6ft [298.715312ms]
    Jun 27 16:27:59.350: INFO: Created: latency-svc-phnsc
    Jun 27 16:27:59.371: INFO: Got endpoints: latency-svc-phnsc [61.850534ms]
    Jun 27 16:27:59.474: INFO: Created: latency-svc-tmpth
    Jun 27 16:27:59.480: INFO: Created: latency-svc-ts97m
    Jun 27 16:27:59.480: INFO: Created: latency-svc-vxnx2
    Jun 27 16:27:59.481: INFO: Created: latency-svc-4zzrc
    Jun 27 16:27:59.481: INFO: Created: latency-svc-qmqp6
    Jun 27 16:27:59.482: INFO: Created: latency-svc-fms52
    Jun 27 16:27:59.483: INFO: Created: latency-svc-rkbgj
    Jun 27 16:27:59.483: INFO: Created: latency-svc-jbtss
    Jun 27 16:27:59.483: INFO: Created: latency-svc-jpvd7
    Jun 27 16:27:59.513: INFO: Created: latency-svc-bvrw8
    Jun 27 16:27:59.514: INFO: Created: latency-svc-szvkg
    Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-fms52 [184.283677ms]
    Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-tmpth [184.335872ms]
    Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-vxnx2 [205.450101ms]
    Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-ts97m [184.132042ms]
    Jun 27 16:27:59.515: INFO: Got endpoints: latency-svc-4zzrc [204.960356ms]
    Jun 27 16:27:59.516: INFO: Got endpoints: latency-svc-qmqp6 [180.782885ms]
    Jun 27 16:27:59.516: INFO: Got endpoints: latency-svc-jbtss [185.530221ms]
    Jun 27 16:27:59.516: INFO: Got endpoints: latency-svc-jpvd7 [185.384368ms]
    Jun 27 16:27:59.516: INFO: Got endpoints: latency-svc-szvkg [205.745427ms]
    Jun 27 16:27:59.517: INFO: Got endpoints: latency-svc-bvrw8 [205.995022ms]
    Jun 27 16:27:59.517: INFO: Got endpoints: latency-svc-rkbgj [178.44243ms]
    Jun 27 16:27:59.517: INFO: Latencies: [45.00075ms 52.413425ms 53.448979ms 54.719254ms 55.389839ms 56.435278ms 61.850534ms 66.357528ms 67.529393ms 69.642226ms 69.987258ms 70.189083ms 71.738153ms 72.501581ms 73.708849ms 74.072056ms 74.363595ms 74.802008ms 74.804744ms 74.927657ms 81.271681ms 81.562045ms 83.257775ms 86.758361ms 89.232563ms 90.021892ms 92.417652ms 92.923959ms 93.76838ms 94.40645ms 99.620439ms 100.718534ms 101.154132ms 104.318347ms 104.95403ms 105.635074ms 110.660227ms 112.928064ms 113.977649ms 114.395945ms 117.611456ms 119.42356ms 123.94136ms 128.388078ms 131.807361ms 133.910412ms 138.434245ms 139.816492ms 142.217931ms 146.946867ms 148.362238ms 148.597295ms 148.843212ms 153.486333ms 155.728617ms 161.017606ms 165.407544ms 169.757291ms 170.187373ms 170.437635ms 171.330628ms 173.207863ms 173.94747ms 174.083682ms 178.44243ms 179.73058ms 180.782885ms 181.504514ms 184.132042ms 184.283677ms 184.335872ms 184.791048ms 185.384368ms 185.530221ms 186.909953ms 187.533613ms 188.001364ms 188.922475ms 189.169604ms 189.604467ms 190.696287ms 193.157918ms 193.245476ms 193.333828ms 194.119153ms 194.708892ms 195.632656ms 196.913225ms 197.446662ms 198.036005ms 198.506454ms 199.12881ms 199.412472ms 199.806787ms 200.22691ms 200.331606ms 200.598998ms 201.046033ms 201.373473ms 201.784585ms 202.261264ms 203.640988ms 203.962541ms 204.276899ms 204.960356ms 205.039471ms 205.450101ms 205.715664ms 205.745427ms 205.995022ms 210.702174ms 211.683965ms 212.661801ms 213.760944ms 214.033819ms 215.710126ms 215.874084ms 218.620684ms 219.215023ms 220.246972ms 223.666872ms 224.862078ms 227.900309ms 230.110892ms 233.766809ms 234.853466ms 235.388513ms 239.842923ms 240.454411ms 243.612122ms 246.530835ms 248.941618ms 251.808286ms 252.207652ms 253.325439ms 264.307518ms 265.963074ms 273.21609ms 274.451228ms 277.293088ms 277.409961ms 278.267145ms 278.677357ms 280.154193ms 280.633535ms 284.606867ms 285.719164ms 288.365166ms 291.634636ms 292.813765ms 293.581323ms 296.625729ms 297.525113ms 298.715312ms 298.911681ms 302.228365ms 302.489815ms 303.239895ms 303.847568ms 305.21445ms 305.93618ms 306.266903ms 306.735344ms 307.340564ms 308.673822ms 315.343154ms 316.585406ms 316.759945ms 318.522129ms 319.472895ms 326.299457ms 326.356932ms 326.588886ms 326.845333ms 334.835929ms 335.317616ms 335.408636ms 335.954575ms 336.524792ms 337.408377ms 338.18936ms 341.573047ms 341.576266ms 349.551941ms 349.98413ms 352.712155ms 353.180294ms 357.010473ms 358.391194ms 361.40955ms 362.595379ms 362.623777ms 364.944177ms 365.609052ms 366.064881ms 377.602338ms 381.500429ms 389.990034ms 391.194454ms 392.250443ms]
    Jun 27 16:27:59.517: INFO: 50 %ile: 202.261264ms
    Jun 27 16:27:59.517: INFO: 90 %ile: 338.18936ms
    Jun 27 16:27:59.517: INFO: 99 %ile: 391.194454ms
    Jun 27 16:27:59.517: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Jun 27 16:27:59.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-8313" for this suite. 06/27/23 16:27:59.546
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:27:59.577
Jun 27 16:27:59.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:27:59.578
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:27:59.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:27:59.7
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 06/27/23 16:27:59.715
Jun 27 16:27:59.715: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-8323 proxy --unix-socket=/tmp/kubectl-proxy-unix34166143/test'
STEP: retrieving proxy /api/ output 06/27/23 16:27:59.804
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:27:59.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8323" for this suite. 06/27/23 16:27:59.831
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":28,"skipped":605,"failed":0}
------------------------------
â€¢ [0.283 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:27:59.577
    Jun 27 16:27:59.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:27:59.578
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:27:59.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:27:59.7
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 06/27/23 16:27:59.715
    Jun 27 16:27:59.715: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-8323 proxy --unix-socket=/tmp/kubectl-proxy-unix34166143/test'
    STEP: retrieving proxy /api/ output 06/27/23 16:27:59.804
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:27:59.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8323" for this suite. 06/27/23 16:27:59.831
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:27:59.863
Jun 27 16:27:59.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 16:27:59.865
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:27:59.939
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:27:59.954
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 06/27/23 16:27:59.969
Jun 27 16:28:00.039: INFO: Waiting up to 5m0s for pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602" in namespace "downward-api-5996" to be "Succeeded or Failed"
Jun 27 16:28:00.061: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602": Phase="Pending", Reason="", readiness=false. Elapsed: 21.784336ms
Jun 27 16:28:02.082: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043452304s
Jun 27 16:28:04.080: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040809287s
Jun 27 16:28:06.077: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038453674s
STEP: Saw pod success 06/27/23 16:28:06.077
Jun 27 16:28:06.078: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602" satisfied condition "Succeeded or Failed"
Jun 27 16:28:06.099: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-dc00d515-ba94-4799-9ab3-b361722b8602 container dapi-container: <nil>
STEP: delete the pod 06/27/23 16:28:06.235
Jun 27 16:28:06.284: INFO: Waiting for pod downward-api-dc00d515-ba94-4799-9ab3-b361722b8602 to disappear
Jun 27 16:28:06.321: INFO: Pod downward-api-dc00d515-ba94-4799-9ab3-b361722b8602 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 27 16:28:06.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5996" for this suite. 06/27/23 16:28:06.381
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":29,"skipped":628,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.579 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:27:59.863
    Jun 27 16:27:59.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 16:27:59.865
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:27:59.939
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:27:59.954
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 06/27/23 16:27:59.969
    Jun 27 16:28:00.039: INFO: Waiting up to 5m0s for pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602" in namespace "downward-api-5996" to be "Succeeded or Failed"
    Jun 27 16:28:00.061: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602": Phase="Pending", Reason="", readiness=false. Elapsed: 21.784336ms
    Jun 27 16:28:02.082: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043452304s
    Jun 27 16:28:04.080: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040809287s
    Jun 27 16:28:06.077: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038453674s
    STEP: Saw pod success 06/27/23 16:28:06.077
    Jun 27 16:28:06.078: INFO: Pod "downward-api-dc00d515-ba94-4799-9ab3-b361722b8602" satisfied condition "Succeeded or Failed"
    Jun 27 16:28:06.099: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-dc00d515-ba94-4799-9ab3-b361722b8602 container dapi-container: <nil>
    STEP: delete the pod 06/27/23 16:28:06.235
    Jun 27 16:28:06.284: INFO: Waiting for pod downward-api-dc00d515-ba94-4799-9ab3-b361722b8602 to disappear
    Jun 27 16:28:06.321: INFO: Pod downward-api-dc00d515-ba94-4799-9ab3-b361722b8602 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 27 16:28:06.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5996" for this suite. 06/27/23 16:28:06.381
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:28:06.45
Jun 27 16:28:06.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:28:06.451
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:06.501
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:06.515
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 06/27/23 16:28:06.529
Jun 27 16:28:06.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 create -f -'
Jun 27 16:28:09.149: INFO: stderr: ""
Jun 27 16:28:09.149: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/27/23 16:28:09.149
Jun 27 16:28:09.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 27 16:28:09.328: INFO: stderr: ""
Jun 27 16:28:09.328: INFO: stdout: "update-demo-nautilus-j2tvz update-demo-nautilus-wzzcg "
Jun 27 16:28:09.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-j2tvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:28:09.449: INFO: stderr: ""
Jun 27 16:28:09.449: INFO: stdout: ""
Jun 27 16:28:09.449: INFO: update-demo-nautilus-j2tvz is created but not running
Jun 27 16:28:14.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 27 16:28:14.597: INFO: stderr: ""
Jun 27 16:28:14.597: INFO: stdout: "update-demo-nautilus-j2tvz update-demo-nautilus-wzzcg "
Jun 27 16:28:14.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-j2tvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:28:14.751: INFO: stderr: ""
Jun 27 16:28:14.751: INFO: stdout: ""
Jun 27 16:28:14.751: INFO: update-demo-nautilus-j2tvz is created but not running
Jun 27 16:28:19.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 27 16:28:19.867: INFO: stderr: ""
Jun 27 16:28:19.867: INFO: stdout: "update-demo-nautilus-j2tvz update-demo-nautilus-wzzcg "
Jun 27 16:28:19.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-j2tvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:28:20.028: INFO: stderr: ""
Jun 27 16:28:20.029: INFO: stdout: "true"
Jun 27 16:28:20.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-j2tvz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 27 16:28:20.164: INFO: stderr: ""
Jun 27 16:28:20.164: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 27 16:28:20.164: INFO: validating pod update-demo-nautilus-j2tvz
Jun 27 16:28:20.205: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 16:28:20.205: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 16:28:20.205: INFO: update-demo-nautilus-j2tvz is verified up and running
Jun 27 16:28:20.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:28:20.346: INFO: stderr: ""
Jun 27 16:28:20.346: INFO: stdout: "true"
Jun 27 16:28:20.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 27 16:28:20.514: INFO: stderr: ""
Jun 27 16:28:20.514: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 27 16:28:20.514: INFO: validating pod update-demo-nautilus-wzzcg
Jun 27 16:28:20.572: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 16:28:20.572: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 16:28:20.572: INFO: update-demo-nautilus-wzzcg is verified up and running
STEP: scaling down the replication controller 06/27/23 16:28:20.572
Jun 27 16:28:20.583: INFO: scanned /root for discovery docs: <nil>
Jun 27 16:28:20.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Jun 27 16:28:21.797: INFO: stderr: ""
Jun 27 16:28:21.797: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/27/23 16:28:21.797
Jun 27 16:28:21.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 27 16:28:21.939: INFO: stderr: ""
Jun 27 16:28:21.939: INFO: stdout: "update-demo-nautilus-j2tvz update-demo-nautilus-wzzcg "
STEP: Replicas for name=update-demo: expected=1 actual=2 06/27/23 16:28:21.939
Jun 27 16:28:26.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 27 16:28:27.065: INFO: stderr: ""
Jun 27 16:28:27.065: INFO: stdout: "update-demo-nautilus-wzzcg "
Jun 27 16:28:27.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:28:27.177: INFO: stderr: ""
Jun 27 16:28:27.177: INFO: stdout: "true"
Jun 27 16:28:27.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 27 16:28:27.295: INFO: stderr: ""
Jun 27 16:28:27.295: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 27 16:28:27.295: INFO: validating pod update-demo-nautilus-wzzcg
Jun 27 16:28:27.347: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 16:28:27.347: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 16:28:27.347: INFO: update-demo-nautilus-wzzcg is verified up and running
STEP: scaling up the replication controller 06/27/23 16:28:27.348
Jun 27 16:28:27.356: INFO: scanned /root for discovery docs: <nil>
Jun 27 16:28:27.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Jun 27 16:28:28.599: INFO: stderr: ""
Jun 27 16:28:28.599: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/27/23 16:28:28.599
Jun 27 16:28:28.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 27 16:28:28.795: INFO: stderr: ""
Jun 27 16:28:28.795: INFO: stdout: "update-demo-nautilus-wzzcg update-demo-nautilus-zc9n8 "
Jun 27 16:28:28.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:28:28.942: INFO: stderr: ""
Jun 27 16:28:28.942: INFO: stdout: "true"
Jun 27 16:28:28.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 27 16:28:29.080: INFO: stderr: ""
Jun 27 16:28:29.080: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 27 16:28:29.080: INFO: validating pod update-demo-nautilus-wzzcg
Jun 27 16:28:29.103: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 16:28:29.103: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 16:28:29.103: INFO: update-demo-nautilus-wzzcg is verified up and running
Jun 27 16:28:29.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-zc9n8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:28:29.232: INFO: stderr: ""
Jun 27 16:28:29.232: INFO: stdout: ""
Jun 27 16:28:29.232: INFO: update-demo-nautilus-zc9n8 is created but not running
Jun 27 16:28:34.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 27 16:28:34.392: INFO: stderr: ""
Jun 27 16:28:34.392: INFO: stdout: "update-demo-nautilus-wzzcg update-demo-nautilus-zc9n8 "
Jun 27 16:28:34.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:28:34.503: INFO: stderr: ""
Jun 27 16:28:34.503: INFO: stdout: "true"
Jun 27 16:28:34.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 27 16:28:34.663: INFO: stderr: ""
Jun 27 16:28:34.663: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 27 16:28:34.663: INFO: validating pod update-demo-nautilus-wzzcg
Jun 27 16:28:34.700: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 16:28:34.700: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 16:28:34.700: INFO: update-demo-nautilus-wzzcg is verified up and running
Jun 27 16:28:34.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-zc9n8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:28:34.851: INFO: stderr: ""
Jun 27 16:28:34.851: INFO: stdout: "true"
Jun 27 16:28:34.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-zc9n8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 27 16:28:34.999: INFO: stderr: ""
Jun 27 16:28:34.999: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 27 16:28:34.999: INFO: validating pod update-demo-nautilus-zc9n8
Jun 27 16:28:35.059: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 16:28:35.059: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 16:28:35.059: INFO: update-demo-nautilus-zc9n8 is verified up and running
STEP: using delete to clean up resources 06/27/23 16:28:35.059
Jun 27 16:28:35.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 delete --grace-period=0 --force -f -'
Jun 27 16:28:35.216: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 16:28:35.216: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 27 16:28:35.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get rc,svc -l name=update-demo --no-headers'
Jun 27 16:28:35.396: INFO: stderr: "No resources found in kubectl-1552 namespace.\n"
Jun 27 16:28:35.396: INFO: stdout: ""
Jun 27 16:28:35.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 27 16:28:35.553: INFO: stderr: ""
Jun 27 16:28:35.553: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:28:35.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1552" for this suite. 06/27/23 16:28:35.593
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":30,"skipped":633,"failed":0}
------------------------------
â€¢ [SLOW TEST] [29.190 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:28:06.45
    Jun 27 16:28:06.450: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:28:06.451
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:06.501
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:06.515
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 06/27/23 16:28:06.529
    Jun 27 16:28:06.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 create -f -'
    Jun 27 16:28:09.149: INFO: stderr: ""
    Jun 27 16:28:09.149: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/27/23 16:28:09.149
    Jun 27 16:28:09.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 27 16:28:09.328: INFO: stderr: ""
    Jun 27 16:28:09.328: INFO: stdout: "update-demo-nautilus-j2tvz update-demo-nautilus-wzzcg "
    Jun 27 16:28:09.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-j2tvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:28:09.449: INFO: stderr: ""
    Jun 27 16:28:09.449: INFO: stdout: ""
    Jun 27 16:28:09.449: INFO: update-demo-nautilus-j2tvz is created but not running
    Jun 27 16:28:14.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 27 16:28:14.597: INFO: stderr: ""
    Jun 27 16:28:14.597: INFO: stdout: "update-demo-nautilus-j2tvz update-demo-nautilus-wzzcg "
    Jun 27 16:28:14.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-j2tvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:28:14.751: INFO: stderr: ""
    Jun 27 16:28:14.751: INFO: stdout: ""
    Jun 27 16:28:14.751: INFO: update-demo-nautilus-j2tvz is created but not running
    Jun 27 16:28:19.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 27 16:28:19.867: INFO: stderr: ""
    Jun 27 16:28:19.867: INFO: stdout: "update-demo-nautilus-j2tvz update-demo-nautilus-wzzcg "
    Jun 27 16:28:19.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-j2tvz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:28:20.028: INFO: stderr: ""
    Jun 27 16:28:20.029: INFO: stdout: "true"
    Jun 27 16:28:20.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-j2tvz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 27 16:28:20.164: INFO: stderr: ""
    Jun 27 16:28:20.164: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 27 16:28:20.164: INFO: validating pod update-demo-nautilus-j2tvz
    Jun 27 16:28:20.205: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 27 16:28:20.205: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 27 16:28:20.205: INFO: update-demo-nautilus-j2tvz is verified up and running
    Jun 27 16:28:20.205: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:28:20.346: INFO: stderr: ""
    Jun 27 16:28:20.346: INFO: stdout: "true"
    Jun 27 16:28:20.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 27 16:28:20.514: INFO: stderr: ""
    Jun 27 16:28:20.514: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 27 16:28:20.514: INFO: validating pod update-demo-nautilus-wzzcg
    Jun 27 16:28:20.572: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 27 16:28:20.572: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 27 16:28:20.572: INFO: update-demo-nautilus-wzzcg is verified up and running
    STEP: scaling down the replication controller 06/27/23 16:28:20.572
    Jun 27 16:28:20.583: INFO: scanned /root for discovery docs: <nil>
    Jun 27 16:28:20.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Jun 27 16:28:21.797: INFO: stderr: ""
    Jun 27 16:28:21.797: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/27/23 16:28:21.797
    Jun 27 16:28:21.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 27 16:28:21.939: INFO: stderr: ""
    Jun 27 16:28:21.939: INFO: stdout: "update-demo-nautilus-j2tvz update-demo-nautilus-wzzcg "
    STEP: Replicas for name=update-demo: expected=1 actual=2 06/27/23 16:28:21.939
    Jun 27 16:28:26.941: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 27 16:28:27.065: INFO: stderr: ""
    Jun 27 16:28:27.065: INFO: stdout: "update-demo-nautilus-wzzcg "
    Jun 27 16:28:27.065: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:28:27.177: INFO: stderr: ""
    Jun 27 16:28:27.177: INFO: stdout: "true"
    Jun 27 16:28:27.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 27 16:28:27.295: INFO: stderr: ""
    Jun 27 16:28:27.295: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 27 16:28:27.295: INFO: validating pod update-demo-nautilus-wzzcg
    Jun 27 16:28:27.347: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 27 16:28:27.347: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 27 16:28:27.347: INFO: update-demo-nautilus-wzzcg is verified up and running
    STEP: scaling up the replication controller 06/27/23 16:28:27.348
    Jun 27 16:28:27.356: INFO: scanned /root for discovery docs: <nil>
    Jun 27 16:28:27.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Jun 27 16:28:28.599: INFO: stderr: ""
    Jun 27 16:28:28.599: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/27/23 16:28:28.599
    Jun 27 16:28:28.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 27 16:28:28.795: INFO: stderr: ""
    Jun 27 16:28:28.795: INFO: stdout: "update-demo-nautilus-wzzcg update-demo-nautilus-zc9n8 "
    Jun 27 16:28:28.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:28:28.942: INFO: stderr: ""
    Jun 27 16:28:28.942: INFO: stdout: "true"
    Jun 27 16:28:28.942: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 27 16:28:29.080: INFO: stderr: ""
    Jun 27 16:28:29.080: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 27 16:28:29.080: INFO: validating pod update-demo-nautilus-wzzcg
    Jun 27 16:28:29.103: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 27 16:28:29.103: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 27 16:28:29.103: INFO: update-demo-nautilus-wzzcg is verified up and running
    Jun 27 16:28:29.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-zc9n8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:28:29.232: INFO: stderr: ""
    Jun 27 16:28:29.232: INFO: stdout: ""
    Jun 27 16:28:29.232: INFO: update-demo-nautilus-zc9n8 is created but not running
    Jun 27 16:28:34.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 27 16:28:34.392: INFO: stderr: ""
    Jun 27 16:28:34.392: INFO: stdout: "update-demo-nautilus-wzzcg update-demo-nautilus-zc9n8 "
    Jun 27 16:28:34.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:28:34.503: INFO: stderr: ""
    Jun 27 16:28:34.503: INFO: stdout: "true"
    Jun 27 16:28:34.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-wzzcg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 27 16:28:34.663: INFO: stderr: ""
    Jun 27 16:28:34.663: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 27 16:28:34.663: INFO: validating pod update-demo-nautilus-wzzcg
    Jun 27 16:28:34.700: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 27 16:28:34.700: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 27 16:28:34.700: INFO: update-demo-nautilus-wzzcg is verified up and running
    Jun 27 16:28:34.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-zc9n8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:28:34.851: INFO: stderr: ""
    Jun 27 16:28:34.851: INFO: stdout: "true"
    Jun 27 16:28:34.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods update-demo-nautilus-zc9n8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 27 16:28:34.999: INFO: stderr: ""
    Jun 27 16:28:34.999: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 27 16:28:34.999: INFO: validating pod update-demo-nautilus-zc9n8
    Jun 27 16:28:35.059: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 27 16:28:35.059: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 27 16:28:35.059: INFO: update-demo-nautilus-zc9n8 is verified up and running
    STEP: using delete to clean up resources 06/27/23 16:28:35.059
    Jun 27 16:28:35.060: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 delete --grace-period=0 --force -f -'
    Jun 27 16:28:35.216: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 27 16:28:35.216: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun 27 16:28:35.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get rc,svc -l name=update-demo --no-headers'
    Jun 27 16:28:35.396: INFO: stderr: "No resources found in kubectl-1552 namespace.\n"
    Jun 27 16:28:35.396: INFO: stdout: ""
    Jun 27 16:28:35.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1552 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 27 16:28:35.553: INFO: stderr: ""
    Jun 27 16:28:35.553: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:28:35.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1552" for this suite. 06/27/23 16:28:35.593
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:28:35.648
Jun 27 16:28:35.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename cronjob 06/27/23 16:28:35.65
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:35.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:35.718
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 06/27/23 16:28:35.733
STEP: creating 06/27/23 16:28:35.734
STEP: getting 06/27/23 16:28:35.749
STEP: listing 06/27/23 16:28:35.784
STEP: watching 06/27/23 16:28:35.797
Jun 27 16:28:35.798: INFO: starting watch
STEP: cluster-wide listing 06/27/23 16:28:35.809
STEP: cluster-wide watching 06/27/23 16:28:35.823
Jun 27 16:28:35.823: INFO: starting watch
STEP: patching 06/27/23 16:28:35.834
STEP: updating 06/27/23 16:28:35.86
Jun 27 16:28:35.896: INFO: waiting for watch events with expected annotations
Jun 27 16:28:35.896: INFO: saw patched and updated annotations
STEP: patching /status 06/27/23 16:28:35.897
STEP: updating /status 06/27/23 16:28:35.917
STEP: get /status 06/27/23 16:28:35.947
STEP: deleting 06/27/23 16:28:35.96
STEP: deleting a collection 06/27/23 16:28:36.021
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 27 16:28:36.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5863" for this suite. 06/27/23 16:28:36.132
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":31,"skipped":675,"failed":0}
------------------------------
â€¢ [0.513 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:28:35.648
    Jun 27 16:28:35.648: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename cronjob 06/27/23 16:28:35.65
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:35.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:35.718
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 06/27/23 16:28:35.733
    STEP: creating 06/27/23 16:28:35.734
    STEP: getting 06/27/23 16:28:35.749
    STEP: listing 06/27/23 16:28:35.784
    STEP: watching 06/27/23 16:28:35.797
    Jun 27 16:28:35.798: INFO: starting watch
    STEP: cluster-wide listing 06/27/23 16:28:35.809
    STEP: cluster-wide watching 06/27/23 16:28:35.823
    Jun 27 16:28:35.823: INFO: starting watch
    STEP: patching 06/27/23 16:28:35.834
    STEP: updating 06/27/23 16:28:35.86
    Jun 27 16:28:35.896: INFO: waiting for watch events with expected annotations
    Jun 27 16:28:35.896: INFO: saw patched and updated annotations
    STEP: patching /status 06/27/23 16:28:35.897
    STEP: updating /status 06/27/23 16:28:35.917
    STEP: get /status 06/27/23 16:28:35.947
    STEP: deleting 06/27/23 16:28:35.96
    STEP: deleting a collection 06/27/23 16:28:36.021
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 27 16:28:36.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5863" for this suite. 06/27/23 16:28:36.132
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:28:36.163
Jun 27 16:28:36.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 16:28:36.17
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:36.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:36.235
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-2b381528-48e1-4da8-afc9-35a2fa16bb2c 06/27/23 16:28:36.25
STEP: Creating a pod to test consume secrets 06/27/23 16:28:36.266
Jun 27 16:28:36.342: INFO: Waiting up to 5m0s for pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce" in namespace "secrets-5200" to be "Succeeded or Failed"
Jun 27 16:28:36.363: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce": Phase="Pending", Reason="", readiness=false. Elapsed: 20.487861ms
Jun 27 16:28:38.382: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03999906s
Jun 27 16:28:40.383: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041126616s
Jun 27 16:28:42.385: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042803744s
STEP: Saw pod success 06/27/23 16:28:42.385
Jun 27 16:28:42.386: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce" satisfied condition "Succeeded or Failed"
Jun 27 16:28:42.404: INFO: Trying to get logs from node 10.113.180.89 pod pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce container secret-volume-test: <nil>
STEP: delete the pod 06/27/23 16:28:42.508
Jun 27 16:28:42.566: INFO: Waiting for pod pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce to disappear
Jun 27 16:28:42.602: INFO: Pod pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 27 16:28:42.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5200" for this suite. 06/27/23 16:28:42.652
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":32,"skipped":682,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.536 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:28:36.163
    Jun 27 16:28:36.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 16:28:36.17
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:36.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:36.235
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-2b381528-48e1-4da8-afc9-35a2fa16bb2c 06/27/23 16:28:36.25
    STEP: Creating a pod to test consume secrets 06/27/23 16:28:36.266
    Jun 27 16:28:36.342: INFO: Waiting up to 5m0s for pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce" in namespace "secrets-5200" to be "Succeeded or Failed"
    Jun 27 16:28:36.363: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce": Phase="Pending", Reason="", readiness=false. Elapsed: 20.487861ms
    Jun 27 16:28:38.382: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03999906s
    Jun 27 16:28:40.383: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041126616s
    Jun 27 16:28:42.385: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.042803744s
    STEP: Saw pod success 06/27/23 16:28:42.385
    Jun 27 16:28:42.386: INFO: Pod "pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce" satisfied condition "Succeeded or Failed"
    Jun 27 16:28:42.404: INFO: Trying to get logs from node 10.113.180.89 pod pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce container secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 16:28:42.508
    Jun 27 16:28:42.566: INFO: Waiting for pod pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce to disappear
    Jun 27 16:28:42.602: INFO: Pod pod-secrets-81878a10-09be-48cd-b3b0-1c3e0d7a30ce no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 16:28:42.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5200" for this suite. 06/27/23 16:28:42.652
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:28:42.706
Jun 27 16:28:42.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 16:28:42.708
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:42.75
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:42.768
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 06/27/23 16:28:42.783
STEP: Creating a ResourceQuota 06/27/23 16:28:47.795
STEP: Ensuring resource quota status is calculated 06/27/23 16:28:47.816
STEP: Creating a Service 06/27/23 16:28:49.83
STEP: Creating a NodePort Service 06/27/23 16:28:49.881
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/27/23 16:28:49.944
STEP: Ensuring resource quota status captures service creation 06/27/23 16:28:49.996
STEP: Deleting Services 06/27/23 16:28:52.011
STEP: Ensuring resource quota status released usage 06/27/23 16:28:52.112
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 16:28:54.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7606" for this suite. 06/27/23 16:28:54.164
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":33,"skipped":713,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.489 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:28:42.706
    Jun 27 16:28:42.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 16:28:42.708
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:42.75
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:42.768
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 06/27/23 16:28:42.783
    STEP: Creating a ResourceQuota 06/27/23 16:28:47.795
    STEP: Ensuring resource quota status is calculated 06/27/23 16:28:47.816
    STEP: Creating a Service 06/27/23 16:28:49.83
    STEP: Creating a NodePort Service 06/27/23 16:28:49.881
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 06/27/23 16:28:49.944
    STEP: Ensuring resource quota status captures service creation 06/27/23 16:28:49.996
    STEP: Deleting Services 06/27/23 16:28:52.011
    STEP: Ensuring resource quota status released usage 06/27/23 16:28:52.112
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 16:28:54.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7606" for this suite. 06/27/23 16:28:54.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:28:54.196
Jun 27 16:28:54.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-watch 06/27/23 16:28:54.199
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:54.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:54.259
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Jun 27 16:28:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Creating first CR  06/27/23 16:28:56.929
Jun 27 16:28:56.948: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:28:56Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:28:56Z]] name:name1 resourceVersion:76002 uid:8ce1fadd-ac46-4a82-9e0e-66764859c3f3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 06/27/23 16:29:06.949
Jun 27 16:29:06.972: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:29:06Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:06Z]] name:name2 resourceVersion:76065 uid:b05da42f-adc9-4f68-9695-6cb006ddad3b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 06/27/23 16:29:16.973
Jun 27 16:29:16.996: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:28:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:16Z]] name:name1 resourceVersion:76116 uid:8ce1fadd-ac46-4a82-9e0e-66764859c3f3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 06/27/23 16:29:26.998
Jun 27 16:29:27.018: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:29:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:27Z]] name:name2 resourceVersion:76167 uid:b05da42f-adc9-4f68-9695-6cb006ddad3b] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 06/27/23 16:29:37.018
Jun 27 16:29:37.084: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:28:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:16Z]] name:name1 resourceVersion:76206 uid:8ce1fadd-ac46-4a82-9e0e-66764859c3f3] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 06/27/23 16:29:47.085
Jun 27 16:29:47.106: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:29:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:27Z]] name:name2 resourceVersion:76259 uid:b05da42f-adc9-4f68-9695-6cb006ddad3b] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:29:57.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6113" for this suite. 06/27/23 16:29:57.686
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":34,"skipped":718,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.516 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:28:54.196
    Jun 27 16:28:54.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-watch 06/27/23 16:28:54.199
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:28:54.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:28:54.259
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Jun 27 16:28:54.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Creating first CR  06/27/23 16:28:56.929
    Jun 27 16:28:56.948: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:28:56Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:28:56Z]] name:name1 resourceVersion:76002 uid:8ce1fadd-ac46-4a82-9e0e-66764859c3f3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 06/27/23 16:29:06.949
    Jun 27 16:29:06.972: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:29:06Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:06Z]] name:name2 resourceVersion:76065 uid:b05da42f-adc9-4f68-9695-6cb006ddad3b] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 06/27/23 16:29:16.973
    Jun 27 16:29:16.996: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:28:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:16Z]] name:name1 resourceVersion:76116 uid:8ce1fadd-ac46-4a82-9e0e-66764859c3f3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 06/27/23 16:29:26.998
    Jun 27 16:29:27.018: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:29:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:27Z]] name:name2 resourceVersion:76167 uid:b05da42f-adc9-4f68-9695-6cb006ddad3b] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 06/27/23 16:29:37.018
    Jun 27 16:29:37.084: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:28:56Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:16Z]] name:name1 resourceVersion:76206 uid:8ce1fadd-ac46-4a82-9e0e-66764859c3f3] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 06/27/23 16:29:47.085
    Jun 27 16:29:47.106: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-06-27T16:29:06Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-06-27T16:29:27Z]] name:name2 resourceVersion:76259 uid:b05da42f-adc9-4f68-9695-6cb006ddad3b] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:29:57.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-6113" for this suite. 06/27/23 16:29:57.686
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:29:57.722
Jun 27 16:29:57.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename security-context-test 06/27/23 16:29:57.724
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:29:57.8
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:29:57.816
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Jun 27 16:29:57.943: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908" in namespace "security-context-test-4473" to be "Succeeded or Failed"
Jun 27 16:29:57.983: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": Phase="Pending", Reason="", readiness=false. Elapsed: 39.896618ms
Jun 27 16:30:00.007: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064216915s
Jun 27 16:30:02.016: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073679346s
Jun 27 16:30:04.006: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062772141s
Jun 27 16:30:04.006: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908" satisfied condition "Succeeded or Failed"
Jun 27 16:30:04.108: INFO: Got logs for pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 27 16:30:04.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-4473" for this suite. 06/27/23 16:30:04.14
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":35,"skipped":759,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.447 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:29:57.722
    Jun 27 16:29:57.722: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename security-context-test 06/27/23 16:29:57.724
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:29:57.8
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:29:57.816
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Jun 27 16:29:57.943: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908" in namespace "security-context-test-4473" to be "Succeeded or Failed"
    Jun 27 16:29:57.983: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": Phase="Pending", Reason="", readiness=false. Elapsed: 39.896618ms
    Jun 27 16:30:00.007: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": Phase="Pending", Reason="", readiness=false. Elapsed: 2.064216915s
    Jun 27 16:30:02.016: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": Phase="Pending", Reason="", readiness=false. Elapsed: 4.073679346s
    Jun 27 16:30:04.006: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.062772141s
    Jun 27 16:30:04.006: INFO: Pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908" satisfied condition "Succeeded or Failed"
    Jun 27 16:30:04.108: INFO: Got logs for pod "busybox-privileged-false-3adc64da-3ab1-4d6b-b919-7b1a049c8908": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 27 16:30:04.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-4473" for this suite. 06/27/23 16:30:04.14
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:30:04.171
Jun 27 16:30:04.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 16:30:04.174
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:04.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:04.243
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 06/27/23 16:30:04.259
STEP: Ensuring ResourceQuota status is calculated 06/27/23 16:30:04.274
STEP: Creating a ResourceQuota with not terminating scope 06/27/23 16:30:06.292
STEP: Ensuring ResourceQuota status is calculated 06/27/23 16:30:06.317
STEP: Creating a long running pod 06/27/23 16:30:08.331
STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/27/23 16:30:08.398
STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/27/23 16:30:10.412
STEP: Deleting the pod 06/27/23 16:30:12.425
STEP: Ensuring resource quota status released the pod usage 06/27/23 16:30:12.47
STEP: Creating a terminating pod 06/27/23 16:30:14.493
STEP: Ensuring resource quota with terminating scope captures the pod usage 06/27/23 16:30:14.556
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/27/23 16:30:16.574
STEP: Deleting the pod 06/27/23 16:30:18.588
STEP: Ensuring resource quota status released the pod usage 06/27/23 16:30:18.639
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 16:30:20.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7926" for this suite. 06/27/23 16:30:20.687
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":36,"skipped":770,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.544 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:30:04.171
    Jun 27 16:30:04.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 16:30:04.174
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:04.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:04.243
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 06/27/23 16:30:04.259
    STEP: Ensuring ResourceQuota status is calculated 06/27/23 16:30:04.274
    STEP: Creating a ResourceQuota with not terminating scope 06/27/23 16:30:06.292
    STEP: Ensuring ResourceQuota status is calculated 06/27/23 16:30:06.317
    STEP: Creating a long running pod 06/27/23 16:30:08.331
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 06/27/23 16:30:08.398
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 06/27/23 16:30:10.412
    STEP: Deleting the pod 06/27/23 16:30:12.425
    STEP: Ensuring resource quota status released the pod usage 06/27/23 16:30:12.47
    STEP: Creating a terminating pod 06/27/23 16:30:14.493
    STEP: Ensuring resource quota with terminating scope captures the pod usage 06/27/23 16:30:14.556
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 06/27/23 16:30:16.574
    STEP: Deleting the pod 06/27/23 16:30:18.588
    STEP: Ensuring resource quota status released the pod usage 06/27/23 16:30:18.639
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 16:30:20.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7926" for this suite. 06/27/23 16:30:20.687
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:30:20.716
Jun 27 16:30:20.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:30:20.719
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:20.776
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:20.791
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 06/27/23 16:30:20.808
Jun 27 16:30:20.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Jun 27 16:30:20.984: INFO: stderr: ""
Jun 27 16:30:20.984: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 06/27/23 16:30:20.984
Jun 27 16:30:20.985: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Jun 27 16:30:20.985: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1591" to be "running and ready, or succeeded"
Jun 27 16:30:21.005: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 20.865284ms
Jun 27 16:30:21.006: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.113.180.96' to be 'Running' but was 'Pending'
Jun 27 16:30:23.027: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042496897s
Jun 27 16:30:23.027: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.113.180.96' to be 'Running' but was 'Pending'
Jun 27 16:30:25.026: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.041514038s
Jun 27 16:30:25.026: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Jun 27 16:30:25.026: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 06/27/23 16:30:25.026
Jun 27 16:30:25.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator'
Jun 27 16:30:25.325: INFO: stderr: ""
Jun 27 16:30:25.325: INFO: stdout: "I0627 16:30:22.595735       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/slcv 499\nI0627 16:30:22.796007       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/wqph 340\nI0627 16:30:22.996736       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/pmz8 356\nI0627 16:30:23.196187       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/km6 538\nI0627 16:30:23.396473       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/7r5 442\nI0627 16:30:23.595943       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/wjp8 309\nI0627 16:30:23.796506       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/2sdf 366\nI0627 16:30:23.995955       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/qzlv 310\nI0627 16:30:24.196461       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/5b4 200\nI0627 16:30:24.395909       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/prds 322\nI0627 16:30:24.596364       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/8hjk 554\nI0627 16:30:24.796845       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/ljlk 478\nI0627 16:30:24.996276       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/tcnb 538\nI0627 16:30:25.196701       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/pp8t 555\n"
STEP: limiting log lines 06/27/23 16:30:25.325
Jun 27 16:30:25.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --tail=1'
Jun 27 16:30:25.564: INFO: stderr: ""
Jun 27 16:30:25.564: INFO: stdout: "I0627 16:30:25.396466       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/vxn 519\n"
Jun 27 16:30:25.564: INFO: got output "I0627 16:30:25.396466       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/vxn 519\n"
STEP: limiting log bytes 06/27/23 16:30:25.564
Jun 27 16:30:25.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --limit-bytes=1'
Jun 27 16:30:25.732: INFO: stderr: ""
Jun 27 16:30:25.732: INFO: stdout: "I"
Jun 27 16:30:25.732: INFO: got output "I"
STEP: exposing timestamps 06/27/23 16:30:25.732
Jun 27 16:30:25.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --tail=1 --timestamps'
Jun 27 16:30:25.934: INFO: stderr: ""
Jun 27 16:30:25.934: INFO: stdout: "2023-06-27T11:30:25.806510250-05:00 I0627 16:30:25.806419       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/558 271\n"
Jun 27 16:30:25.934: INFO: got output "2023-06-27T11:30:25.806510250-05:00 I0627 16:30:25.806419       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/558 271\n"
STEP: restricting to a time range 06/27/23 16:30:25.934
Jun 27 16:30:28.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --since=1s'
Jun 27 16:30:28.630: INFO: stderr: ""
Jun 27 16:30:28.630: INFO: stdout: "I0627 16:30:27.796328       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/p74g 370\nI0627 16:30:27.996859       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/5scj 597\nI0627 16:30:28.196403       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/xfzg 275\nI0627 16:30:28.395797       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/rqdw 518\nI0627 16:30:28.596329       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/d6b7 524\n"
Jun 27 16:30:28.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --since=24h'
Jun 27 16:30:28.876: INFO: stderr: ""
Jun 27 16:30:28.876: INFO: stdout: "I0627 16:30:22.595735       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/slcv 499\nI0627 16:30:22.796007       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/wqph 340\nI0627 16:30:22.996736       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/pmz8 356\nI0627 16:30:23.196187       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/km6 538\nI0627 16:30:23.396473       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/7r5 442\nI0627 16:30:23.595943       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/wjp8 309\nI0627 16:30:23.796506       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/2sdf 366\nI0627 16:30:23.995955       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/qzlv 310\nI0627 16:30:24.196461       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/5b4 200\nI0627 16:30:24.395909       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/prds 322\nI0627 16:30:24.596364       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/8hjk 554\nI0627 16:30:24.796845       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/ljlk 478\nI0627 16:30:24.996276       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/tcnb 538\nI0627 16:30:25.196701       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/pp8t 555\nI0627 16:30:25.396466       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/vxn 519\nI0627 16:30:25.595888       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/tttd 292\nI0627 16:30:25.806419       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/558 271\nI0627 16:30:25.996414       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/9472 581\nI0627 16:30:26.197093       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/t4nh 571\nI0627 16:30:26.396373       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fzb 562\nI0627 16:30:26.596725       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/gtg 586\nI0627 16:30:26.796191       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/dps 375\nI0627 16:30:26.996718       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/vp8x 248\nI0627 16:30:27.196197       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/6nwd 374\nI0627 16:30:27.396735       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/hr58 315\nI0627 16:30:27.596721       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/bwl 340\nI0627 16:30:27.796328       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/p74g 370\nI0627 16:30:27.996859       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/5scj 597\nI0627 16:30:28.196403       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/xfzg 275\nI0627 16:30:28.395797       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/rqdw 518\nI0627 16:30:28.596329       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/d6b7 524\nI0627 16:30:28.796884       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/hvq 556\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Jun 27 16:30:28.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 delete pod logs-generator'
Jun 27 16:30:30.740: INFO: stderr: ""
Jun 27 16:30:30.741: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:30:30.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1591" for this suite. 06/27/23 16:30:30.793
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":37,"skipped":773,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.102 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:30:20.716
    Jun 27 16:30:20.717: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:30:20.719
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:20.776
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:20.791
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 06/27/23 16:30:20.808
    Jun 27 16:30:20.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Jun 27 16:30:20.984: INFO: stderr: ""
    Jun 27 16:30:20.984: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 06/27/23 16:30:20.984
    Jun 27 16:30:20.985: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Jun 27 16:30:20.985: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-1591" to be "running and ready, or succeeded"
    Jun 27 16:30:21.005: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 20.865284ms
    Jun 27 16:30:21.006: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.113.180.96' to be 'Running' but was 'Pending'
    Jun 27 16:30:23.027: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042496897s
    Jun 27 16:30:23.027: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on '10.113.180.96' to be 'Running' but was 'Pending'
    Jun 27 16:30:25.026: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 4.041514038s
    Jun 27 16:30:25.026: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Jun 27 16:30:25.026: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 06/27/23 16:30:25.026
    Jun 27 16:30:25.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator'
    Jun 27 16:30:25.325: INFO: stderr: ""
    Jun 27 16:30:25.325: INFO: stdout: "I0627 16:30:22.595735       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/slcv 499\nI0627 16:30:22.796007       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/wqph 340\nI0627 16:30:22.996736       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/pmz8 356\nI0627 16:30:23.196187       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/km6 538\nI0627 16:30:23.396473       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/7r5 442\nI0627 16:30:23.595943       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/wjp8 309\nI0627 16:30:23.796506       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/2sdf 366\nI0627 16:30:23.995955       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/qzlv 310\nI0627 16:30:24.196461       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/5b4 200\nI0627 16:30:24.395909       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/prds 322\nI0627 16:30:24.596364       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/8hjk 554\nI0627 16:30:24.796845       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/ljlk 478\nI0627 16:30:24.996276       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/tcnb 538\nI0627 16:30:25.196701       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/pp8t 555\n"
    STEP: limiting log lines 06/27/23 16:30:25.325
    Jun 27 16:30:25.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --tail=1'
    Jun 27 16:30:25.564: INFO: stderr: ""
    Jun 27 16:30:25.564: INFO: stdout: "I0627 16:30:25.396466       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/vxn 519\n"
    Jun 27 16:30:25.564: INFO: got output "I0627 16:30:25.396466       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/vxn 519\n"
    STEP: limiting log bytes 06/27/23 16:30:25.564
    Jun 27 16:30:25.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --limit-bytes=1'
    Jun 27 16:30:25.732: INFO: stderr: ""
    Jun 27 16:30:25.732: INFO: stdout: "I"
    Jun 27 16:30:25.732: INFO: got output "I"
    STEP: exposing timestamps 06/27/23 16:30:25.732
    Jun 27 16:30:25.733: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --tail=1 --timestamps'
    Jun 27 16:30:25.934: INFO: stderr: ""
    Jun 27 16:30:25.934: INFO: stdout: "2023-06-27T11:30:25.806510250-05:00 I0627 16:30:25.806419       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/558 271\n"
    Jun 27 16:30:25.934: INFO: got output "2023-06-27T11:30:25.806510250-05:00 I0627 16:30:25.806419       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/558 271\n"
    STEP: restricting to a time range 06/27/23 16:30:25.934
    Jun 27 16:30:28.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --since=1s'
    Jun 27 16:30:28.630: INFO: stderr: ""
    Jun 27 16:30:28.630: INFO: stdout: "I0627 16:30:27.796328       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/p74g 370\nI0627 16:30:27.996859       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/5scj 597\nI0627 16:30:28.196403       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/xfzg 275\nI0627 16:30:28.395797       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/rqdw 518\nI0627 16:30:28.596329       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/d6b7 524\n"
    Jun 27 16:30:28.630: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 logs logs-generator logs-generator --since=24h'
    Jun 27 16:30:28.876: INFO: stderr: ""
    Jun 27 16:30:28.876: INFO: stdout: "I0627 16:30:22.595735       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/slcv 499\nI0627 16:30:22.796007       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/wqph 340\nI0627 16:30:22.996736       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/pmz8 356\nI0627 16:30:23.196187       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/km6 538\nI0627 16:30:23.396473       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/7r5 442\nI0627 16:30:23.595943       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/wjp8 309\nI0627 16:30:23.796506       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/2sdf 366\nI0627 16:30:23.995955       1 logs_generator.go:76] 7 GET /api/v1/namespaces/ns/pods/qzlv 310\nI0627 16:30:24.196461       1 logs_generator.go:76] 8 GET /api/v1/namespaces/ns/pods/5b4 200\nI0627 16:30:24.395909       1 logs_generator.go:76] 9 POST /api/v1/namespaces/ns/pods/prds 322\nI0627 16:30:24.596364       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/8hjk 554\nI0627 16:30:24.796845       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/ns/pods/ljlk 478\nI0627 16:30:24.996276       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/tcnb 538\nI0627 16:30:25.196701       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/pp8t 555\nI0627 16:30:25.396466       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/vxn 519\nI0627 16:30:25.595888       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/tttd 292\nI0627 16:30:25.806419       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/558 271\nI0627 16:30:25.996414       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/default/pods/9472 581\nI0627 16:30:26.197093       1 logs_generator.go:76] 18 POST /api/v1/namespaces/ns/pods/t4nh 571\nI0627 16:30:26.396373       1 logs_generator.go:76] 19 POST /api/v1/namespaces/default/pods/fzb 562\nI0627 16:30:26.596725       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/gtg 586\nI0627 16:30:26.796191       1 logs_generator.go:76] 21 POST /api/v1/namespaces/default/pods/dps 375\nI0627 16:30:26.996718       1 logs_generator.go:76] 22 POST /api/v1/namespaces/kube-system/pods/vp8x 248\nI0627 16:30:27.196197       1 logs_generator.go:76] 23 POST /api/v1/namespaces/default/pods/6nwd 374\nI0627 16:30:27.396735       1 logs_generator.go:76] 24 POST /api/v1/namespaces/ns/pods/hr58 315\nI0627 16:30:27.596721       1 logs_generator.go:76] 25 GET /api/v1/namespaces/kube-system/pods/bwl 340\nI0627 16:30:27.796328       1 logs_generator.go:76] 26 PUT /api/v1/namespaces/default/pods/p74g 370\nI0627 16:30:27.996859       1 logs_generator.go:76] 27 GET /api/v1/namespaces/kube-system/pods/5scj 597\nI0627 16:30:28.196403       1 logs_generator.go:76] 28 GET /api/v1/namespaces/default/pods/xfzg 275\nI0627 16:30:28.395797       1 logs_generator.go:76] 29 GET /api/v1/namespaces/ns/pods/rqdw 518\nI0627 16:30:28.596329       1 logs_generator.go:76] 30 POST /api/v1/namespaces/ns/pods/d6b7 524\nI0627 16:30:28.796884       1 logs_generator.go:76] 31 GET /api/v1/namespaces/default/pods/hvq 556\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Jun 27 16:30:28.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1591 delete pod logs-generator'
    Jun 27 16:30:30.740: INFO: stderr: ""
    Jun 27 16:30:30.741: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:30:30.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1591" for this suite. 06/27/23 16:30:30.793
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:30:30.831
Jun 27 16:30:30.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubelet-test 06/27/23 16:30:30.833
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:30.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:30.895
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 06/27/23 16:30:30.985
Jun 27 16:30:30.985: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2" in namespace "kubelet-test-2164" to be "completed"
Jun 27 16:30:31.006: INFO: Pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.193578ms
Jun 27 16:30:33.026: INFO: Pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040936232s
Jun 27 16:30:35.030: INFO: Pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044324304s
Jun 27 16:30:35.030: INFO: Pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 27 16:30:35.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2164" for this suite. 06/27/23 16:30:35.125
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":38,"skipped":859,"failed":0}
------------------------------
â€¢ [4.323 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:30:30.831
    Jun 27 16:30:30.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubelet-test 06/27/23 16:30:30.833
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:30.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:30.895
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 06/27/23 16:30:30.985
    Jun 27 16:30:30.985: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2" in namespace "kubelet-test-2164" to be "completed"
    Jun 27 16:30:31.006: INFO: Pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 20.193578ms
    Jun 27 16:30:33.026: INFO: Pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040936232s
    Jun 27 16:30:35.030: INFO: Pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044324304s
    Jun 27 16:30:35.030: INFO: Pod "agnhost-host-aliasesac69de73-3ce6-445e-9b4a-4f8c7add2ea2" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 27 16:30:35.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2164" for this suite. 06/27/23 16:30:35.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:30:35.159
Jun 27 16:30:35.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename runtimeclass 06/27/23 16:30:35.161
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:35.215
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:35.234
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 27 16:30:35.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-361" for this suite. 06/27/23 16:30:35.319
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":39,"skipped":887,"failed":0}
------------------------------
â€¢ [0.186 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:30:35.159
    Jun 27 16:30:35.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename runtimeclass 06/27/23 16:30:35.161
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:35.215
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:35.234
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 27 16:30:35.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-361" for this suite. 06/27/23 16:30:35.319
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:30:35.345
Jun 27 16:30:35.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename events 06/27/23 16:30:35.348
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:35.397
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:35.419
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 06/27/23 16:30:35.433
STEP: get a list of Events with a label in the current namespace 06/27/23 16:30:35.521
STEP: delete a list of events 06/27/23 16:30:35.541
Jun 27 16:30:35.541: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/27/23 16:30:35.637
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jun 27 16:30:35.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6315" for this suite. 06/27/23 16:30:35.675
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":40,"skipped":888,"failed":0}
------------------------------
â€¢ [0.357 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:30:35.345
    Jun 27 16:30:35.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename events 06/27/23 16:30:35.348
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:35.397
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:35.419
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 06/27/23 16:30:35.433
    STEP: get a list of Events with a label in the current namespace 06/27/23 16:30:35.521
    STEP: delete a list of events 06/27/23 16:30:35.541
    Jun 27 16:30:35.541: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/27/23 16:30:35.637
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jun 27 16:30:35.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6315" for this suite. 06/27/23 16:30:35.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:30:35.709
Jun 27 16:30:35.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:30:35.711
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:35.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:35.786
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 06/27/23 16:30:35.801
Jun 27 16:30:35.884: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68" in namespace "projected-5156" to be "Succeeded or Failed"
Jun 27 16:30:35.911: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 27.325939ms
Jun 27 16:30:37.932: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047913776s
Jun 27 16:30:39.934: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050145774s
Jun 27 16:30:41.935: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050512243s
STEP: Saw pod success 06/27/23 16:30:41.935
Jun 27 16:30:41.935: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68" satisfied condition "Succeeded or Failed"
Jun 27 16:30:41.955: INFO: Trying to get logs from node 10.113.180.96 pod downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68 container client-container: <nil>
STEP: delete the pod 06/27/23 16:30:41.994
Jun 27 16:30:42.049: INFO: Waiting for pod downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68 to disappear
Jun 27 16:30:42.069: INFO: Pod downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 16:30:42.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5156" for this suite. 06/27/23 16:30:42.099
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":41,"skipped":898,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.454 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:30:35.709
    Jun 27 16:30:35.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:30:35.711
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:35.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:35.786
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 06/27/23 16:30:35.801
    Jun 27 16:30:35.884: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68" in namespace "projected-5156" to be "Succeeded or Failed"
    Jun 27 16:30:35.911: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 27.325939ms
    Jun 27 16:30:37.932: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047913776s
    Jun 27 16:30:39.934: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68": Phase="Pending", Reason="", readiness=false. Elapsed: 4.050145774s
    Jun 27 16:30:41.935: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.050512243s
    STEP: Saw pod success 06/27/23 16:30:41.935
    Jun 27 16:30:41.935: INFO: Pod "downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68" satisfied condition "Succeeded or Failed"
    Jun 27 16:30:41.955: INFO: Trying to get logs from node 10.113.180.96 pod downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68 container client-container: <nil>
    STEP: delete the pod 06/27/23 16:30:41.994
    Jun 27 16:30:42.049: INFO: Waiting for pod downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68 to disappear
    Jun 27 16:30:42.069: INFO: Pod downwardapi-volume-f255d991-c5eb-4977-b46a-103f330e2d68 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 16:30:42.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5156" for this suite. 06/27/23 16:30:42.099
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:30:42.163
Jun 27 16:30:42.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 16:30:42.166
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:42.235
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:42.252
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 06/27/23 16:30:42.268
Jun 27 16:30:42.335: INFO: Waiting up to 5m0s for pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f" in namespace "emptydir-8346" to be "Succeeded or Failed"
Jun 27 16:30:42.355: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.162377ms
Jun 27 16:30:44.376: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040866739s
Jun 27 16:30:46.374: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039190458s
Jun 27 16:30:48.374: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038544771s
STEP: Saw pod success 06/27/23 16:30:48.374
Jun 27 16:30:48.374: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f" satisfied condition "Succeeded or Failed"
Jun 27 16:30:48.398: INFO: Trying to get logs from node 10.113.180.96 pod pod-ccab0fa2-0428-451f-a7f0-adba0813e20f container test-container: <nil>
STEP: delete the pod 06/27/23 16:30:48.44
Jun 27 16:30:48.502: INFO: Waiting for pod pod-ccab0fa2-0428-451f-a7f0-adba0813e20f to disappear
Jun 27 16:30:48.529: INFO: Pod pod-ccab0fa2-0428-451f-a7f0-adba0813e20f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 16:30:48.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8346" for this suite. 06/27/23 16:30:48.56
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":42,"skipped":898,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.424 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:30:42.163
    Jun 27 16:30:42.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 16:30:42.166
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:42.235
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:42.252
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 06/27/23 16:30:42.268
    Jun 27 16:30:42.335: INFO: Waiting up to 5m0s for pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f" in namespace "emptydir-8346" to be "Succeeded or Failed"
    Jun 27 16:30:42.355: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f": Phase="Pending", Reason="", readiness=false. Elapsed: 20.162377ms
    Jun 27 16:30:44.376: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040866739s
    Jun 27 16:30:46.374: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039190458s
    Jun 27 16:30:48.374: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038544771s
    STEP: Saw pod success 06/27/23 16:30:48.374
    Jun 27 16:30:48.374: INFO: Pod "pod-ccab0fa2-0428-451f-a7f0-adba0813e20f" satisfied condition "Succeeded or Failed"
    Jun 27 16:30:48.398: INFO: Trying to get logs from node 10.113.180.96 pod pod-ccab0fa2-0428-451f-a7f0-adba0813e20f container test-container: <nil>
    STEP: delete the pod 06/27/23 16:30:48.44
    Jun 27 16:30:48.502: INFO: Waiting for pod pod-ccab0fa2-0428-451f-a7f0-adba0813e20f to disappear
    Jun 27 16:30:48.529: INFO: Pod pod-ccab0fa2-0428-451f-a7f0-adba0813e20f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 16:30:48.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8346" for this suite. 06/27/23 16:30:48.56
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:30:48.588
Jun 27 16:30:48.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename statefulset 06/27/23 16:30:48.591
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:48.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:48.663
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7356 06/27/23 16:30:48.676
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-7356 06/27/23 16:30:48.696
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7356 06/27/23 16:30:48.755
Jun 27 16:30:48.773: INFO: Found 0 stateful pods, waiting for 1
Jun 27 16:30:58.795: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/27/23 16:30:58.795
Jun 27 16:30:58.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 16:30:59.209: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 16:30:59.209: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 16:30:59.209: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 27 16:30:59.227: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 27 16:31:09.251: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 16:31:09.251: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 16:31:09.325: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 27 16:31:09.325: INFO: ss-0  10.113.180.90  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  }]
Jun 27 16:31:09.325: INFO: 
Jun 27 16:31:09.325: INFO: StatefulSet ss has not reached scale 3, at 1
Jun 27 16:31:10.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980127915s
Jun 27 16:31:11.375: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.959423443s
Jun 27 16:31:12.405: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.929025841s
Jun 27 16:31:13.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.899448541s
Jun 27 16:31:14.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.861132536s
Jun 27 16:31:15.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.83417622s
Jun 27 16:31:16.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.807770446s
Jun 27 16:31:17.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.780777643s
Jun 27 16:31:18.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 756.08273ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7356 06/27/23 16:31:19.579
Jun 27 16:31:19.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 27 16:31:19.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 27 16:31:19.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 27 16:31:19.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 27 16:31:19.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 27 16:31:20.388: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 27 16:31:20.397: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 27 16:31:20.397: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 27 16:31:20.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 27 16:31:20.779: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Jun 27 16:31:20.779: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 27 16:31:20.779: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 27 16:31:20.804: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 16:31:20.804: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 16:31:20.804: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 06/27/23 16:31:20.804
Jun 27 16:31:20.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 16:31:21.211: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 16:31:21.211: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 16:31:21.211: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 27 16:31:21.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 16:31:21.581: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 16:31:21.581: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 16:31:21.581: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 27 16:31:21.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 16:31:21.957: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 16:31:21.957: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 16:31:21.957: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 27 16:31:21.957: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 16:31:21.973: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 27 16:31:32.018: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 16:31:32.018: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 16:31:32.018: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 16:31:32.081: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 27 16:31:32.081: INFO: ss-0  10.113.180.90  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  }]
Jun 27 16:31:32.081: INFO: ss-1  10.113.180.96  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
Jun 27 16:31:32.081: INFO: ss-2  10.113.180.89  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
Jun 27 16:31:32.081: INFO: 
Jun 27 16:31:32.081: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 27 16:31:33.110: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 27 16:31:33.110: INFO: ss-0  10.113.180.90  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  }]
Jun 27 16:31:33.110: INFO: ss-1  10.113.180.96  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
Jun 27 16:31:33.110: INFO: ss-2  10.113.180.89  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
Jun 27 16:31:33.110: INFO: 
Jun 27 16:31:33.110: INFO: StatefulSet ss has not reached scale 0, at 3
Jun 27 16:31:34.150: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
Jun 27 16:31:34.150: INFO: ss-1  10.113.180.96  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
Jun 27 16:31:34.150: INFO: 
Jun 27 16:31:34.150: INFO: StatefulSet ss has not reached scale 0, at 1
Jun 27 16:31:35.171: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.90444526s
Jun 27 16:31:36.201: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.884441697s
Jun 27 16:31:37.219: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.854509641s
Jun 27 16:31:38.241: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.835735675s
Jun 27 16:31:39.261: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.81463437s
Jun 27 16:31:40.280: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.79456638s
Jun 27 16:31:41.299: INFO: Verifying statefulset ss doesn't scale past 0 for another 774.520739ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7356 06/27/23 16:31:42.299
Jun 27 16:31:42.317: INFO: Scaling statefulset ss to 0
Jun 27 16:31:42.368: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 27 16:31:42.383: INFO: Deleting all statefulset in ns statefulset-7356
Jun 27 16:31:42.398: INFO: Scaling statefulset ss to 0
Jun 27 16:31:42.453: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 16:31:42.468: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 27 16:31:42.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7356" for this suite. 06/27/23 16:31:42.552
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":43,"skipped":898,"failed":0}
------------------------------
â€¢ [SLOW TEST] [53.988 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:30:48.588
    Jun 27 16:30:48.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename statefulset 06/27/23 16:30:48.591
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:30:48.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:30:48.663
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7356 06/27/23 16:30:48.676
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-7356 06/27/23 16:30:48.696
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7356 06/27/23 16:30:48.755
    Jun 27 16:30:48.773: INFO: Found 0 stateful pods, waiting for 1
    Jun 27 16:30:58.795: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 06/27/23 16:30:58.795
    Jun 27 16:30:58.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 16:30:59.209: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 16:30:59.209: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 16:30:59.209: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 27 16:30:59.227: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun 27 16:31:09.251: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 27 16:31:09.251: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 16:31:09.325: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Jun 27 16:31:09.325: INFO: ss-0  10.113.180.90  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:00 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  }]
    Jun 27 16:31:09.325: INFO: 
    Jun 27 16:31:09.325: INFO: StatefulSet ss has not reached scale 3, at 1
    Jun 27 16:31:10.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.980127915s
    Jun 27 16:31:11.375: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.959423443s
    Jun 27 16:31:12.405: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.929025841s
    Jun 27 16:31:13.444: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.899448541s
    Jun 27 16:31:14.471: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.861132536s
    Jun 27 16:31:15.496: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.83417622s
    Jun 27 16:31:16.524: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.807770446s
    Jun 27 16:31:17.549: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.780777643s
    Jun 27 16:31:18.578: INFO: Verifying statefulset ss doesn't scale past 3 for another 756.08273ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7356 06/27/23 16:31:19.579
    Jun 27 16:31:19.606: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 27 16:31:19.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 27 16:31:19.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 27 16:31:19.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 27 16:31:19.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 27 16:31:20.388: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun 27 16:31:20.397: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 27 16:31:20.397: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 27 16:31:20.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 27 16:31:20.779: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Jun 27 16:31:20.779: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 27 16:31:20.779: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 27 16:31:20.804: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 16:31:20.804: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 16:31:20.804: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 06/27/23 16:31:20.804
    Jun 27 16:31:20.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 16:31:21.211: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 16:31:21.211: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 16:31:21.211: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 27 16:31:21.211: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 16:31:21.581: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 16:31:21.581: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 16:31:21.581: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 27 16:31:21.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-7356 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 16:31:21.957: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 16:31:21.957: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 16:31:21.957: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 27 16:31:21.957: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 16:31:21.973: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jun 27 16:31:32.018: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 27 16:31:32.018: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun 27 16:31:32.018: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun 27 16:31:32.081: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Jun 27 16:31:32.081: INFO: ss-0  10.113.180.90  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  }]
    Jun 27 16:31:32.081: INFO: ss-1  10.113.180.96  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
    Jun 27 16:31:32.081: INFO: ss-2  10.113.180.89  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
    Jun 27 16:31:32.081: INFO: 
    Jun 27 16:31:32.081: INFO: StatefulSet ss has not reached scale 0, at 3
    Jun 27 16:31:33.110: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Jun 27 16:31:33.110: INFO: ss-0  10.113.180.90  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:30:48 +0000 UTC  }]
    Jun 27 16:31:33.110: INFO: ss-1  10.113.180.96  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
    Jun 27 16:31:33.110: INFO: ss-2  10.113.180.89  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
    Jun 27 16:31:33.110: INFO: 
    Jun 27 16:31:33.110: INFO: StatefulSet ss has not reached scale 0, at 3
    Jun 27 16:31:34.150: INFO: POD   NODE           PHASE    GRACE  CONDITIONS
    Jun 27 16:31:34.150: INFO: ss-1  10.113.180.96  Running  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 16:31:09 +0000 UTC  }]
    Jun 27 16:31:34.150: INFO: 
    Jun 27 16:31:34.150: INFO: StatefulSet ss has not reached scale 0, at 1
    Jun 27 16:31:35.171: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.90444526s
    Jun 27 16:31:36.201: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.884441697s
    Jun 27 16:31:37.219: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.854509641s
    Jun 27 16:31:38.241: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.835735675s
    Jun 27 16:31:39.261: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.81463437s
    Jun 27 16:31:40.280: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.79456638s
    Jun 27 16:31:41.299: INFO: Verifying statefulset ss doesn't scale past 0 for another 774.520739ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7356 06/27/23 16:31:42.299
    Jun 27 16:31:42.317: INFO: Scaling statefulset ss to 0
    Jun 27 16:31:42.368: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 27 16:31:42.383: INFO: Deleting all statefulset in ns statefulset-7356
    Jun 27 16:31:42.398: INFO: Scaling statefulset ss to 0
    Jun 27 16:31:42.453: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 16:31:42.468: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 27 16:31:42.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7356" for this suite. 06/27/23 16:31:42.552
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:31:42.579
Jun 27 16:31:42.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename dns 06/27/23 16:31:42.581
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:31:42.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:31:42.657
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 06/27/23 16:31:42.674
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3590.svc.cluster.local;sleep 1; done
 06/27/23 16:31:42.699
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3590.svc.cluster.local;sleep 1; done
 06/27/23 16:31:42.699
STEP: creating a pod to probe DNS 06/27/23 16:31:42.699
STEP: submitting the pod to kubernetes 06/27/23 16:31:42.699
Jun 27 16:31:42.777: INFO: Waiting up to 15m0s for pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0" in namespace "dns-3590" to be "running"
Jun 27 16:31:42.796: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.461222ms
Jun 27 16:31:44.815: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037803438s
Jun 27 16:31:46.820: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042933974s
Jun 27 16:31:48.835: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057837725s
Jun 27 16:31:50.825: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048180612s
Jun 27 16:31:52.822: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Running", Reason="", readiness=true. Elapsed: 10.044882371s
Jun 27 16:31:52.822: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0" satisfied condition "running"
STEP: retrieving the pod 06/27/23 16:31:52.822
STEP: looking for the results for each expected name from probers 06/27/23 16:31:52.838
Jun 27 16:31:52.875: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
Jun 27 16:31:52.904: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
Jun 27 16:31:52.928: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
Jun 27 16:31:53.019: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
Jun 27 16:31:53.058: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
Jun 27 16:31:53.095: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
Jun 27 16:31:53.117: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
Jun 27 16:31:53.158: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
Jun 27 16:31:53.158: INFO: Lookups using dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3590.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3590.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local jessie_udp@dns-test-service-2.dns-3590.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3590.svc.cluster.local]

Jun 27 16:31:58.378: INFO: DNS probes using dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0 succeeded

STEP: deleting the pod 06/27/23 16:31:58.378
STEP: deleting the test headless service 06/27/23 16:31:58.439
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 27 16:31:58.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3590" for this suite. 06/27/23 16:31:58.532
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":44,"skipped":901,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.984 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:31:42.579
    Jun 27 16:31:42.579: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename dns 06/27/23 16:31:42.581
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:31:42.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:31:42.657
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 06/27/23 16:31:42.674
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-3590.svc.cluster.local;sleep 1; done
     06/27/23 16:31:42.699
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-3590.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-3590.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-3590.svc.cluster.local;sleep 1; done
     06/27/23 16:31:42.699
    STEP: creating a pod to probe DNS 06/27/23 16:31:42.699
    STEP: submitting the pod to kubernetes 06/27/23 16:31:42.699
    Jun 27 16:31:42.777: INFO: Waiting up to 15m0s for pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0" in namespace "dns-3590" to be "running"
    Jun 27 16:31:42.796: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.461222ms
    Jun 27 16:31:44.815: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037803438s
    Jun 27 16:31:46.820: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042933974s
    Jun 27 16:31:48.835: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.057837725s
    Jun 27 16:31:50.825: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048180612s
    Jun 27 16:31:52.822: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0": Phase="Running", Reason="", readiness=true. Elapsed: 10.044882371s
    Jun 27 16:31:52.822: INFO: Pod "dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0" satisfied condition "running"
    STEP: retrieving the pod 06/27/23 16:31:52.822
    STEP: looking for the results for each expected name from probers 06/27/23 16:31:52.838
    Jun 27 16:31:52.875: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
    Jun 27 16:31:52.904: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
    Jun 27 16:31:52.928: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
    Jun 27 16:31:53.019: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
    Jun 27 16:31:53.058: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
    Jun 27 16:31:53.095: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
    Jun 27 16:31:53.117: INFO: Unable to read jessie_udp@dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
    Jun 27 16:31:53.158: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-3590.svc.cluster.local from pod dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0: the server could not find the requested resource (get pods dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0)
    Jun 27 16:31:53.158: INFO: Lookups using dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local wheezy_udp@dns-test-service-2.dns-3590.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-3590.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-3590.svc.cluster.local jessie_udp@dns-test-service-2.dns-3590.svc.cluster.local jessie_tcp@dns-test-service-2.dns-3590.svc.cluster.local]

    Jun 27 16:31:58.378: INFO: DNS probes using dns-3590/dns-test-5de86a3c-90eb-45e3-9dca-68f5154462a0 succeeded

    STEP: deleting the pod 06/27/23 16:31:58.378
    STEP: deleting the test headless service 06/27/23 16:31:58.439
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 27 16:31:58.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3590" for this suite. 06/27/23 16:31:58.532
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:31:58.563
Jun 27 16:31:58.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename disruption 06/27/23 16:31:58.566
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:31:58.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:31:58.641
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 06/27/23 16:31:58.684
STEP: Waiting for the pdb to be processed 06/27/23 16:31:58.705
STEP: updating the pdb 06/27/23 16:32:00.737
STEP: Waiting for the pdb to be processed 06/27/23 16:32:00.776
STEP: patching the pdb 06/27/23 16:32:00.797
STEP: Waiting for the pdb to be processed 06/27/23 16:32:00.833
STEP: Waiting for the pdb to be deleted 06/27/23 16:32:00.87
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 27 16:32:00.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-9162" for this suite. 06/27/23 16:32:00.923
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":45,"skipped":902,"failed":0}
------------------------------
â€¢ [2.390 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:31:58.563
    Jun 27 16:31:58.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename disruption 06/27/23 16:31:58.566
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:31:58.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:31:58.641
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 06/27/23 16:31:58.684
    STEP: Waiting for the pdb to be processed 06/27/23 16:31:58.705
    STEP: updating the pdb 06/27/23 16:32:00.737
    STEP: Waiting for the pdb to be processed 06/27/23 16:32:00.776
    STEP: patching the pdb 06/27/23 16:32:00.797
    STEP: Waiting for the pdb to be processed 06/27/23 16:32:00.833
    STEP: Waiting for the pdb to be deleted 06/27/23 16:32:00.87
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 27 16:32:00.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-9162" for this suite. 06/27/23 16:32:00.923
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:32:00.955
Jun 27 16:32:00.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename prestop 06/27/23 16:32:00.956
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:01.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:01.034
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-9914 06/27/23 16:32:01.048
STEP: Waiting for pods to come up. 06/27/23 16:32:01.118
Jun 27 16:32:01.119: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9914" to be "running"
Jun 27 16:32:01.138: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 19.594072ms
Jun 27 16:32:03.159: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039835591s
Jun 27 16:32:05.157: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.037948022s
Jun 27 16:32:05.157: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-9914 06/27/23 16:32:05.173
Jun 27 16:32:05.221: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9914" to be "running"
Jun 27 16:32:05.240: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 18.888824ms
Jun 27 16:32:07.258: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036605343s
Jun 27 16:32:09.260: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.039019292s
Jun 27 16:32:09.260: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 06/27/23 16:32:09.261
Jun 27 16:32:14.333: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 06/27/23 16:32:14.333
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Jun 27 16:32:14.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-9914" for this suite. 06/27/23 16:32:14.421
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":46,"skipped":915,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.494 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:32:00.955
    Jun 27 16:32:00.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename prestop 06/27/23 16:32:00.956
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:01.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:01.034
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-9914 06/27/23 16:32:01.048
    STEP: Waiting for pods to come up. 06/27/23 16:32:01.118
    Jun 27 16:32:01.119: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-9914" to be "running"
    Jun 27 16:32:01.138: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 19.594072ms
    Jun 27 16:32:03.159: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039835591s
    Jun 27 16:32:05.157: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.037948022s
    Jun 27 16:32:05.157: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-9914 06/27/23 16:32:05.173
    Jun 27 16:32:05.221: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-9914" to be "running"
    Jun 27 16:32:05.240: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 18.888824ms
    Jun 27 16:32:07.258: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036605343s
    Jun 27 16:32:09.260: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 4.039019292s
    Jun 27 16:32:09.260: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 06/27/23 16:32:09.261
    Jun 27 16:32:14.333: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 06/27/23 16:32:14.333
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Jun 27 16:32:14.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-9914" for this suite. 06/27/23 16:32:14.421
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:32:14.451
Jun 27 16:32:14.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename namespaces 06/27/23 16:32:14.455
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:14.502
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:14.529
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 06/27/23 16:32:14.541
STEP: patching the Namespace 06/27/23 16:32:14.595
STEP: get the Namespace and ensuring it has the label 06/27/23 16:32:14.638
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 27 16:32:14.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9262" for this suite. 06/27/23 16:32:14.727
STEP: Destroying namespace "nspatchtest-9d6dde3c-f7c2-45cf-968c-02f15c1d3a1a-7969" for this suite. 06/27/23 16:32:14.756
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":47,"skipped":922,"failed":0}
------------------------------
â€¢ [0.330 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:32:14.451
    Jun 27 16:32:14.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename namespaces 06/27/23 16:32:14.455
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:14.502
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:14.529
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 06/27/23 16:32:14.541
    STEP: patching the Namespace 06/27/23 16:32:14.595
    STEP: get the Namespace and ensuring it has the label 06/27/23 16:32:14.638
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 16:32:14.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9262" for this suite. 06/27/23 16:32:14.727
    STEP: Destroying namespace "nspatchtest-9d6dde3c-f7c2-45cf-968c-02f15c1d3a1a-7969" for this suite. 06/27/23 16:32:14.756
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:32:14.782
Jun 27 16:32:14.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename gc 06/27/23 16:32:14.783
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:14.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:14.841
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 06/27/23 16:32:14.912
STEP: delete the rc 06/27/23 16:32:19.962
STEP: wait for the rc to be deleted 06/27/23 16:32:19.996
STEP: Gathering metrics 06/27/23 16:32:21.078
W0627 16:32:21.118417      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 27 16:32:21.118: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 27 16:32:21.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4055" for this suite. 06/27/23 16:32:21.18
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":48,"skipped":923,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.423 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:32:14.782
    Jun 27 16:32:14.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename gc 06/27/23 16:32:14.783
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:14.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:14.841
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 06/27/23 16:32:14.912
    STEP: delete the rc 06/27/23 16:32:19.962
    STEP: wait for the rc to be deleted 06/27/23 16:32:19.996
    STEP: Gathering metrics 06/27/23 16:32:21.078
    W0627 16:32:21.118417      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 27 16:32:21.118: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 27 16:32:21.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4055" for this suite. 06/27/23 16:32:21.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:32:21.209
Jun 27 16:32:21.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename daemonsets 06/27/23 16:32:21.211
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:21.266
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:21.282
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 06/27/23 16:32:21.477
STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 16:32:21.503
Jun 27 16:32:21.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:32:21.546: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:32:22.604: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:32:22.604: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:32:23.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:32:23.599: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:32:24.611: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:32:24.611: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:32:25.598: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 27 16:32:25.598: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:32:26.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 27 16:32:26.601: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:32:27.594: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 27 16:32:27.594: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:32:28.614: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 27 16:32:28.614: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:32:29.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 27 16:32:29.597: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:32:30.611: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 27 16:32:30.611: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: listing all DeamonSets 06/27/23 16:32:30.628
STEP: DeleteCollection of the DaemonSets 06/27/23 16:32:30.661
STEP: Verify that ReplicaSets have been deleted 06/27/23 16:32:30.703
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Jun 27 16:32:30.785: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"79822"},"items":null}

Jun 27 16:32:30.806: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"79822"},"items":[{"metadata":{"name":"daemon-set-6dfxd","generateName":"daemon-set-","namespace":"daemonsets-7163","uid":"f6012141-fe81-46de-9c65-ddf0fe9e290e","resourceVersion":"79816","creationTimestamp":"2023-06-27T16:32:21Z","deletionTimestamp":"2023-06-27T16:33:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fc3729b549bd643cfdea0d6eeaa33c5ca5125c57f4f78a0124dfc2abb8a7244a","cni.projectcalico.org/podIP":"172.30.60.107/32","cni.projectcalico.org/podIPs":"172.30.60.107/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.60.107\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.60.107\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e6534d7-5275-4c97-9b8f-5f62b52420cd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e6534d7-5275-4c97-9b8f-5f62b52420cd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-sm2kt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-sm2kt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.113.180.96","securityContext":{"seLinuxOptions":{"level":"s0:c37,c19"}},"imagePullSecrets":[{"name":"default-dockercfg-wvsmg"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.113.180.96"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"}],"hostIP":"10.113.180.96","podIP":"172.30.60.107","podIPs":[{"ip":"172.30.60.107"}],"startTime":"2023-06-27T16:32:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-27T16:32:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://e66ff3b2d6a131d9b661353c4c649f7cc6807e4729e102d485b033b2af162b6a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gwcnb","generateName":"daemon-set-","namespace":"daemonsets-7163","uid":"cbaa68d1-8d71-42f5-909f-9c3c0be96c37","resourceVersion":"79814","creationTimestamp":"2023-06-27T16:32:21Z","deletionTimestamp":"2023-06-27T16:33:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f6bbec8f96be33bead85f109bfbe3473bf7d577b3bdd5dc4284b6ebfd3d766fb","cni.projectcalico.org/podIP":"172.30.106.145/32","cni.projectcalico.org/podIPs":"172.30.106.145/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.106.145\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.106.145\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e6534d7-5275-4c97-9b8f-5f62b52420cd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e6534d7-5275-4c97-9b8f-5f62b52420cd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l9p5l","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l9p5l","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.113.180.89","securityContext":{"seLinuxOptions":{"level":"s0:c37,c19"}},"imagePullSecrets":[{"name":"default-dockercfg-wvsmg"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.113.180.89"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"}],"hostIP":"10.113.180.89","podIP":"172.30.106.145","podIPs":[{"ip":"172.30.106.145"}],"startTime":"2023-06-27T16:32:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-27T16:32:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://936826e9fb06cfd3c69fc3ef3e092e81ddea77796998e0e55b6b519a23bf076f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qdlh2","generateName":"daemon-set-","namespace":"daemonsets-7163","uid":"f131d213-fa0b-4930-9506-ebfd5bcbbb04","resourceVersion":"79815","creationTimestamp":"2023-06-27T16:32:21Z","deletionTimestamp":"2023-06-27T16:33:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d8493a35725ceb11af2f813c591c079f52166e9af4b81ba78db47996e84ee677","cni.projectcalico.org/podIP":"172.30.250.226/32","cni.projectcalico.org/podIPs":"172.30.250.226/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.250.226\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.250.226\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e6534d7-5275-4c97-9b8f-5f62b52420cd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e6534d7-5275-4c97-9b8f-5f62b52420cd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c6qjc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c6qjc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.113.180.90","securityContext":{"seLinuxOptions":{"level":"s0:c37,c19"}},"imagePullSecrets":[{"name":"default-dockercfg-wvsmg"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.113.180.90"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"}],"hostIP":"10.113.180.90","podIP":"172.30.250.226","podIPs":[{"ip":"172.30.250.226"}],"startTime":"2023-06-27T16:32:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-27T16:32:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://2bb1db11c4f109a12994c96bf6d833baca6c3206118e353793f31dbee389b5cd","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 27 16:32:30.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7163" for this suite. 06/27/23 16:32:30.919
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":49,"skipped":990,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.759 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:32:21.209
    Jun 27 16:32:21.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename daemonsets 06/27/23 16:32:21.211
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:21.266
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:21.282
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 06/27/23 16:32:21.477
    STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 16:32:21.503
    Jun 27 16:32:21.546: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:32:21.546: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:32:22.604: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:32:22.604: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:32:23.599: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:32:23.599: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:32:24.611: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:32:24.611: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:32:25.598: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 27 16:32:25.598: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:32:26.601: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 27 16:32:26.601: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:32:27.594: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 27 16:32:27.594: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:32:28.614: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 27 16:32:28.614: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:32:29.597: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 27 16:32:29.597: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:32:30.611: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 27 16:32:30.611: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: listing all DeamonSets 06/27/23 16:32:30.628
    STEP: DeleteCollection of the DaemonSets 06/27/23 16:32:30.661
    STEP: Verify that ReplicaSets have been deleted 06/27/23 16:32:30.703
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Jun 27 16:32:30.785: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"79822"},"items":null}

    Jun 27 16:32:30.806: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"79822"},"items":[{"metadata":{"name":"daemon-set-6dfxd","generateName":"daemon-set-","namespace":"daemonsets-7163","uid":"f6012141-fe81-46de-9c65-ddf0fe9e290e","resourceVersion":"79816","creationTimestamp":"2023-06-27T16:32:21Z","deletionTimestamp":"2023-06-27T16:33:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"fc3729b549bd643cfdea0d6eeaa33c5ca5125c57f4f78a0124dfc2abb8a7244a","cni.projectcalico.org/podIP":"172.30.60.107/32","cni.projectcalico.org/podIPs":"172.30.60.107/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.60.107\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.60.107\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e6534d7-5275-4c97-9b8f-5f62b52420cd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e6534d7-5275-4c97-9b8f-5f62b52420cd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:29Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.107\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-sm2kt","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-sm2kt","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.113.180.96","securityContext":{"seLinuxOptions":{"level":"s0:c37,c19"}},"imagePullSecrets":[{"name":"default-dockercfg-wvsmg"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.113.180.96"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"}],"hostIP":"10.113.180.96","podIP":"172.30.60.107","podIPs":[{"ip":"172.30.60.107"}],"startTime":"2023-06-27T16:32:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-27T16:32:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://e66ff3b2d6a131d9b661353c4c649f7cc6807e4729e102d485b033b2af162b6a","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gwcnb","generateName":"daemon-set-","namespace":"daemonsets-7163","uid":"cbaa68d1-8d71-42f5-909f-9c3c0be96c37","resourceVersion":"79814","creationTimestamp":"2023-06-27T16:32:21Z","deletionTimestamp":"2023-06-27T16:33:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"f6bbec8f96be33bead85f109bfbe3473bf7d577b3bdd5dc4284b6ebfd3d766fb","cni.projectcalico.org/podIP":"172.30.106.145/32","cni.projectcalico.org/podIPs":"172.30.106.145/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.106.145\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.106.145\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e6534d7-5275-4c97-9b8f-5f62b52420cd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e6534d7-5275-4c97-9b8f-5f62b52420cd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:23Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:25Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.145\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l9p5l","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l9p5l","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.113.180.89","securityContext":{"seLinuxOptions":{"level":"s0:c37,c19"}},"imagePullSecrets":[{"name":"default-dockercfg-wvsmg"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.113.180.89"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:25Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:25Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"}],"hostIP":"10.113.180.89","podIP":"172.30.106.145","podIPs":[{"ip":"172.30.106.145"}],"startTime":"2023-06-27T16:32:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-27T16:32:24Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://936826e9fb06cfd3c69fc3ef3e092e81ddea77796998e0e55b6b519a23bf076f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qdlh2","generateName":"daemon-set-","namespace":"daemonsets-7163","uid":"f131d213-fa0b-4930-9506-ebfd5bcbbb04","resourceVersion":"79815","creationTimestamp":"2023-06-27T16:32:21Z","deletionTimestamp":"2023-06-27T16:33:00Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d8493a35725ceb11af2f813c591c079f52166e9af4b81ba78db47996e84ee677","cni.projectcalico.org/podIP":"172.30.250.226/32","cni.projectcalico.org/podIPs":"172.30.250.226/32","k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.250.226\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.250.226\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]","openshift.io/scc":"anyuid"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"9e6534d7-5275-4c97-9b8f-5f62b52420cd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:21Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9e6534d7-5275-4c97-9b8f-5f62b52420cd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"multus","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-06-27T16:32:30Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.226\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-c6qjc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}},{"configMap":{"name":"openshift-service-ca.crt","items":[{"key":"service-ca.crt","path":"service-ca.crt"}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-c6qjc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{"capabilities":{"drop":["MKNOD"]}}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"10.113.180.90","securityContext":{"seLinuxOptions":{"level":"s0:c37,c19"}},"imagePullSecrets":[{"name":"default-dockercfg-wvsmg"}],"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["10.113.180.90"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:30Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:30Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-06-27T16:32:21Z"}],"hostIP":"10.113.180.90","podIP":"172.30.250.226","podIPs":[{"ip":"172.30.250.226"}],"startTime":"2023-06-27T16:32:21Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-06-27T16:32:29Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"cri-o://2bb1db11c4f109a12994c96bf6d833baca6c3206118e353793f31dbee389b5cd","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 16:32:30.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7163" for this suite. 06/27/23 16:32:30.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:32:30.972
Jun 27 16:32:30.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename proxy 06/27/23 16:32:30.976
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:31.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:31.036
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Jun 27 16:32:31.049: INFO: Creating pod...
Jun 27 16:32:31.123: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9518" to be "running"
Jun 27 16:32:31.142: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 19.036682ms
Jun 27 16:32:33.159: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036010733s
Jun 27 16:32:35.160: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036839131s
Jun 27 16:32:37.162: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038806137s
Jun 27 16:32:39.162: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 8.039206284s
Jun 27 16:32:39.163: INFO: Pod "agnhost" satisfied condition "running"
Jun 27 16:32:39.163: INFO: Creating service...
Jun 27 16:32:39.199: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/DELETE
Jun 27 16:32:39.251: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 27 16:32:39.251: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/GET
Jun 27 16:32:39.283: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 27 16:32:39.283: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/HEAD
Jun 27 16:32:39.311: INFO: http.Client request:HEAD | StatusCode:200
Jun 27 16:32:39.311: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/OPTIONS
Jun 27 16:32:39.341: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 27 16:32:39.341: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/PATCH
Jun 27 16:32:39.383: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 27 16:32:39.383: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/POST
Jun 27 16:32:39.408: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 27 16:32:39.408: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/PUT
Jun 27 16:32:39.436: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 27 16:32:39.436: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/DELETE
Jun 27 16:32:39.470: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 27 16:32:39.470: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/GET
Jun 27 16:32:39.503: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Jun 27 16:32:39.503: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/HEAD
Jun 27 16:32:39.533: INFO: http.Client request:HEAD | StatusCode:200
Jun 27 16:32:39.533: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/OPTIONS
Jun 27 16:32:39.578: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 27 16:32:39.578: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/PATCH
Jun 27 16:32:39.609: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 27 16:32:39.609: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/POST
Jun 27 16:32:39.639: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 27 16:32:39.639: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/PUT
Jun 27 16:32:39.679: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jun 27 16:32:39.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-9518" for this suite. 06/27/23 16:32:39.725
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":50,"skipped":1068,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.778 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:32:30.972
    Jun 27 16:32:30.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename proxy 06/27/23 16:32:30.976
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:31.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:31.036
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Jun 27 16:32:31.049: INFO: Creating pod...
    Jun 27 16:32:31.123: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-9518" to be "running"
    Jun 27 16:32:31.142: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 19.036682ms
    Jun 27 16:32:33.159: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036010733s
    Jun 27 16:32:35.160: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036839131s
    Jun 27 16:32:37.162: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 6.038806137s
    Jun 27 16:32:39.162: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 8.039206284s
    Jun 27 16:32:39.163: INFO: Pod "agnhost" satisfied condition "running"
    Jun 27 16:32:39.163: INFO: Creating service...
    Jun 27 16:32:39.199: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/DELETE
    Jun 27 16:32:39.251: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 27 16:32:39.251: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/GET
    Jun 27 16:32:39.283: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun 27 16:32:39.283: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/HEAD
    Jun 27 16:32:39.311: INFO: http.Client request:HEAD | StatusCode:200
    Jun 27 16:32:39.311: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/OPTIONS
    Jun 27 16:32:39.341: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 27 16:32:39.341: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/PATCH
    Jun 27 16:32:39.383: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 27 16:32:39.383: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/POST
    Jun 27 16:32:39.408: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 27 16:32:39.408: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/pods/agnhost/proxy/some/path/with/PUT
    Jun 27 16:32:39.436: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 27 16:32:39.436: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/DELETE
    Jun 27 16:32:39.470: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 27 16:32:39.470: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/GET
    Jun 27 16:32:39.503: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Jun 27 16:32:39.503: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/HEAD
    Jun 27 16:32:39.533: INFO: http.Client request:HEAD | StatusCode:200
    Jun 27 16:32:39.533: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/OPTIONS
    Jun 27 16:32:39.578: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 27 16:32:39.578: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/PATCH
    Jun 27 16:32:39.609: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 27 16:32:39.609: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/POST
    Jun 27 16:32:39.639: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 27 16:32:39.639: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-9518/services/test-service/proxy/some/path/with/PUT
    Jun 27 16:32:39.679: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jun 27 16:32:39.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-9518" for this suite. 06/27/23 16:32:39.725
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:32:39.751
Jun 27 16:32:39.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 16:32:39.752
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:39.801
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:39.82
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 16:32:39.887
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:32:40.304
STEP: Deploying the webhook pod 06/27/23 16:32:40.35
STEP: Wait for the deployment to be ready 06/27/23 16:32:40.399
Jun 27 16:32:40.436: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 27 16:32:42.486: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 16:32:44.504
STEP: Verifying the service has paired with the endpoint 06/27/23 16:32:44.538
Jun 27 16:32:45.539: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 06/27/23 16:32:45.555
STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/27/23 16:32:45.633
STEP: Creating a configMap that should not be mutated 06/27/23 16:32:45.658
STEP: Patching a mutating webhook configuration's rules to include the create operation 06/27/23 16:32:45.703
STEP: Creating a configMap that should be mutated 06/27/23 16:32:45.726
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:32:45.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8957" for this suite. 06/27/23 16:32:45.863
STEP: Destroying namespace "webhook-8957-markers" for this suite. 06/27/23 16:32:45.892
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":51,"skipped":1082,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.318 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:32:39.751
    Jun 27 16:32:39.751: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 16:32:39.752
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:39.801
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:39.82
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 16:32:39.887
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:32:40.304
    STEP: Deploying the webhook pod 06/27/23 16:32:40.35
    STEP: Wait for the deployment to be ready 06/27/23 16:32:40.399
    Jun 27 16:32:40.436: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 27 16:32:42.486: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 40, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 40, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 40, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 16:32:44.504
    STEP: Verifying the service has paired with the endpoint 06/27/23 16:32:44.538
    Jun 27 16:32:45.539: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 06/27/23 16:32:45.555
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 06/27/23 16:32:45.633
    STEP: Creating a configMap that should not be mutated 06/27/23 16:32:45.658
    STEP: Patching a mutating webhook configuration's rules to include the create operation 06/27/23 16:32:45.703
    STEP: Creating a configMap that should be mutated 06/27/23 16:32:45.726
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:32:45.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8957" for this suite. 06/27/23 16:32:45.863
    STEP: Destroying namespace "webhook-8957-markers" for this suite. 06/27/23 16:32:45.892
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:32:46.076
Jun 27 16:32:46.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename deployment 06/27/23 16:32:46.081
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:46.225
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:46.242
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 06/27/23 16:32:46.316
Jun 27 16:32:46.316: INFO: Creating simple deployment test-deployment-gfrqd
Jun 27 16:32:46.421: INFO: deployment "test-deployment-gfrqd" doesn't have the required revision set
Jun 27 16:32:48.474: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-gfrqd-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Getting /status 06/27/23 16:32:50.525
Jun 27 16:32:50.547: INFO: Deployment test-deployment-gfrqd has Conditions: [{Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 06/27/23 16:32:50.547
Jun 27 16:32:50.591: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gfrqd-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 06/27/23 16:32:50.591
Jun 27 16:32:50.602: INFO: Observed &Deployment event: ADDED
Jun 27 16:32:50.603: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gfrqd-777898ffcc"}
Jun 27 16:32:50.603: INFO: Observed &Deployment event: MODIFIED
Jun 27 16:32:50.603: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gfrqd-777898ffcc"}
Jun 27 16:32:50.603: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 27 16:32:50.603: INFO: Observed &Deployment event: MODIFIED
Jun 27 16:32:50.604: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 27 16:32:50.604: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gfrqd-777898ffcc" is progressing.}
Jun 27 16:32:50.604: INFO: Observed &Deployment event: MODIFIED
Jun 27 16:32:50.604: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 27 16:32:50.604: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}
Jun 27 16:32:50.605: INFO: Observed &Deployment event: MODIFIED
Jun 27 16:32:50.605: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 27 16:32:50.605: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}
Jun 27 16:32:50.605: INFO: Found Deployment test-deployment-gfrqd in namespace deployment-8198 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 27 16:32:50.605: INFO: Deployment test-deployment-gfrqd has an updated status
STEP: patching the Statefulset Status 06/27/23 16:32:50.605
Jun 27 16:32:50.606: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 27 16:32:50.634: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 06/27/23 16:32:50.634
Jun 27 16:32:50.645: INFO: Observed &Deployment event: ADDED
Jun 27 16:32:50.645: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gfrqd-777898ffcc"}
Jun 27 16:32:50.646: INFO: Observed &Deployment event: MODIFIED
Jun 27 16:32:50.646: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gfrqd-777898ffcc"}
Jun 27 16:32:50.646: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 27 16:32:50.646: INFO: Observed &Deployment event: MODIFIED
Jun 27 16:32:50.647: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Jun 27 16:32:50.647: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gfrqd-777898ffcc" is progressing.}
Jun 27 16:32:50.647: INFO: Observed &Deployment event: MODIFIED
Jun 27 16:32:50.648: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 27 16:32:50.648: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}
Jun 27 16:32:50.648: INFO: Observed &Deployment event: MODIFIED
Jun 27 16:32:50.648: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Jun 27 16:32:50.649: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}
Jun 27 16:32:50.649: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 27 16:32:50.649: INFO: Observed &Deployment event: MODIFIED
Jun 27 16:32:50.649: INFO: Found deployment test-deployment-gfrqd in namespace deployment-8198 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Jun 27 16:32:50.649: INFO: Deployment test-deployment-gfrqd has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 27 16:32:50.674: INFO: Deployment "test-deployment-gfrqd":
&Deployment{ObjectMeta:{test-deployment-gfrqd  deployment-8198  98d01dd2-6f0d-43bc-9d91-fd509edd62a1 80438 1 2023-06-27 16:32:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-27 16:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-27 16:32:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-27 16:32:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039a9418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-27 16:32:50 +0000 UTC,LastTransitionTime:2023-06-27 16:32:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.,LastUpdateTime:2023-06-27 16:32:50 +0000 UTC,LastTransitionTime:2023-06-27 16:32:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 27 16:32:50.699: INFO: New ReplicaSet "test-deployment-gfrqd-777898ffcc" of Deployment "test-deployment-gfrqd":
&ReplicaSet{ObjectMeta:{test-deployment-gfrqd-777898ffcc  deployment-8198  f3d6e3eb-043f-427d-b52b-c5734181e3b9 80417 1 2023-06-27 16:32:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gfrqd 98d01dd2-6f0d-43bc-9d91-fd509edd62a1 0xc00c9f00f0 0xc00c9f00f1}] [] [{kube-controller-manager Update apps/v1 2023-06-27 16:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98d01dd2-6f0d-43bc-9d91-fd509edd62a1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:32:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c9f0198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 27 16:32:50.720: INFO: Pod "test-deployment-gfrqd-777898ffcc-llgfw" is available:
&Pod{ObjectMeta:{test-deployment-gfrqd-777898ffcc-llgfw test-deployment-gfrqd-777898ffcc- deployment-8198  9571e395-95b5-4ede-9f4b-f0cffa723fda 80416 0 2023-06-27 16:32:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:da4fb45649f9c6e4531a96d1626edbaec05965ef24b141ee4140731975b82b1f cni.projectcalico.org/podIP:172.30.250.253/32 cni.projectcalico.org/podIPs:172.30.250.253/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.253"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.253"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-gfrqd-777898ffcc f3d6e3eb-043f-427d-b52b-c5734181e3b9 0xc009ee3c67 0xc009ee3c68}] [] [{kube-controller-manager Update v1 2023-06-27 16:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3d6e3eb-043f-427d-b52b-c5734181e3b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 16:32:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 16:32:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 16:32:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2v75r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2v75r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-z8fl4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:32:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:32:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.253,StartTime:2023-06-27 16:32:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 16:32:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://16d2e9e6a61731f8eb0b1d55e8bf18dae0f88695f158376b787f7b74f63e069c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 27 16:32:50.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8198" for this suite. 06/27/23 16:32:50.764
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":52,"skipped":1101,"failed":0}
------------------------------
â€¢ [4.729 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:32:46.076
    Jun 27 16:32:46.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename deployment 06/27/23 16:32:46.081
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:46.225
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:46.242
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 06/27/23 16:32:46.316
    Jun 27 16:32:46.316: INFO: Creating simple deployment test-deployment-gfrqd
    Jun 27 16:32:46.421: INFO: deployment "test-deployment-gfrqd" doesn't have the required revision set
    Jun 27 16:32:48.474: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-gfrqd-777898ffcc\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Getting /status 06/27/23 16:32:50.525
    Jun 27 16:32:50.547: INFO: Deployment test-deployment-gfrqd has Conditions: [{Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 06/27/23 16:32:50.547
    Jun 27 16:32:50.591: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 48, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 32, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 32, 46, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-gfrqd-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 06/27/23 16:32:50.591
    Jun 27 16:32:50.602: INFO: Observed &Deployment event: ADDED
    Jun 27 16:32:50.603: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gfrqd-777898ffcc"}
    Jun 27 16:32:50.603: INFO: Observed &Deployment event: MODIFIED
    Jun 27 16:32:50.603: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gfrqd-777898ffcc"}
    Jun 27 16:32:50.603: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 27 16:32:50.603: INFO: Observed &Deployment event: MODIFIED
    Jun 27 16:32:50.604: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 27 16:32:50.604: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gfrqd-777898ffcc" is progressing.}
    Jun 27 16:32:50.604: INFO: Observed &Deployment event: MODIFIED
    Jun 27 16:32:50.604: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 27 16:32:50.604: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}
    Jun 27 16:32:50.605: INFO: Observed &Deployment event: MODIFIED
    Jun 27 16:32:50.605: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 27 16:32:50.605: INFO: Observed Deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}
    Jun 27 16:32:50.605: INFO: Found Deployment test-deployment-gfrqd in namespace deployment-8198 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 27 16:32:50.605: INFO: Deployment test-deployment-gfrqd has an updated status
    STEP: patching the Statefulset Status 06/27/23 16:32:50.605
    Jun 27 16:32:50.606: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 27 16:32:50.634: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 06/27/23 16:32:50.634
    Jun 27 16:32:50.645: INFO: Observed &Deployment event: ADDED
    Jun 27 16:32:50.645: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gfrqd-777898ffcc"}
    Jun 27 16:32:50.646: INFO: Observed &Deployment event: MODIFIED
    Jun 27 16:32:50.646: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-gfrqd-777898ffcc"}
    Jun 27 16:32:50.646: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 27 16:32:50.646: INFO: Observed &Deployment event: MODIFIED
    Jun 27 16:32:50.647: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Jun 27 16:32:50.647: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:46 +0000 UTC 2023-06-27 16:32:46 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-gfrqd-777898ffcc" is progressing.}
    Jun 27 16:32:50.647: INFO: Observed &Deployment event: MODIFIED
    Jun 27 16:32:50.648: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 27 16:32:50.648: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}
    Jun 27 16:32:50.648: INFO: Observed &Deployment event: MODIFIED
    Jun 27 16:32:50.648: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:48 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Jun 27 16:32:50.649: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-06-27 16:32:48 +0000 UTC 2023-06-27 16:32:46 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.}
    Jun 27 16:32:50.649: INFO: Observed deployment test-deployment-gfrqd in namespace deployment-8198 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 27 16:32:50.649: INFO: Observed &Deployment event: MODIFIED
    Jun 27 16:32:50.649: INFO: Found deployment test-deployment-gfrqd in namespace deployment-8198 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Jun 27 16:32:50.649: INFO: Deployment test-deployment-gfrqd has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 27 16:32:50.674: INFO: Deployment "test-deployment-gfrqd":
    &Deployment{ObjectMeta:{test-deployment-gfrqd  deployment-8198  98d01dd2-6f0d-43bc-9d91-fd509edd62a1 80438 1 2023-06-27 16:32:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-27 16:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-06-27 16:32:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-06-27 16:32:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0039a9418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-27 16:32:50 +0000 UTC,LastTransitionTime:2023-06-27 16:32:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-deployment-gfrqd-777898ffcc" has successfully progressed.,LastUpdateTime:2023-06-27 16:32:50 +0000 UTC,LastTransitionTime:2023-06-27 16:32:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 27 16:32:50.699: INFO: New ReplicaSet "test-deployment-gfrqd-777898ffcc" of Deployment "test-deployment-gfrqd":
    &ReplicaSet{ObjectMeta:{test-deployment-gfrqd-777898ffcc  deployment-8198  f3d6e3eb-043f-427d-b52b-c5734181e3b9 80417 1 2023-06-27 16:32:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-gfrqd 98d01dd2-6f0d-43bc-9d91-fd509edd62a1 0xc00c9f00f0 0xc00c9f00f1}] [] [{kube-controller-manager Update apps/v1 2023-06-27 16:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"98d01dd2-6f0d-43bc-9d91-fd509edd62a1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:32:48 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00c9f0198 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 16:32:50.720: INFO: Pod "test-deployment-gfrqd-777898ffcc-llgfw" is available:
    &Pod{ObjectMeta:{test-deployment-gfrqd-777898ffcc-llgfw test-deployment-gfrqd-777898ffcc- deployment-8198  9571e395-95b5-4ede-9f4b-f0cffa723fda 80416 0 2023-06-27 16:32:46 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[cni.projectcalico.org/containerID:da4fb45649f9c6e4531a96d1626edbaec05965ef24b141ee4140731975b82b1f cni.projectcalico.org/podIP:172.30.250.253/32 cni.projectcalico.org/podIPs:172.30.250.253/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.253"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.253"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-gfrqd-777898ffcc f3d6e3eb-043f-427d-b52b-c5734181e3b9 0xc009ee3c67 0xc009ee3c68}] [] [{kube-controller-manager Update v1 2023-06-27 16:32:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f3d6e3eb-043f-427d-b52b-c5734181e3b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 16:32:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 16:32:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 16:32:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.253\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2v75r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2v75r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c38,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-z8fl4,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:32:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:32:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:32:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.253,StartTime:2023-06-27 16:32:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 16:32:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://16d2e9e6a61731f8eb0b1d55e8bf18dae0f88695f158376b787f7b74f63e069c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.253,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 27 16:32:50.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8198" for this suite. 06/27/23 16:32:50.764
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:32:50.806
Jun 27 16:32:50.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 16:32:50.809
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:50.899
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:50.919
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
STEP: creating service in namespace services-7080 06/27/23 16:32:50.937
STEP: creating service affinity-nodeport in namespace services-7080 06/27/23 16:32:50.938
STEP: creating replication controller affinity-nodeport in namespace services-7080 06/27/23 16:32:50.989
I0627 16:32:51.038101      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7080, replica count: 3
I0627 16:32:54.089657      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 16:32:54.168: INFO: Creating new exec pod
Jun 27 16:32:54.275: INFO: Waiting up to 5m0s for pod "execpod-affinityqx658" in namespace "services-7080" to be "running"
Jun 27 16:32:54.292: INFO: Pod "execpod-affinityqx658": Phase="Pending", Reason="", readiness=false. Elapsed: 16.767748ms
Jun 27 16:32:56.330: INFO: Pod "execpod-affinityqx658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054462745s
Jun 27 16:32:58.314: INFO: Pod "execpod-affinityqx658": Phase="Running", Reason="", readiness=true. Elapsed: 4.039195784s
Jun 27 16:32:58.315: INFO: Pod "execpod-affinityqx658" satisfied condition "running"
Jun 27 16:32:59.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Jun 27 16:32:59.872: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Jun 27 16:32:59.872: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 16:32:59.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.218.136 80'
Jun 27 16:33:00.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.218.136 80\nConnection to 172.21.218.136 80 port [tcp/http] succeeded!\n"
Jun 27 16:33:00.289: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 16:33:00.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31161'
Jun 27 16:33:00.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31161\nConnection to 10.113.180.89 31161 port [tcp/*] succeeded!\n"
Jun 27 16:33:00.645: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 16:33:00.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.96 31161'
Jun 27 16:33:01.077: INFO: stderr: "+ nc -v -t -w 2 10.113.180.96 31161\n+ echo hostName\nConnection to 10.113.180.96 31161 port [tcp/*] succeeded!\n"
Jun 27 16:33:01.077: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 16:33:01.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.180.89:31161/ ; done'
Jun 27 16:33:01.654: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n"
Jun 27 16:33:01.654: INFO: stdout: "\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2"
Jun 27 16:33:01.654: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.654: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.654: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.654: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
Jun 27 16:33:01.655: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-7080, will wait for the garbage collector to delete the pods 06/27/23 16:33:01.724
Jun 27 16:33:01.823: INFO: Deleting ReplicationController affinity-nodeport took: 30.107653ms
Jun 27 16:33:01.924: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.325473ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 16:33:04.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7080" for this suite. 06/27/23 16:33:04.826
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":53,"skipped":1101,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.076 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:32:50.806
    Jun 27 16:32:50.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 16:32:50.809
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:32:50.899
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:32:50.919
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2194
    STEP: creating service in namespace services-7080 06/27/23 16:32:50.937
    STEP: creating service affinity-nodeport in namespace services-7080 06/27/23 16:32:50.938
    STEP: creating replication controller affinity-nodeport in namespace services-7080 06/27/23 16:32:50.989
    I0627 16:32:51.038101      23 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-7080, replica count: 3
    I0627 16:32:54.089657      23 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 27 16:32:54.168: INFO: Creating new exec pod
    Jun 27 16:32:54.275: INFO: Waiting up to 5m0s for pod "execpod-affinityqx658" in namespace "services-7080" to be "running"
    Jun 27 16:32:54.292: INFO: Pod "execpod-affinityqx658": Phase="Pending", Reason="", readiness=false. Elapsed: 16.767748ms
    Jun 27 16:32:56.330: INFO: Pod "execpod-affinityqx658": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054462745s
    Jun 27 16:32:58.314: INFO: Pod "execpod-affinityqx658": Phase="Running", Reason="", readiness=true. Elapsed: 4.039195784s
    Jun 27 16:32:58.315: INFO: Pod "execpod-affinityqx658" satisfied condition "running"
    Jun 27 16:32:59.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Jun 27 16:32:59.872: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Jun 27 16:32:59.872: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 16:32:59.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.218.136 80'
    Jun 27 16:33:00.289: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.218.136 80\nConnection to 172.21.218.136 80 port [tcp/http] succeeded!\n"
    Jun 27 16:33:00.289: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 16:33:00.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31161'
    Jun 27 16:33:00.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31161\nConnection to 10.113.180.89 31161 port [tcp/*] succeeded!\n"
    Jun 27 16:33:00.645: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 16:33:00.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.96 31161'
    Jun 27 16:33:01.077: INFO: stderr: "+ nc -v -t -w 2 10.113.180.96 31161\n+ echo hostName\nConnection to 10.113.180.96 31161 port [tcp/*] succeeded!\n"
    Jun 27 16:33:01.077: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 16:33:01.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-7080 exec execpod-affinityqx658 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.180.89:31161/ ; done'
    Jun 27 16:33:01.654: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31161/\n"
    Jun 27 16:33:01.654: INFO: stdout: "\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2\naffinity-nodeport-pvtv2"
    Jun 27 16:33:01.654: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.654: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.654: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.654: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Received response from host: affinity-nodeport-pvtv2
    Jun 27 16:33:01.655: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-7080, will wait for the garbage collector to delete the pods 06/27/23 16:33:01.724
    Jun 27 16:33:01.823: INFO: Deleting ReplicationController affinity-nodeport took: 30.107653ms
    Jun 27 16:33:01.924: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.325473ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 16:33:04.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7080" for this suite. 06/27/23 16:33:04.826
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:33:04.883
Jun 27 16:33:04.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pod-network-test 06/27/23 16:33:04.885
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:33:04.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:33:04.975
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3381 06/27/23 16:33:04.995
STEP: creating a selector 06/27/23 16:33:04.996
STEP: Creating the service pods in kubernetes 06/27/23 16:33:04.996
Jun 27 16:33:04.996: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 27 16:33:05.191: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3381" to be "running and ready"
Jun 27 16:33:05.213: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.427915ms
Jun 27 16:33:05.213: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:33:07.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.041912389s
Jun 27 16:33:07.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:09.235: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.044288118s
Jun 27 16:33:09.235: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:11.231: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.040192191s
Jun 27 16:33:11.231: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:13.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.042313256s
Jun 27 16:33:13.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:15.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.041754217s
Jun 27 16:33:15.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:17.235: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.044086803s
Jun 27 16:33:17.235: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:19.253: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.062122098s
Jun 27 16:33:19.253: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:21.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.041773308s
Jun 27 16:33:21.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:23.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.042486962s
Jun 27 16:33:23.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:25.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.041647054s
Jun 27 16:33:25.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 16:33:27.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.046870937s
Jun 27 16:33:27.238: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 27 16:33:27.238: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 27 16:33:27.257: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3381" to be "running and ready"
Jun 27 16:33:27.275: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 17.702472ms
Jun 27 16:33:27.276: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 27 16:33:27.276: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 27 16:33:27.293: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3381" to be "running and ready"
Jun 27 16:33:27.312: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 18.756221ms
Jun 27 16:33:27.312: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 27 16:33:27.312: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/27/23 16:33:27.331
Jun 27 16:33:27.375: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3381" to be "running"
Jun 27 16:33:27.393: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.499026ms
Jun 27 16:33:29.413: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037492037s
Jun 27 16:33:31.414: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.038764186s
Jun 27 16:33:31.414: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 27 16:33:31.431: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 27 16:33:31.431: INFO: Breadth first check of 172.30.106.170 on host 10.113.180.89...
Jun 27 16:33:31.448: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.60.114:9080/dial?request=hostname&protocol=http&host=172.30.106.170&port=8083&tries=1'] Namespace:pod-network-test-3381 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 16:33:31.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 16:33:31.449: INFO: ExecWithOptions: Clientset creation
Jun 27 16:33:31.449: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3381/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.60.114%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.106.170%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 27 16:33:31.671: INFO: Waiting for responses: map[]
Jun 27 16:33:31.671: INFO: reached 172.30.106.170 after 0/1 tries
Jun 27 16:33:31.671: INFO: Breadth first check of 172.30.250.204 on host 10.113.180.90...
Jun 27 16:33:31.691: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.60.114:9080/dial?request=hostname&protocol=http&host=172.30.250.204&port=8083&tries=1'] Namespace:pod-network-test-3381 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 16:33:31.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 16:33:31.693: INFO: ExecWithOptions: Clientset creation
Jun 27 16:33:31.693: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3381/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.60.114%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.250.204%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 27 16:33:32.006: INFO: Waiting for responses: map[]
Jun 27 16:33:32.006: INFO: reached 172.30.250.204 after 0/1 tries
Jun 27 16:33:32.006: INFO: Breadth first check of 172.30.60.122 on host 10.113.180.96...
Jun 27 16:33:32.023: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.60.114:9080/dial?request=hostname&protocol=http&host=172.30.60.122&port=8083&tries=1'] Namespace:pod-network-test-3381 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 16:33:32.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 16:33:32.023: INFO: ExecWithOptions: Clientset creation
Jun 27 16:33:32.023: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3381/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.60.114%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.60.122%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 27 16:33:32.283: INFO: Waiting for responses: map[]
Jun 27 16:33:32.283: INFO: reached 172.30.60.122 after 0/1 tries
Jun 27 16:33:32.283: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 27 16:33:32.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3381" for this suite. 06/27/23 16:33:32.315
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":54,"skipped":1113,"failed":0}
------------------------------
â€¢ [SLOW TEST] [27.460 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:33:04.883
    Jun 27 16:33:04.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pod-network-test 06/27/23 16:33:04.885
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:33:04.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:33:04.975
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3381 06/27/23 16:33:04.995
    STEP: creating a selector 06/27/23 16:33:04.996
    STEP: Creating the service pods in kubernetes 06/27/23 16:33:04.996
    Jun 27 16:33:04.996: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 27 16:33:05.191: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3381" to be "running and ready"
    Jun 27 16:33:05.213: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 22.427915ms
    Jun 27 16:33:05.213: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:33:07.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.041912389s
    Jun 27 16:33:07.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:09.235: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.044288118s
    Jun 27 16:33:09.235: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:11.231: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.040192191s
    Jun 27 16:33:11.231: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:13.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.042313256s
    Jun 27 16:33:13.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:15.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.041754217s
    Jun 27 16:33:15.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:17.235: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.044086803s
    Jun 27 16:33:17.235: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:19.253: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.062122098s
    Jun 27 16:33:19.253: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:21.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.041773308s
    Jun 27 16:33:21.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:23.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.042486962s
    Jun 27 16:33:23.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:25.233: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.041647054s
    Jun 27 16:33:25.233: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 16:33:27.238: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.046870937s
    Jun 27 16:33:27.238: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 27 16:33:27.238: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 27 16:33:27.257: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3381" to be "running and ready"
    Jun 27 16:33:27.275: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 17.702472ms
    Jun 27 16:33:27.276: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 27 16:33:27.276: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 27 16:33:27.293: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-3381" to be "running and ready"
    Jun 27 16:33:27.312: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 18.756221ms
    Jun 27 16:33:27.312: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 27 16:33:27.312: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/27/23 16:33:27.331
    Jun 27 16:33:27.375: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3381" to be "running"
    Jun 27 16:33:27.393: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 17.499026ms
    Jun 27 16:33:29.413: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037492037s
    Jun 27 16:33:31.414: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.038764186s
    Jun 27 16:33:31.414: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 27 16:33:31.431: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 27 16:33:31.431: INFO: Breadth first check of 172.30.106.170 on host 10.113.180.89...
    Jun 27 16:33:31.448: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.60.114:9080/dial?request=hostname&protocol=http&host=172.30.106.170&port=8083&tries=1'] Namespace:pod-network-test-3381 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 16:33:31.448: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 16:33:31.449: INFO: ExecWithOptions: Clientset creation
    Jun 27 16:33:31.449: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3381/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.60.114%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.106.170%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 27 16:33:31.671: INFO: Waiting for responses: map[]
    Jun 27 16:33:31.671: INFO: reached 172.30.106.170 after 0/1 tries
    Jun 27 16:33:31.671: INFO: Breadth first check of 172.30.250.204 on host 10.113.180.90...
    Jun 27 16:33:31.691: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.60.114:9080/dial?request=hostname&protocol=http&host=172.30.250.204&port=8083&tries=1'] Namespace:pod-network-test-3381 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 16:33:31.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 16:33:31.693: INFO: ExecWithOptions: Clientset creation
    Jun 27 16:33:31.693: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3381/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.60.114%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.250.204%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 27 16:33:32.006: INFO: Waiting for responses: map[]
    Jun 27 16:33:32.006: INFO: reached 172.30.250.204 after 0/1 tries
    Jun 27 16:33:32.006: INFO: Breadth first check of 172.30.60.122 on host 10.113.180.96...
    Jun 27 16:33:32.023: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.60.114:9080/dial?request=hostname&protocol=http&host=172.30.60.122&port=8083&tries=1'] Namespace:pod-network-test-3381 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 16:33:32.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 16:33:32.023: INFO: ExecWithOptions: Clientset creation
    Jun 27 16:33:32.023: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-3381/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.60.114%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.30.60.122%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 27 16:33:32.283: INFO: Waiting for responses: map[]
    Jun 27 16:33:32.283: INFO: reached 172.30.60.122 after 0/1 tries
    Jun 27 16:33:32.283: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 27 16:33:32.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3381" for this suite. 06/27/23 16:33:32.315
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:33:32.344
Jun 27 16:33:32.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:33:32.348
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:33:32.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:33:32.437
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 06/27/23 16:33:32.451
Jun 27 16:33:32.558: INFO: Waiting up to 5m0s for pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb" in namespace "projected-9657" to be "running and ready"
Jun 27 16:33:32.593: INFO: Pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 34.475813ms
Jun 27 16:33:32.593: INFO: The phase of Pod annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:33:34.616: INFO: Pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057103599s
Jun 27 16:33:34.616: INFO: The phase of Pod annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:33:36.612: INFO: Pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.053250215s
Jun 27 16:33:36.612: INFO: The phase of Pod annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb is Running (Ready = true)
Jun 27 16:33:36.612: INFO: Pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb" satisfied condition "running and ready"
Jun 27 16:33:37.303: INFO: Successfully updated pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 16:33:39.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9657" for this suite. 06/27/23 16:33:39.433
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":55,"skipped":1114,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.114 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:33:32.344
    Jun 27 16:33:32.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:33:32.348
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:33:32.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:33:32.437
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 06/27/23 16:33:32.451
    Jun 27 16:33:32.558: INFO: Waiting up to 5m0s for pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb" in namespace "projected-9657" to be "running and ready"
    Jun 27 16:33:32.593: INFO: Pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 34.475813ms
    Jun 27 16:33:32.593: INFO: The phase of Pod annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:33:34.616: INFO: Pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.057103599s
    Jun 27 16:33:34.616: INFO: The phase of Pod annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:33:36.612: INFO: Pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb": Phase="Running", Reason="", readiness=true. Elapsed: 4.053250215s
    Jun 27 16:33:36.612: INFO: The phase of Pod annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb is Running (Ready = true)
    Jun 27 16:33:36.612: INFO: Pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb" satisfied condition "running and ready"
    Jun 27 16:33:37.303: INFO: Successfully updated pod "annotationupdatef1c96d34-767f-4d17-912d-642d255bd3bb"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 16:33:39.403: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9657" for this suite. 06/27/23 16:33:39.433
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:33:39.458
Jun 27 16:33:39.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 16:33:39.46
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:33:39.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:33:39.523
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-663 06/27/23 16:33:39.539
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/27/23 16:33:39.589
STEP: creating service externalsvc in namespace services-663 06/27/23 16:33:39.59
STEP: creating replication controller externalsvc in namespace services-663 06/27/23 16:33:39.621
I0627 16:33:39.657996      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-663, replica count: 2
I0627 16:33:42.712180      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 06/27/23 16:33:42.728
Jun 27 16:33:42.791: INFO: Creating new exec pod
Jun 27 16:33:42.857: INFO: Waiting up to 5m0s for pod "execpodmb289" in namespace "services-663" to be "running"
Jun 27 16:33:42.879: INFO: Pod "execpodmb289": Phase="Pending", Reason="", readiness=false. Elapsed: 21.72217ms
Jun 27 16:33:44.901: INFO: Pod "execpodmb289": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044111867s
Jun 27 16:33:46.901: INFO: Pod "execpodmb289": Phase="Running", Reason="", readiness=true. Elapsed: 4.043235985s
Jun 27 16:33:46.901: INFO: Pod "execpodmb289" satisfied condition "running"
Jun 27 16:33:46.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-663 exec execpodmb289 -- /bin/sh -x -c nslookup nodeport-service.services-663.svc.cluster.local'
Jun 27 16:33:47.347: INFO: stderr: "+ nslookup nodeport-service.services-663.svc.cluster.local\n"
Jun 27 16:33:47.347: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-663.svc.cluster.local\tcanonical name = externalsvc.services-663.svc.cluster.local.\nName:\texternalsvc.services-663.svc.cluster.local\nAddress: 172.21.48.50\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-663, will wait for the garbage collector to delete the pods 06/27/23 16:33:47.347
Jun 27 16:33:47.451: INFO: Deleting ReplicationController externalsvc took: 31.222419ms
Jun 27 16:33:47.551: INFO: Terminating ReplicationController externalsvc pods took: 100.137542ms
Jun 27 16:33:50.217: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 16:33:50.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-663" for this suite. 06/27/23 16:33:50.296
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":56,"skipped":1117,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.866 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:33:39.458
    Jun 27 16:33:39.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 16:33:39.46
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:33:39.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:33:39.523
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-663 06/27/23 16:33:39.539
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/27/23 16:33:39.589
    STEP: creating service externalsvc in namespace services-663 06/27/23 16:33:39.59
    STEP: creating replication controller externalsvc in namespace services-663 06/27/23 16:33:39.621
    I0627 16:33:39.657996      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-663, replica count: 2
    I0627 16:33:42.712180      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 06/27/23 16:33:42.728
    Jun 27 16:33:42.791: INFO: Creating new exec pod
    Jun 27 16:33:42.857: INFO: Waiting up to 5m0s for pod "execpodmb289" in namespace "services-663" to be "running"
    Jun 27 16:33:42.879: INFO: Pod "execpodmb289": Phase="Pending", Reason="", readiness=false. Elapsed: 21.72217ms
    Jun 27 16:33:44.901: INFO: Pod "execpodmb289": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044111867s
    Jun 27 16:33:46.901: INFO: Pod "execpodmb289": Phase="Running", Reason="", readiness=true. Elapsed: 4.043235985s
    Jun 27 16:33:46.901: INFO: Pod "execpodmb289" satisfied condition "running"
    Jun 27 16:33:46.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-663 exec execpodmb289 -- /bin/sh -x -c nslookup nodeport-service.services-663.svc.cluster.local'
    Jun 27 16:33:47.347: INFO: stderr: "+ nslookup nodeport-service.services-663.svc.cluster.local\n"
    Jun 27 16:33:47.347: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nnodeport-service.services-663.svc.cluster.local\tcanonical name = externalsvc.services-663.svc.cluster.local.\nName:\texternalsvc.services-663.svc.cluster.local\nAddress: 172.21.48.50\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-663, will wait for the garbage collector to delete the pods 06/27/23 16:33:47.347
    Jun 27 16:33:47.451: INFO: Deleting ReplicationController externalsvc took: 31.222419ms
    Jun 27 16:33:47.551: INFO: Terminating ReplicationController externalsvc pods took: 100.137542ms
    Jun 27 16:33:50.217: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 16:33:50.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-663" for this suite. 06/27/23 16:33:50.296
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:33:50.327
Jun 27 16:33:50.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replication-controller 06/27/23 16:33:50.328
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:33:50.398
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:33:50.41
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5 06/27/23 16:33:50.429
Jun 27 16:33:50.477: INFO: Pod name my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5: Found 0 pods out of 1
Jun 27 16:33:55.498: INFO: Pod name my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5: Found 1 pods out of 1
Jun 27 16:33:55.498: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5" are running
Jun 27 16:33:55.498: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7" in namespace "replication-controller-5088" to be "running"
Jun 27 16:33:55.517: INFO: Pod "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7": Phase="Running", Reason="", readiness=true. Elapsed: 19.402348ms
Jun 27 16:33:55.517: INFO: Pod "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7" satisfied condition "running"
Jun 27 16:33:55.517: INFO: Pod "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:33:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:33:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:33:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:33:50 +0000 UTC Reason: Message:}])
Jun 27 16:33:55.517: INFO: Trying to dial the pod
Jun 27 16:34:00.597: INFO: Controller my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5: Got expected result from replica 1 [my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7]: "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 27 16:34:00.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-5088" for this suite. 06/27/23 16:34:00.629
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":57,"skipped":1133,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.333 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:33:50.327
    Jun 27 16:33:50.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replication-controller 06/27/23 16:33:50.328
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:33:50.398
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:33:50.41
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5 06/27/23 16:33:50.429
    Jun 27 16:33:50.477: INFO: Pod name my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5: Found 0 pods out of 1
    Jun 27 16:33:55.498: INFO: Pod name my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5: Found 1 pods out of 1
    Jun 27 16:33:55.498: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5" are running
    Jun 27 16:33:55.498: INFO: Waiting up to 5m0s for pod "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7" in namespace "replication-controller-5088" to be "running"
    Jun 27 16:33:55.517: INFO: Pod "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7": Phase="Running", Reason="", readiness=true. Elapsed: 19.402348ms
    Jun 27 16:33:55.517: INFO: Pod "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7" satisfied condition "running"
    Jun 27 16:33:55.517: INFO: Pod "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:33:50 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:33:52 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:33:52 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:33:50 +0000 UTC Reason: Message:}])
    Jun 27 16:33:55.517: INFO: Trying to dial the pod
    Jun 27 16:34:00.597: INFO: Controller my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5: Got expected result from replica 1 [my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7]: "my-hostname-basic-5d084eae-c4d8-4857-8587-a9bbb8714ee5-bs2x7", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 27 16:34:00.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-5088" for this suite. 06/27/23 16:34:00.629
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:34:00.661
Jun 27 16:34:00.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-probe 06/27/23 16:34:00.664
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:34:00.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:34:00.726
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197 in namespace container-probe-2445 06/27/23 16:34:00.746
Jun 27 16:34:00.837: INFO: Waiting up to 5m0s for pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197" in namespace "container-probe-2445" to be "not pending"
Jun 27 16:34:00.857: INFO: Pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197": Phase="Pending", Reason="", readiness=false. Elapsed: 20.28987ms
Jun 27 16:34:02.879: INFO: Pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041936909s
Jun 27 16:34:04.877: INFO: Pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197": Phase="Running", Reason="", readiness=true. Elapsed: 4.040111731s
Jun 27 16:34:04.877: INFO: Pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197" satisfied condition "not pending"
Jun 27 16:34:04.877: INFO: Started pod test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197 in namespace container-probe-2445
STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 16:34:04.877
Jun 27 16:34:04.896: INFO: Initial restart count of pod test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197 is 0
STEP: deleting the pod 06/27/23 16:38:05.649
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 27 16:38:05.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2445" for this suite. 06/27/23 16:38:05.745
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":58,"skipped":1138,"failed":0}
------------------------------
â€¢ [SLOW TEST] [245.113 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:34:00.661
    Jun 27 16:34:00.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-probe 06/27/23 16:34:00.664
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:34:00.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:34:00.726
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197 in namespace container-probe-2445 06/27/23 16:34:00.746
    Jun 27 16:34:00.837: INFO: Waiting up to 5m0s for pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197" in namespace "container-probe-2445" to be "not pending"
    Jun 27 16:34:00.857: INFO: Pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197": Phase="Pending", Reason="", readiness=false. Elapsed: 20.28987ms
    Jun 27 16:34:02.879: INFO: Pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041936909s
    Jun 27 16:34:04.877: INFO: Pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197": Phase="Running", Reason="", readiness=true. Elapsed: 4.040111731s
    Jun 27 16:34:04.877: INFO: Pod "test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197" satisfied condition "not pending"
    Jun 27 16:34:04.877: INFO: Started pod test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197 in namespace container-probe-2445
    STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 16:34:04.877
    Jun 27 16:34:04.896: INFO: Initial restart count of pod test-webserver-fe406bcf-7013-4018-b1a1-90466dbbd197 is 0
    STEP: deleting the pod 06/27/23 16:38:05.649
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 27 16:38:05.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2445" for this suite. 06/27/23 16:38:05.745
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:38:05.774
Jun 27 16:38:05.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename gc 06/27/23 16:38:05.775
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:05.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:05.856
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 06/27/23 16:38:05.871
STEP: delete the rc 06/27/23 16:38:10.918
STEP: wait for all pods to be garbage collected 06/27/23 16:38:10.95
STEP: Gathering metrics 06/27/23 16:38:15.994
W0627 16:38:16.031582      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 27 16:38:16.031: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 27 16:38:16.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-6263" for this suite. 06/27/23 16:38:16.058
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":59,"skipped":1147,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.310 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:38:05.774
    Jun 27 16:38:05.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename gc 06/27/23 16:38:05.775
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:05.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:05.856
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 06/27/23 16:38:05.871
    STEP: delete the rc 06/27/23 16:38:10.918
    STEP: wait for all pods to be garbage collected 06/27/23 16:38:10.95
    STEP: Gathering metrics 06/27/23 16:38:15.994
    W0627 16:38:16.031582      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 27 16:38:16.031: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 27 16:38:16.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-6263" for this suite. 06/27/23 16:38:16.058
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:38:16.087
Jun 27 16:38:16.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sysctl 06/27/23 16:38:16.09
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:16.137
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:16.154
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/27/23 16:38:16.173
STEP: Watching for error events or started pod 06/27/23 16:38:16.242
STEP: Waiting for pod completion 06/27/23 16:38:18.261
Jun 27 16:38:18.261: INFO: Waiting up to 3m0s for pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232" in namespace "sysctl-7250" to be "completed"
Jun 27 16:38:18.279: INFO: Pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232": Phase="Pending", Reason="", readiness=false. Elapsed: 18.200947ms
Jun 27 16:38:20.300: INFO: Pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039119127s
Jun 27 16:38:22.297: INFO: Pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036143329s
Jun 27 16:38:22.297: INFO: Pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232" satisfied condition "completed"
STEP: Checking that the pod succeeded 06/27/23 16:38:22.318
STEP: Getting logs from the pod 06/27/23 16:38:22.319
STEP: Checking that the sysctl is actually updated 06/27/23 16:38:22.407
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 27 16:38:22.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-7250" for this suite. 06/27/23 16:38:22.441
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":60,"skipped":1160,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.380 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:38:16.087
    Jun 27 16:38:16.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sysctl 06/27/23 16:38:16.09
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:16.137
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:16.154
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 06/27/23 16:38:16.173
    STEP: Watching for error events or started pod 06/27/23 16:38:16.242
    STEP: Waiting for pod completion 06/27/23 16:38:18.261
    Jun 27 16:38:18.261: INFO: Waiting up to 3m0s for pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232" in namespace "sysctl-7250" to be "completed"
    Jun 27 16:38:18.279: INFO: Pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232": Phase="Pending", Reason="", readiness=false. Elapsed: 18.200947ms
    Jun 27 16:38:20.300: INFO: Pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039119127s
    Jun 27 16:38:22.297: INFO: Pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036143329s
    Jun 27 16:38:22.297: INFO: Pod "sysctl-35532c10-5b09-4c48-95f7-47f14ed72232" satisfied condition "completed"
    STEP: Checking that the pod succeeded 06/27/23 16:38:22.318
    STEP: Getting logs from the pod 06/27/23 16:38:22.319
    STEP: Checking that the sysctl is actually updated 06/27/23 16:38:22.407
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 27 16:38:22.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-7250" for this suite. 06/27/23 16:38:22.441
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:38:22.47
Jun 27 16:38:22.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-pred 06/27/23 16:38:22.473
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:22.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:22.55
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 27 16:38:22.567: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 27 16:38:22.631: INFO: Waiting for terminating namespaces to be deleted...
Jun 27 16:38:22.667: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.89 before test
Jun 27 16:38:22.767: INFO: calico-node-vplt6 from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 16:38:22.767: INFO: calico-typha-657df678d7-65kdh from calico-system started at 2023-06-27 13:54:48 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container calico-typha ready: true, restart count 0
Jun 27 16:38:22.767: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
Jun 27 16:38:22.767: INFO: ibm-keepalived-watcher-9rf8d from kube-system started at 2023-06-27 13:53:32 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 16:38:22.767: INFO: ibm-master-proxy-static-10.113.180.89 from kube-system started at 2023-06-27 13:53:30 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container pause ready: true, restart count 0
Jun 27 16:38:22.767: INFO: ibmcloud-block-storage-driver-5ml2s from kube-system started at 2023-06-27 13:53:43 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 16:38:22.767: INFO: vpn-546f6bf578-7ks4f from kube-system started at 2023-06-27 14:06:39 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container vpn ready: true, restart count 0
Jun 27 16:38:22.767: INFO: tuned-mwm8w from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container tuned ready: true, restart count 0
Jun 27 16:38:22.767: INFO: csi-snapshot-controller-5694c47cbb-9pvhd from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 27 16:38:22.767: INFO: csi-snapshot-webhook-5d7cc7f6cb-lwgl9 from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container webhook ready: true, restart count 0
Jun 27 16:38:22.767: INFO: console-6c8658586b-jlcfh from openshift-console started at 2023-06-27 14:03:39 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container console ready: true, restart count 0
Jun 27 16:38:22.767: INFO: downloads-57bd479866-2fpqm from openshift-console started at 2023-06-27 13:56:16 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container download-server ready: true, restart count 0
Jun 27 16:38:22.767: INFO: dns-default-z76hf from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container dns ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: node-resolver-cwltw from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 16:38:22.767: INFO: node-ca-d4q6s from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 16:38:22.767: INFO: registry-pvc-permissions-4bk2b from openshift-image-registry started at 2023-06-27 14:02:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 27 16:38:22.767: INFO: ingress-canary-bmwtq from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 16:38:22.767: INFO: router-default-7f97cd5c5f-m5pcx from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container router ready: true, restart count 0
Jun 27 16:38:22.767: INFO: openshift-kube-proxy-b4224 from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: redhat-marketplace-qzk6b from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 16:38:22.767: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-06-27 14:02:16 +0000 UTC (6 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container alertmanager ready: true, restart count 1
Jun 27 16:38:22.767: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: kube-state-metrics-685444f59b-5dlq4 from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 27 16:38:22.767: INFO: node-exporter-lqlws from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 16:38:22.767: INFO: openshift-state-metrics-6d79d8c586-9mgzr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 27 16:38:22.767: INFO: prometheus-adapter-648f68fcc-h27rt from openshift-monitoring started at 2023-06-27 14:01:39 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 27 16:38:22.767: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-06-27 14:01:59 +0000 UTC (6 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container prometheus ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 27 16:38:22.767: INFO: prometheus-operator-admission-webhook-6c667b594b-bds22 from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jun 27 16:38:22.767: INFO: telemeter-client-7dc8fdddc8-kpccw from openshift-monitoring started at 2023-06-27 14:00:33 +0000 UTC (3 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container reload ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 27 16:38:22.767: INFO: thanos-querier-754f675f77-6nf9l from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container thanos-query ready: true, restart count 0
Jun 27 16:38:22.767: INFO: multus-7zznf from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 16:38:22.767: INFO: multus-additional-cni-plugins-pn2hb from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 16:38:22.767: INFO: multus-admission-controller-7fb7cfdd7c-krjbc from openshift-multus started at 2023-06-27 13:56:57 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 27 16:38:22.767: INFO: network-metrics-daemon-p8dh4 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 16:38:22.767: INFO: network-check-target-7qghz from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 16:38:22.767: INFO: sonobuoy from sonobuoy started at 2023-06-27 16:21:42 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 27 16:38:22.767: INFO: sonobuoy-e2e-job-0e516cd149fa4871 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container e2e ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 16:38:22.767: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 27 16:38:22.767: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.90 before test
Jun 27 16:38:22.845: INFO: calico-node-6j4xp from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 16:38:22.845: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-npmrp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
Jun 27 16:38:22.845: INFO: ibm-keepalived-watcher-bpqvm from kube-system started at 2023-06-27 13:53:26 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 16:38:22.845: INFO: ibm-master-proxy-static-10.113.180.90 from kube-system started at 2023-06-27 13:53:22 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 16:38:22.845: INFO: 	Container pause ready: true, restart count 0
Jun 27 16:38:22.845: INFO: ibmcloud-block-storage-driver-jb9wr from kube-system started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 16:38:22.845: INFO: tuned-md8mr from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container tuned ready: true, restart count 0
Jun 27 16:38:22.845: INFO: csi-snapshot-controller-5694c47cbb-tbtws from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 27 16:38:22.845: INFO: csi-snapshot-webhook-5d7cc7f6cb-gkzhr from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container webhook ready: true, restart count 0
Jun 27 16:38:22.845: INFO: console-6c8658586b-nbbk5 from openshift-console started at 2023-06-27 14:03:13 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container console ready: true, restart count 0
Jun 27 16:38:22.845: INFO: downloads-57bd479866-q8vlr from openshift-console started at 2023-06-27 13:56:15 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container download-server ready: true, restart count 0
Jun 27 16:38:22.845: INFO: dns-default-jp65p from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container dns ready: true, restart count 0
Jun 27 16:38:22.845: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.845: INFO: node-resolver-dtvx7 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 16:38:22.845: INFO: image-registry-7f546fc5bb-htm8z from openshift-image-registry started at 2023-06-27 14:02:00 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container registry ready: true, restart count 0
Jun 27 16:38:22.845: INFO: node-ca-8pgp8 from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 16:38:22.845: INFO: ingress-canary-nwx9b from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 16:38:22.845: INFO: router-default-7f97cd5c5f-fwv8n from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container router ready: true, restart count 0
Jun 27 16:38:22.845: INFO: openshift-kube-proxy-bf9cj from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 16:38:22.845: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.845: INFO: migrator-6795cdbdb7-sg6pf from openshift-kube-storage-version-migrator started at 2023-06-27 13:56:10 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container migrator ready: true, restart count 0
Jun 27 16:38:22.845: INFO: certified-operators-qdmmw from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 16:38:22.845: INFO: community-operators-pc9md from openshift-marketplace started at 2023-06-27 16:16:34 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 16:38:22.845: INFO: redhat-operators-z76kp from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 16:38:22.845: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-06-27 14:01:43 +0000 UTC (6 container statuses recorded)
Jun 27 16:38:22.845: INFO: 	Container alertmanager ready: true, restart count 1
Jun 27 16:38:22.845: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 27 16:38:22.845: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 16:38:22.845: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.845: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: node-exporter-5vgpm from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 16:38:22.846: INFO: prometheus-adapter-648f68fcc-hpjp5 from openshift-monitoring started at 2023-06-27 14:01:40 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 27 16:38:22.846: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-06-27 14:02:17 +0000 UTC (6 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container prometheus ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 27 16:38:22.846: INFO: prometheus-operator-68dfcc5c8-ngd9f from openshift-monitoring started at 2023-06-27 14:00:13 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 27 16:38:22.846: INFO: prometheus-operator-admission-webhook-6c667b594b-6ls8t from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jun 27 16:38:22.846: INFO: thanos-querier-754f675f77-86766 from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container thanos-query ready: true, restart count 0
Jun 27 16:38:22.846: INFO: multus-additional-cni-plugins-qszdk from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 16:38:22.846: INFO: multus-admission-controller-7fb7cfdd7c-v2xrp from openshift-multus started at 2023-06-27 13:56:50 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 27 16:38:22.846: INFO: multus-skrpb from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 16:38:22.846: INFO: network-metrics-daemon-4jmw9 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 16:38:22.846: INFO: network-check-target-zmtm2 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 16:38:22.846: INFO: network-operator-ffb9884c5-qnhcd from openshift-network-operator started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container network-operator ready: true, restart count 1
Jun 27 16:38:22.846: INFO: collect-profiles-28131360-g6twq from openshift-operator-lifecycle-manager started at 2023-06-27 16:00:00 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 27 16:38:22.846: INFO: collect-profiles-28131375-xgdd2 from openshift-operator-lifecycle-manager started at 2023-06-27 16:15:00 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 27 16:38:22.846: INFO: collect-profiles-28131390-j5lzt from openshift-operator-lifecycle-manager started at 2023-06-27 16:30:00 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 27 16:38:22.846: INFO: packageserver-8d78bf5dd-7lzpw from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container packageserver ready: true, restart count 0
Jun 27 16:38:22.846: INFO: service-ca-6f86485857-zj4pd from openshift-service-ca started at 2023-06-27 13:56:17 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container service-ca-controller ready: true, restart count 0
Jun 27 16:38:22.846: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 16:38:22.846: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 27 16:38:22.846: INFO: sysctl-35532c10-5b09-4c48-95f7-47f14ed72232 from sysctl-7250 started at 2023-06-27 16:38:16 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container test-container ready: false, restart count 0
Jun 27 16:38:22.846: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-06-27 13:57:19 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
Jun 27 16:38:22.846: INFO: tigera-operator-687c49f5c8-7jdkr from tigera-operator started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.846: INFO: 	Container tigera-operator ready: true, restart count 2
Jun 27 16:38:22.846: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.96 before test
Jun 27 16:38:22.919: INFO: calico-kube-controllers-79f474fb8-m45qw from calico-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 27 16:38:22.919: INFO: calico-node-z28vk from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 16:38:22.919: INFO: calico-typha-657df678d7-4ch2t from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container calico-typha ready: true, restart count 0
Jun 27 16:38:22.919: INFO: managed-storage-validation-webhooks-665764b6b4-r77pv from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 16:38:22.919: INFO: managed-storage-validation-webhooks-665764b6b4-v4g8s from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 16:38:22.919: INFO: managed-storage-validation-webhooks-665764b6b4-vhqr7 from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 16:38:22.919: INFO: ibm-file-plugin-84cc66cdb-97nzr from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 27 16:38:22.919: INFO: ibm-keepalived-watcher-h8p9q from kube-system started at 2023-06-27 13:53:48 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 16:38:22.919: INFO: ibm-master-proxy-static-10.113.180.96 from kube-system started at 2023-06-27 13:53:45 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 16:38:22.919: INFO: 	Container pause ready: true, restart count 0
Jun 27 16:38:22.919: INFO: ibm-storage-metrics-agent-746d76fb56-nvvw9 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Jun 27 16:38:22.919: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Jun 27 16:38:22.919: INFO: ibm-storage-watcher-676b4ddbc6-dz54r from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 27 16:38:22.919: INFO: ibmcloud-block-storage-driver-pkhxd from kube-system started at 2023-06-27 13:53:53 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 16:38:22.919: INFO: ibmcloud-block-storage-plugin-5f677b8577-8f7s7 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 27 16:38:22.919: INFO: cluster-node-tuning-operator-848d57c5bb-7ln5t from openshift-cluster-node-tuning-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 27 16:38:22.919: INFO: tuned-kk4gn from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container tuned ready: true, restart count 0
Jun 27 16:38:22.919: INFO: cluster-samples-operator-769ddfc8cc-88z62 from openshift-cluster-samples-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 27 16:38:22.919: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 27 16:38:22.919: INFO: cluster-storage-operator-5fc995c76b-jhmzr from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 27 16:38:22.919: INFO: csi-snapshot-controller-operator-64ff6856c8-l29px from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 27 16:38:22.919: INFO: console-operator-5694fff656-5wp7b from openshift-console-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container console-operator ready: true, restart count 1
Jun 27 16:38:22.919: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Jun 27 16:38:22.919: INFO: dns-operator-b64d4b4c7-5p5hs from openshift-dns-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container dns-operator ready: true, restart count 0
Jun 27 16:38:22.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.919: INFO: dns-default-q8pl2 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container dns ready: true, restart count 0
Jun 27 16:38:22.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.919: INFO: node-resolver-nbx64 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 16:38:22.919: INFO: cluster-image-registry-operator-55bdbc85cd-vpldl from openshift-image-registry started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 27 16:38:22.919: INFO: node-ca-tbfkf from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 16:38:22.919: INFO: ingress-canary-mtqfl from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 16:38:22.919: INFO: ingress-operator-5c457b57b4-67rvh from openshift-ingress-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 27 16:38:22.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.919: INFO: insights-operator-8f4d6c949-n2nlt from openshift-insights started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container insights-operator ready: true, restart count 1
Jun 27 16:38:22.919: INFO: openshift-kube-proxy-mpv6g from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 16:38:22.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.919: INFO: kube-storage-version-migrator-operator-59f4c7c696-jq8gm from openshift-kube-storage-version-migrator-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.919: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 27 16:38:22.919: INFO: marketplace-operator-57bd87cfb8-5hhm8 from openshift-marketplace started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 27 16:38:22.920: INFO: cluster-monitoring-operator-644d445879-l5jj8 from openshift-monitoring started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 27 16:38:22.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.920: INFO: node-exporter-l5dxr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.920: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 16:38:22.920: INFO: multus-additional-cni-plugins-xqsgs from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 16:38:22.920: INFO: multus-kx7qm from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 16:38:22.920: INFO: network-metrics-daemon-cbrq2 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:38:22.920: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 16:38:22.920: INFO: network-check-source-6b8766f86f-s8s7j from openshift-network-diagnostics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 27 16:38:22.920: INFO: network-check-target-6lv98 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 16:38:22.920: INFO: catalog-operator-58898485cd-qjmms from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 27 16:38:22.920: INFO: olm-operator-7959cfcd9f-dt77g from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container olm-operator ready: true, restart count 0
Jun 27 16:38:22.920: INFO: package-server-manager-f7b6c466f-9bbzw from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container package-server-manager ready: true, restart count 0
Jun 27 16:38:22.920: INFO: packageserver-8d78bf5dd-r9g9t from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container packageserver ready: true, restart count 0
Jun 27 16:38:22.920: INFO: metrics-6d46d44d8f-jbnwq from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container metrics ready: true, restart count 3
Jun 27 16:38:22.920: INFO: push-gateway-5465b544cc-f8v8q from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container push-gateway ready: true, restart count 0
Jun 27 16:38:22.920: INFO: service-ca-operator-74dd8bfc8-cl4bp from openshift-service-ca-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 27 16:38:22.920: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 16:38:22.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 16:38:22.920: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 06/27/23 16:38:22.92
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.176c917792f38190], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 06/27/23 16:38:23.13
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 27 16:38:24.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3095" for this suite. 06/27/23 16:38:24.165
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":61,"skipped":1174,"failed":0}
------------------------------
â€¢ [1.720 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:38:22.47
    Jun 27 16:38:22.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-pred 06/27/23 16:38:22.473
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:22.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:22.55
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 27 16:38:22.567: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 27 16:38:22.631: INFO: Waiting for terminating namespaces to be deleted...
    Jun 27 16:38:22.667: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.89 before test
    Jun 27 16:38:22.767: INFO: calico-node-vplt6 from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: calico-typha-657df678d7-65kdh from calico-system started at 2023-06-27 13:54:48 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: ibm-keepalived-watcher-9rf8d from kube-system started at 2023-06-27 13:53:32 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: ibm-master-proxy-static-10.113.180.89 from kube-system started at 2023-06-27 13:53:30 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container pause ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: ibmcloud-block-storage-driver-5ml2s from kube-system started at 2023-06-27 13:53:43 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: vpn-546f6bf578-7ks4f from kube-system started at 2023-06-27 14:06:39 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container vpn ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: tuned-mwm8w from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: csi-snapshot-controller-5694c47cbb-9pvhd from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: csi-snapshot-webhook-5d7cc7f6cb-lwgl9 from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container webhook ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: console-6c8658586b-jlcfh from openshift-console started at 2023-06-27 14:03:39 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container console ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: downloads-57bd479866-2fpqm from openshift-console started at 2023-06-27 13:56:16 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container download-server ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: dns-default-z76hf from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container dns ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: node-resolver-cwltw from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: node-ca-d4q6s from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: registry-pvc-permissions-4bk2b from openshift-image-registry started at 2023-06-27 14:02:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container pvc-permissions ready: false, restart count 0
    Jun 27 16:38:22.767: INFO: ingress-canary-bmwtq from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: router-default-7f97cd5c5f-m5pcx from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container router ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: openshift-kube-proxy-b4224 from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: redhat-marketplace-qzk6b from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-06-27 14:02:16 +0000 UTC (6 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container alertmanager ready: true, restart count 1
    Jun 27 16:38:22.767: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: kube-state-metrics-685444f59b-5dlq4 from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: node-exporter-lqlws from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: openshift-state-metrics-6d79d8c586-9mgzr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: prometheus-adapter-648f68fcc-h27rt from openshift-monitoring started at 2023-06-27 14:01:39 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-06-27 14:01:59 +0000 UTC (6 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container prometheus ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: prometheus-operator-admission-webhook-6c667b594b-bds22 from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: telemeter-client-7dc8fdddc8-kpccw from openshift-monitoring started at 2023-06-27 14:00:33 +0000 UTC (3 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container reload ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container telemeter-client ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: thanos-querier-754f675f77-6nf9l from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container thanos-query ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: multus-7zznf from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: multus-additional-cni-plugins-pn2hb from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: multus-admission-controller-7fb7cfdd7c-krjbc from openshift-multus started at 2023-06-27 13:56:57 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: network-metrics-daemon-p8dh4 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: network-check-target-7qghz from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: sonobuoy from sonobuoy started at 2023-06-27 16:21:42 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: sonobuoy-e2e-job-0e516cd149fa4871 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container e2e ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.767: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 27 16:38:22.767: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.90 before test
    Jun 27 16:38:22.845: INFO: calico-node-6j4xp from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-npmrp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: ibm-keepalived-watcher-bpqvm from kube-system started at 2023-06-27 13:53:26 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: ibm-master-proxy-static-10.113.180.90 from kube-system started at 2023-06-27 13:53:22 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: 	Container pause ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: ibmcloud-block-storage-driver-jb9wr from kube-system started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: tuned-md8mr from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: csi-snapshot-controller-5694c47cbb-tbtws from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: csi-snapshot-webhook-5d7cc7f6cb-gkzhr from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container webhook ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: console-6c8658586b-nbbk5 from openshift-console started at 2023-06-27 14:03:13 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container console ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: downloads-57bd479866-q8vlr from openshift-console started at 2023-06-27 13:56:15 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container download-server ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: dns-default-jp65p from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container dns ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: node-resolver-dtvx7 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: image-registry-7f546fc5bb-htm8z from openshift-image-registry started at 2023-06-27 14:02:00 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container registry ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: node-ca-8pgp8 from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: ingress-canary-nwx9b from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: router-default-7f97cd5c5f-fwv8n from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container router ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: openshift-kube-proxy-bf9cj from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: migrator-6795cdbdb7-sg6pf from openshift-kube-storage-version-migrator started at 2023-06-27 13:56:10 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container migrator ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: certified-operators-qdmmw from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: community-operators-pc9md from openshift-marketplace started at 2023-06-27 16:16:34 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: redhat-operators-z76kp from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-06-27 14:01:43 +0000 UTC (6 container statuses recorded)
    Jun 27 16:38:22.845: INFO: 	Container alertmanager ready: true, restart count 1
    Jun 27 16:38:22.845: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.845: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: node-exporter-5vgpm from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: prometheus-adapter-648f68fcc-hpjp5 from openshift-monitoring started at 2023-06-27 14:01:40 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-06-27 14:02:17 +0000 UTC (6 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container prometheus ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: prometheus-operator-68dfcc5c8-ngd9f from openshift-monitoring started at 2023-06-27 14:00:13 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: prometheus-operator-admission-webhook-6c667b594b-6ls8t from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: thanos-querier-754f675f77-86766 from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container thanos-query ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: multus-additional-cni-plugins-qszdk from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: multus-admission-controller-7fb7cfdd7c-v2xrp from openshift-multus started at 2023-06-27 13:56:50 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: multus-skrpb from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: network-metrics-daemon-4jmw9 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: network-check-target-zmtm2 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: network-operator-ffb9884c5-qnhcd from openshift-network-operator started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container network-operator ready: true, restart count 1
    Jun 27 16:38:22.846: INFO: collect-profiles-28131360-g6twq from openshift-operator-lifecycle-manager started at 2023-06-27 16:00:00 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container collect-profiles ready: false, restart count 0
    Jun 27 16:38:22.846: INFO: collect-profiles-28131375-xgdd2 from openshift-operator-lifecycle-manager started at 2023-06-27 16:15:00 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container collect-profiles ready: false, restart count 0
    Jun 27 16:38:22.846: INFO: collect-profiles-28131390-j5lzt from openshift-operator-lifecycle-manager started at 2023-06-27 16:30:00 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container collect-profiles ready: false, restart count 0
    Jun 27 16:38:22.846: INFO: packageserver-8d78bf5dd-7lzpw from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container packageserver ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: service-ca-6f86485857-zj4pd from openshift-service-ca started at 2023-06-27 13:56:17 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container service-ca-controller ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: sysctl-35532c10-5b09-4c48-95f7-47f14ed72232 from sysctl-7250 started at 2023-06-27 16:38:16 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container test-container ready: false, restart count 0
    Jun 27 16:38:22.846: INFO: test-k8s-e2e-pvg-master-verification from test-k8s-e2e-pvg-privileged started at 2023-06-27 13:57:19 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container test-k8s-e2e-pvg-master-verification ready: true, restart count 0
    Jun 27 16:38:22.846: INFO: tigera-operator-687c49f5c8-7jdkr from tigera-operator started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.846: INFO: 	Container tigera-operator ready: true, restart count 2
    Jun 27 16:38:22.846: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.96 before test
    Jun 27 16:38:22.919: INFO: calico-kube-controllers-79f474fb8-m45qw from calico-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: calico-node-z28vk from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: calico-typha-657df678d7-4ch2t from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: managed-storage-validation-webhooks-665764b6b4-r77pv from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: managed-storage-validation-webhooks-665764b6b4-v4g8s from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: managed-storage-validation-webhooks-665764b6b4-vhqr7 from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: ibm-file-plugin-84cc66cdb-97nzr from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: ibm-keepalived-watcher-h8p9q from kube-system started at 2023-06-27 13:53:48 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: ibm-master-proxy-static-10.113.180.96 from kube-system started at 2023-06-27 13:53:45 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: 	Container pause ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: ibm-storage-metrics-agent-746d76fb56-nvvw9 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: ibm-storage-watcher-676b4ddbc6-dz54r from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: ibmcloud-block-storage-driver-pkhxd from kube-system started at 2023-06-27 13:53:53 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: ibmcloud-block-storage-plugin-5f677b8577-8f7s7 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: cluster-node-tuning-operator-848d57c5bb-7ln5t from openshift-cluster-node-tuning-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: tuned-kk4gn from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: cluster-samples-operator-769ddfc8cc-88z62 from openshift-cluster-samples-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: cluster-storage-operator-5fc995c76b-jhmzr from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Jun 27 16:38:22.919: INFO: csi-snapshot-controller-operator-64ff6856c8-l29px from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: console-operator-5694fff656-5wp7b from openshift-console-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container console-operator ready: true, restart count 1
    Jun 27 16:38:22.919: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Jun 27 16:38:22.919: INFO: dns-operator-b64d4b4c7-5p5hs from openshift-dns-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container dns-operator ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: dns-default-q8pl2 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container dns ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: node-resolver-nbx64 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: cluster-image-registry-operator-55bdbc85cd-vpldl from openshift-image-registry started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: node-ca-tbfkf from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: ingress-canary-mtqfl from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: ingress-operator-5c457b57b4-67rvh from openshift-ingress-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container ingress-operator ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: insights-operator-8f4d6c949-n2nlt from openshift-insights started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container insights-operator ready: true, restart count 1
    Jun 27 16:38:22.919: INFO: openshift-kube-proxy-mpv6g from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.919: INFO: kube-storage-version-migrator-operator-59f4c7c696-jq8gm from openshift-kube-storage-version-migrator-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.919: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Jun 27 16:38:22.919: INFO: marketplace-operator-57bd87cfb8-5hhm8 from openshift-marketplace started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container marketplace-operator ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: cluster-monitoring-operator-644d445879-l5jj8 from openshift-monitoring started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: node-exporter-l5dxr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: multus-additional-cni-plugins-xqsgs from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: multus-kx7qm from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: network-metrics-daemon-cbrq2 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: network-check-source-6b8766f86f-s8s7j from openshift-network-diagnostics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container check-endpoints ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: network-check-target-6lv98 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: catalog-operator-58898485cd-qjmms from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container catalog-operator ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: olm-operator-7959cfcd9f-dt77g from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container olm-operator ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: package-server-manager-f7b6c466f-9bbzw from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container package-server-manager ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: packageserver-8d78bf5dd-r9g9t from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container packageserver ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: metrics-6d46d44d8f-jbnwq from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container metrics ready: true, restart count 3
    Jun 27 16:38:22.920: INFO: push-gateway-5465b544cc-f8v8q from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container push-gateway ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: service-ca-operator-74dd8bfc8-cl4bp from openshift-service-ca-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container service-ca-operator ready: true, restart count 1
    Jun 27 16:38:22.920: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 16:38:22.920: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 16:38:22.920: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 06/27/23 16:38:22.92
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.176c917792f38190], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling.] 06/27/23 16:38:23.13
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 16:38:24.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3095" for this suite. 06/27/23 16:38:24.165
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:38:24.209
Jun 27 16:38:24.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename gc 06/27/23 16:38:24.21
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:24.262
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:24.278
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 06/27/23 16:38:24.292
STEP: Wait for the Deployment to create new ReplicaSet 06/27/23 16:38:24.315
STEP: delete the deployment 06/27/23 16:38:24.337
STEP: wait for all rs to be garbage collected 06/27/23 16:38:24.375
STEP: expected 0 pods, got 1 pods 06/27/23 16:38:24.407
STEP: Gathering metrics 06/27/23 16:38:25.057
W0627 16:38:25.126095      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 27 16:38:25.126: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 27 16:38:25.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-8787" for this suite. 06/27/23 16:38:25.15
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":62,"skipped":1250,"failed":0}
------------------------------
â€¢ [0.967 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:38:24.209
    Jun 27 16:38:24.209: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename gc 06/27/23 16:38:24.21
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:24.262
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:24.278
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 06/27/23 16:38:24.292
    STEP: Wait for the Deployment to create new ReplicaSet 06/27/23 16:38:24.315
    STEP: delete the deployment 06/27/23 16:38:24.337
    STEP: wait for all rs to be garbage collected 06/27/23 16:38:24.375
    STEP: expected 0 pods, got 1 pods 06/27/23 16:38:24.407
    STEP: Gathering metrics 06/27/23 16:38:25.057
    W0627 16:38:25.126095      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 27 16:38:25.126: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 27 16:38:25.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-8787" for this suite. 06/27/23 16:38:25.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:38:25.178
Jun 27 16:38:25.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 16:38:25.18
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:25.232
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:25.24
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 16:38:25.357
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:38:25.708
STEP: Deploying the webhook pod 06/27/23 16:38:25.756
STEP: Wait for the deployment to be ready 06/27/23 16:38:25.85
Jun 27 16:38:25.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 38, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 38, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 38, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 38, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 16:38:27.972
STEP: Verifying the service has paired with the endpoint 06/27/23 16:38:28.008
Jun 27 16:38:29.009: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 06/27/23 16:38:29.222
STEP: Creating a configMap that should be mutated 06/27/23 16:38:29.286
STEP: Deleting the collection of validation webhooks 06/27/23 16:38:29.435
STEP: Creating a configMap that should not be mutated 06/27/23 16:38:29.554
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:38:29.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8353" for this suite. 06/27/23 16:38:29.669
STEP: Destroying namespace "webhook-8353-markers" for this suite. 06/27/23 16:38:29.694
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":63,"skipped":1270,"failed":0}
------------------------------
â€¢ [4.684 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:38:25.178
    Jun 27 16:38:25.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 16:38:25.18
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:25.232
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:25.24
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 16:38:25.357
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:38:25.708
    STEP: Deploying the webhook pod 06/27/23 16:38:25.756
    STEP: Wait for the deployment to be ready 06/27/23 16:38:25.85
    Jun 27 16:38:25.951: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 38, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 38, 25, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 38, 25, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 38, 25, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 16:38:27.972
    STEP: Verifying the service has paired with the endpoint 06/27/23 16:38:28.008
    Jun 27 16:38:29.009: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 06/27/23 16:38:29.222
    STEP: Creating a configMap that should be mutated 06/27/23 16:38:29.286
    STEP: Deleting the collection of validation webhooks 06/27/23 16:38:29.435
    STEP: Creating a configMap that should not be mutated 06/27/23 16:38:29.554
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:38:29.615: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8353" for this suite. 06/27/23 16:38:29.669
    STEP: Destroying namespace "webhook-8353-markers" for this suite. 06/27/23 16:38:29.694
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:38:29.864
Jun 27 16:38:29.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 16:38:29.867
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:29.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:29.986
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
Jun 27 16:38:30.033: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-c9d88c26-0a23-4ebc-95a2-942a10e0edf1 06/27/23 16:38:30.033
STEP: Creating secret with name s-test-opt-upd-70dfb791-46d5-4da2-892a-02a6ea553587 06/27/23 16:38:30.053
STEP: Creating the pod 06/27/23 16:38:30.07
Jun 27 16:38:30.133: INFO: Waiting up to 5m0s for pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c" in namespace "secrets-8309" to be "running and ready"
Jun 27 16:38:30.161: INFO: Pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c": Phase="Pending", Reason="", readiness=false. Elapsed: 27.227924ms
Jun 27 16:38:30.161: INFO: The phase of Pod pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:38:32.181: INFO: Pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047120661s
Jun 27 16:38:32.181: INFO: The phase of Pod pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:38:34.180: INFO: Pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c": Phase="Running", Reason="", readiness=true. Elapsed: 4.046749697s
Jun 27 16:38:34.181: INFO: The phase of Pod pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c is Running (Ready = true)
Jun 27 16:38:34.181: INFO: Pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-c9d88c26-0a23-4ebc-95a2-942a10e0edf1 06/27/23 16:38:34.398
STEP: Updating secret s-test-opt-upd-70dfb791-46d5-4da2-892a-02a6ea553587 06/27/23 16:38:34.418
STEP: Creating secret with name s-test-opt-create-4e7620db-17ac-42c3-9dec-39e49e83f28f 06/27/23 16:38:34.438
STEP: waiting to observe update in volume 06/27/23 16:38:34.455
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 27 16:39:48.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8309" for this suite. 06/27/23 16:39:48.469
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":64,"skipped":1283,"failed":0}
------------------------------
â€¢ [SLOW TEST] [78.637 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:38:29.864
    Jun 27 16:38:29.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 16:38:29.867
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:38:29.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:38:29.986
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    Jun 27 16:38:30.033: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-c9d88c26-0a23-4ebc-95a2-942a10e0edf1 06/27/23 16:38:30.033
    STEP: Creating secret with name s-test-opt-upd-70dfb791-46d5-4da2-892a-02a6ea553587 06/27/23 16:38:30.053
    STEP: Creating the pod 06/27/23 16:38:30.07
    Jun 27 16:38:30.133: INFO: Waiting up to 5m0s for pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c" in namespace "secrets-8309" to be "running and ready"
    Jun 27 16:38:30.161: INFO: Pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c": Phase="Pending", Reason="", readiness=false. Elapsed: 27.227924ms
    Jun 27 16:38:30.161: INFO: The phase of Pod pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:38:32.181: INFO: Pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047120661s
    Jun 27 16:38:32.181: INFO: The phase of Pod pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:38:34.180: INFO: Pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c": Phase="Running", Reason="", readiness=true. Elapsed: 4.046749697s
    Jun 27 16:38:34.181: INFO: The phase of Pod pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c is Running (Ready = true)
    Jun 27 16:38:34.181: INFO: Pod "pod-secrets-a6368121-23e2-4d4c-98a9-aef17c37a33c" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-c9d88c26-0a23-4ebc-95a2-942a10e0edf1 06/27/23 16:38:34.398
    STEP: Updating secret s-test-opt-upd-70dfb791-46d5-4da2-892a-02a6ea553587 06/27/23 16:38:34.418
    STEP: Creating secret with name s-test-opt-create-4e7620db-17ac-42c3-9dec-39e49e83f28f 06/27/23 16:38:34.438
    STEP: waiting to observe update in volume 06/27/23 16:38:34.455
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 16:39:48.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8309" for this suite. 06/27/23 16:39:48.469
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:39:48.502
Jun 27 16:39:48.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 16:39:48.504
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:39:48.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:39:48.576
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 06/27/23 16:39:48.596
Jun 27 16:39:48.673: INFO: Waiting up to 5m0s for pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03" in namespace "emptydir-4520" to be "Succeeded or Failed"
Jun 27 16:39:48.696: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03": Phase="Pending", Reason="", readiness=false. Elapsed: 22.841706ms
Jun 27 16:39:50.716: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043174229s
Jun 27 16:39:52.716: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043091277s
Jun 27 16:39:54.720: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047024854s
STEP: Saw pod success 06/27/23 16:39:54.72
Jun 27 16:39:54.720: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03" satisfied condition "Succeeded or Failed"
Jun 27 16:39:54.744: INFO: Trying to get logs from node 10.113.180.89 pod pod-b737aa28-b299-4807-b262-cd7bf1aa9b03 container test-container: <nil>
STEP: delete the pod 06/27/23 16:39:54.844
Jun 27 16:39:54.904: INFO: Waiting for pod pod-b737aa28-b299-4807-b262-cd7bf1aa9b03 to disappear
Jun 27 16:39:54.921: INFO: Pod pod-b737aa28-b299-4807-b262-cd7bf1aa9b03 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 16:39:54.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4520" for this suite. 06/27/23 16:39:54.957
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":65,"skipped":1290,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.483 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:39:48.502
    Jun 27 16:39:48.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 16:39:48.504
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:39:48.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:39:48.576
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/27/23 16:39:48.596
    Jun 27 16:39:48.673: INFO: Waiting up to 5m0s for pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03" in namespace "emptydir-4520" to be "Succeeded or Failed"
    Jun 27 16:39:48.696: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03": Phase="Pending", Reason="", readiness=false. Elapsed: 22.841706ms
    Jun 27 16:39:50.716: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043174229s
    Jun 27 16:39:52.716: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043091277s
    Jun 27 16:39:54.720: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.047024854s
    STEP: Saw pod success 06/27/23 16:39:54.72
    Jun 27 16:39:54.720: INFO: Pod "pod-b737aa28-b299-4807-b262-cd7bf1aa9b03" satisfied condition "Succeeded or Failed"
    Jun 27 16:39:54.744: INFO: Trying to get logs from node 10.113.180.89 pod pod-b737aa28-b299-4807-b262-cd7bf1aa9b03 container test-container: <nil>
    STEP: delete the pod 06/27/23 16:39:54.844
    Jun 27 16:39:54.904: INFO: Waiting for pod pod-b737aa28-b299-4807-b262-cd7bf1aa9b03 to disappear
    Jun 27 16:39:54.921: INFO: Pod pod-b737aa28-b299-4807-b262-cd7bf1aa9b03 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 16:39:54.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4520" for this suite. 06/27/23 16:39:54.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:39:54.99
Jun 27 16:39:54.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename deployment 06/27/23 16:39:54.993
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:39:55.049
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:39:55.065
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Jun 27 16:39:55.134: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Jun 27 16:40:00.156: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/27/23 16:40:00.156
Jun 27 16:40:00.156: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/27/23 16:40:00.208
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 27 16:40:04.295: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4642  3e830543-2b8c-43dc-a278-27462e3a2d66 84147 1 2023-06-27 16:40:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-27 16:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f4028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-27 16:40:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-06-27 16:40:02 +0000 UTC,LastTransitionTime:2023-06-27 16:40:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 27 16:40:04.311: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4642  ff4cfe8e-247a-4214-a73b-b520a3d1a4f6 84137 1 2023-06-27 16:40:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 3e830543-2b8c-43dc-a278-27462e3a2d66 0xc0037f43f7 0xc0037f43f8}] [] [{kube-controller-manager Update apps/v1 2023-06-27 16:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e830543-2b8c-43dc-a278-27462e3a2d66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f44a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 27 16:40:04.329: INFO: Pod "test-cleanup-deployment-69cb9c5497-lb8cz" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-lb8cz test-cleanup-deployment-69cb9c5497- deployment-4642  e24101b0-ccb6-4b08-811f-f6e3082401d9 84135 0 2023-06-27 16:40:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:837ca1885211d376bb7c616bdd7b4ca75533d0a2dbd9eadf5205db61364eb82c cni.projectcalico.org/podIP:172.30.60.70/32 cni.projectcalico.org/podIPs:172.30.60.70/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.70"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.70"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 ff4cfe8e-247a-4214-a73b-b520a3d1a4f6 0xc0037f4867 0xc0037f4868}] [] [{kube-controller-manager Update v1 2023-06-27 16:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff4cfe8e-247a-4214-a73b-b520a3d1a4f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 16:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 16:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 16:40:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7gnl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7gnl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-8w847,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.70,StartTime:2023-06-27 16:40:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 16:40:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://51f6c86a7681d1f2856bca202ee017a977dea3502fc5e3c1cd457f7d0e62de7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 27 16:40:04.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4642" for this suite. 06/27/23 16:40:04.363
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":66,"skipped":1300,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.398 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:39:54.99
    Jun 27 16:39:54.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename deployment 06/27/23 16:39:54.993
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:39:55.049
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:39:55.065
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Jun 27 16:39:55.134: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Jun 27 16:40:00.156: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/27/23 16:40:00.156
    Jun 27 16:40:00.156: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 06/27/23 16:40:00.208
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 27 16:40:04.295: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-4642  3e830543-2b8c-43dc-a278-27462e3a2d66 84147 1 2023-06-27 16:40:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-06-27 16:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f4028 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-27 16:40:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-06-27 16:40:02 +0000 UTC,LastTransitionTime:2023-06-27 16:40:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 27 16:40:04.311: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-4642  ff4cfe8e-247a-4214-a73b-b520a3d1a4f6 84137 1 2023-06-27 16:40:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 3e830543-2b8c-43dc-a278-27462e3a2d66 0xc0037f43f7 0xc0037f43f8}] [] [{kube-controller-manager Update apps/v1 2023-06-27 16:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3e830543-2b8c-43dc-a278-27462e3a2d66\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0037f44a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 16:40:04.329: INFO: Pod "test-cleanup-deployment-69cb9c5497-lb8cz" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-lb8cz test-cleanup-deployment-69cb9c5497- deployment-4642  e24101b0-ccb6-4b08-811f-f6e3082401d9 84135 0 2023-06-27 16:40:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[cni.projectcalico.org/containerID:837ca1885211d376bb7c616bdd7b4ca75533d0a2dbd9eadf5205db61364eb82c cni.projectcalico.org/podIP:172.30.60.70/32 cni.projectcalico.org/podIPs:172.30.60.70/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.70"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.70"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 ff4cfe8e-247a-4214-a73b-b520a3d1a4f6 0xc0037f4867 0xc0037f4868}] [] [{kube-controller-manager Update v1 2023-06-27 16:40:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff4cfe8e-247a-4214-a73b-b520a3d1a4f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 16:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 16:40:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 16:40:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.70\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g7gnl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g7gnl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c0,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-8w847,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:00 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:00 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.70,StartTime:2023-06-27 16:40:00 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 16:40:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://51f6c86a7681d1f2856bca202ee017a977dea3502fc5e3c1cd457f7d0e62de7f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.70,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 27 16:40:04.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4642" for this suite. 06/27/23 16:40:04.363
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:40:04.39
Jun 27 16:40:04.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename deployment 06/27/23 16:40:04.393
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:04.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:04.465
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Jun 27 16:40:04.527: INFO: Pod name rollover-pod: Found 0 pods out of 1
Jun 27 16:40:09.553: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/27/23 16:40:09.553
Jun 27 16:40:09.553: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Jun 27 16:40:11.569: INFO: Creating deployment "test-rollover-deployment"
Jun 27 16:40:11.619: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Jun 27 16:40:13.656: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Jun 27 16:40:13.695: INFO: Ensure that both replica sets have 1 created replica
Jun 27 16:40:13.724: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Jun 27 16:40:13.773: INFO: Updating deployment test-rollover-deployment
Jun 27 16:40:13.773: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Jun 27 16:40:15.810: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Jun 27 16:40:15.842: INFO: Make sure deployment "test-rollover-deployment" is complete
Jun 27 16:40:15.877: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 16:40:15.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 16:40:17.911: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 16:40:17.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 16:40:19.916: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 16:40:19.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 16:40:21.916: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 16:40:21.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 16:40:23.932: INFO: all replica sets need to contain the pod-template-hash label
Jun 27 16:40:23.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 16:40:25.913: INFO: 
Jun 27 16:40:25.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 16:40:27.916: INFO: 
Jun 27 16:40:27.916: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 27 16:40:27.964: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-627  b7ce26c4-27e0-41df-b8f4-bfd090eea016 84411 2 2023-06-27 16:40:11 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b12328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-27 16:40:11 +0000 UTC,LastTransitionTime:2023-06-27 16:40:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-06-27 16:40:25 +0000 UTC,LastTransitionTime:2023-06-27 16:40:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 27 16:40:27.980: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-627  7ae42869-9386-46fe-9cc7-540838b9d933 84401 2 2023-06-27 16:40:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b7ce26c4-27e0-41df-b8f4-bfd090eea016 0xc003f80437 0xc003f80438}] [] [{kube-controller-manager Update apps/v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7ce26c4-27e0-41df-b8f4-bfd090eea016\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f804e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 27 16:40:27.980: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Jun 27 16:40:27.981: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-627  430bfad6-6fa6-4d4d-b846-2e88c2017d16 84409 2 2023-06-27 16:40:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b7ce26c4-27e0-41df-b8f4-bfd090eea016 0xc003f801d7 0xc003f801d8}] [] [{e2e.test Update apps/v1 2023-06-27 16:40:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7ce26c4-27e0-41df-b8f4-bfd090eea016\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f802a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 27 16:40:27.981: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-627  3b035f15-2ce2-4330-87a7-70d474d603e7 84331 2 2023-06-27 16:40:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b7ce26c4-27e0-41df-b8f4-bfd090eea016 0xc003f80317 0xc003f80318}] [] [{kube-controller-manager Update apps/v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7ce26c4-27e0-41df-b8f4-bfd090eea016\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f803c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 27 16:40:28.000: INFO: Pod "test-rollover-deployment-6d45fd857b-6wrqr" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-6wrqr test-rollover-deployment-6d45fd857b- deployment-627  00481924-f0b7-4395-88e0-14e31dff8b6c 84361 0 2023-06-27 16:40:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:79d49980c747b1a718a96611793caa806ef623ddbdc4873513e4b748d84c0218 cni.projectcalico.org/podIP:172.30.60.85/32 cni.projectcalico.org/podIPs:172.30.60.85/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.85"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.85"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 7ae42869-9386-46fe-9cc7-540838b9d933 0xc003f80ca7 0xc003f80ca8}] [] [{kube-controller-manager Update v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ae42869-9386-46fe-9cc7-540838b9d933\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 16:40:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 16:40:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 16:40:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67nkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67nkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c5,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gkmqn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.85,StartTime:2023-06-27 16:40:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 16:40:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://157a7b9c31b10f465848c4e77985821e3c6bee3105834a27a0b798d62856a509,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 27 16:40:28.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-627" for this suite. 06/27/23 16:40:28.028
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":67,"skipped":1300,"failed":0}
------------------------------
â€¢ [SLOW TEST] [23.665 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:40:04.39
    Jun 27 16:40:04.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename deployment 06/27/23 16:40:04.393
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:04.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:04.465
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Jun 27 16:40:04.527: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Jun 27 16:40:09.553: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/27/23 16:40:09.553
    Jun 27 16:40:09.553: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Jun 27 16:40:11.569: INFO: Creating deployment "test-rollover-deployment"
    Jun 27 16:40:11.619: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Jun 27 16:40:13.656: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Jun 27 16:40:13.695: INFO: Ensure that both replica sets have 1 created replica
    Jun 27 16:40:13.724: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Jun 27 16:40:13.773: INFO: Updating deployment test-rollover-deployment
    Jun 27 16:40:13.773: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Jun 27 16:40:15.810: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Jun 27 16:40:15.842: INFO: Make sure deployment "test-rollover-deployment" is complete
    Jun 27 16:40:15.877: INFO: all replica sets need to contain the pod-template-hash label
    Jun 27 16:40:15.877: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:1, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 16:40:17.911: INFO: all replica sets need to contain the pod-template-hash label
    Jun 27 16:40:17.911: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 16:40:19.916: INFO: all replica sets need to contain the pod-template-hash label
    Jun 27 16:40:19.916: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 16:40:21.916: INFO: all replica sets need to contain the pod-template-hash label
    Jun 27 16:40:21.917: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 16:40:23.932: INFO: all replica sets need to contain the pod-template-hash label
    Jun 27 16:40:23.932: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 16:40:25.913: INFO: 
    Jun 27 16:40:25.913: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 40, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 40, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 16:40:27.916: INFO: 
    Jun 27 16:40:27.916: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 27 16:40:27.964: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-627  b7ce26c4-27e0-41df-b8f4-bfd090eea016 84411 2 2023-06-27 16:40:11 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002b12328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-27 16:40:11 +0000 UTC,LastTransitionTime:2023-06-27 16:40:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-06-27 16:40:25 +0000 UTC,LastTransitionTime:2023-06-27 16:40:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 27 16:40:27.980: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-627  7ae42869-9386-46fe-9cc7-540838b9d933 84401 2 2023-06-27 16:40:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment b7ce26c4-27e0-41df-b8f4-bfd090eea016 0xc003f80437 0xc003f80438}] [] [{kube-controller-manager Update apps/v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7ce26c4-27e0-41df-b8f4-bfd090eea016\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:25 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f804e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 16:40:27.980: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Jun 27 16:40:27.981: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-627  430bfad6-6fa6-4d4d-b846-2e88c2017d16 84409 2 2023-06-27 16:40:04 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment b7ce26c4-27e0-41df-b8f4-bfd090eea016 0xc003f801d7 0xc003f801d8}] [] [{e2e.test Update apps/v1 2023-06-27 16:40:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7ce26c4-27e0-41df-b8f4-bfd090eea016\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003f802a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 16:40:27.981: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-627  3b035f15-2ce2-4330-87a7-70d474d603e7 84331 2 2023-06-27 16:40:11 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment b7ce26c4-27e0-41df-b8f4-bfd090eea016 0xc003f80317 0xc003f80318}] [] [{kube-controller-manager Update apps/v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7ce26c4-27e0-41df-b8f4-bfd090eea016\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003f803c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 16:40:28.000: INFO: Pod "test-rollover-deployment-6d45fd857b-6wrqr" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-6wrqr test-rollover-deployment-6d45fd857b- deployment-627  00481924-f0b7-4395-88e0-14e31dff8b6c 84361 0 2023-06-27 16:40:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[cni.projectcalico.org/containerID:79d49980c747b1a718a96611793caa806ef623ddbdc4873513e4b748d84c0218 cni.projectcalico.org/podIP:172.30.60.85/32 cni.projectcalico.org/podIPs:172.30.60.85/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.85"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.85"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b 7ae42869-9386-46fe-9cc7-540838b9d933 0xc003f80ca7 0xc003f80ca8}] [] [{kube-controller-manager Update v1 2023-06-27 16:40:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ae42869-9386-46fe-9cc7-540838b9d933\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 16:40:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 16:40:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 16:40:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.85\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67nkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67nkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c40,c5,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-gkmqn,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 16:40:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.85,StartTime:2023-06-27 16:40:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 16:40:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://157a7b9c31b10f465848c4e77985821e3c6bee3105834a27a0b798d62856a509,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.85,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 27 16:40:28.000: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-627" for this suite. 06/27/23 16:40:28.028
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:40:28.058
Jun 27 16:40:28.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename podtemplate 06/27/23 16:40:28.061
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:28.109
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:28.127
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 06/27/23 16:40:28.142
STEP: Replace a pod template 06/27/23 16:40:28.168
Jun 27 16:40:28.211: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jun 27 16:40:28.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9972" for this suite. 06/27/23 16:40:28.234
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":68,"skipped":1308,"failed":0}
------------------------------
â€¢ [0.202 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:40:28.058
    Jun 27 16:40:28.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename podtemplate 06/27/23 16:40:28.061
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:28.109
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:28.127
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 06/27/23 16:40:28.142
    STEP: Replace a pod template 06/27/23 16:40:28.168
    Jun 27 16:40:28.211: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jun 27 16:40:28.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9972" for this suite. 06/27/23 16:40:28.234
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:40:28.267
Jun 27 16:40:28.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:40:28.269
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:28.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:28.339
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-9c8ed443-9c43-49b9-93ed-6f9ef4ee084a 06/27/23 16:40:28.354
STEP: Creating a pod to test consume configMaps 06/27/23 16:40:28.379
Jun 27 16:40:28.444: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e" in namespace "projected-3144" to be "Succeeded or Failed"
Jun 27 16:40:28.462: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.931601ms
Jun 27 16:40:30.481: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036593513s
Jun 27 16:40:32.482: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037903495s
Jun 27 16:40:34.481: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037161297s
STEP: Saw pod success 06/27/23 16:40:34.482
Jun 27 16:40:34.482: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e" satisfied condition "Succeeded or Failed"
Jun 27 16:40:34.501: INFO: Trying to get logs from node 10.113.180.89 pod pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e container agnhost-container: <nil>
STEP: delete the pod 06/27/23 16:40:34.545
Jun 27 16:40:34.626: INFO: Waiting for pod pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e to disappear
Jun 27 16:40:34.646: INFO: Pod pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 27 16:40:34.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3144" for this suite. 06/27/23 16:40:34.683
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":69,"skipped":1338,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.443 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:40:28.267
    Jun 27 16:40:28.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:40:28.269
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:28.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:28.339
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-9c8ed443-9c43-49b9-93ed-6f9ef4ee084a 06/27/23 16:40:28.354
    STEP: Creating a pod to test consume configMaps 06/27/23 16:40:28.379
    Jun 27 16:40:28.444: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e" in namespace "projected-3144" to be "Succeeded or Failed"
    Jun 27 16:40:28.462: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e": Phase="Pending", Reason="", readiness=false. Elapsed: 17.931601ms
    Jun 27 16:40:30.481: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036593513s
    Jun 27 16:40:32.482: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037903495s
    Jun 27 16:40:34.481: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037161297s
    STEP: Saw pod success 06/27/23 16:40:34.482
    Jun 27 16:40:34.482: INFO: Pod "pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e" satisfied condition "Succeeded or Failed"
    Jun 27 16:40:34.501: INFO: Trying to get logs from node 10.113.180.89 pod pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 16:40:34.545
    Jun 27 16:40:34.626: INFO: Waiting for pod pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e to disappear
    Jun 27 16:40:34.646: INFO: Pod pod-projected-configmaps-d565f3c3-eb4a-484b-af8a-2ce19b0cc55e no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 27 16:40:34.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3144" for this suite. 06/27/23 16:40:34.683
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:40:34.711
Jun 27 16:40:34.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename watch 06/27/23 16:40:34.712
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:34.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:34.776
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 06/27/23 16:40:34.79
STEP: starting a background goroutine to produce watch events 06/27/23 16:40:34.807
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/27/23 16:40:34.807
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 27 16:40:37.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-3262" for this suite. 06/27/23 16:40:37.591
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":70,"skipped":1339,"failed":0}
------------------------------
â€¢ [2.928 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:40:34.711
    Jun 27 16:40:34.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename watch 06/27/23 16:40:34.712
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:34.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:34.776
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 06/27/23 16:40:34.79
    STEP: starting a background goroutine to produce watch events 06/27/23 16:40:34.807
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 06/27/23 16:40:34.807
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 27 16:40:37.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-3262" for this suite. 06/27/23 16:40:37.591
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:40:37.646
Jun 27 16:40:37.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 16:40:37.648
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:37.699
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:37.719
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-083e26e7-cf5a-4201-92c5-1a1a2aabfe0b 06/27/23 16:40:37.735
STEP: Creating a pod to test consume configMaps 06/27/23 16:40:37.785
Jun 27 16:40:37.843: INFO: Waiting up to 5m0s for pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef" in namespace "configmap-481" to be "Succeeded or Failed"
Jun 27 16:40:37.862: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef": Phase="Pending", Reason="", readiness=false. Elapsed: 19.563448ms
Jun 27 16:40:39.889: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046534647s
Jun 27 16:40:41.884: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041315782s
Jun 27 16:40:43.886: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043465845s
STEP: Saw pod success 06/27/23 16:40:43.887
Jun 27 16:40:43.887: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef" satisfied condition "Succeeded or Failed"
Jun 27 16:40:43.908: INFO: Trying to get logs from node 10.113.180.96 pod pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef container agnhost-container: <nil>
STEP: delete the pod 06/27/23 16:40:43.95
Jun 27 16:40:44.009: INFO: Waiting for pod pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef to disappear
Jun 27 16:40:44.028: INFO: Pod pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 16:40:44.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-481" for this suite. 06/27/23 16:40:44.061
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":71,"skipped":1348,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.443 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:40:37.646
    Jun 27 16:40:37.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 16:40:37.648
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:37.699
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:37.719
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-083e26e7-cf5a-4201-92c5-1a1a2aabfe0b 06/27/23 16:40:37.735
    STEP: Creating a pod to test consume configMaps 06/27/23 16:40:37.785
    Jun 27 16:40:37.843: INFO: Waiting up to 5m0s for pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef" in namespace "configmap-481" to be "Succeeded or Failed"
    Jun 27 16:40:37.862: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef": Phase="Pending", Reason="", readiness=false. Elapsed: 19.563448ms
    Jun 27 16:40:39.889: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046534647s
    Jun 27 16:40:41.884: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef": Phase="Pending", Reason="", readiness=false. Elapsed: 4.041315782s
    Jun 27 16:40:43.886: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043465845s
    STEP: Saw pod success 06/27/23 16:40:43.887
    Jun 27 16:40:43.887: INFO: Pod "pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef" satisfied condition "Succeeded or Failed"
    Jun 27 16:40:43.908: INFO: Trying to get logs from node 10.113.180.96 pod pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 16:40:43.95
    Jun 27 16:40:44.009: INFO: Waiting for pod pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef to disappear
    Jun 27 16:40:44.028: INFO: Pod pod-configmaps-329ba2be-8d13-4f32-93d6-61b3e2b147ef no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 16:40:44.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-481" for this suite. 06/27/23 16:40:44.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:40:44.094
Jun 27 16:40:44.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 16:40:44.097
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:44.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:44.17
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-8642 06/27/23 16:40:44.189
STEP: creating replication controller nodeport-test in namespace services-8642 06/27/23 16:40:44.244
I0627 16:40:44.275025      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8642, replica count: 2
I0627 16:40:47.327375      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 16:40:47.327: INFO: Creating new exec pod
Jun 27 16:40:47.391: INFO: Waiting up to 5m0s for pod "execpod5ncjn" in namespace "services-8642" to be "running"
Jun 27 16:40:47.418: INFO: Pod "execpod5ncjn": Phase="Pending", Reason="", readiness=false. Elapsed: 26.794665ms
Jun 27 16:40:49.443: INFO: Pod "execpod5ncjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.051270092s
Jun 27 16:40:49.443: INFO: Pod "execpod5ncjn" satisfied condition "running"
Jun 27 16:40:50.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 27 16:40:50.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 27 16:40:50.865: INFO: stdout: ""
Jun 27 16:40:51.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 27 16:40:52.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 27 16:40:52.240: INFO: stdout: ""
Jun 27 16:40:52.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 27 16:40:53.246: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 27 16:40:53.246: INFO: stdout: ""
Jun 27 16:40:53.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 27 16:40:54.509: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 27 16:40:54.509: INFO: stdout: ""
Jun 27 16:40:54.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 27 16:40:55.375: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 27 16:40:55.375: INFO: stdout: ""
Jun 27 16:40:55.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 27 16:40:56.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 27 16:40:56.232: INFO: stdout: ""
Jun 27 16:40:56.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Jun 27 16:40:57.216: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Jun 27 16:40:57.216: INFO: stdout: "nodeport-test-shkvm"
Jun 27 16:40:57.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.239.94 80'
Jun 27 16:40:57.691: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.239.94 80\nConnection to 172.21.239.94 80 port [tcp/http] succeeded!\n"
Jun 27 16:40:57.691: INFO: stdout: "nodeport-test-jkczk"
Jun 27 16:40:57.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.96 32491'
Jun 27 16:40:58.118: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.96 32491\nConnection to 10.113.180.96 32491 port [tcp/*] succeeded!\n"
Jun 27 16:40:58.118: INFO: stdout: "nodeport-test-jkczk"
Jun 27 16:40:58.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.90 32491'
Jun 27 16:40:58.500: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.90 32491\nConnection to 10.113.180.90 32491 port [tcp/*] succeeded!\n"
Jun 27 16:40:58.500: INFO: stdout: "nodeport-test-jkczk"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 16:40:58.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8642" for this suite. 06/27/23 16:40:58.534
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":72,"skipped":1387,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.484 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:40:44.094
    Jun 27 16:40:44.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 16:40:44.097
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:44.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:44.17
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-8642 06/27/23 16:40:44.189
    STEP: creating replication controller nodeport-test in namespace services-8642 06/27/23 16:40:44.244
    I0627 16:40:44.275025      23 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-8642, replica count: 2
    I0627 16:40:47.327375      23 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 27 16:40:47.327: INFO: Creating new exec pod
    Jun 27 16:40:47.391: INFO: Waiting up to 5m0s for pod "execpod5ncjn" in namespace "services-8642" to be "running"
    Jun 27 16:40:47.418: INFO: Pod "execpod5ncjn": Phase="Pending", Reason="", readiness=false. Elapsed: 26.794665ms
    Jun 27 16:40:49.443: INFO: Pod "execpod5ncjn": Phase="Running", Reason="", readiness=true. Elapsed: 2.051270092s
    Jun 27 16:40:49.443: INFO: Pod "execpod5ncjn" satisfied condition "running"
    Jun 27 16:40:50.478: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 27 16:40:50.865: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 27 16:40:50.865: INFO: stdout: ""
    Jun 27 16:40:51.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 27 16:40:52.240: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 27 16:40:52.240: INFO: stdout: ""
    Jun 27 16:40:52.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 27 16:40:53.246: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 27 16:40:53.246: INFO: stdout: ""
    Jun 27 16:40:53.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 27 16:40:54.509: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 27 16:40:54.509: INFO: stdout: ""
    Jun 27 16:40:54.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 27 16:40:55.375: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 27 16:40:55.375: INFO: stdout: ""
    Jun 27 16:40:55.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 27 16:40:56.231: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 27 16:40:56.232: INFO: stdout: ""
    Jun 27 16:40:56.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Jun 27 16:40:57.216: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Jun 27 16:40:57.216: INFO: stdout: "nodeport-test-shkvm"
    Jun 27 16:40:57.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.239.94 80'
    Jun 27 16:40:57.691: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.239.94 80\nConnection to 172.21.239.94 80 port [tcp/http] succeeded!\n"
    Jun 27 16:40:57.691: INFO: stdout: "nodeport-test-jkczk"
    Jun 27 16:40:57.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.96 32491'
    Jun 27 16:40:58.118: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.96 32491\nConnection to 10.113.180.96 32491 port [tcp/*] succeeded!\n"
    Jun 27 16:40:58.118: INFO: stdout: "nodeport-test-jkczk"
    Jun 27 16:40:58.118: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-8642 exec execpod5ncjn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.90 32491'
    Jun 27 16:40:58.500: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.90 32491\nConnection to 10.113.180.90 32491 port [tcp/*] succeeded!\n"
    Jun 27 16:40:58.500: INFO: stdout: "nodeport-test-jkczk"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 16:40:58.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8642" for this suite. 06/27/23 16:40:58.534
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:40:58.584
Jun 27 16:40:58.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 16:40:58.586
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:58.635
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:58.65
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
Jun 27 16:40:58.706: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-b597de6f-afe1-4856-809c-e907e16bcae0 06/27/23 16:40:58.706
STEP: Creating the pod 06/27/23 16:40:58.727
Jun 27 16:40:58.788: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28" in namespace "configmap-689" to be "running"
Jun 27 16:40:58.816: INFO: Pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28": Phase="Pending", Reason="", readiness=false. Elapsed: 27.634416ms
Jun 27 16:41:00.840: INFO: Pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052002812s
Jun 27 16:41:02.854: INFO: Pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28": Phase="Running", Reason="", readiness=false. Elapsed: 4.066158343s
Jun 27 16:41:02.854: INFO: Pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28" satisfied condition "running"
STEP: Waiting for pod with text data 06/27/23 16:41:02.854
STEP: Waiting for pod with binary data 06/27/23 16:41:02.964
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 16:41:03.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-689" for this suite. 06/27/23 16:41:03.032
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":73,"skipped":1410,"failed":0}
------------------------------
â€¢ [4.477 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:40:58.584
    Jun 27 16:40:58.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 16:40:58.586
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:40:58.635
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:40:58.65
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    Jun 27 16:40:58.706: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-b597de6f-afe1-4856-809c-e907e16bcae0 06/27/23 16:40:58.706
    STEP: Creating the pod 06/27/23 16:40:58.727
    Jun 27 16:40:58.788: INFO: Waiting up to 5m0s for pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28" in namespace "configmap-689" to be "running"
    Jun 27 16:40:58.816: INFO: Pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28": Phase="Pending", Reason="", readiness=false. Elapsed: 27.634416ms
    Jun 27 16:41:00.840: INFO: Pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052002812s
    Jun 27 16:41:02.854: INFO: Pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28": Phase="Running", Reason="", readiness=false. Elapsed: 4.066158343s
    Jun 27 16:41:02.854: INFO: Pod "pod-configmaps-4b8d32e5-7dcb-4ca7-bc84-280618299b28" satisfied condition "running"
    STEP: Waiting for pod with text data 06/27/23 16:41:02.854
    STEP: Waiting for pod with binary data 06/27/23 16:41:02.964
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 16:41:03.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-689" for this suite. 06/27/23 16:41:03.032
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:41:03.061
Jun 27 16:41:03.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:41:03.071
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:03.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:03.135
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-62ff43a7-b36a-412a-9f19-cc7283111705 06/27/23 16:41:03.161
STEP: Creating a pod to test consume configMaps 06/27/23 16:41:03.185
Jun 27 16:41:03.248: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a" in namespace "projected-9157" to be "Succeeded or Failed"
Jun 27 16:41:03.271: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.284211ms
Jun 27 16:41:05.291: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042284057s
Jun 27 16:41:07.291: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042608605s
Jun 27 16:41:09.293: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044364931s
STEP: Saw pod success 06/27/23 16:41:09.293
Jun 27 16:41:09.293: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a" satisfied condition "Succeeded or Failed"
Jun 27 16:41:09.314: INFO: Trying to get logs from node 10.113.180.96 pod pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a container agnhost-container: <nil>
STEP: delete the pod 06/27/23 16:41:09.357
Jun 27 16:41:09.422: INFO: Waiting for pod pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a to disappear
Jun 27 16:41:09.439: INFO: Pod pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 27 16:41:09.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9157" for this suite. 06/27/23 16:41:09.471
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":74,"skipped":1414,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.449 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:41:03.061
    Jun 27 16:41:03.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:41:03.071
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:03.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:03.135
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-62ff43a7-b36a-412a-9f19-cc7283111705 06/27/23 16:41:03.161
    STEP: Creating a pod to test consume configMaps 06/27/23 16:41:03.185
    Jun 27 16:41:03.248: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a" in namespace "projected-9157" to be "Succeeded or Failed"
    Jun 27 16:41:03.271: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 22.284211ms
    Jun 27 16:41:05.291: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042284057s
    Jun 27 16:41:07.291: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042608605s
    Jun 27 16:41:09.293: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044364931s
    STEP: Saw pod success 06/27/23 16:41:09.293
    Jun 27 16:41:09.293: INFO: Pod "pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a" satisfied condition "Succeeded or Failed"
    Jun 27 16:41:09.314: INFO: Trying to get logs from node 10.113.180.96 pod pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 16:41:09.357
    Jun 27 16:41:09.422: INFO: Waiting for pod pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a to disappear
    Jun 27 16:41:09.439: INFO: Pod pod-projected-configmaps-57467dd4-830a-4fa5-82fe-f1de78bafc6a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 27 16:41:09.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9157" for this suite. 06/27/23 16:41:09.471
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:41:09.523
Jun 27 16:41:09.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename statefulset 06/27/23 16:41:09.525
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:09.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:09.612
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5660 06/27/23 16:41:09.627
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Jun 27 16:41:09.760: INFO: Found 0 stateful pods, waiting for 1
Jun 27 16:41:19.780: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 06/27/23 16:41:19.812
W0627 16:41:19.846227      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun 27 16:41:19.879: INFO: Found 1 stateful pods, waiting for 2
Jun 27 16:41:29.916: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 16:41:29.916: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 06/27/23 16:41:29.951
STEP: Delete all of the StatefulSets 06/27/23 16:41:29.985
STEP: Verify that StatefulSets have been deleted 06/27/23 16:41:30.018
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 27 16:41:30.042: INFO: Deleting all statefulset in ns statefulset-5660
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 27 16:41:30.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5660" for this suite. 06/27/23 16:41:30.121
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":75,"skipped":1457,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.643 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:41:09.523
    Jun 27 16:41:09.524: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename statefulset 06/27/23 16:41:09.525
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:09.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:09.612
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5660 06/27/23 16:41:09.627
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Jun 27 16:41:09.760: INFO: Found 0 stateful pods, waiting for 1
    Jun 27 16:41:19.780: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 06/27/23 16:41:19.812
    W0627 16:41:19.846227      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun 27 16:41:19.879: INFO: Found 1 stateful pods, waiting for 2
    Jun 27 16:41:29.916: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 16:41:29.916: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 06/27/23 16:41:29.951
    STEP: Delete all of the StatefulSets 06/27/23 16:41:29.985
    STEP: Verify that StatefulSets have been deleted 06/27/23 16:41:30.018
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 27 16:41:30.042: INFO: Deleting all statefulset in ns statefulset-5660
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 27 16:41:30.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5660" for this suite. 06/27/23 16:41:30.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:41:30.177
Jun 27 16:41:30.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename dns 06/27/23 16:41:30.179
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:30.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:30.241
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 06/27/23 16:41:30.259
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9061 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9061;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9061 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9061;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9061.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9061.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9061.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9061.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9061.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9061.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9061.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9061.svc;check="$$(dig +notcp +noall +answer +search 172.35.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.35.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.35.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.35.172_tcp@PTR;sleep 1; done
 06/27/23 16:41:30.319
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9061 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9061;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9061 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9061;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9061.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9061.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9061.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9061.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9061.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9061.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9061.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9061.svc;check="$$(dig +notcp +noall +answer +search 172.35.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.35.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.35.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.35.172_tcp@PTR;sleep 1; done
 06/27/23 16:41:30.319
STEP: creating a pod to probe DNS 06/27/23 16:41:30.319
STEP: submitting the pod to kubernetes 06/27/23 16:41:30.32
Jun 27 16:41:30.383: INFO: Waiting up to 15m0s for pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288" in namespace "dns-9061" to be "running"
Jun 27 16:41:30.407: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 23.814496ms
Jun 27 16:41:32.431: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047587489s
Jun 27 16:41:34.428: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044877737s
Jun 27 16:41:36.428: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044806248s
Jun 27 16:41:38.427: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044350847s
Jun 27 16:41:40.429: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Running", Reason="", readiness=true. Elapsed: 10.045477156s
Jun 27 16:41:40.429: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288" satisfied condition "running"
STEP: retrieving the pod 06/27/23 16:41:40.429
STEP: looking for the results for each expected name from probers 06/27/23 16:41:40.449
Jun 27 16:41:40.490: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.520: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.555: INFO: Unable to read wheezy_udp@dns-test-service.dns-9061 from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.606: INFO: Unable to read wheezy_udp@dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.630: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.654: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.807: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.861: INFO: Unable to read jessie_udp@dns-test-service.dns-9061 from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.887: INFO: Unable to read jessie_tcp@dns-test-service.dns-9061 from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.917: INFO: Unable to read jessie_udp@dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.956: INFO: Unable to read jessie_tcp@dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:40.984: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
Jun 27 16:41:41.110: INFO: Lookups using dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9061 wheezy_udp@dns-test-service.dns-9061.svc wheezy_tcp@dns-test-service.dns-9061.svc wheezy_udp@_http._tcp.dns-test-service.dns-9061.svc jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-9061 jessie_tcp@dns-test-service.dns-9061 jessie_udp@dns-test-service.dns-9061.svc jessie_tcp@dns-test-service.dns-9061.svc jessie_udp@_http._tcp.dns-test-service.dns-9061.svc]

Jun 27 16:41:46.728: INFO: DNS probes using dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288 succeeded

STEP: deleting the pod 06/27/23 16:41:46.728
STEP: deleting the test service 06/27/23 16:41:46.803
STEP: deleting the test headless service 06/27/23 16:41:46.862
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 27 16:41:46.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9061" for this suite. 06/27/23 16:41:46.943
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":76,"skipped":1530,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.792 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:41:30.177
    Jun 27 16:41:30.177: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename dns 06/27/23 16:41:30.179
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:30.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:30.241
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 06/27/23 16:41:30.259
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9061 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9061;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9061 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9061;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9061.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-9061.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9061.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-9061.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-9061.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-9061.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-9061.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-9061.svc;check="$$(dig +notcp +noall +answer +search 172.35.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.35.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.35.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.35.172_tcp@PTR;sleep 1; done
     06/27/23 16:41:30.319
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9061 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9061;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9061 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9061;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-9061.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-9061.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-9061.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-9061.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-9061.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-9061.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-9061.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-9061.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-9061.svc;check="$$(dig +notcp +noall +answer +search 172.35.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.35.172_udp@PTR;check="$$(dig +tcp +noall +answer +search 172.35.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.35.172_tcp@PTR;sleep 1; done
     06/27/23 16:41:30.319
    STEP: creating a pod to probe DNS 06/27/23 16:41:30.319
    STEP: submitting the pod to kubernetes 06/27/23 16:41:30.32
    Jun 27 16:41:30.383: INFO: Waiting up to 15m0s for pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288" in namespace "dns-9061" to be "running"
    Jun 27 16:41:30.407: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 23.814496ms
    Jun 27 16:41:32.431: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047587489s
    Jun 27 16:41:34.428: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044877737s
    Jun 27 16:41:36.428: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044806248s
    Jun 27 16:41:38.427: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Pending", Reason="", readiness=false. Elapsed: 8.044350847s
    Jun 27 16:41:40.429: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288": Phase="Running", Reason="", readiness=true. Elapsed: 10.045477156s
    Jun 27 16:41:40.429: INFO: Pod "dns-test-3c303f24-9973-4497-8a43-f68e3322b288" satisfied condition "running"
    STEP: retrieving the pod 06/27/23 16:41:40.429
    STEP: looking for the results for each expected name from probers 06/27/23 16:41:40.449
    Jun 27 16:41:40.490: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.520: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.555: INFO: Unable to read wheezy_udp@dns-test-service.dns-9061 from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.606: INFO: Unable to read wheezy_udp@dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.630: INFO: Unable to read wheezy_tcp@dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.654: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.807: INFO: Unable to read jessie_udp@dns-test-service from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.861: INFO: Unable to read jessie_udp@dns-test-service.dns-9061 from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.887: INFO: Unable to read jessie_tcp@dns-test-service.dns-9061 from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.917: INFO: Unable to read jessie_udp@dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.956: INFO: Unable to read jessie_tcp@dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:40.984: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-9061.svc from pod dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288: the server could not find the requested resource (get pods dns-test-3c303f24-9973-4497-8a43-f68e3322b288)
    Jun 27 16:41:41.110: INFO: Lookups using dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-9061 wheezy_udp@dns-test-service.dns-9061.svc wheezy_tcp@dns-test-service.dns-9061.svc wheezy_udp@_http._tcp.dns-test-service.dns-9061.svc jessie_udp@dns-test-service jessie_udp@dns-test-service.dns-9061 jessie_tcp@dns-test-service.dns-9061 jessie_udp@dns-test-service.dns-9061.svc jessie_tcp@dns-test-service.dns-9061.svc jessie_udp@_http._tcp.dns-test-service.dns-9061.svc]

    Jun 27 16:41:46.728: INFO: DNS probes using dns-9061/dns-test-3c303f24-9973-4497-8a43-f68e3322b288 succeeded

    STEP: deleting the pod 06/27/23 16:41:46.728
    STEP: deleting the test service 06/27/23 16:41:46.803
    STEP: deleting the test headless service 06/27/23 16:41:46.862
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 27 16:41:46.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9061" for this suite. 06/27/23 16:41:46.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:41:46.98
Jun 27 16:41:46.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename endpointslicemirroring 06/27/23 16:41:46.981
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:47.028
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:47.042
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 06/27/23 16:41:47.098
Jun 27 16:41:47.137: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 06/27/23 16:41:49.157
STEP: mirroring deletion of a custom Endpoint 06/27/23 16:41:49.197
Jun 27 16:41:49.240: INFO: Waiting for 0 EndpointSlices to exist, got 1
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Jun 27 16:41:51.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-9372" for this suite. 06/27/23 16:41:51.293
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":77,"skipped":1614,"failed":0}
------------------------------
â€¢ [4.339 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:41:46.98
    Jun 27 16:41:46.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename endpointslicemirroring 06/27/23 16:41:46.981
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:47.028
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:47.042
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 06/27/23 16:41:47.098
    Jun 27 16:41:47.137: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 06/27/23 16:41:49.157
    STEP: mirroring deletion of a custom Endpoint 06/27/23 16:41:49.197
    Jun 27 16:41:49.240: INFO: Waiting for 0 EndpointSlices to exist, got 1
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Jun 27 16:41:51.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-9372" for this suite. 06/27/23 16:41:51.293
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:41:51.32
Jun 27 16:41:51.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename dns 06/27/23 16:41:51.323
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:51.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:51.392
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 06/27/23 16:41:51.406
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3512.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3512.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 06/27/23 16:41:51.425
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3512.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3512.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 06/27/23 16:41:51.426
STEP: creating a pod to probe DNS 06/27/23 16:41:51.426
STEP: submitting the pod to kubernetes 06/27/23 16:41:51.426
Jun 27 16:41:51.494: INFO: Waiting up to 15m0s for pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902" in namespace "dns-3512" to be "running"
Jun 27 16:41:51.519: INFO: Pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902": Phase="Pending", Reason="", readiness=false. Elapsed: 24.926851ms
Jun 27 16:41:53.538: INFO: Pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043094637s
Jun 27 16:41:55.539: INFO: Pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902": Phase="Running", Reason="", readiness=true. Elapsed: 4.044463802s
Jun 27 16:41:55.539: INFO: Pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902" satisfied condition "running"
STEP: retrieving the pod 06/27/23 16:41:55.539
STEP: looking for the results for each expected name from probers 06/27/23 16:41:55.559
Jun 27 16:41:55.725: INFO: DNS probes using dns-3512/dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902 succeeded

STEP: deleting the pod 06/27/23 16:41:55.725
STEP: deleting the test headless service 06/27/23 16:41:55.782
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 27 16:41:55.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3512" for this suite. 06/27/23 16:41:55.92
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":78,"skipped":1622,"failed":0}
------------------------------
â€¢ [4.633 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:41:51.32
    Jun 27 16:41:51.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename dns 06/27/23 16:41:51.323
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:51.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:51.392
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 06/27/23 16:41:51.406
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3512.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-3512.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     06/27/23 16:41:51.425
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-3512.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-3512.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     06/27/23 16:41:51.426
    STEP: creating a pod to probe DNS 06/27/23 16:41:51.426
    STEP: submitting the pod to kubernetes 06/27/23 16:41:51.426
    Jun 27 16:41:51.494: INFO: Waiting up to 15m0s for pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902" in namespace "dns-3512" to be "running"
    Jun 27 16:41:51.519: INFO: Pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902": Phase="Pending", Reason="", readiness=false. Elapsed: 24.926851ms
    Jun 27 16:41:53.538: INFO: Pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043094637s
    Jun 27 16:41:55.539: INFO: Pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902": Phase="Running", Reason="", readiness=true. Elapsed: 4.044463802s
    Jun 27 16:41:55.539: INFO: Pod "dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902" satisfied condition "running"
    STEP: retrieving the pod 06/27/23 16:41:55.539
    STEP: looking for the results for each expected name from probers 06/27/23 16:41:55.559
    Jun 27 16:41:55.725: INFO: DNS probes using dns-3512/dns-test-710e41d7-fc14-4ccb-977a-f5090eb74902 succeeded

    STEP: deleting the pod 06/27/23 16:41:55.725
    STEP: deleting the test headless service 06/27/23 16:41:55.782
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 27 16:41:55.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3512" for this suite. 06/27/23 16:41:55.92
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:41:55.96
Jun 27 16:41:55.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 16:41:55.964
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:56.053
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:56.074
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-f07e2632-a9f5-462a-adf0-e4af6631b34b 06/27/23 16:41:56.091
STEP: Creating a pod to test consume secrets 06/27/23 16:41:56.123
Jun 27 16:41:56.200: INFO: Waiting up to 5m0s for pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a" in namespace "secrets-8131" to be "Succeeded or Failed"
Jun 27 16:41:56.226: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.147298ms
Jun 27 16:41:58.246: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046367816s
Jun 27 16:42:00.249: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049044013s
Jun 27 16:42:02.246: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045787725s
STEP: Saw pod success 06/27/23 16:42:02.246
Jun 27 16:42:02.247: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a" satisfied condition "Succeeded or Failed"
Jun 27 16:42:02.266: INFO: Trying to get logs from node 10.113.180.96 pod pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a container secret-volume-test: <nil>
STEP: delete the pod 06/27/23 16:42:02.327
Jun 27 16:42:02.384: INFO: Waiting for pod pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a to disappear
Jun 27 16:42:02.405: INFO: Pod pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 27 16:42:02.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8131" for this suite. 06/27/23 16:42:02.438
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":79,"skipped":1651,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.504 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:41:55.96
    Jun 27 16:41:55.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 16:41:55.964
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:41:56.053
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:41:56.074
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-f07e2632-a9f5-462a-adf0-e4af6631b34b 06/27/23 16:41:56.091
    STEP: Creating a pod to test consume secrets 06/27/23 16:41:56.123
    Jun 27 16:41:56.200: INFO: Waiting up to 5m0s for pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a" in namespace "secrets-8131" to be "Succeeded or Failed"
    Jun 27 16:41:56.226: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 26.147298ms
    Jun 27 16:41:58.246: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046367816s
    Jun 27 16:42:00.249: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.049044013s
    Jun 27 16:42:02.246: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045787725s
    STEP: Saw pod success 06/27/23 16:42:02.246
    Jun 27 16:42:02.247: INFO: Pod "pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a" satisfied condition "Succeeded or Failed"
    Jun 27 16:42:02.266: INFO: Trying to get logs from node 10.113.180.96 pod pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a container secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 16:42:02.327
    Jun 27 16:42:02.384: INFO: Waiting for pod pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a to disappear
    Jun 27 16:42:02.405: INFO: Pod pod-secrets-f2ace91e-fe88-4b66-a029-e322239c3e8a no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 16:42:02.405: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8131" for this suite. 06/27/23 16:42:02.438
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:42:02.464
Jun 27 16:42:02.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:42:02.466
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:02.515
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:02.531
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 06/27/23 16:42:02.547
Jun 27 16:42:02.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 create -f -'
Jun 27 16:42:03.335: INFO: stderr: ""
Jun 27 16:42:03.335: INFO: stdout: "pod/pause created\n"
Jun 27 16:42:03.335: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Jun 27 16:42:03.335: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1954" to be "running and ready"
Jun 27 16:42:03.372: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 36.417775ms
Jun 27 16:42:03.372: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.113.180.90' to be 'Running' but was 'Pending'
Jun 27 16:42:05.392: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056243924s
Jun 27 16:42:05.392: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.113.180.90' to be 'Running' but was 'Pending'
Jun 27 16:42:07.390: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.055222576s
Jun 27 16:42:07.391: INFO: Pod "pause" satisfied condition "running and ready"
Jun 27 16:42:07.391: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 06/27/23 16:42:07.391
Jun 27 16:42:07.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 label pods pause testing-label=testing-label-value'
Jun 27 16:42:07.579: INFO: stderr: ""
Jun 27 16:42:07.579: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 06/27/23 16:42:07.579
Jun 27 16:42:07.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 get pod pause -L testing-label'
Jun 27 16:42:07.704: INFO: stderr: ""
Jun 27 16:42:07.704: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
STEP: removing the label testing-label of a pod 06/27/23 16:42:07.704
Jun 27 16:42:07.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 label pods pause testing-label-'
Jun 27 16:42:07.848: INFO: stderr: ""
Jun 27 16:42:07.848: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 06/27/23 16:42:07.848
Jun 27 16:42:07.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 get pod pause -L testing-label'
Jun 27 16:42:07.970: INFO: stderr: ""
Jun 27 16:42:07.970: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 06/27/23 16:42:07.97
Jun 27 16:42:07.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 delete --grace-period=0 --force -f -'
Jun 27 16:42:08.128: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 16:42:08.128: INFO: stdout: "pod \"pause\" force deleted\n"
Jun 27 16:42:08.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 get rc,svc -l name=pause --no-headers'
Jun 27 16:42:08.287: INFO: stderr: "No resources found in kubectl-1954 namespace.\n"
Jun 27 16:42:08.287: INFO: stdout: ""
Jun 27 16:42:08.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 27 16:42:08.414: INFO: stderr: ""
Jun 27 16:42:08.414: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:42:08.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1954" for this suite. 06/27/23 16:42:08.448
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":80,"skipped":1651,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.013 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:42:02.464
    Jun 27 16:42:02.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:42:02.466
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:02.515
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:02.531
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 06/27/23 16:42:02.547
    Jun 27 16:42:02.548: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 create -f -'
    Jun 27 16:42:03.335: INFO: stderr: ""
    Jun 27 16:42:03.335: INFO: stdout: "pod/pause created\n"
    Jun 27 16:42:03.335: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Jun 27 16:42:03.335: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-1954" to be "running and ready"
    Jun 27 16:42:03.372: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 36.417775ms
    Jun 27 16:42:03.372: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.113.180.90' to be 'Running' but was 'Pending'
    Jun 27 16:42:05.392: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056243924s
    Jun 27 16:42:05.392: INFO: Error evaluating pod condition running and ready: want pod 'pause' on '10.113.180.90' to be 'Running' but was 'Pending'
    Jun 27 16:42:07.390: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 4.055222576s
    Jun 27 16:42:07.391: INFO: Pod "pause" satisfied condition "running and ready"
    Jun 27 16:42:07.391: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 06/27/23 16:42:07.391
    Jun 27 16:42:07.391: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 label pods pause testing-label=testing-label-value'
    Jun 27 16:42:07.579: INFO: stderr: ""
    Jun 27 16:42:07.579: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 06/27/23 16:42:07.579
    Jun 27 16:42:07.579: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 get pod pause -L testing-label'
    Jun 27 16:42:07.704: INFO: stderr: ""
    Jun 27 16:42:07.704: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 06/27/23 16:42:07.704
    Jun 27 16:42:07.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 label pods pause testing-label-'
    Jun 27 16:42:07.848: INFO: stderr: ""
    Jun 27 16:42:07.848: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 06/27/23 16:42:07.848
    Jun 27 16:42:07.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 get pod pause -L testing-label'
    Jun 27 16:42:07.970: INFO: stderr: ""
    Jun 27 16:42:07.970: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          4s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 06/27/23 16:42:07.97
    Jun 27 16:42:07.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 delete --grace-period=0 --force -f -'
    Jun 27 16:42:08.128: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 27 16:42:08.128: INFO: stdout: "pod \"pause\" force deleted\n"
    Jun 27 16:42:08.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 get rc,svc -l name=pause --no-headers'
    Jun 27 16:42:08.287: INFO: stderr: "No resources found in kubectl-1954 namespace.\n"
    Jun 27 16:42:08.287: INFO: stdout: ""
    Jun 27 16:42:08.288: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-1954 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 27 16:42:08.414: INFO: stderr: ""
    Jun 27 16:42:08.414: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:42:08.414: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1954" for this suite. 06/27/23 16:42:08.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:42:08.481
Jun 27 16:42:08.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-lifecycle-hook 06/27/23 16:42:08.484
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:08.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:08.552
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/27/23 16:42:08.637
Jun 27 16:42:08.698: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5866" to be "running and ready"
Jun 27 16:42:08.716: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.44216ms
Jun 27 16:42:08.716: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:42:10.739: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.04010398s
Jun 27 16:42:10.739: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 27 16:42:10.739: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 06/27/23 16:42:10.764
Jun 27 16:42:10.817: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5866" to be "running and ready"
Jun 27 16:42:10.852: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 34.98144ms
Jun 27 16:42:10.852: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:42:12.873: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.055658577s
Jun 27 16:42:12.873: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Jun 27 16:42:12.873: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/27/23 16:42:12.89
Jun 27 16:42:12.926: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 16:42:12.957: INFO: Pod pod-with-prestop-http-hook still exists
Jun 27 16:42:14.958: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 16:42:15.009: INFO: Pod pod-with-prestop-http-hook still exists
Jun 27 16:42:16.958: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Jun 27 16:42:16.977: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 06/27/23 16:42:16.977
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 27 16:42:17.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5866" for this suite. 06/27/23 16:42:17.084
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":81,"skipped":1672,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.629 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:42:08.481
    Jun 27 16:42:08.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/27/23 16:42:08.484
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:08.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:08.552
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/27/23 16:42:08.637
    Jun 27 16:42:08.698: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5866" to be "running and ready"
    Jun 27 16:42:08.716: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 17.44216ms
    Jun 27 16:42:08.716: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:42:10.739: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.04010398s
    Jun 27 16:42:10.739: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 27 16:42:10.739: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 06/27/23 16:42:10.764
    Jun 27 16:42:10.817: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-5866" to be "running and ready"
    Jun 27 16:42:10.852: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 34.98144ms
    Jun 27 16:42:10.852: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:42:12.873: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.055658577s
    Jun 27 16:42:12.873: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Jun 27 16:42:12.873: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/27/23 16:42:12.89
    Jun 27 16:42:12.926: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 27 16:42:12.957: INFO: Pod pod-with-prestop-http-hook still exists
    Jun 27 16:42:14.958: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 27 16:42:15.009: INFO: Pod pod-with-prestop-http-hook still exists
    Jun 27 16:42:16.958: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Jun 27 16:42:16.977: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 06/27/23 16:42:16.977
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 27 16:42:17.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5866" for this suite. 06/27/23 16:42:17.084
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:42:17.111
Jun 27 16:42:17.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:42:17.113
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:17.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:17.178
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 06/27/23 16:42:17.2
Jun 27 16:42:17.200: INFO: namespace kubectl-6693
Jun 27 16:42:17.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-6693 create -f -'
Jun 27 16:42:17.819: INFO: stderr: ""
Jun 27 16:42:17.819: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/27/23 16:42:17.819
Jun 27 16:42:18.848: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 16:42:18.848: INFO: Found 0 / 1
Jun 27 16:42:19.840: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 16:42:19.840: INFO: Found 0 / 1
Jun 27 16:42:20.839: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 16:42:20.839: INFO: Found 1 / 1
Jun 27 16:42:20.839: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 27 16:42:20.859: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 16:42:20.859: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 27 16:42:20.859: INFO: wait on agnhost-primary startup in kubectl-6693 
Jun 27 16:42:20.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-6693 logs agnhost-primary-6vw2v agnhost-primary'
Jun 27 16:42:21.095: INFO: stderr: ""
Jun 27 16:42:21.095: INFO: stdout: "Paused\n"
STEP: exposing RC 06/27/23 16:42:21.096
Jun 27 16:42:21.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-6693 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Jun 27 16:42:21.298: INFO: stderr: ""
Jun 27 16:42:21.298: INFO: stdout: "service/rm2 exposed\n"
Jun 27 16:42:21.314: INFO: Service rm2 in namespace kubectl-6693 found.
STEP: exposing service 06/27/23 16:42:23.35
Jun 27 16:42:23.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-6693 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Jun 27 16:42:23.501: INFO: stderr: ""
Jun 27 16:42:23.501: INFO: stdout: "service/rm3 exposed\n"
Jun 27 16:42:23.519: INFO: Service rm3 in namespace kubectl-6693 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:42:25.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-6693" for this suite. 06/27/23 16:42:25.581
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":82,"skipped":1686,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.496 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:42:17.111
    Jun 27 16:42:17.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:42:17.113
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:17.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:17.178
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 06/27/23 16:42:17.2
    Jun 27 16:42:17.200: INFO: namespace kubectl-6693
    Jun 27 16:42:17.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-6693 create -f -'
    Jun 27 16:42:17.819: INFO: stderr: ""
    Jun 27 16:42:17.819: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/27/23 16:42:17.819
    Jun 27 16:42:18.848: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 16:42:18.848: INFO: Found 0 / 1
    Jun 27 16:42:19.840: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 16:42:19.840: INFO: Found 0 / 1
    Jun 27 16:42:20.839: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 16:42:20.839: INFO: Found 1 / 1
    Jun 27 16:42:20.839: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun 27 16:42:20.859: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 16:42:20.859: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 27 16:42:20.859: INFO: wait on agnhost-primary startup in kubectl-6693 
    Jun 27 16:42:20.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-6693 logs agnhost-primary-6vw2v agnhost-primary'
    Jun 27 16:42:21.095: INFO: stderr: ""
    Jun 27 16:42:21.095: INFO: stdout: "Paused\n"
    STEP: exposing RC 06/27/23 16:42:21.096
    Jun 27 16:42:21.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-6693 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Jun 27 16:42:21.298: INFO: stderr: ""
    Jun 27 16:42:21.298: INFO: stdout: "service/rm2 exposed\n"
    Jun 27 16:42:21.314: INFO: Service rm2 in namespace kubectl-6693 found.
    STEP: exposing service 06/27/23 16:42:23.35
    Jun 27 16:42:23.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-6693 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Jun 27 16:42:23.501: INFO: stderr: ""
    Jun 27 16:42:23.501: INFO: stdout: "service/rm3 exposed\n"
    Jun 27 16:42:23.519: INFO: Service rm3 in namespace kubectl-6693 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:42:25.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-6693" for this suite. 06/27/23 16:42:25.581
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:42:25.612
Jun 27 16:42:25.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 16:42:25.614
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:25.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:25.675
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 16:42:25.763
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:42:26.43
STEP: Deploying the webhook pod 06/27/23 16:42:26.476
STEP: Wait for the deployment to be ready 06/27/23 16:42:26.522
Jun 27 16:42:26.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun 27 16:42:28.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 16:42:30.634
STEP: Verifying the service has paired with the endpoint 06/27/23 16:42:30.687
Jun 27 16:42:31.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 06/27/23 16:42:31.703
STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 16:42:31.77
STEP: Updating a validating webhook configuration's rules to not include the create operation 06/27/23 16:42:31.818
STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 16:42:31.851
STEP: Patching a validating webhook configuration's rules to include the create operation 06/27/23 16:42:31.898
STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 16:42:31.921
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:42:31.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7453" for this suite. 06/27/23 16:42:31.984
STEP: Destroying namespace "webhook-7453-markers" for this suite. 06/27/23 16:42:32.009
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":83,"skipped":1704,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.601 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:42:25.612
    Jun 27 16:42:25.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 16:42:25.614
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:25.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:25.675
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 16:42:25.763
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:42:26.43
    STEP: Deploying the webhook pod 06/27/23 16:42:26.476
    STEP: Wait for the deployment to be ready 06/27/23 16:42:26.522
    Jun 27 16:42:26.614: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jun 27 16:42:28.671: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 42, 26, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 16:42:30.634
    STEP: Verifying the service has paired with the endpoint 06/27/23 16:42:30.687
    Jun 27 16:42:31.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 06/27/23 16:42:31.703
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 16:42:31.77
    STEP: Updating a validating webhook configuration's rules to not include the create operation 06/27/23 16:42:31.818
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 16:42:31.851
    STEP: Patching a validating webhook configuration's rules to include the create operation 06/27/23 16:42:31.898
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 16:42:31.921
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:42:31.955: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7453" for this suite. 06/27/23 16:42:31.984
    STEP: Destroying namespace "webhook-7453-markers" for this suite. 06/27/23 16:42:32.009
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:42:32.216
Jun 27 16:42:32.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 16:42:32.217
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:32.263
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:32.283
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 06/27/23 16:42:32.302
Jun 27 16:42:32.367: INFO: Waiting up to 5m0s for pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605" in namespace "emptydir-389" to be "Succeeded or Failed"
Jun 27 16:42:32.388: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605": Phase="Pending", Reason="", readiness=false. Elapsed: 20.533151ms
Jun 27 16:42:34.407: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039566601s
Jun 27 16:42:36.407: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039420506s
Jun 27 16:42:38.414: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046361909s
STEP: Saw pod success 06/27/23 16:42:38.414
Jun 27 16:42:38.414: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605" satisfied condition "Succeeded or Failed"
Jun 27 16:42:38.433: INFO: Trying to get logs from node 10.113.180.89 pod pod-409f0c86-f883-491d-a22a-a3446b2cd605 container test-container: <nil>
STEP: delete the pod 06/27/23 16:42:38.476
Jun 27 16:42:38.538: INFO: Waiting for pod pod-409f0c86-f883-491d-a22a-a3446b2cd605 to disappear
Jun 27 16:42:38.555: INFO: Pod pod-409f0c86-f883-491d-a22a-a3446b2cd605 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 16:42:38.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-389" for this suite. 06/27/23 16:42:38.587
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1739,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.429 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:42:32.216
    Jun 27 16:42:32.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 16:42:32.217
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:32.263
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:32.283
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 06/27/23 16:42:32.302
    Jun 27 16:42:32.367: INFO: Waiting up to 5m0s for pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605" in namespace "emptydir-389" to be "Succeeded or Failed"
    Jun 27 16:42:32.388: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605": Phase="Pending", Reason="", readiness=false. Elapsed: 20.533151ms
    Jun 27 16:42:34.407: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039566601s
    Jun 27 16:42:36.407: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039420506s
    Jun 27 16:42:38.414: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.046361909s
    STEP: Saw pod success 06/27/23 16:42:38.414
    Jun 27 16:42:38.414: INFO: Pod "pod-409f0c86-f883-491d-a22a-a3446b2cd605" satisfied condition "Succeeded or Failed"
    Jun 27 16:42:38.433: INFO: Trying to get logs from node 10.113.180.89 pod pod-409f0c86-f883-491d-a22a-a3446b2cd605 container test-container: <nil>
    STEP: delete the pod 06/27/23 16:42:38.476
    Jun 27 16:42:38.538: INFO: Waiting for pod pod-409f0c86-f883-491d-a22a-a3446b2cd605 to disappear
    Jun 27 16:42:38.555: INFO: Pod pod-409f0c86-f883-491d-a22a-a3446b2cd605 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 16:42:38.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-389" for this suite. 06/27/23 16:42:38.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:42:38.651
Jun 27 16:42:38.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:42:38.654
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:38.706
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:38.73
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Jun 27 16:42:38.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-8571 version'
Jun 27 16:42:38.888: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Jun 27 16:42:38.888: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.10\", GitCommit:\"e770bdbb87cccdc2daa790ecd69f40cf4df3cc9d\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:12:20Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.10+3fe2906\", GitCommit:\"379f6fe03321f9149edea7f20e11ce88f8d99c25\", GitTreeState:\"clean\", BuildDate:\"2023-05-26T02:20:22Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:42:38.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8571" for this suite. 06/27/23 16:42:38.919
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":85,"skipped":1789,"failed":0}
------------------------------
â€¢ [0.302 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:42:38.651
    Jun 27 16:42:38.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:42:38.654
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:38.706
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:38.73
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Jun 27 16:42:38.744: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-8571 version'
    Jun 27 16:42:38.888: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Jun 27 16:42:38.888: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.10\", GitCommit:\"e770bdbb87cccdc2daa790ecd69f40cf4df3cc9d\", GitTreeState:\"clean\", BuildDate:\"2023-05-17T14:12:20Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.10+3fe2906\", GitCommit:\"379f6fe03321f9149edea7f20e11ce88f8d99c25\", GitTreeState:\"clean\", BuildDate:\"2023-05-26T02:20:22Z\", GoVersion:\"go1.19.9\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:42:38.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8571" for this suite. 06/27/23 16:42:38.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:42:38.953
Jun 27 16:42:38.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename daemonsets 06/27/23 16:42:38.954
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:39.037
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:39.052
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Jun 27 16:42:39.209: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 06/27/23 16:42:39.235
Jun 27 16:42:39.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:39.256: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 06/27/23 16:42:39.256
Jun 27 16:42:39.355: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:39.356: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:42:40.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:40.376: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:42:41.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:41.379: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:42:42.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 27 16:42:42.375: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 06/27/23 16:42:42.391
Jun 27 16:42:42.470: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:42.471: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/27/23 16:42:42.471
Jun 27 16:42:42.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:42.542: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:42:43.563: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:43.563: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:42:44.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:44.568: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:42:45.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:45.562: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:42:46.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:46.562: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:42:47.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:47.562: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
Jun 27 16:42:48.563: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 27 16:42:48.563: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/27/23 16:42:48.605
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6702, will wait for the garbage collector to delete the pods 06/27/23 16:42:48.606
Jun 27 16:42:48.708: INFO: Deleting DaemonSet.extensions daemon-set took: 31.028028ms
Jun 27 16:42:48.809: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.694764ms
Jun 27 16:42:52.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:42:52.429: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 27 16:42:52.448: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"87111"},"items":null}

Jun 27 16:42:52.468: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"87111"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 27 16:42:52.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-6702" for this suite. 06/27/23 16:42:52.634
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":86,"skipped":1798,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.708 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:42:38.953
    Jun 27 16:42:38.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename daemonsets 06/27/23 16:42:38.954
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:39.037
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:39.052
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Jun 27 16:42:39.209: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 06/27/23 16:42:39.235
    Jun 27 16:42:39.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:39.256: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 06/27/23 16:42:39.256
    Jun 27 16:42:39.355: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:39.356: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:42:40.376: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:40.376: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:42:41.379: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:41.379: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:42:42.375: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 27 16:42:42.375: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 06/27/23 16:42:42.391
    Jun 27 16:42:42.470: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:42.471: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 06/27/23 16:42:42.471
    Jun 27 16:42:42.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:42.542: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:42:43.563: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:43.563: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:42:44.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:44.568: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:42:45.561: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:45.562: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:42:46.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:46.562: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:42:47.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:47.562: INFO: Node 10.113.180.90 is running 0 daemon pod, expected 1
    Jun 27 16:42:48.563: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 27 16:42:48.563: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/27/23 16:42:48.605
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6702, will wait for the garbage collector to delete the pods 06/27/23 16:42:48.606
    Jun 27 16:42:48.708: INFO: Deleting DaemonSet.extensions daemon-set took: 31.028028ms
    Jun 27 16:42:48.809: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.694764ms
    Jun 27 16:42:52.429: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:42:52.429: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 27 16:42:52.448: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"87111"},"items":null}

    Jun 27 16:42:52.468: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"87111"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 16:42:52.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-6702" for this suite. 06/27/23 16:42:52.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:42:52.663
Jun 27 16:42:52.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename dns 06/27/23 16:42:52.665
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:52.715
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:52.737
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 06/27/23 16:42:52.753
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 53.227.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.227.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.227.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.227.53_tcp@PTR;sleep 1; done
 06/27/23 16:42:52.812
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 53.227.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.227.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.227.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.227.53_tcp@PTR;sleep 1; done
 06/27/23 16:42:52.812
STEP: creating a pod to probe DNS 06/27/23 16:42:52.812
STEP: submitting the pod to kubernetes 06/27/23 16:42:52.812
Jun 27 16:42:52.879: INFO: Waiting up to 15m0s for pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf" in namespace "dns-839" to be "running"
Jun 27 16:42:52.902: INFO: Pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 23.255206ms
Jun 27 16:42:54.923: INFO: Pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044304479s
Jun 27 16:42:56.924: INFO: Pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.044510642s
Jun 27 16:42:56.924: INFO: Pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf" satisfied condition "running"
STEP: retrieving the pod 06/27/23 16:42:56.924
STEP: looking for the results for each expected name from probers 06/27/23 16:42:56.944
Jun 27 16:42:56.976: INFO: Unable to read wheezy_udp@dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
Jun 27 16:42:57.002: INFO: Unable to read wheezy_tcp@dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
Jun 27 16:42:57.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
Jun 27 16:42:57.053: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
Jun 27 16:42:57.181: INFO: Unable to read jessie_udp@dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
Jun 27 16:42:57.233: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
Jun 27 16:42:57.370: INFO: Lookups using dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf failed for: [wheezy_udp@dns-test-service.dns-839.svc.cluster.local wheezy_tcp@dns-test-service.dns-839.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-839.svc.cluster.local jessie_udp@dns-test-service.dns-839.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local]

Jun 27 16:43:02.894: INFO: DNS probes using dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf succeeded

STEP: deleting the pod 06/27/23 16:43:02.894
STEP: deleting the test service 06/27/23 16:43:02.95
STEP: deleting the test headless service 06/27/23 16:43:03.024
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 27 16:43:03.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-839" for this suite. 06/27/23 16:43:03.093
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":87,"skipped":1813,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.459 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:42:52.663
    Jun 27 16:42:52.663: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename dns 06/27/23 16:42:52.665
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:42:52.715
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:42:52.737
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 06/27/23 16:42:52.753
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 53.227.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.227.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.227.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.227.53_tcp@PTR;sleep 1; done
     06/27/23 16:42:52.812
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-839.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-839.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-839.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-839.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 53.227.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.227.53_udp@PTR;check="$$(dig +tcp +noall +answer +search 53.227.21.172.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/172.21.227.53_tcp@PTR;sleep 1; done
     06/27/23 16:42:52.812
    STEP: creating a pod to probe DNS 06/27/23 16:42:52.812
    STEP: submitting the pod to kubernetes 06/27/23 16:42:52.812
    Jun 27 16:42:52.879: INFO: Waiting up to 15m0s for pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf" in namespace "dns-839" to be "running"
    Jun 27 16:42:52.902: INFO: Pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 23.255206ms
    Jun 27 16:42:54.923: INFO: Pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044304479s
    Jun 27 16:42:56.924: INFO: Pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf": Phase="Running", Reason="", readiness=true. Elapsed: 4.044510642s
    Jun 27 16:42:56.924: INFO: Pod "dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf" satisfied condition "running"
    STEP: retrieving the pod 06/27/23 16:42:56.924
    STEP: looking for the results for each expected name from probers 06/27/23 16:42:56.944
    Jun 27 16:42:56.976: INFO: Unable to read wheezy_udp@dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
    Jun 27 16:42:57.002: INFO: Unable to read wheezy_tcp@dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
    Jun 27 16:42:57.027: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
    Jun 27 16:42:57.053: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
    Jun 27 16:42:57.181: INFO: Unable to read jessie_udp@dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
    Jun 27 16:42:57.233: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local from pod dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf: the server could not find the requested resource (get pods dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf)
    Jun 27 16:42:57.370: INFO: Lookups using dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf failed for: [wheezy_udp@dns-test-service.dns-839.svc.cluster.local wheezy_tcp@dns-test-service.dns-839.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-839.svc.cluster.local jessie_udp@dns-test-service.dns-839.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-839.svc.cluster.local]

    Jun 27 16:43:02.894: INFO: DNS probes using dns-839/dns-test-8b9c17ee-6034-4be1-a67a-eab48a06b7cf succeeded

    STEP: deleting the pod 06/27/23 16:43:02.894
    STEP: deleting the test service 06/27/23 16:43:02.95
    STEP: deleting the test headless service 06/27/23 16:43:03.024
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 27 16:43:03.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-839" for this suite. 06/27/23 16:43:03.093
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:43:03.13
Jun 27 16:43:03.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 16:43:03.132
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:43:03.201
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:43:03.218
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Jun 27 16:43:03.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:43:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-2598" for this suite. 06/27/23 16:43:03.952
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":88,"skipped":1832,"failed":0}
------------------------------
â€¢ [0.878 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:43:03.13
    Jun 27 16:43:03.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 16:43:03.132
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:43:03.201
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:43:03.218
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Jun 27 16:43:03.236: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:43:03.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-2598" for this suite. 06/27/23 16:43:03.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:43:04.018
Jun 27 16:43:04.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename taint-multiple-pods 06/27/23 16:43:04.019
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:43:04.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:43:04.297
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Jun 27 16:43:04.312: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 27 16:44:04.553: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Jun 27 16:44:04.581: INFO: Starting informer...
STEP: Starting pods... 06/27/23 16:44:04.581
Jun 27 16:44:04.892: INFO: Pod1 is running on 10.113.180.90. Tainting Node
Jun 27 16:44:05.181: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9011" to be "running"
Jun 27 16:44:05.201: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.825806ms
Jun 27 16:44:07.233: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.052009391s
Jun 27 16:44:07.233: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Jun 27 16:44:07.233: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9011" to be "running"
Jun 27 16:44:07.251: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 17.64261ms
Jun 27 16:44:07.251: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Jun 27 16:44:07.251: INFO: Pod2 is running on 10.113.180.90. Tainting Node
STEP: Trying to apply a taint on the Node 06/27/23 16:44:07.251
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/27/23 16:44:07.308
STEP: Waiting for Pod1 and Pod2 to be deleted 06/27/23 16:44:07.337
Jun 27 16:44:14.554: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Jun 27 16:44:33.757: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/27/23 16:44:33.817
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Jun 27 16:44:33.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-9011" for this suite. 06/27/23 16:44:33.896
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":89,"skipped":1847,"failed":0}
------------------------------
â€¢ [SLOW TEST] [89.932 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:43:04.018
    Jun 27 16:43:04.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename taint-multiple-pods 06/27/23 16:43:04.019
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:43:04.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:43:04.297
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Jun 27 16:43:04.312: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 27 16:44:04.553: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Jun 27 16:44:04.581: INFO: Starting informer...
    STEP: Starting pods... 06/27/23 16:44:04.581
    Jun 27 16:44:04.892: INFO: Pod1 is running on 10.113.180.90. Tainting Node
    Jun 27 16:44:05.181: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-9011" to be "running"
    Jun 27 16:44:05.201: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 19.825806ms
    Jun 27 16:44:07.233: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.052009391s
    Jun 27 16:44:07.233: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Jun 27 16:44:07.233: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-9011" to be "running"
    Jun 27 16:44:07.251: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 17.64261ms
    Jun 27 16:44:07.251: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Jun 27 16:44:07.251: INFO: Pod2 is running on 10.113.180.90. Tainting Node
    STEP: Trying to apply a taint on the Node 06/27/23 16:44:07.251
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/27/23 16:44:07.308
    STEP: Waiting for Pod1 and Pod2 to be deleted 06/27/23 16:44:07.337
    Jun 27 16:44:14.554: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Jun 27 16:44:33.757: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/27/23 16:44:33.817
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 16:44:33.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-9011" for this suite. 06/27/23 16:44:33.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:44:33.951
Jun 27 16:44:33.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 16:44:33.952
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:44:34.007
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:44:34.03
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-d45e0ea3-9b54-4d37-984e-455782f35838 06/27/23 16:44:34.051
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 27 16:44:34.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6240" for this suite. 06/27/23 16:44:34.11
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":90,"skipped":1858,"failed":0}
------------------------------
â€¢ [0.197 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:44:33.951
    Jun 27 16:44:33.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 16:44:33.952
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:44:34.007
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:44:34.03
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-d45e0ea3-9b54-4d37-984e-455782f35838 06/27/23 16:44:34.051
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 16:44:34.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6240" for this suite. 06/27/23 16:44:34.11
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:44:34.158
Jun 27 16:44:34.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 16:44:34.159
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:44:34.218
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:44:34.248
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-7f3787e3-264a-4861-8d05-575c0ad76ce2 06/27/23 16:44:34.265
STEP: Creating a pod to test consume secrets 06/27/23 16:44:34.288
Jun 27 16:44:34.364: INFO: Waiting up to 5m0s for pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa" in namespace "secrets-605" to be "Succeeded or Failed"
Jun 27 16:44:34.385: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa": Phase="Pending", Reason="", readiness=false. Elapsed: 20.215227ms
Jun 27 16:44:36.411: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046181837s
Jun 27 16:44:38.417: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052486935s
Jun 27 16:44:40.425: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060605097s
STEP: Saw pod success 06/27/23 16:44:40.425
Jun 27 16:44:40.425: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa" satisfied condition "Succeeded or Failed"
Jun 27 16:44:40.459: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa container secret-volume-test: <nil>
STEP: delete the pod 06/27/23 16:44:40.589
Jun 27 16:44:40.648: INFO: Waiting for pod pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa to disappear
Jun 27 16:44:40.664: INFO: Pod pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 27 16:44:40.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-605" for this suite. 06/27/23 16:44:40.715
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":91,"skipped":1862,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.601 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:44:34.158
    Jun 27 16:44:34.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 16:44:34.159
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:44:34.218
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:44:34.248
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-7f3787e3-264a-4861-8d05-575c0ad76ce2 06/27/23 16:44:34.265
    STEP: Creating a pod to test consume secrets 06/27/23 16:44:34.288
    Jun 27 16:44:34.364: INFO: Waiting up to 5m0s for pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa" in namespace "secrets-605" to be "Succeeded or Failed"
    Jun 27 16:44:34.385: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa": Phase="Pending", Reason="", readiness=false. Elapsed: 20.215227ms
    Jun 27 16:44:36.411: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046181837s
    Jun 27 16:44:38.417: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052486935s
    Jun 27 16:44:40.425: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.060605097s
    STEP: Saw pod success 06/27/23 16:44:40.425
    Jun 27 16:44:40.425: INFO: Pod "pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa" satisfied condition "Succeeded or Failed"
    Jun 27 16:44:40.459: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa container secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 16:44:40.589
    Jun 27 16:44:40.648: INFO: Waiting for pod pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa to disappear
    Jun 27 16:44:40.664: INFO: Pod pod-secrets-135fc34e-0679-471f-8afd-564dddd34afa no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 16:44:40.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-605" for this suite. 06/27/23 16:44:40.715
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:44:40.764
Jun 27 16:44:40.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename cronjob 06/27/23 16:44:40.765
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:44:40.841
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:44:40.855
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 06/27/23 16:44:40.872
STEP: Ensuring a job is scheduled 06/27/23 16:44:40.895
STEP: Ensuring exactly one is scheduled 06/27/23 16:45:00.908
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/27/23 16:45:00.93
STEP: Ensuring no more jobs are scheduled 06/27/23 16:45:00.951
STEP: Removing cronjob 06/27/23 16:50:00.98
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 27 16:50:00.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5328" for this suite. 06/27/23 16:50:01.033
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":92,"skipped":1879,"failed":0}
------------------------------
â€¢ [SLOW TEST] [320.299 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:44:40.764
    Jun 27 16:44:40.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename cronjob 06/27/23 16:44:40.765
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:44:40.841
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:44:40.855
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 06/27/23 16:44:40.872
    STEP: Ensuring a job is scheduled 06/27/23 16:44:40.895
    STEP: Ensuring exactly one is scheduled 06/27/23 16:45:00.908
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/27/23 16:45:00.93
    STEP: Ensuring no more jobs are scheduled 06/27/23 16:45:00.951
    STEP: Removing cronjob 06/27/23 16:50:00.98
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 27 16:50:00.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5328" for this suite. 06/27/23 16:50:01.033
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:50:01.077
Jun 27 16:50:01.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename var-expansion 06/27/23 16:50:01.079
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:50:01.127
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:50:01.143
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 06/27/23 16:50:01.158
Jun 27 16:50:01.237: INFO: Waiting up to 5m0s for pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854" in namespace "var-expansion-5804" to be "Succeeded or Failed"
Jun 27 16:50:01.257: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854": Phase="Pending", Reason="", readiness=false. Elapsed: 20.060484ms
Jun 27 16:50:03.277: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039563859s
Jun 27 16:50:05.292: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054772245s
Jun 27 16:50:07.281: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043766793s
STEP: Saw pod success 06/27/23 16:50:07.281
Jun 27 16:50:07.282: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854" satisfied condition "Succeeded or Failed"
Jun 27 16:50:07.301: INFO: Trying to get logs from node 10.113.180.90 pod var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854 container dapi-container: <nil>
STEP: delete the pod 06/27/23 16:50:07.386
Jun 27 16:50:07.437: INFO: Waiting for pod var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854 to disappear
Jun 27 16:50:07.456: INFO: Pod var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 27 16:50:07.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5804" for this suite. 06/27/23 16:50:07.486
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":93,"skipped":1927,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.439 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:50:01.077
    Jun 27 16:50:01.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename var-expansion 06/27/23 16:50:01.079
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:50:01.127
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:50:01.143
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 06/27/23 16:50:01.158
    Jun 27 16:50:01.237: INFO: Waiting up to 5m0s for pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854" in namespace "var-expansion-5804" to be "Succeeded or Failed"
    Jun 27 16:50:01.257: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854": Phase="Pending", Reason="", readiness=false. Elapsed: 20.060484ms
    Jun 27 16:50:03.277: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039563859s
    Jun 27 16:50:05.292: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854": Phase="Pending", Reason="", readiness=false. Elapsed: 4.054772245s
    Jun 27 16:50:07.281: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043766793s
    STEP: Saw pod success 06/27/23 16:50:07.281
    Jun 27 16:50:07.282: INFO: Pod "var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854" satisfied condition "Succeeded or Failed"
    Jun 27 16:50:07.301: INFO: Trying to get logs from node 10.113.180.90 pod var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854 container dapi-container: <nil>
    STEP: delete the pod 06/27/23 16:50:07.386
    Jun 27 16:50:07.437: INFO: Waiting for pod var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854 to disappear
    Jun 27 16:50:07.456: INFO: Pod var-expansion-92f3f4f6-a564-46b6-bf51-163c19e4c854 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 27 16:50:07.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5804" for this suite. 06/27/23 16:50:07.486
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:50:07.522
Jun 27 16:50:07.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename controllerrevisions 06/27/23 16:50:07.527
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:50:07.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:50:07.602
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-99f8h-daemon-set" 06/27/23 16:50:07.743
STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 16:50:07.798
Jun 27 16:50:07.843: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 0
Jun 27 16:50:07.843: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:50:08.899: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 0
Jun 27 16:50:08.899: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:50:09.901: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 1
Jun 27 16:50:09.902: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:50:10.902: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 3
Jun 27 16:50:10.903: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-99f8h-daemon-set
STEP: Confirm DaemonSet "e2e-99f8h-daemon-set" successfully created with "daemonset-name=e2e-99f8h-daemon-set" label 06/27/23 16:50:10.92
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-99f8h-daemon-set" 06/27/23 16:50:10.967
Jun 27 16:50:10.990: INFO: Located ControllerRevision: "e2e-99f8h-daemon-set-8696844555"
STEP: Patching ControllerRevision "e2e-99f8h-daemon-set-8696844555" 06/27/23 16:50:11.002
Jun 27 16:50:11.026: INFO: e2e-99f8h-daemon-set-8696844555 has been patched
STEP: Create a new ControllerRevision 06/27/23 16:50:11.026
Jun 27 16:50:11.065: INFO: Created ControllerRevision: e2e-99f8h-daemon-set-7bdfb9b467
STEP: Confirm that there are two ControllerRevisions 06/27/23 16:50:11.065
Jun 27 16:50:11.065: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 27 16:50:11.078: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-99f8h-daemon-set-8696844555" 06/27/23 16:50:11.078
STEP: Confirm that there is only one ControllerRevision 06/27/23 16:50:11.1
Jun 27 16:50:11.100: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 27 16:50:11.113: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-99f8h-daemon-set-7bdfb9b467" 06/27/23 16:50:11.125
Jun 27 16:50:11.152: INFO: e2e-99f8h-daemon-set-7bdfb9b467 has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 06/27/23 16:50:11.152
W0627 16:50:11.180025      23 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 06/27/23 16:50:11.18
Jun 27 16:50:11.180: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 27 16:50:12.195: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 27 16:50:12.208: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-99f8h-daemon-set-7bdfb9b467=updated" 06/27/23 16:50:12.208
STEP: Confirm that there is only one ControllerRevision 06/27/23 16:50:12.231
Jun 27 16:50:12.231: INFO: Requesting list of ControllerRevisions to confirm quantity
Jun 27 16:50:12.243: INFO: Found 1 ControllerRevisions
Jun 27 16:50:12.254: INFO: ControllerRevision "e2e-99f8h-daemon-set-786d7d59f4" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-99f8h-daemon-set" 06/27/23 16:50:12.271
STEP: deleting DaemonSet.extensions e2e-99f8h-daemon-set in namespace controllerrevisions-8846, will wait for the garbage collector to delete the pods 06/27/23 16:50:12.271
Jun 27 16:50:12.372: INFO: Deleting DaemonSet.extensions e2e-99f8h-daemon-set took: 31.341103ms
Jun 27 16:50:12.476: INFO: Terminating DaemonSet.extensions e2e-99f8h-daemon-set pods took: 104.048302ms
Jun 27 16:50:15.305: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 0
Jun 27 16:50:15.305: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-99f8h-daemon-set
Jun 27 16:50:15.321: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"91559"},"items":null}

Jun 27 16:50:15.339: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"91559"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Jun 27 16:50:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-8846" for this suite. 06/27/23 16:50:15.493
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":94,"skipped":1929,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.000 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:50:07.522
    Jun 27 16:50:07.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename controllerrevisions 06/27/23 16:50:07.527
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:50:07.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:50:07.602
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-99f8h-daemon-set" 06/27/23 16:50:07.743
    STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 16:50:07.798
    Jun 27 16:50:07.843: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 0
    Jun 27 16:50:07.843: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:50:08.899: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 0
    Jun 27 16:50:08.899: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:50:09.901: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 1
    Jun 27 16:50:09.902: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:50:10.902: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 3
    Jun 27 16:50:10.903: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-99f8h-daemon-set
    STEP: Confirm DaemonSet "e2e-99f8h-daemon-set" successfully created with "daemonset-name=e2e-99f8h-daemon-set" label 06/27/23 16:50:10.92
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-99f8h-daemon-set" 06/27/23 16:50:10.967
    Jun 27 16:50:10.990: INFO: Located ControllerRevision: "e2e-99f8h-daemon-set-8696844555"
    STEP: Patching ControllerRevision "e2e-99f8h-daemon-set-8696844555" 06/27/23 16:50:11.002
    Jun 27 16:50:11.026: INFO: e2e-99f8h-daemon-set-8696844555 has been patched
    STEP: Create a new ControllerRevision 06/27/23 16:50:11.026
    Jun 27 16:50:11.065: INFO: Created ControllerRevision: e2e-99f8h-daemon-set-7bdfb9b467
    STEP: Confirm that there are two ControllerRevisions 06/27/23 16:50:11.065
    Jun 27 16:50:11.065: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 27 16:50:11.078: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-99f8h-daemon-set-8696844555" 06/27/23 16:50:11.078
    STEP: Confirm that there is only one ControllerRevision 06/27/23 16:50:11.1
    Jun 27 16:50:11.100: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 27 16:50:11.113: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-99f8h-daemon-set-7bdfb9b467" 06/27/23 16:50:11.125
    Jun 27 16:50:11.152: INFO: e2e-99f8h-daemon-set-7bdfb9b467 has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 06/27/23 16:50:11.152
    W0627 16:50:11.180025      23 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 06/27/23 16:50:11.18
    Jun 27 16:50:11.180: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 27 16:50:12.195: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 27 16:50:12.208: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-99f8h-daemon-set-7bdfb9b467=updated" 06/27/23 16:50:12.208
    STEP: Confirm that there is only one ControllerRevision 06/27/23 16:50:12.231
    Jun 27 16:50:12.231: INFO: Requesting list of ControllerRevisions to confirm quantity
    Jun 27 16:50:12.243: INFO: Found 1 ControllerRevisions
    Jun 27 16:50:12.254: INFO: ControllerRevision "e2e-99f8h-daemon-set-786d7d59f4" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-99f8h-daemon-set" 06/27/23 16:50:12.271
    STEP: deleting DaemonSet.extensions e2e-99f8h-daemon-set in namespace controllerrevisions-8846, will wait for the garbage collector to delete the pods 06/27/23 16:50:12.271
    Jun 27 16:50:12.372: INFO: Deleting DaemonSet.extensions e2e-99f8h-daemon-set took: 31.341103ms
    Jun 27 16:50:12.476: INFO: Terminating DaemonSet.extensions e2e-99f8h-daemon-set pods took: 104.048302ms
    Jun 27 16:50:15.305: INFO: Number of nodes with available pods controlled by daemonset e2e-99f8h-daemon-set: 0
    Jun 27 16:50:15.305: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-99f8h-daemon-set
    Jun 27 16:50:15.321: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"91559"},"items":null}

    Jun 27 16:50:15.339: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"91559"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 16:50:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-8846" for this suite. 06/27/23 16:50:15.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:50:15.523
Jun 27 16:50:15.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-probe 06/27/23 16:50:15.524
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:50:15.574
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:50:15.596
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319 in namespace container-probe-5948 06/27/23 16:50:15.609
Jun 27 16:50:15.685: INFO: Waiting up to 5m0s for pod "liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319" in namespace "container-probe-5948" to be "not pending"
Jun 27 16:50:15.710: INFO: Pod "liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319": Phase="Pending", Reason="", readiness=false. Elapsed: 24.93067ms
Jun 27 16:50:17.730: INFO: Pod "liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319": Phase="Running", Reason="", readiness=true. Elapsed: 2.044999797s
Jun 27 16:50:17.730: INFO: Pod "liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319" satisfied condition "not pending"
Jun 27 16:50:17.730: INFO: Started pod liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319 in namespace container-probe-5948
STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 16:50:17.73
Jun 27 16:50:17.750: INFO: Initial restart count of pod liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319 is 0
STEP: deleting the pod 06/27/23 16:54:18.27
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 27 16:54:18.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5948" for this suite. 06/27/23 16:54:18.352
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":95,"skipped":1941,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.882 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:50:15.523
    Jun 27 16:50:15.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-probe 06/27/23 16:50:15.524
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:50:15.574
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:50:15.596
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319 in namespace container-probe-5948 06/27/23 16:50:15.609
    Jun 27 16:50:15.685: INFO: Waiting up to 5m0s for pod "liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319" in namespace "container-probe-5948" to be "not pending"
    Jun 27 16:50:15.710: INFO: Pod "liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319": Phase="Pending", Reason="", readiness=false. Elapsed: 24.93067ms
    Jun 27 16:50:17.730: INFO: Pod "liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319": Phase="Running", Reason="", readiness=true. Elapsed: 2.044999797s
    Jun 27 16:50:17.730: INFO: Pod "liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319" satisfied condition "not pending"
    Jun 27 16:50:17.730: INFO: Started pod liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319 in namespace container-probe-5948
    STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 16:50:17.73
    Jun 27 16:50:17.750: INFO: Initial restart count of pod liveness-5f1b518d-5ebe-49c4-adca-d1d8d3729319 is 0
    STEP: deleting the pod 06/27/23 16:54:18.27
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 27 16:54:18.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5948" for this suite. 06/27/23 16:54:18.352
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:54:18.405
Jun 27 16:54:18.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename security-context-test 06/27/23 16:54:18.416
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:18.534
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:18.621
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Jun 27 16:54:18.726: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee" in namespace "security-context-test-2599" to be "Succeeded or Failed"
Jun 27 16:54:18.813: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee": Phase="Pending", Reason="", readiness=false. Elapsed: 86.604563ms
Jun 27 16:54:20.831: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10478147s
Jun 27 16:54:22.833: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106959713s
Jun 27 16:54:24.835: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.108773317s
Jun 27 16:54:24.835: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 27 16:54:24.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-2599" for this suite. 06/27/23 16:54:24.862
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":96,"skipped":1959,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.489 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:54:18.405
    Jun 27 16:54:18.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename security-context-test 06/27/23 16:54:18.416
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:18.534
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:18.621
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Jun 27 16:54:18.726: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee" in namespace "security-context-test-2599" to be "Succeeded or Failed"
    Jun 27 16:54:18.813: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee": Phase="Pending", Reason="", readiness=false. Elapsed: 86.604563ms
    Jun 27 16:54:20.831: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee": Phase="Pending", Reason="", readiness=false. Elapsed: 2.10478147s
    Jun 27 16:54:22.833: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee": Phase="Pending", Reason="", readiness=false. Elapsed: 4.106959713s
    Jun 27 16:54:24.835: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.108773317s
    Jun 27 16:54:24.835: INFO: Pod "busybox-readonly-false-ed217371-7719-4641-ab7a-44c3335451ee" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 27 16:54:24.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-2599" for this suite. 06/27/23 16:54:24.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:54:24.91
Jun 27 16:54:24.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename containers 06/27/23 16:54:24.911
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:24.959
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:24.975
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 06/27/23 16:54:24.989
Jun 27 16:54:25.058: INFO: Waiting up to 5m0s for pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9" in namespace "containers-7545" to be "Succeeded or Failed"
Jun 27 16:54:25.079: INFO: Pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.55218ms
Jun 27 16:54:27.098: INFO: Pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039874977s
Jun 27 16:54:29.104: INFO: Pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045633697s
STEP: Saw pod success 06/27/23 16:54:29.104
Jun 27 16:54:29.105: INFO: Pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9" satisfied condition "Succeeded or Failed"
Jun 27 16:54:29.122: INFO: Trying to get logs from node 10.113.180.90 pod client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9 container agnhost-container: <nil>
STEP: delete the pod 06/27/23 16:54:29.193
Jun 27 16:54:29.243: INFO: Waiting for pod client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9 to disappear
Jun 27 16:54:29.261: INFO: Pod client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 27 16:54:29.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-7545" for this suite. 06/27/23 16:54:29.287
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":97,"skipped":2040,"failed":0}
------------------------------
â€¢ [4.403 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:54:24.91
    Jun 27 16:54:24.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename containers 06/27/23 16:54:24.911
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:24.959
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:24.975
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 06/27/23 16:54:24.989
    Jun 27 16:54:25.058: INFO: Waiting up to 5m0s for pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9" in namespace "containers-7545" to be "Succeeded or Failed"
    Jun 27 16:54:25.079: INFO: Pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.55218ms
    Jun 27 16:54:27.098: INFO: Pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039874977s
    Jun 27 16:54:29.104: INFO: Pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.045633697s
    STEP: Saw pod success 06/27/23 16:54:29.104
    Jun 27 16:54:29.105: INFO: Pod "client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9" satisfied condition "Succeeded or Failed"
    Jun 27 16:54:29.122: INFO: Trying to get logs from node 10.113.180.90 pod client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9 container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 16:54:29.193
    Jun 27 16:54:29.243: INFO: Waiting for pod client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9 to disappear
    Jun 27 16:54:29.261: INFO: Pod client-containers-2eb67cab-3a7d-4a6c-9aa7-190e9d8071f9 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 27 16:54:29.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-7545" for this suite. 06/27/23 16:54:29.287
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:54:29.317
Jun 27 16:54:29.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:54:29.32
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:29.372
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:29.387
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 06/27/23 16:54:29.399
Jun 27 16:54:29.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a" in namespace "projected-5019" to be "Succeeded or Failed"
Jun 27 16:54:29.493: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.142191ms
Jun 27 16:54:31.512: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035482789s
Jun 27 16:54:33.513: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036518494s
Jun 27 16:54:35.513: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036685691s
STEP: Saw pod success 06/27/23 16:54:35.513
Jun 27 16:54:35.513: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a" satisfied condition "Succeeded or Failed"
Jun 27 16:54:35.534: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a container client-container: <nil>
STEP: delete the pod 06/27/23 16:54:35.584
Jun 27 16:54:35.657: INFO: Waiting for pod downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a to disappear
Jun 27 16:54:35.705: INFO: Pod downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 16:54:35.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5019" for this suite. 06/27/23 16:54:35.737
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":98,"skipped":2072,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.452 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:54:29.317
    Jun 27 16:54:29.317: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:54:29.32
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:29.372
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:29.387
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 06/27/23 16:54:29.399
    Jun 27 16:54:29.476: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a" in namespace "projected-5019" to be "Succeeded or Failed"
    Jun 27 16:54:29.493: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a": Phase="Pending", Reason="", readiness=false. Elapsed: 17.142191ms
    Jun 27 16:54:31.512: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035482789s
    Jun 27 16:54:33.513: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.036518494s
    Jun 27 16:54:35.513: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.036685691s
    STEP: Saw pod success 06/27/23 16:54:35.513
    Jun 27 16:54:35.513: INFO: Pod "downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a" satisfied condition "Succeeded or Failed"
    Jun 27 16:54:35.534: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a container client-container: <nil>
    STEP: delete the pod 06/27/23 16:54:35.584
    Jun 27 16:54:35.657: INFO: Waiting for pod downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a to disappear
    Jun 27 16:54:35.705: INFO: Pod downwardapi-volume-0063e543-c7b9-4c95-b19b-aa16def4405a no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 16:54:35.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5019" for this suite. 06/27/23 16:54:35.737
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:54:35.774
Jun 27 16:54:35.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename daemonsets 06/27/23 16:54:35.777
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:35.856
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:35.875
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 06/27/23 16:54:36.046
STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 16:54:36.078
Jun 27 16:54:36.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:54:36.118: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:54:37.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:54:37.190: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:54:38.173: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 27 16:54:38.173: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 16:54:39.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 27 16:54:39.168: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Getting /status 06/27/23 16:54:39.186
Jun 27 16:54:39.220: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 06/27/23 16:54:39.22
Jun 27 16:54:39.264: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 06/27/23 16:54:39.265
Jun 27 16:54:39.274: INFO: Observed &DaemonSet event: ADDED
Jun 27 16:54:39.274: INFO: Observed &DaemonSet event: MODIFIED
Jun 27 16:54:39.274: INFO: Observed &DaemonSet event: MODIFIED
Jun 27 16:54:39.274: INFO: Observed &DaemonSet event: MODIFIED
Jun 27 16:54:39.275: INFO: Observed &DaemonSet event: MODIFIED
Jun 27 16:54:39.275: INFO: Found daemon set daemon-set in namespace daemonsets-5564 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 27 16:54:39.275: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 06/27/23 16:54:39.275
STEP: watching for the daemon set status to be patched 06/27/23 16:54:39.306
Jun 27 16:54:39.313: INFO: Observed &DaemonSet event: ADDED
Jun 27 16:54:39.313: INFO: Observed &DaemonSet event: MODIFIED
Jun 27 16:54:39.314: INFO: Observed &DaemonSet event: MODIFIED
Jun 27 16:54:39.315: INFO: Observed &DaemonSet event: MODIFIED
Jun 27 16:54:39.316: INFO: Observed &DaemonSet event: MODIFIED
Jun 27 16:54:39.316: INFO: Observed daemon set daemon-set in namespace daemonsets-5564 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 27 16:54:39.317: INFO: Observed &DaemonSet event: MODIFIED
Jun 27 16:54:39.318: INFO: Found daemon set daemon-set in namespace daemonsets-5564 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Jun 27 16:54:39.318: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/27/23 16:54:39.339
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5564, will wait for the garbage collector to delete the pods 06/27/23 16:54:39.339
Jun 27 16:54:39.505: INFO: Deleting DaemonSet.extensions daemon-set took: 94.792728ms
Jun 27 16:54:39.606: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.820427ms
Jun 27 16:54:42.625: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 16:54:42.625: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 27 16:54:42.642: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"93252"},"items":null}

Jun 27 16:54:42.658: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"93252"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 27 16:54:42.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5564" for this suite. 06/27/23 16:54:42.778
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":99,"skipped":2097,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.034 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:54:35.774
    Jun 27 16:54:35.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename daemonsets 06/27/23 16:54:35.777
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:35.856
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:35.875
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 06/27/23 16:54:36.046
    STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 16:54:36.078
    Jun 27 16:54:36.118: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:54:36.118: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:54:37.190: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:54:37.190: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:54:38.173: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 27 16:54:38.173: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 16:54:39.168: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 27 16:54:39.168: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Getting /status 06/27/23 16:54:39.186
    Jun 27 16:54:39.220: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 06/27/23 16:54:39.22
    Jun 27 16:54:39.264: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 06/27/23 16:54:39.265
    Jun 27 16:54:39.274: INFO: Observed &DaemonSet event: ADDED
    Jun 27 16:54:39.274: INFO: Observed &DaemonSet event: MODIFIED
    Jun 27 16:54:39.274: INFO: Observed &DaemonSet event: MODIFIED
    Jun 27 16:54:39.274: INFO: Observed &DaemonSet event: MODIFIED
    Jun 27 16:54:39.275: INFO: Observed &DaemonSet event: MODIFIED
    Jun 27 16:54:39.275: INFO: Found daemon set daemon-set in namespace daemonsets-5564 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 27 16:54:39.275: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 06/27/23 16:54:39.275
    STEP: watching for the daemon set status to be patched 06/27/23 16:54:39.306
    Jun 27 16:54:39.313: INFO: Observed &DaemonSet event: ADDED
    Jun 27 16:54:39.313: INFO: Observed &DaemonSet event: MODIFIED
    Jun 27 16:54:39.314: INFO: Observed &DaemonSet event: MODIFIED
    Jun 27 16:54:39.315: INFO: Observed &DaemonSet event: MODIFIED
    Jun 27 16:54:39.316: INFO: Observed &DaemonSet event: MODIFIED
    Jun 27 16:54:39.316: INFO: Observed daemon set daemon-set in namespace daemonsets-5564 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 27 16:54:39.317: INFO: Observed &DaemonSet event: MODIFIED
    Jun 27 16:54:39.318: INFO: Found daemon set daemon-set in namespace daemonsets-5564 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Jun 27 16:54:39.318: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/27/23 16:54:39.339
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5564, will wait for the garbage collector to delete the pods 06/27/23 16:54:39.339
    Jun 27 16:54:39.505: INFO: Deleting DaemonSet.extensions daemon-set took: 94.792728ms
    Jun 27 16:54:39.606: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.820427ms
    Jun 27 16:54:42.625: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 16:54:42.625: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 27 16:54:42.642: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"93252"},"items":null}

    Jun 27 16:54:42.658: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"93252"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 16:54:42.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5564" for this suite. 06/27/23 16:54:42.778
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:54:42.811
Jun 27 16:54:42.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename events 06/27/23 16:54:42.813
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:42.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:42.89
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 06/27/23 16:54:42.906
Jun 27 16:54:42.945: INFO: created test-event-1
Jun 27 16:54:42.967: INFO: created test-event-2
Jun 27 16:54:42.991: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 06/27/23 16:54:42.991
STEP: delete collection of events 06/27/23 16:54:43.009
Jun 27 16:54:43.010: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 06/27/23 16:54:43.132
Jun 27 16:54:43.132: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jun 27 16:54:43.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3100" for this suite. 06/27/23 16:54:43.178
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":100,"skipped":2103,"failed":0}
------------------------------
â€¢ [0.398 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:54:42.811
    Jun 27 16:54:42.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename events 06/27/23 16:54:42.813
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:42.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:42.89
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 06/27/23 16:54:42.906
    Jun 27 16:54:42.945: INFO: created test-event-1
    Jun 27 16:54:42.967: INFO: created test-event-2
    Jun 27 16:54:42.991: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 06/27/23 16:54:42.991
    STEP: delete collection of events 06/27/23 16:54:43.009
    Jun 27 16:54:43.010: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 06/27/23 16:54:43.132
    Jun 27 16:54:43.132: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jun 27 16:54:43.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3100" for this suite. 06/27/23 16:54:43.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:54:43.216
Jun 27 16:54:43.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-pred 06/27/23 16:54:43.221
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:43.267
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:43.286
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 27 16:54:43.313: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 27 16:54:43.366: INFO: Waiting for terminating namespaces to be deleted...
Jun 27 16:54:43.394: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.89 before test
Jun 27 16:54:43.473: INFO: calico-node-vplt6 from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.474: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 16:54:43.474: INFO: calico-typha-657df678d7-65kdh from calico-system started at 2023-06-27 13:54:48 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.474: INFO: 	Container calico-typha ready: true, restart count 0
Jun 27 16:54:43.474: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.474: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
Jun 27 16:54:43.474: INFO: ibm-keepalived-watcher-9rf8d from kube-system started at 2023-06-27 13:53:32 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.474: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 16:54:43.474: INFO: ibm-master-proxy-static-10.113.180.89 from kube-system started at 2023-06-27 13:53:30 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.474: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 16:54:43.474: INFO: 	Container pause ready: true, restart count 0
Jun 27 16:54:43.474: INFO: ibmcloud-block-storage-driver-5ml2s from kube-system started at 2023-06-27 13:53:43 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.475: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 16:54:43.475: INFO: vpn-546f6bf578-7ks4f from kube-system started at 2023-06-27 14:06:39 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.475: INFO: 	Container vpn ready: true, restart count 0
Jun 27 16:54:43.475: INFO: tuned-mwm8w from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.475: INFO: 	Container tuned ready: true, restart count 0
Jun 27 16:54:43.475: INFO: csi-snapshot-controller-5694c47cbb-9pvhd from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.475: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 27 16:54:43.475: INFO: csi-snapshot-webhook-5d7cc7f6cb-lwgl9 from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.475: INFO: 	Container webhook ready: true, restart count 0
Jun 27 16:54:43.475: INFO: console-6c8658586b-jlcfh from openshift-console started at 2023-06-27 14:03:39 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.475: INFO: 	Container console ready: true, restart count 0
Jun 27 16:54:43.476: INFO: downloads-57bd479866-2fpqm from openshift-console started at 2023-06-27 13:56:16 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.476: INFO: 	Container download-server ready: true, restart count 0
Jun 27 16:54:43.476: INFO: dns-default-z76hf from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.476: INFO: 	Container dns ready: true, restart count 0
Jun 27 16:54:43.476: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.476: INFO: node-resolver-cwltw from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.476: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 16:54:43.476: INFO: image-registry-7f546fc5bb-n7w67 from openshift-image-registry started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.476: INFO: 	Container registry ready: true, restart count 0
Jun 27 16:54:43.476: INFO: node-ca-d4q6s from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.476: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 16:54:43.476: INFO: registry-pvc-permissions-4bk2b from openshift-image-registry started at 2023-06-27 14:02:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.476: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 27 16:54:43.476: INFO: ingress-canary-bmwtq from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.477: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 16:54:43.477: INFO: router-default-7f97cd5c5f-m5pcx from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.477: INFO: 	Container router ready: true, restart count 0
Jun 27 16:54:43.477: INFO: openshift-kube-proxy-b4224 from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.477: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 16:54:43.477: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.477: INFO: certified-operators-799hx from openshift-marketplace started at 2023-06-27 16:44:15 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.477: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 16:54:43.477: INFO: community-operators-9jptm from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.477: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 16:54:43.477: INFO: redhat-marketplace-qzk6b from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.477: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 16:54:43.477: INFO: redhat-operators-jqjb9 from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.477: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 16:54:43.477: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-06-27 14:02:16 +0000 UTC (6 container statuses recorded)
Jun 27 16:54:43.477: INFO: 	Container alertmanager ready: true, restart count 1
Jun 27 16:54:43.477: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 27 16:54:43.478: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jun 27 16:54:43.478: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 16:54:43.478: INFO: kube-state-metrics-685444f59b-5dlq4 from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 27 16:54:43.478: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 27 16:54:43.478: INFO: node-exporter-lqlws from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.478: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 16:54:43.478: INFO: openshift-state-metrics-6d79d8c586-9mgzr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 27 16:54:43.478: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 27 16:54:43.478: INFO: prometheus-adapter-648f68fcc-h27rt from openshift-monitoring started at 2023-06-27 14:01:39 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.478: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 27 16:54:43.479: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-06-27 14:01:59 +0000 UTC (6 container statuses recorded)
Jun 27 16:54:43.479: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 27 16:54:43.479: INFO: 	Container prometheus ready: true, restart count 0
Jun 27 16:54:43.479: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 27 16:54:43.479: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 27 16:54:43.479: INFO: prometheus-operator-68dfcc5c8-kn6tn from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.479: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 27 16:54:43.479: INFO: prometheus-operator-admission-webhook-6c667b594b-bds22 from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.479: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jun 27 16:54:43.479: INFO: telemeter-client-7dc8fdddc8-kpccw from openshift-monitoring started at 2023-06-27 14:00:33 +0000 UTC (3 container statuses recorded)
Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.479: INFO: 	Container reload ready: true, restart count 0
Jun 27 16:54:43.479: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 27 16:54:43.479: INFO: thanos-querier-754f675f77-6nf9l from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.480: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jun 27 16:54:43.480: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 27 16:54:43.480: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 27 16:54:43.480: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 16:54:43.480: INFO: 	Container thanos-query ready: true, restart count 0
Jun 27 16:54:43.480: INFO: multus-7zznf from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.480: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 16:54:43.480: INFO: multus-additional-cni-plugins-pn2hb from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.480: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 16:54:43.480: INFO: multus-admission-controller-754d449d79-strmv from openshift-multus started at 2023-06-27 16:44:18 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.480: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 27 16:54:43.480: INFO: network-metrics-daemon-p8dh4 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.480: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 16:54:43.480: INFO: network-check-target-7qghz from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.481: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 16:54:43.481: INFO: packageserver-8d78bf5dd-q8ct5 from openshift-operator-lifecycle-manager started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.481: INFO: 	Container packageserver ready: true, restart count 0
Jun 27 16:54:43.481: INFO: sonobuoy from sonobuoy started at 2023-06-27 16:21:42 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.481: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 27 16:54:43.481: INFO: sonobuoy-e2e-job-0e516cd149fa4871 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.481: INFO: 	Container e2e ready: true, restart count 0
Jun 27 16:54:43.481: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 16:54:43.481: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.481: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 16:54:43.481: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 27 16:54:43.481: INFO: tigera-operator-687c49f5c8-qcfmm from tigera-operator started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.481: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 27 16:54:43.481: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.90 before test
Jun 27 16:54:43.537: INFO: calico-node-6j4xp from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 16:54:43.537: INFO: ibm-keepalived-watcher-bpqvm from kube-system started at 2023-06-27 13:53:26 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 16:54:43.537: INFO: ibm-master-proxy-static-10.113.180.90 from kube-system started at 2023-06-27 13:53:22 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 16:54:43.537: INFO: 	Container pause ready: true, restart count 0
Jun 27 16:54:43.537: INFO: ibmcloud-block-storage-driver-jb9wr from kube-system started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 16:54:43.537: INFO: tuned-md8mr from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container tuned ready: true, restart count 0
Jun 27 16:54:43.537: INFO: dns-default-q6q9l from openshift-dns started at 2023-06-27 16:44:33 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container dns ready: true, restart count 0
Jun 27 16:54:43.537: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.537: INFO: node-resolver-dtvx7 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 16:54:43.537: INFO: node-ca-8pgp8 from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 16:54:43.537: INFO: ingress-canary-hfzkr from openshift-ingress-canary started at 2023-06-27 16:44:33 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 16:54:43.537: INFO: openshift-kube-proxy-bf9cj from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 16:54:43.537: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.537: INFO: node-exporter-5vgpm from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.537: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 16:54:43.537: INFO: multus-additional-cni-plugins-qszdk from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 16:54:43.537: INFO: multus-skrpb from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 16:54:43.537: INFO: network-metrics-daemon-4jmw9 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.537: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 16:54:43.537: INFO: network-check-target-zmtm2 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 16:54:43.537: INFO: collect-profiles-28131405-8w5hd from openshift-operator-lifecycle-manager started at 2023-06-27 16:45:00 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 27 16:54:43.537: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.537: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 16:54:43.537: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 27 16:54:43.537: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.96 before test
Jun 27 16:54:43.647: INFO: calico-kube-controllers-79f474fb8-m45qw from calico-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 27 16:54:43.647: INFO: calico-node-z28vk from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 16:54:43.647: INFO: calico-typha-657df678d7-4ch2t from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container calico-typha ready: true, restart count 0
Jun 27 16:54:43.647: INFO: managed-storage-validation-webhooks-665764b6b4-r77pv from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 16:54:43.647: INFO: managed-storage-validation-webhooks-665764b6b4-v4g8s from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 16:54:43.647: INFO: managed-storage-validation-webhooks-665764b6b4-vhqr7 from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 16:54:43.647: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-tnr88 from ibm-system started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
Jun 27 16:54:43.647: INFO: ibm-file-plugin-84cc66cdb-97nzr from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 27 16:54:43.647: INFO: ibm-keepalived-watcher-h8p9q from kube-system started at 2023-06-27 13:53:48 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 16:54:43.647: INFO: ibm-master-proxy-static-10.113.180.96 from kube-system started at 2023-06-27 13:53:45 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 16:54:43.647: INFO: 	Container pause ready: true, restart count 0
Jun 27 16:54:43.647: INFO: ibm-storage-metrics-agent-746d76fb56-nvvw9 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.647: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Jun 27 16:54:43.648: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Jun 27 16:54:43.648: INFO: ibm-storage-watcher-676b4ddbc6-dz54r from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 27 16:54:43.648: INFO: ibmcloud-block-storage-driver-pkhxd from kube-system started at 2023-06-27 13:53:53 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 16:54:43.648: INFO: ibmcloud-block-storage-plugin-5f677b8577-8f7s7 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 27 16:54:43.648: INFO: cluster-node-tuning-operator-848d57c5bb-7ln5t from openshift-cluster-node-tuning-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 27 16:54:43.648: INFO: tuned-kk4gn from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container tuned ready: true, restart count 0
Jun 27 16:54:43.648: INFO: cluster-samples-operator-769ddfc8cc-88z62 from openshift-cluster-samples-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 27 16:54:43.648: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 27 16:54:43.648: INFO: cluster-storage-operator-5fc995c76b-jhmzr from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 27 16:54:43.648: INFO: csi-snapshot-controller-5694c47cbb-qfvbs from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 27 16:54:43.648: INFO: csi-snapshot-controller-operator-64ff6856c8-l29px from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 27 16:54:43.648: INFO: csi-snapshot-webhook-5d7cc7f6cb-gzt6x from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container webhook ready: true, restart count 0
Jun 27 16:54:43.648: INFO: console-operator-5694fff656-5wp7b from openshift-console-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container console-operator ready: true, restart count 1
Jun 27 16:54:43.648: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Jun 27 16:54:43.648: INFO: console-6c8658586b-rc55z from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.648: INFO: 	Container console ready: true, restart count 0
Jun 27 16:54:43.651: INFO: downloads-57bd479866-8bj2l from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container download-server ready: true, restart count 0
Jun 27 16:54:43.651: INFO: dns-operator-b64d4b4c7-5p5hs from openshift-dns-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container dns-operator ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: dns-default-q8pl2 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container dns ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: node-resolver-nbx64 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 16:54:43.651: INFO: cluster-image-registry-operator-55bdbc85cd-vpldl from openshift-image-registry started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 27 16:54:43.651: INFO: node-ca-tbfkf from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 16:54:43.651: INFO: ingress-canary-mtqfl from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 16:54:43.651: INFO: ingress-operator-5c457b57b4-67rvh from openshift-ingress-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: router-default-7f97cd5c5f-n56jj from openshift-ingress started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container router ready: true, restart count 0
Jun 27 16:54:43.651: INFO: insights-operator-8f4d6c949-n2nlt from openshift-insights started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container insights-operator ready: true, restart count 1
Jun 27 16:54:43.651: INFO: openshift-kube-proxy-mpv6g from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: kube-storage-version-migrator-operator-59f4c7c696-jq8gm from openshift-kube-storage-version-migrator-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 27 16:54:43.651: INFO: migrator-6795cdbdb7-qsfqv from openshift-kube-storage-version-migrator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container migrator ready: true, restart count 0
Jun 27 16:54:43.651: INFO: marketplace-operator-57bd87cfb8-5hhm8 from openshift-marketplace started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 27 16:54:43.651: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-06-27 16:44:14 +0000 UTC (6 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container alertmanager ready: true, restart count 1
Jun 27 16:54:43.651: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: cluster-monitoring-operator-644d445879-l5jj8 from openshift-monitoring started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: node-exporter-l5dxr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 16:54:43.651: INFO: prometheus-adapter-648f68fcc-mpvkt from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 27 16:54:43.651: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-06-27 16:44:13 +0000 UTC (6 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container prometheus ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 27 16:54:43.651: INFO: prometheus-operator-admission-webhook-6c667b594b-c9dw4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jun 27 16:54:43.651: INFO: thanos-querier-754f675f77-gs6k4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (6 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container thanos-query ready: true, restart count 0
Jun 27 16:54:43.651: INFO: multus-additional-cni-plugins-xqsgs from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 16:54:43.651: INFO: multus-admission-controller-754d449d79-6cbgx from openshift-multus started at 2023-06-27 16:44:27 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 27 16:54:43.651: INFO: multus-kx7qm from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 16:54:43.651: INFO: network-metrics-daemon-cbrq2 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 16:54:43.651: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 16:54:43.651: INFO: network-check-source-6b8766f86f-s8s7j from openshift-network-diagnostics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 27 16:54:43.651: INFO: network-check-target-6lv98 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 16:54:43.651: INFO: network-operator-ffb9884c5-25r42 from openshift-network-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container network-operator ready: true, restart count 0
Jun 27 16:54:43.651: INFO: catalog-operator-58898485cd-qjmms from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.651: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 27 16:54:43.651: INFO: olm-operator-7959cfcd9f-dt77g from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.652: INFO: 	Container olm-operator ready: true, restart count 0
Jun 27 16:54:43.652: INFO: package-server-manager-f7b6c466f-9bbzw from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.652: INFO: 	Container package-server-manager ready: true, restart count 0
Jun 27 16:54:43.652: INFO: packageserver-8d78bf5dd-r9g9t from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.652: INFO: 	Container packageserver ready: true, restart count 0
Jun 27 16:54:43.652: INFO: metrics-6d46d44d8f-jbnwq from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.652: INFO: 	Container metrics ready: true, restart count 3
Jun 27 16:54:43.652: INFO: push-gateway-5465b544cc-f8v8q from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.652: INFO: 	Container push-gateway ready: true, restart count 0
Jun 27 16:54:43.652: INFO: service-ca-operator-74dd8bfc8-cl4bp from openshift-service-ca-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.652: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 27 16:54:43.652: INFO: service-ca-6f86485857-drj9g from openshift-service-ca started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
Jun 27 16:54:43.652: INFO: 	Container service-ca-controller ready: false, restart count 0
Jun 27 16:54:43.652: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 16:54:43.652: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 16:54:43.652: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/27/23 16:54:43.652
Jun 27 16:54:43.713: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8078" to be "running"
Jun 27 16:54:43.738: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 25.151145ms
Jun 27 16:54:45.761: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048286558s
Jun 27 16:54:47.762: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.049689961s
Jun 27 16:54:47.762: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/27/23 16:54:47.781
STEP: Trying to apply a random label on the found node. 06/27/23 16:54:47.835
STEP: verifying the node has the label kubernetes.io/e2e-1ee0ebd5-e5a5-4fab-9b6a-d479f03dc007 42 06/27/23 16:54:47.887
STEP: Trying to relaunch the pod, now with labels. 06/27/23 16:54:47.911
Jun 27 16:54:47.960: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8078" to be "not pending"
Jun 27 16:54:47.981: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 21.031201ms
Jun 27 16:54:50.009: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049231668s
Jun 27 16:54:52.035: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.074854383s
Jun 27 16:54:52.035: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-1ee0ebd5-e5a5-4fab-9b6a-d479f03dc007 off the node 10.113.180.90 06/27/23 16:54:52.072
STEP: verifying the node doesn't have the label kubernetes.io/e2e-1ee0ebd5-e5a5-4fab-9b6a-d479f03dc007 06/27/23 16:54:52.169
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 27 16:54:52.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-8078" for this suite. 06/27/23 16:54:52.294
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":101,"skipped":2118,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.106 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:54:43.216
    Jun 27 16:54:43.217: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-pred 06/27/23 16:54:43.221
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:43.267
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:43.286
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 27 16:54:43.313: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 27 16:54:43.366: INFO: Waiting for terminating namespaces to be deleted...
    Jun 27 16:54:43.394: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.89 before test
    Jun 27 16:54:43.473: INFO: calico-node-vplt6 from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.474: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 16:54:43.474: INFO: calico-typha-657df678d7-65kdh from calico-system started at 2023-06-27 13:54:48 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.474: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 27 16:54:43.474: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.474: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
    Jun 27 16:54:43.474: INFO: ibm-keepalived-watcher-9rf8d from kube-system started at 2023-06-27 13:53:32 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.474: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 16:54:43.474: INFO: ibm-master-proxy-static-10.113.180.89 from kube-system started at 2023-06-27 13:53:30 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.474: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 16:54:43.474: INFO: 	Container pause ready: true, restart count 0
    Jun 27 16:54:43.474: INFO: ibmcloud-block-storage-driver-5ml2s from kube-system started at 2023-06-27 13:53:43 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.475: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 16:54:43.475: INFO: vpn-546f6bf578-7ks4f from kube-system started at 2023-06-27 14:06:39 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.475: INFO: 	Container vpn ready: true, restart count 0
    Jun 27 16:54:43.475: INFO: tuned-mwm8w from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.475: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 16:54:43.475: INFO: csi-snapshot-controller-5694c47cbb-9pvhd from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.475: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 27 16:54:43.475: INFO: csi-snapshot-webhook-5d7cc7f6cb-lwgl9 from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.475: INFO: 	Container webhook ready: true, restart count 0
    Jun 27 16:54:43.475: INFO: console-6c8658586b-jlcfh from openshift-console started at 2023-06-27 14:03:39 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.475: INFO: 	Container console ready: true, restart count 0
    Jun 27 16:54:43.476: INFO: downloads-57bd479866-2fpqm from openshift-console started at 2023-06-27 13:56:16 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.476: INFO: 	Container download-server ready: true, restart count 0
    Jun 27 16:54:43.476: INFO: dns-default-z76hf from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.476: INFO: 	Container dns ready: true, restart count 0
    Jun 27 16:54:43.476: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.476: INFO: node-resolver-cwltw from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.476: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 16:54:43.476: INFO: image-registry-7f546fc5bb-n7w67 from openshift-image-registry started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.476: INFO: 	Container registry ready: true, restart count 0
    Jun 27 16:54:43.476: INFO: node-ca-d4q6s from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.476: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 16:54:43.476: INFO: registry-pvc-permissions-4bk2b from openshift-image-registry started at 2023-06-27 14:02:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.476: INFO: 	Container pvc-permissions ready: false, restart count 0
    Jun 27 16:54:43.476: INFO: ingress-canary-bmwtq from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.477: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 16:54:43.477: INFO: router-default-7f97cd5c5f-m5pcx from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.477: INFO: 	Container router ready: true, restart count 0
    Jun 27 16:54:43.477: INFO: openshift-kube-proxy-b4224 from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.477: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 16:54:43.477: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.477: INFO: certified-operators-799hx from openshift-marketplace started at 2023-06-27 16:44:15 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.477: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 16:54:43.477: INFO: community-operators-9jptm from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.477: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 16:54:43.477: INFO: redhat-marketplace-qzk6b from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.477: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 16:54:43.477: INFO: redhat-operators-jqjb9 from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.477: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 16:54:43.477: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-06-27 14:02:16 +0000 UTC (6 container statuses recorded)
    Jun 27 16:54:43.477: INFO: 	Container alertmanager ready: true, restart count 1
    Jun 27 16:54:43.477: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: kube-state-metrics-685444f59b-5dlq4 from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
    Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: node-exporter-lqlws from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: openshift-state-metrics-6d79d8c586-9mgzr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
    Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jun 27 16:54:43.478: INFO: prometheus-adapter-648f68fcc-h27rt from openshift-monitoring started at 2023-06-27 14:01:39 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.478: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-06-27 14:01:59 +0000 UTC (6 container statuses recorded)
    Jun 27 16:54:43.479: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: 	Container prometheus ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: prometheus-operator-68dfcc5c8-kn6tn from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: prometheus-operator-admission-webhook-6c667b594b-bds22 from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.479: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: telemeter-client-7dc8fdddc8-kpccw from openshift-monitoring started at 2023-06-27 14:00:33 +0000 UTC (3 container statuses recorded)
    Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: 	Container reload ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: 	Container telemeter-client ready: true, restart count 0
    Jun 27 16:54:43.479: INFO: thanos-querier-754f675f77-6nf9l from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
    Jun 27 16:54:43.479: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: 	Container thanos-query ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: multus-7zznf from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.480: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: multus-additional-cni-plugins-pn2hb from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.480: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: multus-admission-controller-754d449d79-strmv from openshift-multus started at 2023-06-27 16:44:18 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: network-metrics-daemon-p8dh4 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.480: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 16:54:43.480: INFO: network-check-target-7qghz from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.481: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 16:54:43.481: INFO: packageserver-8d78bf5dd-q8ct5 from openshift-operator-lifecycle-manager started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.481: INFO: 	Container packageserver ready: true, restart count 0
    Jun 27 16:54:43.481: INFO: sonobuoy from sonobuoy started at 2023-06-27 16:21:42 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.481: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 27 16:54:43.481: INFO: sonobuoy-e2e-job-0e516cd149fa4871 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.481: INFO: 	Container e2e ready: true, restart count 0
    Jun 27 16:54:43.481: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 16:54:43.481: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.481: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 16:54:43.481: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 27 16:54:43.481: INFO: tigera-operator-687c49f5c8-qcfmm from tigera-operator started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.481: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun 27 16:54:43.481: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.90 before test
    Jun 27 16:54:43.537: INFO: calico-node-6j4xp from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: ibm-keepalived-watcher-bpqvm from kube-system started at 2023-06-27 13:53:26 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: ibm-master-proxy-static-10.113.180.90 from kube-system started at 2023-06-27 13:53:22 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: 	Container pause ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: ibmcloud-block-storage-driver-jb9wr from kube-system started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: tuned-md8mr from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: dns-default-q6q9l from openshift-dns started at 2023-06-27 16:44:33 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container dns ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: node-resolver-dtvx7 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: node-ca-8pgp8 from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: ingress-canary-hfzkr from openshift-ingress-canary started at 2023-06-27 16:44:33 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: openshift-kube-proxy-bf9cj from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: node-exporter-5vgpm from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: multus-additional-cni-plugins-qszdk from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: multus-skrpb from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: network-metrics-daemon-4jmw9 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: network-check-target-zmtm2 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: collect-profiles-28131405-8w5hd from openshift-operator-lifecycle-manager started at 2023-06-27 16:45:00 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container collect-profiles ready: false, restart count 0
    Jun 27 16:54:43.537: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.537: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 27 16:54:43.537: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.96 before test
    Jun 27 16:54:43.647: INFO: calico-kube-controllers-79f474fb8-m45qw from calico-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: calico-node-z28vk from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: calico-typha-657df678d7-4ch2t from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: managed-storage-validation-webhooks-665764b6b4-r77pv from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: managed-storage-validation-webhooks-665764b6b4-v4g8s from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: managed-storage-validation-webhooks-665764b6b4-vhqr7 from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-tnr88 from ibm-system started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: ibm-file-plugin-84cc66cdb-97nzr from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: ibm-keepalived-watcher-h8p9q from kube-system started at 2023-06-27 13:53:48 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: ibm-master-proxy-static-10.113.180.96 from kube-system started at 2023-06-27 13:53:45 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: 	Container pause ready: true, restart count 0
    Jun 27 16:54:43.647: INFO: ibm-storage-metrics-agent-746d76fb56-nvvw9 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.647: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: ibm-storage-watcher-676b4ddbc6-dz54r from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: ibmcloud-block-storage-driver-pkhxd from kube-system started at 2023-06-27 13:53:53 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: ibmcloud-block-storage-plugin-5f677b8577-8f7s7 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: cluster-node-tuning-operator-848d57c5bb-7ln5t from openshift-cluster-node-tuning-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: tuned-kk4gn from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: cluster-samples-operator-769ddfc8cc-88z62 from openshift-cluster-samples-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: cluster-storage-operator-5fc995c76b-jhmzr from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Jun 27 16:54:43.648: INFO: csi-snapshot-controller-5694c47cbb-qfvbs from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: csi-snapshot-controller-operator-64ff6856c8-l29px from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: csi-snapshot-webhook-5d7cc7f6cb-gzt6x from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container webhook ready: true, restart count 0
    Jun 27 16:54:43.648: INFO: console-operator-5694fff656-5wp7b from openshift-console-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container console-operator ready: true, restart count 1
    Jun 27 16:54:43.648: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Jun 27 16:54:43.648: INFO: console-6c8658586b-rc55z from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.648: INFO: 	Container console ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: downloads-57bd479866-8bj2l from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container download-server ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: dns-operator-b64d4b4c7-5p5hs from openshift-dns-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container dns-operator ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: dns-default-q8pl2 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container dns ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: node-resolver-nbx64 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: cluster-image-registry-operator-55bdbc85cd-vpldl from openshift-image-registry started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: node-ca-tbfkf from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: ingress-canary-mtqfl from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: ingress-operator-5c457b57b4-67rvh from openshift-ingress-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container ingress-operator ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: router-default-7f97cd5c5f-n56jj from openshift-ingress started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container router ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: insights-operator-8f4d6c949-n2nlt from openshift-insights started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container insights-operator ready: true, restart count 1
    Jun 27 16:54:43.651: INFO: openshift-kube-proxy-mpv6g from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: kube-storage-version-migrator-operator-59f4c7c696-jq8gm from openshift-kube-storage-version-migrator-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Jun 27 16:54:43.651: INFO: migrator-6795cdbdb7-qsfqv from openshift-kube-storage-version-migrator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container migrator ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: marketplace-operator-57bd87cfb8-5hhm8 from openshift-marketplace started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container marketplace-operator ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-06-27 16:44:14 +0000 UTC (6 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container alertmanager ready: true, restart count 1
    Jun 27 16:54:43.651: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: cluster-monitoring-operator-644d445879-l5jj8 from openshift-monitoring started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: node-exporter-l5dxr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: prometheus-adapter-648f68fcc-mpvkt from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-06-27 16:44:13 +0000 UTC (6 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container prometheus ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: prometheus-operator-admission-webhook-6c667b594b-c9dw4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: thanos-querier-754f675f77-gs6k4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (6 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container thanos-query ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: multus-additional-cni-plugins-xqsgs from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: multus-admission-controller-754d449d79-6cbgx from openshift-multus started at 2023-06-27 16:44:27 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: multus-kx7qm from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: network-metrics-daemon-cbrq2 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: network-check-source-6b8766f86f-s8s7j from openshift-network-diagnostics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container check-endpoints ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: network-check-target-6lv98 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: network-operator-ffb9884c5-25r42 from openshift-network-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container network-operator ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: catalog-operator-58898485cd-qjmms from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.651: INFO: 	Container catalog-operator ready: true, restart count 0
    Jun 27 16:54:43.651: INFO: olm-operator-7959cfcd9f-dt77g from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.652: INFO: 	Container olm-operator ready: true, restart count 0
    Jun 27 16:54:43.652: INFO: package-server-manager-f7b6c466f-9bbzw from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.652: INFO: 	Container package-server-manager ready: true, restart count 0
    Jun 27 16:54:43.652: INFO: packageserver-8d78bf5dd-r9g9t from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.652: INFO: 	Container packageserver ready: true, restart count 0
    Jun 27 16:54:43.652: INFO: metrics-6d46d44d8f-jbnwq from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.652: INFO: 	Container metrics ready: true, restart count 3
    Jun 27 16:54:43.652: INFO: push-gateway-5465b544cc-f8v8q from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.652: INFO: 	Container push-gateway ready: true, restart count 0
    Jun 27 16:54:43.652: INFO: service-ca-operator-74dd8bfc8-cl4bp from openshift-service-ca-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.652: INFO: 	Container service-ca-operator ready: true, restart count 1
    Jun 27 16:54:43.652: INFO: service-ca-6f86485857-drj9g from openshift-service-ca started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
    Jun 27 16:54:43.652: INFO: 	Container service-ca-controller ready: false, restart count 0
    Jun 27 16:54:43.652: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 16:54:43.652: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 16:54:43.652: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/27/23 16:54:43.652
    Jun 27 16:54:43.713: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-8078" to be "running"
    Jun 27 16:54:43.738: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 25.151145ms
    Jun 27 16:54:45.761: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048286558s
    Jun 27 16:54:47.762: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.049689961s
    Jun 27 16:54:47.762: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/27/23 16:54:47.781
    STEP: Trying to apply a random label on the found node. 06/27/23 16:54:47.835
    STEP: verifying the node has the label kubernetes.io/e2e-1ee0ebd5-e5a5-4fab-9b6a-d479f03dc007 42 06/27/23 16:54:47.887
    STEP: Trying to relaunch the pod, now with labels. 06/27/23 16:54:47.911
    Jun 27 16:54:47.960: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-8078" to be "not pending"
    Jun 27 16:54:47.981: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 21.031201ms
    Jun 27 16:54:50.009: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 2.049231668s
    Jun 27 16:54:52.035: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 4.074854383s
    Jun 27 16:54:52.035: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-1ee0ebd5-e5a5-4fab-9b6a-d479f03dc007 off the node 10.113.180.90 06/27/23 16:54:52.072
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-1ee0ebd5-e5a5-4fab-9b6a-d479f03dc007 06/27/23 16:54:52.169
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 16:54:52.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-8078" for this suite. 06/27/23 16:54:52.294
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:54:52.323
Jun 27 16:54:52.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:54:52.325
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:52.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:52.418
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-90dbdfa9-a6ae-4ff4-b500-55756854ad92 06/27/23 16:54:52.458
STEP: Creating a pod to test consume configMaps 06/27/23 16:54:52.485
Jun 27 16:54:52.553: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57" in namespace "projected-9654" to be "Succeeded or Failed"
Jun 27 16:54:52.579: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57": Phase="Pending", Reason="", readiness=false. Elapsed: 25.799625ms
Jun 27 16:54:54.597: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043978371s
Jun 27 16:54:56.600: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046725216s
Jun 27 16:54:58.599: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045750884s
STEP: Saw pod success 06/27/23 16:54:58.599
Jun 27 16:54:58.599: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57" satisfied condition "Succeeded or Failed"
Jun 27 16:54:58.629: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57 container agnhost-container: <nil>
STEP: delete the pod 06/27/23 16:54:58.692
Jun 27 16:54:58.741: INFO: Waiting for pod pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57 to disappear
Jun 27 16:54:58.759: INFO: Pod pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 27 16:54:58.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9654" for this suite. 06/27/23 16:54:58.798
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":102,"skipped":2120,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.500 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:54:52.323
    Jun 27 16:54:52.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:54:52.325
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:52.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:52.418
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-90dbdfa9-a6ae-4ff4-b500-55756854ad92 06/27/23 16:54:52.458
    STEP: Creating a pod to test consume configMaps 06/27/23 16:54:52.485
    Jun 27 16:54:52.553: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57" in namespace "projected-9654" to be "Succeeded or Failed"
    Jun 27 16:54:52.579: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57": Phase="Pending", Reason="", readiness=false. Elapsed: 25.799625ms
    Jun 27 16:54:54.597: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043978371s
    Jun 27 16:54:56.600: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046725216s
    Jun 27 16:54:58.599: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.045750884s
    STEP: Saw pod success 06/27/23 16:54:58.599
    Jun 27 16:54:58.599: INFO: Pod "pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57" satisfied condition "Succeeded or Failed"
    Jun 27 16:54:58.629: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57 container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 16:54:58.692
    Jun 27 16:54:58.741: INFO: Waiting for pod pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57 to disappear
    Jun 27 16:54:58.759: INFO: Pod pod-projected-configmaps-3ae4f5e5-232e-4994-8e9a-dfd81b9c8c57 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 27 16:54:58.760: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9654" for this suite. 06/27/23 16:54:58.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:54:58.824
Jun 27 16:54:58.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubelet-test 06/27/23 16:54:58.832
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:58.876
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:58.893
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Jun 27 16:54:58.967: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625" in namespace "kubelet-test-2297" to be "running and ready"
Jun 27 16:54:58.986: INFO: Pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625": Phase="Pending", Reason="", readiness=false. Elapsed: 19.308148ms
Jun 27 16:54:58.986: INFO: The phase of Pod busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:55:01.007: INFO: Pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04050877s
Jun 27 16:55:01.007: INFO: The phase of Pod busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:55:03.009: INFO: Pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625": Phase="Running", Reason="", readiness=true. Elapsed: 4.042657734s
Jun 27 16:55:03.009: INFO: The phase of Pod busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625 is Running (Ready = true)
Jun 27 16:55:03.009: INFO: Pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 27 16:55:03.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-2297" for this suite. 06/27/23 16:55:03.159
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":103,"skipped":2138,"failed":0}
------------------------------
â€¢ [4.361 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:54:58.824
    Jun 27 16:54:58.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubelet-test 06/27/23 16:54:58.832
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:54:58.876
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:54:58.893
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Jun 27 16:54:58.967: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625" in namespace "kubelet-test-2297" to be "running and ready"
    Jun 27 16:54:58.986: INFO: Pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625": Phase="Pending", Reason="", readiness=false. Elapsed: 19.308148ms
    Jun 27 16:54:58.986: INFO: The phase of Pod busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:55:01.007: INFO: Pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04050877s
    Jun 27 16:55:01.007: INFO: The phase of Pod busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:55:03.009: INFO: Pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625": Phase="Running", Reason="", readiness=true. Elapsed: 4.042657734s
    Jun 27 16:55:03.009: INFO: The phase of Pod busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625 is Running (Ready = true)
    Jun 27 16:55:03.009: INFO: Pod "busybox-readonly-fs8c7ca563-b05a-4407-ae30-4e037547c625" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 27 16:55:03.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-2297" for this suite. 06/27/23 16:55:03.159
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:55:03.191
Jun 27 16:55:03.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:55:03.192
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:55:03.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:55:03.268
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-1aa614f8-a362-4e06-a08c-9c82f46ba3d6 06/27/23 16:55:03.285
STEP: Creating a pod to test consume secrets 06/27/23 16:55:03.303
Jun 27 16:55:03.400: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6" in namespace "projected-8855" to be "Succeeded or Failed"
Jun 27 16:55:03.430: INFO: Pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6": Phase="Pending", Reason="", readiness=false. Elapsed: 29.820899ms
Jun 27 16:55:05.448: INFO: Pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048496193s
Jun 27 16:55:07.450: INFO: Pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050494791s
STEP: Saw pod success 06/27/23 16:55:07.45
Jun 27 16:55:07.451: INFO: Pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6" satisfied condition "Succeeded or Failed"
Jun 27 16:55:07.469: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/27/23 16:55:07.519
Jun 27 16:55:07.586: INFO: Waiting for pod pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6 to disappear
Jun 27 16:55:07.604: INFO: Pod pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 27 16:55:07.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8855" for this suite. 06/27/23 16:55:07.63
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":104,"skipped":2168,"failed":0}
------------------------------
â€¢ [4.465 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:55:03.191
    Jun 27 16:55:03.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:55:03.192
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:55:03.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:55:03.268
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-1aa614f8-a362-4e06-a08c-9c82f46ba3d6 06/27/23 16:55:03.285
    STEP: Creating a pod to test consume secrets 06/27/23 16:55:03.303
    Jun 27 16:55:03.400: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6" in namespace "projected-8855" to be "Succeeded or Failed"
    Jun 27 16:55:03.430: INFO: Pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6": Phase="Pending", Reason="", readiness=false. Elapsed: 29.820899ms
    Jun 27 16:55:05.448: INFO: Pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.048496193s
    Jun 27 16:55:07.450: INFO: Pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.050494791s
    STEP: Saw pod success 06/27/23 16:55:07.45
    Jun 27 16:55:07.451: INFO: Pod "pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6" satisfied condition "Succeeded or Failed"
    Jun 27 16:55:07.469: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 16:55:07.519
    Jun 27 16:55:07.586: INFO: Waiting for pod pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6 to disappear
    Jun 27 16:55:07.604: INFO: Pod pod-projected-secrets-f4dc40c7-1fa3-456d-b1d4-1dc09db92ab6 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 27 16:55:07.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8855" for this suite. 06/27/23 16:55:07.63
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:55:07.664
Jun 27 16:55:07.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 16:55:07.666
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:55:07.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:55:07.741
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 16:55:07.821
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:55:08.36
STEP: Deploying the webhook pod 06/27/23 16:55:08.407
STEP: Wait for the deployment to be ready 06/27/23 16:55:08.482
Jun 27 16:55:08.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun 27 16:55:10.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 16:55:12.574
STEP: Verifying the service has paired with the endpoint 06/27/23 16:55:12.635
Jun 27 16:55:13.638: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Jun 27 16:55:13.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4602-crds.webhook.example.com via the AdmissionRegistration API 06/27/23 16:55:14.214
STEP: Creating a custom resource that should be mutated by the webhook 06/27/23 16:55:14.302
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:55:17.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2293" for this suite. 06/27/23 16:55:17.051
STEP: Destroying namespace "webhook-2293-markers" for this suite. 06/27/23 16:55:17.079
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":105,"skipped":2171,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.583 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:55:07.664
    Jun 27 16:55:07.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 16:55:07.666
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:55:07.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:55:07.741
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 16:55:07.821
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 16:55:08.36
    STEP: Deploying the webhook pod 06/27/23 16:55:08.407
    STEP: Wait for the deployment to be ready 06/27/23 16:55:08.482
    Jun 27 16:55:08.546: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jun 27 16:55:10.565: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 16, 55, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 16:55:12.574
    STEP: Verifying the service has paired with the endpoint 06/27/23 16:55:12.635
    Jun 27 16:55:13.638: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Jun 27 16:55:13.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4602-crds.webhook.example.com via the AdmissionRegistration API 06/27/23 16:55:14.214
    STEP: Creating a custom resource that should be mutated by the webhook 06/27/23 16:55:14.302
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:55:17.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2293" for this suite. 06/27/23 16:55:17.051
    STEP: Destroying namespace "webhook-2293-markers" for this suite. 06/27/23 16:55:17.079
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:55:17.25
Jun 27 16:55:17.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 16:55:17.251
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:55:17.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:55:17.329
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-80e55224-38d7-4744-a128-9cb4b4692de9 06/27/23 16:55:17.357
STEP: Creating a pod to test consume secrets 06/27/23 16:55:17.382
Jun 27 16:55:17.470: INFO: Waiting up to 5m0s for pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32" in namespace "secrets-5425" to be "Succeeded or Failed"
Jun 27 16:55:17.505: INFO: Pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32": Phase="Pending", Reason="", readiness=false. Elapsed: 35.092476ms
Jun 27 16:55:19.525: INFO: Pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054720432s
Jun 27 16:55:21.524: INFO: Pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054188061s
STEP: Saw pod success 06/27/23 16:55:21.525
Jun 27 16:55:21.525: INFO: Pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32" satisfied condition "Succeeded or Failed"
Jun 27 16:55:21.542: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32 container secret-volume-test: <nil>
STEP: delete the pod 06/27/23 16:55:21.582
Jun 27 16:55:21.626: INFO: Waiting for pod pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32 to disappear
Jun 27 16:55:21.644: INFO: Pod pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 27 16:55:21.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5425" for this suite. 06/27/23 16:55:21.671
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":106,"skipped":2185,"failed":0}
------------------------------
â€¢ [4.448 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:55:17.25
    Jun 27 16:55:17.250: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 16:55:17.251
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:55:17.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:55:17.329
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-80e55224-38d7-4744-a128-9cb4b4692de9 06/27/23 16:55:17.357
    STEP: Creating a pod to test consume secrets 06/27/23 16:55:17.382
    Jun 27 16:55:17.470: INFO: Waiting up to 5m0s for pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32" in namespace "secrets-5425" to be "Succeeded or Failed"
    Jun 27 16:55:17.505: INFO: Pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32": Phase="Pending", Reason="", readiness=false. Elapsed: 35.092476ms
    Jun 27 16:55:19.525: INFO: Pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054720432s
    Jun 27 16:55:21.524: INFO: Pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.054188061s
    STEP: Saw pod success 06/27/23 16:55:21.525
    Jun 27 16:55:21.525: INFO: Pod "pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32" satisfied condition "Succeeded or Failed"
    Jun 27 16:55:21.542: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32 container secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 16:55:21.582
    Jun 27 16:55:21.626: INFO: Waiting for pod pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32 to disappear
    Jun 27 16:55:21.644: INFO: Pod pod-secrets-9ff60807-01b9-415e-aa52-e1d10aaebd32 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 16:55:21.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5425" for this suite. 06/27/23 16:55:21.671
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:55:21.716
Jun 27 16:55:21.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 16:55:21.727
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:55:21.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:55:21.803
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/27/23 16:55:21.817
Jun 27 16:55:21.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/27/23 16:55:57.665
Jun 27 16:55:57.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 16:56:07.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:56:43.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8358" for this suite. 06/27/23 16:56:43.703
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":107,"skipped":2276,"failed":0}
------------------------------
â€¢ [SLOW TEST] [82.015 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:55:21.716
    Jun 27 16:55:21.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 16:55:21.727
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:55:21.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:55:21.803
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 06/27/23 16:55:21.817
    Jun 27 16:55:21.822: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 06/27/23 16:55:57.665
    Jun 27 16:55:57.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 16:56:07.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:56:43.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8358" for this suite. 06/27/23 16:56:43.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:56:43.734
Jun 27 16:56:43.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 16:56:43.736
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:56:43.779
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:56:43.801
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 06/27/23 16:56:43.818
Jun 27 16:56:43.869: INFO: Waiting up to 5m0s for pod "pod-4d96df12-7249-462f-b160-947c983754d0" in namespace "emptydir-4181" to be "Succeeded or Failed"
Jun 27 16:56:43.880: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.903372ms
Jun 27 16:56:45.894: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025319852s
Jun 27 16:56:47.893: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024276575s
Jun 27 16:56:49.893: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02349131s
STEP: Saw pod success 06/27/23 16:56:49.893
Jun 27 16:56:49.894: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0" satisfied condition "Succeeded or Failed"
Jun 27 16:56:49.906: INFO: Trying to get logs from node 10.113.180.90 pod pod-4d96df12-7249-462f-b160-947c983754d0 container test-container: <nil>
STEP: delete the pod 06/27/23 16:56:49.971
Jun 27 16:56:50.005: INFO: Waiting for pod pod-4d96df12-7249-462f-b160-947c983754d0 to disappear
Jun 27 16:56:50.017: INFO: Pod pod-4d96df12-7249-462f-b160-947c983754d0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 16:56:50.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4181" for this suite. 06/27/23 16:56:50.04
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":108,"skipped":2298,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.339 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:56:43.734
    Jun 27 16:56:43.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 16:56:43.736
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:56:43.779
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:56:43.801
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 06/27/23 16:56:43.818
    Jun 27 16:56:43.869: INFO: Waiting up to 5m0s for pod "pod-4d96df12-7249-462f-b160-947c983754d0" in namespace "emptydir-4181" to be "Succeeded or Failed"
    Jun 27 16:56:43.880: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.903372ms
    Jun 27 16:56:45.894: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025319852s
    Jun 27 16:56:47.893: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024276575s
    Jun 27 16:56:49.893: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02349131s
    STEP: Saw pod success 06/27/23 16:56:49.893
    Jun 27 16:56:49.894: INFO: Pod "pod-4d96df12-7249-462f-b160-947c983754d0" satisfied condition "Succeeded or Failed"
    Jun 27 16:56:49.906: INFO: Trying to get logs from node 10.113.180.90 pod pod-4d96df12-7249-462f-b160-947c983754d0 container test-container: <nil>
    STEP: delete the pod 06/27/23 16:56:49.971
    Jun 27 16:56:50.005: INFO: Waiting for pod pod-4d96df12-7249-462f-b160-947c983754d0 to disappear
    Jun 27 16:56:50.017: INFO: Pod pod-4d96df12-7249-462f-b160-947c983754d0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 16:56:50.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4181" for this suite. 06/27/23 16:56:50.04
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:56:50.074
Jun 27 16:56:50.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename gc 06/27/23 16:56:50.077
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:56:50.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:56:50.138
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Jun 27 16:56:50.327: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a3bab3d9-ff91-4891-a60a-3c15798da42a", Controller:(*bool)(0xc00ce23792), BlockOwnerDeletion:(*bool)(0xc00ce23793)}}
Jun 27 16:56:50.344: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"23e3217f-80d9-45ec-b4cf-5e9c3f9ec30d", Controller:(*bool)(0xc0094dd0f2), BlockOwnerDeletion:(*bool)(0xc0094dd0f3)}}
Jun 27 16:56:50.365: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f7adb624-0946-489a-b5b6-f10ac09db969", Controller:(*bool)(0xc009580456), BlockOwnerDeletion:(*bool)(0xc009580457)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 27 16:56:55.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-5325" for this suite. 06/27/23 16:56:55.417
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":109,"skipped":2298,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.380 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:56:50.074
    Jun 27 16:56:50.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename gc 06/27/23 16:56:50.077
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:56:50.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:56:50.138
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Jun 27 16:56:50.327: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"a3bab3d9-ff91-4891-a60a-3c15798da42a", Controller:(*bool)(0xc00ce23792), BlockOwnerDeletion:(*bool)(0xc00ce23793)}}
    Jun 27 16:56:50.344: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"23e3217f-80d9-45ec-b4cf-5e9c3f9ec30d", Controller:(*bool)(0xc0094dd0f2), BlockOwnerDeletion:(*bool)(0xc0094dd0f3)}}
    Jun 27 16:56:50.365: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"f7adb624-0946-489a-b5b6-f10ac09db969", Controller:(*bool)(0xc009580456), BlockOwnerDeletion:(*bool)(0xc009580457)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 27 16:56:55.397: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-5325" for this suite. 06/27/23 16:56:55.417
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:56:55.458
Jun 27 16:56:55.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename containers 06/27/23 16:56:55.461
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:56:55.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:56:55.528
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 06/27/23 16:56:55.544
Jun 27 16:56:55.619: INFO: Waiting up to 5m0s for pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068" in namespace "containers-9640" to be "Succeeded or Failed"
Jun 27 16:56:55.631: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068": Phase="Pending", Reason="", readiness=false. Elapsed: 11.676221ms
Jun 27 16:56:57.645: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025277242s
Jun 27 16:56:59.659: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039184603s
Jun 27 16:57:01.645: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02585103s
STEP: Saw pod success 06/27/23 16:57:01.645
Jun 27 16:57:01.646: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068" satisfied condition "Succeeded or Failed"
Jun 27 16:57:01.659: INFO: Trying to get logs from node 10.113.180.90 pod client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068 container agnhost-container: <nil>
STEP: delete the pod 06/27/23 16:57:01.688
Jun 27 16:57:01.721: INFO: Waiting for pod client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068 to disappear
Jun 27 16:57:01.733: INFO: Pod client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 27 16:57:01.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9640" for this suite. 06/27/23 16:57:01.762
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":110,"skipped":2310,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.332 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:56:55.458
    Jun 27 16:56:55.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename containers 06/27/23 16:56:55.461
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:56:55.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:56:55.528
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 06/27/23 16:56:55.544
    Jun 27 16:56:55.619: INFO: Waiting up to 5m0s for pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068" in namespace "containers-9640" to be "Succeeded or Failed"
    Jun 27 16:56:55.631: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068": Phase="Pending", Reason="", readiness=false. Elapsed: 11.676221ms
    Jun 27 16:56:57.645: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025277242s
    Jun 27 16:56:59.659: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068": Phase="Pending", Reason="", readiness=false. Elapsed: 4.039184603s
    Jun 27 16:57:01.645: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02585103s
    STEP: Saw pod success 06/27/23 16:57:01.645
    Jun 27 16:57:01.646: INFO: Pod "client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068" satisfied condition "Succeeded or Failed"
    Jun 27 16:57:01.659: INFO: Trying to get logs from node 10.113.180.90 pod client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068 container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 16:57:01.688
    Jun 27 16:57:01.721: INFO: Waiting for pod client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068 to disappear
    Jun 27 16:57:01.733: INFO: Pod client-containers-241bca0d-ca06-4cec-bf3b-8b0a98906068 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 27 16:57:01.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9640" for this suite. 06/27/23 16:57:01.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:01.798
Jun 27 16:57:01.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 16:57:01.802
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:01.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:01.871
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
STEP: fetching services 06/27/23 16:57:01.892
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 16:57:01.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-395" for this suite. 06/27/23 16:57:01.977
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":111,"skipped":2321,"failed":0}
------------------------------
â€¢ [0.207 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:01.798
    Jun 27 16:57:01.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 16:57:01.802
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:01.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:01.871
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3185
    STEP: fetching services 06/27/23 16:57:01.892
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 16:57:01.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-395" for this suite. 06/27/23 16:57:01.977
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:02.023
Jun 27 16:57:02.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:57:02.024
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:02.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:02.084
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 06/27/23 16:57:02.099
Jun 27 16:57:02.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da" in namespace "projected-1456" to be "Succeeded or Failed"
Jun 27 16:57:02.162: INFO: Pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da": Phase="Pending", Reason="", readiness=false. Elapsed: 11.10009ms
Jun 27 16:57:04.176: INFO: Pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025673555s
Jun 27 16:57:06.177: INFO: Pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025915957s
STEP: Saw pod success 06/27/23 16:57:06.177
Jun 27 16:57:06.177: INFO: Pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da" satisfied condition "Succeeded or Failed"
Jun 27 16:57:06.193: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da container client-container: <nil>
STEP: delete the pod 06/27/23 16:57:06.232
Jun 27 16:57:06.268: INFO: Waiting for pod downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da to disappear
Jun 27 16:57:06.280: INFO: Pod downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 16:57:06.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1456" for this suite. 06/27/23 16:57:06.328
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":112,"skipped":2397,"failed":0}
------------------------------
â€¢ [4.341 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:02.023
    Jun 27 16:57:02.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:57:02.024
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:02.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:02.084
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 06/27/23 16:57:02.099
    Jun 27 16:57:02.150: INFO: Waiting up to 5m0s for pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da" in namespace "projected-1456" to be "Succeeded or Failed"
    Jun 27 16:57:02.162: INFO: Pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da": Phase="Pending", Reason="", readiness=false. Elapsed: 11.10009ms
    Jun 27 16:57:04.176: INFO: Pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025673555s
    Jun 27 16:57:06.177: INFO: Pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025915957s
    STEP: Saw pod success 06/27/23 16:57:06.177
    Jun 27 16:57:06.177: INFO: Pod "downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da" satisfied condition "Succeeded or Failed"
    Jun 27 16:57:06.193: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da container client-container: <nil>
    STEP: delete the pod 06/27/23 16:57:06.232
    Jun 27 16:57:06.268: INFO: Waiting for pod downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da to disappear
    Jun 27 16:57:06.280: INFO: Pod downwardapi-volume-19b2dafc-bab2-4d77-b0ee-ac73acf436da no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 16:57:06.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1456" for this suite. 06/27/23 16:57:06.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:06.377
Jun 27 16:57:06.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename job 06/27/23 16:57:06.38
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:06.445
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:06.461
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 06/27/23 16:57:06.47
W0627 16:57:06.497007      23 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring job reaches completions 06/27/23 16:57:06.497
STEP: Ensuring pods with index for job exist 06/27/23 16:57:18.511
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 27 16:57:18.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4594" for this suite. 06/27/23 16:57:18.552
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":113,"skipped":2428,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.221 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:06.377
    Jun 27 16:57:06.378: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename job 06/27/23 16:57:06.38
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:06.445
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:06.461
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 06/27/23 16:57:06.47
    W0627 16:57:06.497007      23 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring job reaches completions 06/27/23 16:57:06.497
    STEP: Ensuring pods with index for job exist 06/27/23 16:57:18.511
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 27 16:57:18.532: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4594" for this suite. 06/27/23 16:57:18.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:18.606
Jun 27 16:57:18.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replicaset 06/27/23 16:57:18.609
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:18.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:18.687
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Jun 27 16:57:18.702: INFO: Creating ReplicaSet my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59
Jun 27 16:57:18.741: INFO: Pod name my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59: Found 0 pods out of 1
Jun 27 16:57:23.761: INFO: Pod name my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59: Found 1 pods out of 1
Jun 27 16:57:23.761: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59" is running
Jun 27 16:57:23.762: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k" in namespace "replicaset-1140" to be "running"
Jun 27 16:57:23.774: INFO: Pod "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k": Phase="Running", Reason="", readiness=true. Elapsed: 12.590399ms
Jun 27 16:57:23.774: INFO: Pod "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k" satisfied condition "running"
Jun 27 16:57:23.774: INFO: Pod "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:57:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:57:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:57:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:57:18 +0000 UTC Reason: Message:}])
Jun 27 16:57:23.774: INFO: Trying to dial the pod
Jun 27 16:57:28.840: INFO: Controller my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59: Got expected result from replica 1 [my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k]: "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 27 16:57:28.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-1140" for this suite. 06/27/23 16:57:28.867
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":114,"skipped":2437,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.279 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:18.606
    Jun 27 16:57:18.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replicaset 06/27/23 16:57:18.609
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:18.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:18.687
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Jun 27 16:57:18.702: INFO: Creating ReplicaSet my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59
    Jun 27 16:57:18.741: INFO: Pod name my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59: Found 0 pods out of 1
    Jun 27 16:57:23.761: INFO: Pod name my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59: Found 1 pods out of 1
    Jun 27 16:57:23.761: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59" is running
    Jun 27 16:57:23.762: INFO: Waiting up to 5m0s for pod "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k" in namespace "replicaset-1140" to be "running"
    Jun 27 16:57:23.774: INFO: Pod "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k": Phase="Running", Reason="", readiness=true. Elapsed: 12.590399ms
    Jun 27 16:57:23.774: INFO: Pod "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k" satisfied condition "running"
    Jun 27 16:57:23.774: INFO: Pod "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:57:18 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:57:20 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:57:20 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-06-27 16:57:18 +0000 UTC Reason: Message:}])
    Jun 27 16:57:23.774: INFO: Trying to dial the pod
    Jun 27 16:57:28.840: INFO: Controller my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59: Got expected result from replica 1 [my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k]: "my-hostname-basic-9b434f82-bc09-4f79-889f-a58647856e59-lb28k", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 27 16:57:28.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-1140" for this suite. 06/27/23 16:57:28.867
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:28.888
Jun 27 16:57:28.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:57:28.889
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:28.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:28.949
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 06/27/23 16:57:28.965
Jun 27 16:57:29.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8" in namespace "projected-3072" to be "Succeeded or Failed"
Jun 27 16:57:29.052: INFO: Pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.339612ms
Jun 27 16:57:31.066: INFO: Pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026195488s
Jun 27 16:57:33.065: INFO: Pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024727723s
STEP: Saw pod success 06/27/23 16:57:33.065
Jun 27 16:57:33.065: INFO: Pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8" satisfied condition "Succeeded or Failed"
Jun 27 16:57:33.077: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8 container client-container: <nil>
STEP: delete the pod 06/27/23 16:57:33.11
Jun 27 16:57:33.137: INFO: Waiting for pod downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8 to disappear
Jun 27 16:57:33.148: INFO: Pod downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 16:57:33.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3072" for this suite. 06/27/23 16:57:33.169
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":115,"skipped":2439,"failed":0}
------------------------------
â€¢ [4.304 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:28.888
    Jun 27 16:57:28.888: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:57:28.889
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:28.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:28.949
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 06/27/23 16:57:28.965
    Jun 27 16:57:29.040: INFO: Waiting up to 5m0s for pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8" in namespace "projected-3072" to be "Succeeded or Failed"
    Jun 27 16:57:29.052: INFO: Pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.339612ms
    Jun 27 16:57:31.066: INFO: Pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026195488s
    Jun 27 16:57:33.065: INFO: Pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.024727723s
    STEP: Saw pod success 06/27/23 16:57:33.065
    Jun 27 16:57:33.065: INFO: Pod "downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8" satisfied condition "Succeeded or Failed"
    Jun 27 16:57:33.077: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8 container client-container: <nil>
    STEP: delete the pod 06/27/23 16:57:33.11
    Jun 27 16:57:33.137: INFO: Waiting for pod downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8 to disappear
    Jun 27 16:57:33.148: INFO: Pod downwardapi-volume-10bdafc0-7b70-40a8-88bb-350e6526bbe8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 16:57:33.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3072" for this suite. 06/27/23 16:57:33.169
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:33.193
Jun 27 16:57:33.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 16:57:33.195
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:33.237
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:33.257
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 06/27/23 16:57:33.271
Jun 27 16:57:33.330: INFO: Waiting up to 5m0s for pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e" in namespace "downward-api-2326" to be "running and ready"
Jun 27 16:57:33.342: INFO: Pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.303079ms
Jun 27 16:57:33.342: INFO: The phase of Pod labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:57:35.392: INFO: Pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e": Phase="Running", Reason="", readiness=true. Elapsed: 2.061901865s
Jun 27 16:57:35.392: INFO: The phase of Pod labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e is Running (Ready = true)
Jun 27 16:57:35.392: INFO: Pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e" satisfied condition "running and ready"
Jun 27 16:57:36.004: INFO: Successfully updated pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 16:57:38.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2326" for this suite. 06/27/23 16:57:38.089
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":116,"skipped":2442,"failed":0}
------------------------------
â€¢ [4.917 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:33.193
    Jun 27 16:57:33.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 16:57:33.195
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:33.237
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:33.257
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 06/27/23 16:57:33.271
    Jun 27 16:57:33.330: INFO: Waiting up to 5m0s for pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e" in namespace "downward-api-2326" to be "running and ready"
    Jun 27 16:57:33.342: INFO: Pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e": Phase="Pending", Reason="", readiness=false. Elapsed: 12.303079ms
    Jun 27 16:57:33.342: INFO: The phase of Pod labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:57:35.392: INFO: Pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e": Phase="Running", Reason="", readiness=true. Elapsed: 2.061901865s
    Jun 27 16:57:35.392: INFO: The phase of Pod labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e is Running (Ready = true)
    Jun 27 16:57:35.392: INFO: Pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e" satisfied condition "running and ready"
    Jun 27 16:57:36.004: INFO: Successfully updated pod "labelsupdate64ca51d9-f23d-415b-a2b9-e226ae79028e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 16:57:38.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2326" for this suite. 06/27/23 16:57:38.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:38.113
Jun 27 16:57:38.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:57:38.116
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:38.155
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:38.172
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-7e385258-a198-499b-a8c2-4001b3a15f0a 06/27/23 16:57:38.193
STEP: Creating a pod to test consume configMaps 06/27/23 16:57:38.224
Jun 27 16:57:38.315: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0" in namespace "projected-9491" to be "Succeeded or Failed"
Jun 27 16:57:38.346: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.744501ms
Jun 27 16:57:40.360: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044650025s
Jun 27 16:57:42.360: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044546896s
Jun 27 16:57:44.359: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043365772s
STEP: Saw pod success 06/27/23 16:57:44.359
Jun 27 16:57:44.359: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0" satisfied condition "Succeeded or Failed"
Jun 27 16:57:44.371: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0 container agnhost-container: <nil>
STEP: delete the pod 06/27/23 16:57:44.401
Jun 27 16:57:44.436: INFO: Waiting for pod pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0 to disappear
Jun 27 16:57:44.447: INFO: Pod pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 27 16:57:44.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9491" for this suite. 06/27/23 16:57:44.471
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":117,"skipped":2447,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.385 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:38.113
    Jun 27 16:57:38.114: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:57:38.116
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:38.155
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:38.172
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-7e385258-a198-499b-a8c2-4001b3a15f0a 06/27/23 16:57:38.193
    STEP: Creating a pod to test consume configMaps 06/27/23 16:57:38.224
    Jun 27 16:57:38.315: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0" in namespace "projected-9491" to be "Succeeded or Failed"
    Jun 27 16:57:38.346: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0": Phase="Pending", Reason="", readiness=false. Elapsed: 30.744501ms
    Jun 27 16:57:40.360: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.044650025s
    Jun 27 16:57:42.360: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044546896s
    Jun 27 16:57:44.359: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.043365772s
    STEP: Saw pod success 06/27/23 16:57:44.359
    Jun 27 16:57:44.359: INFO: Pod "pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0" satisfied condition "Succeeded or Failed"
    Jun 27 16:57:44.371: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0 container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 16:57:44.401
    Jun 27 16:57:44.436: INFO: Waiting for pod pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0 to disappear
    Jun 27 16:57:44.447: INFO: Pod pod-projected-configmaps-c6d65c31-e4ea-47bd-aeac-278295c8acd0 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 27 16:57:44.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9491" for this suite. 06/27/23 16:57:44.471
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:44.499
Jun 27 16:57:44.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 16:57:44.5
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:44.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:44.556
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
STEP: creating a collection of services 06/27/23 16:57:44.568
Jun 27 16:57:44.568: INFO: Creating e2e-svc-a-bz9nd
Jun 27 16:57:44.623: INFO: Creating e2e-svc-b-xnzhv
Jun 27 16:57:44.688: INFO: Creating e2e-svc-c-fp942
STEP: deleting service collection 06/27/23 16:57:44.752
Jun 27 16:57:44.888: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 16:57:44.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8497" for this suite. 06/27/23 16:57:44.908
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":118,"skipped":2448,"failed":0}
------------------------------
â€¢ [0.434 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:44.499
    Jun 27 16:57:44.499: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 16:57:44.5
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:44.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:44.556
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3620
    STEP: creating a collection of services 06/27/23 16:57:44.568
    Jun 27 16:57:44.568: INFO: Creating e2e-svc-a-bz9nd
    Jun 27 16:57:44.623: INFO: Creating e2e-svc-b-xnzhv
    Jun 27 16:57:44.688: INFO: Creating e2e-svc-c-fp942
    STEP: deleting service collection 06/27/23 16:57:44.752
    Jun 27 16:57:44.888: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 16:57:44.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8497" for this suite. 06/27/23 16:57:44.908
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:44.932
Jun 27 16:57:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename ingressclass 06/27/23 16:57:44.934
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:44.971
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:44.984
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 06/27/23 16:57:44.997
STEP: getting /apis/networking.k8s.io 06/27/23 16:57:45.022
STEP: getting /apis/networking.k8s.iov1 06/27/23 16:57:45.028
STEP: creating 06/27/23 16:57:45.035
STEP: getting 06/27/23 16:57:45.083
STEP: listing 06/27/23 16:57:45.097
STEP: watching 06/27/23 16:57:45.114
Jun 27 16:57:45.114: INFO: starting watch
STEP: patching 06/27/23 16:57:45.12
STEP: updating 06/27/23 16:57:45.136
Jun 27 16:57:45.151: INFO: waiting for watch events with expected annotations
Jun 27 16:57:45.151: INFO: saw patched and updated annotations
STEP: deleting 06/27/23 16:57:45.151
STEP: deleting a collection 06/27/23 16:57:45.193
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Jun 27 16:57:45.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-295" for this suite. 06/27/23 16:57:45.285
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":119,"skipped":2450,"failed":0}
------------------------------
â€¢ [0.371 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:44.932
    Jun 27 16:57:44.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename ingressclass 06/27/23 16:57:44.934
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:44.971
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:44.984
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 06/27/23 16:57:44.997
    STEP: getting /apis/networking.k8s.io 06/27/23 16:57:45.022
    STEP: getting /apis/networking.k8s.iov1 06/27/23 16:57:45.028
    STEP: creating 06/27/23 16:57:45.035
    STEP: getting 06/27/23 16:57:45.083
    STEP: listing 06/27/23 16:57:45.097
    STEP: watching 06/27/23 16:57:45.114
    Jun 27 16:57:45.114: INFO: starting watch
    STEP: patching 06/27/23 16:57:45.12
    STEP: updating 06/27/23 16:57:45.136
    Jun 27 16:57:45.151: INFO: waiting for watch events with expected annotations
    Jun 27 16:57:45.151: INFO: saw patched and updated annotations
    STEP: deleting 06/27/23 16:57:45.151
    STEP: deleting a collection 06/27/23 16:57:45.193
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Jun 27 16:57:45.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-295" for this suite. 06/27/23 16:57:45.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:45.308
Jun 27 16:57:45.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename events 06/27/23 16:57:45.31
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:45.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:45.368
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 06/27/23 16:57:45.381
STEP: listing events in all namespaces 06/27/23 16:57:45.411
STEP: listing events in test namespace 06/27/23 16:57:45.473
STEP: listing events with field selection filtering on source 06/27/23 16:57:45.501
STEP: listing events with field selection filtering on reportingController 06/27/23 16:57:45.541
STEP: getting the test event 06/27/23 16:57:45.555
STEP: patching the test event 06/27/23 16:57:45.569
STEP: getting the test event 06/27/23 16:57:45.592
STEP: updating the test event 06/27/23 16:57:45.607
STEP: getting the test event 06/27/23 16:57:45.629
STEP: deleting the test event 06/27/23 16:57:45.641
STEP: listing events in all namespaces 06/27/23 16:57:45.671
STEP: listing events in test namespace 06/27/23 16:57:45.733
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Jun 27 16:57:45.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2528" for this suite. 06/27/23 16:57:45.763
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":120,"skipped":2467,"failed":0}
------------------------------
â€¢ [0.476 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:45.308
    Jun 27 16:57:45.308: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename events 06/27/23 16:57:45.31
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:45.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:45.368
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 06/27/23 16:57:45.381
    STEP: listing events in all namespaces 06/27/23 16:57:45.411
    STEP: listing events in test namespace 06/27/23 16:57:45.473
    STEP: listing events with field selection filtering on source 06/27/23 16:57:45.501
    STEP: listing events with field selection filtering on reportingController 06/27/23 16:57:45.541
    STEP: getting the test event 06/27/23 16:57:45.555
    STEP: patching the test event 06/27/23 16:57:45.569
    STEP: getting the test event 06/27/23 16:57:45.592
    STEP: updating the test event 06/27/23 16:57:45.607
    STEP: getting the test event 06/27/23 16:57:45.629
    STEP: deleting the test event 06/27/23 16:57:45.641
    STEP: listing events in all namespaces 06/27/23 16:57:45.671
    STEP: listing events in test namespace 06/27/23 16:57:45.733
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Jun 27 16:57:45.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2528" for this suite. 06/27/23 16:57:45.763
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:57:45.79
Jun 27 16:57:45.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 16:57:45.792
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:45.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:45.849
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/27/23 16:57:45.865
Jun 27 16:57:45.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 16:57:55.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 16:58:31.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3721" for this suite. 06/27/23 16:58:31.464
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":121,"skipped":2512,"failed":0}
------------------------------
â€¢ [SLOW TEST] [45.704 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:57:45.79
    Jun 27 16:57:45.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 16:57:45.792
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:57:45.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:57:45.849
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 06/27/23 16:57:45.865
    Jun 27 16:57:45.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 16:57:55.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 16:58:31.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3721" for this suite. 06/27/23 16:58:31.464
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:58:31.5
Jun 27 16:58:31.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-lifecycle-hook 06/27/23 16:58:31.502
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:58:31.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:58:31.574
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/27/23 16:58:31.621
Jun 27 16:58:31.687: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4666" to be "running and ready"
Jun 27 16:58:31.710: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 23.194965ms
Jun 27 16:58:31.710: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:58:33.747: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.05944949s
Jun 27 16:58:33.747: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 27 16:58:33.747: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 06/27/23 16:58:33.772
Jun 27 16:58:33.817: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4666" to be "running and ready"
Jun 27 16:58:33.840: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 23.089968ms
Jun 27 16:58:33.840: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:58:35.857: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.039956946s
Jun 27 16:58:35.857: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Jun 27 16:58:35.857: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/27/23 16:58:35.872
STEP: delete the pod with lifecycle hook 06/27/23 16:58:35.944
Jun 27 16:58:35.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 16:58:36.010: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 16:58:38.010: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 16:58:38.030: INFO: Pod pod-with-poststart-exec-hook still exists
Jun 27 16:58:40.012: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Jun 27 16:58:40.030: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 27 16:58:40.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4666" for this suite. 06/27/23 16:58:40.057
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":122,"skipped":2544,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.584 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:58:31.5
    Jun 27 16:58:31.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/27/23 16:58:31.502
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:58:31.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:58:31.574
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/27/23 16:58:31.621
    Jun 27 16:58:31.687: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4666" to be "running and ready"
    Jun 27 16:58:31.710: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 23.194965ms
    Jun 27 16:58:31.710: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:58:33.747: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.05944949s
    Jun 27 16:58:33.747: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 27 16:58:33.747: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 06/27/23 16:58:33.772
    Jun 27 16:58:33.817: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-4666" to be "running and ready"
    Jun 27 16:58:33.840: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 23.089968ms
    Jun 27 16:58:33.840: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:58:35.857: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.039956946s
    Jun 27 16:58:35.857: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Jun 27 16:58:35.857: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/27/23 16:58:35.872
    STEP: delete the pod with lifecycle hook 06/27/23 16:58:35.944
    Jun 27 16:58:35.984: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 27 16:58:36.010: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun 27 16:58:38.010: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 27 16:58:38.030: INFO: Pod pod-with-poststart-exec-hook still exists
    Jun 27 16:58:40.012: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Jun 27 16:58:40.030: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 27 16:58:40.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4666" for this suite. 06/27/23 16:58:40.057
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:58:40.086
Jun 27 16:58:40.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 16:58:40.088
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:58:40.158
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:58:40.175
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
STEP: creating service in namespace services-4912 06/27/23 16:58:40.191
STEP: creating service affinity-nodeport-transition in namespace services-4912 06/27/23 16:58:40.191
STEP: creating replication controller affinity-nodeport-transition in namespace services-4912 06/27/23 16:58:40.237
I0627 16:58:40.274567      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4912, replica count: 3
I0627 16:58:43.325525      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 16:58:43.378: INFO: Creating new exec pod
Jun 27 16:58:43.433: INFO: Waiting up to 5m0s for pod "execpod-affinitypbq5f" in namespace "services-4912" to be "running"
Jun 27 16:58:43.451: INFO: Pod "execpod-affinitypbq5f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.156289ms
Jun 27 16:58:45.470: INFO: Pod "execpod-affinitypbq5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036222343s
Jun 27 16:58:47.470: INFO: Pod "execpod-affinitypbq5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.036510023s
Jun 27 16:58:47.470: INFO: Pod "execpod-affinitypbq5f" satisfied condition "running"
Jun 27 16:58:48.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Jun 27 16:58:48.831: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Jun 27 16:58:48.831: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 16:58:48.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.28.113 80'
Jun 27 16:58:49.350: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.28.113 80\nConnection to 172.21.28.113 80 port [tcp/http] succeeded!\n"
Jun 27 16:58:49.350: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 16:58:49.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31851'
Jun 27 16:58:49.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31851\nConnection to 10.113.180.89 31851 port [tcp/*] succeeded!\n"
Jun 27 16:58:49.767: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 16:58:49.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.90 31851'
Jun 27 16:58:50.133: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.90 31851\nConnection to 10.113.180.90 31851 port [tcp/*] succeeded!\n"
Jun 27 16:58:50.133: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 16:58:50.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.180.89:31851/ ; done'
Jun 27 16:58:50.845: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n"
Jun 27 16:58:50.846: INFO: stdout: "\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-fwwgl\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-fwwgl\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-fwwgl\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-fwwgl\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-5cfpn"
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-fwwgl
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-fwwgl
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-fwwgl
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-fwwgl
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:50.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.180.89:31851/ ; done'
Jun 27 16:58:51.404: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n"
Jun 27 16:58:51.404: INFO: stdout: "\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn"
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.405: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.405: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.405: INFO: Received response from host: affinity-nodeport-transition-5cfpn
Jun 27 16:58:51.405: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4912, will wait for the garbage collector to delete the pods 06/27/23 16:58:51.461
Jun 27 16:58:51.575: INFO: Deleting ReplicationController affinity-nodeport-transition took: 44.948883ms
Jun 27 16:58:51.676: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.23514ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 16:58:54.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4912" for this suite. 06/27/23 16:58:55.054
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":123,"skipped":2554,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.004 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:58:40.086
    Jun 27 16:58:40.086: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 16:58:40.088
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:58:40.158
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:58:40.175
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2216
    STEP: creating service in namespace services-4912 06/27/23 16:58:40.191
    STEP: creating service affinity-nodeport-transition in namespace services-4912 06/27/23 16:58:40.191
    STEP: creating replication controller affinity-nodeport-transition in namespace services-4912 06/27/23 16:58:40.237
    I0627 16:58:40.274567      23 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-4912, replica count: 3
    I0627 16:58:43.325525      23 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 27 16:58:43.378: INFO: Creating new exec pod
    Jun 27 16:58:43.433: INFO: Waiting up to 5m0s for pod "execpod-affinitypbq5f" in namespace "services-4912" to be "running"
    Jun 27 16:58:43.451: INFO: Pod "execpod-affinitypbq5f": Phase="Pending", Reason="", readiness=false. Elapsed: 17.156289ms
    Jun 27 16:58:45.470: INFO: Pod "execpod-affinitypbq5f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036222343s
    Jun 27 16:58:47.470: INFO: Pod "execpod-affinitypbq5f": Phase="Running", Reason="", readiness=true. Elapsed: 4.036510023s
    Jun 27 16:58:47.470: INFO: Pod "execpod-affinitypbq5f" satisfied condition "running"
    Jun 27 16:58:48.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Jun 27 16:58:48.831: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Jun 27 16:58:48.831: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 16:58:48.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.28.113 80'
    Jun 27 16:58:49.350: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.28.113 80\nConnection to 172.21.28.113 80 port [tcp/http] succeeded!\n"
    Jun 27 16:58:49.350: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 16:58:49.350: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31851'
    Jun 27 16:58:49.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31851\nConnection to 10.113.180.89 31851 port [tcp/*] succeeded!\n"
    Jun 27 16:58:49.767: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 16:58:49.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.90 31851'
    Jun 27 16:58:50.133: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.90 31851\nConnection to 10.113.180.90 31851 port [tcp/*] succeeded!\n"
    Jun 27 16:58:50.133: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 16:58:50.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.180.89:31851/ ; done'
    Jun 27 16:58:50.845: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n"
    Jun 27 16:58:50.846: INFO: stdout: "\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-fwwgl\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-fwwgl\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-fwwgl\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-fwwgl\naffinity-nodeport-transition-7cm9k\naffinity-nodeport-transition-5cfpn"
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-fwwgl
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-fwwgl
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-fwwgl
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-fwwgl
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-7cm9k
    Jun 27 16:58:50.846: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:50.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4912 exec execpod-affinitypbq5f -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.113.180.89:31851/ ; done'
    Jun 27 16:58:51.404: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.113.180.89:31851/\n"
    Jun 27 16:58:51.404: INFO: stdout: "\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn\naffinity-nodeport-transition-5cfpn"
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.404: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.405: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.405: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.405: INFO: Received response from host: affinity-nodeport-transition-5cfpn
    Jun 27 16:58:51.405: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4912, will wait for the garbage collector to delete the pods 06/27/23 16:58:51.461
    Jun 27 16:58:51.575: INFO: Deleting ReplicationController affinity-nodeport-transition took: 44.948883ms
    Jun 27 16:58:51.676: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.23514ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 16:58:54.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4912" for this suite. 06/27/23 16:58:55.054
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:58:55.093
Jun 27 16:58:55.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 16:58:55.095
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:58:55.182
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:58:55.205
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 06/27/23 16:58:55.227
Jun 27 16:58:55.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 create -f -'
Jun 27 16:58:57.001: INFO: stderr: ""
Jun 27 16:58:57.001: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 06/27/23 16:58:57.001
Jun 27 16:58:57.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 27 16:58:57.143: INFO: stderr: ""
Jun 27 16:58:57.143: INFO: stdout: "update-demo-nautilus-6m9x4 update-demo-nautilus-h5bv8 "
Jun 27 16:58:57.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-6m9x4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:58:57.270: INFO: stderr: ""
Jun 27 16:58:57.270: INFO: stdout: ""
Jun 27 16:58:57.270: INFO: update-demo-nautilus-6m9x4 is created but not running
Jun 27 16:59:02.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Jun 27 16:59:02.407: INFO: stderr: ""
Jun 27 16:59:02.407: INFO: stdout: "update-demo-nautilus-6m9x4 update-demo-nautilus-h5bv8 "
Jun 27 16:59:02.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-6m9x4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:59:02.569: INFO: stderr: ""
Jun 27 16:59:02.569: INFO: stdout: "true"
Jun 27 16:59:02.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-6m9x4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 27 16:59:02.711: INFO: stderr: ""
Jun 27 16:59:02.711: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 27 16:59:02.711: INFO: validating pod update-demo-nautilus-6m9x4
Jun 27 16:59:02.747: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 16:59:02.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 16:59:02.747: INFO: update-demo-nautilus-6m9x4 is verified up and running
Jun 27 16:59:02.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-h5bv8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Jun 27 16:59:02.876: INFO: stderr: ""
Jun 27 16:59:02.876: INFO: stdout: "true"
Jun 27 16:59:02.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-h5bv8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Jun 27 16:59:03.047: INFO: stderr: ""
Jun 27 16:59:03.047: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Jun 27 16:59:03.047: INFO: validating pod update-demo-nautilus-h5bv8
Jun 27 16:59:03.110: INFO: got data: {
  "image": "nautilus.jpg"
}

Jun 27 16:59:03.110: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Jun 27 16:59:03.110: INFO: update-demo-nautilus-h5bv8 is verified up and running
STEP: using delete to clean up resources 06/27/23 16:59:03.11
Jun 27 16:59:03.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 delete --grace-period=0 --force -f -'
Jun 27 16:59:03.242: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 16:59:03.242: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Jun 27 16:59:03.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get rc,svc -l name=update-demo --no-headers'
Jun 27 16:59:03.409: INFO: stderr: "No resources found in kubectl-5858 namespace.\n"
Jun 27 16:59:03.409: INFO: stdout: ""
Jun 27 16:59:03.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Jun 27 16:59:03.603: INFO: stderr: ""
Jun 27 16:59:03.603: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 16:59:03.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5858" for this suite. 06/27/23 16:59:03.628
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":124,"skipped":2590,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.561 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:58:55.093
    Jun 27 16:58:55.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 16:58:55.095
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:58:55.182
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:58:55.205
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 06/27/23 16:58:55.227
    Jun 27 16:58:55.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 create -f -'
    Jun 27 16:58:57.001: INFO: stderr: ""
    Jun 27 16:58:57.001: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 06/27/23 16:58:57.001
    Jun 27 16:58:57.001: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 27 16:58:57.143: INFO: stderr: ""
    Jun 27 16:58:57.143: INFO: stdout: "update-demo-nautilus-6m9x4 update-demo-nautilus-h5bv8 "
    Jun 27 16:58:57.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-6m9x4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:58:57.270: INFO: stderr: ""
    Jun 27 16:58:57.270: INFO: stdout: ""
    Jun 27 16:58:57.270: INFO: update-demo-nautilus-6m9x4 is created but not running
    Jun 27 16:59:02.270: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Jun 27 16:59:02.407: INFO: stderr: ""
    Jun 27 16:59:02.407: INFO: stdout: "update-demo-nautilus-6m9x4 update-demo-nautilus-h5bv8 "
    Jun 27 16:59:02.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-6m9x4 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:59:02.569: INFO: stderr: ""
    Jun 27 16:59:02.569: INFO: stdout: "true"
    Jun 27 16:59:02.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-6m9x4 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 27 16:59:02.711: INFO: stderr: ""
    Jun 27 16:59:02.711: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 27 16:59:02.711: INFO: validating pod update-demo-nautilus-6m9x4
    Jun 27 16:59:02.747: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 27 16:59:02.747: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 27 16:59:02.747: INFO: update-demo-nautilus-6m9x4 is verified up and running
    Jun 27 16:59:02.747: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-h5bv8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Jun 27 16:59:02.876: INFO: stderr: ""
    Jun 27 16:59:02.876: INFO: stdout: "true"
    Jun 27 16:59:02.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods update-demo-nautilus-h5bv8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Jun 27 16:59:03.047: INFO: stderr: ""
    Jun 27 16:59:03.047: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Jun 27 16:59:03.047: INFO: validating pod update-demo-nautilus-h5bv8
    Jun 27 16:59:03.110: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Jun 27 16:59:03.110: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Jun 27 16:59:03.110: INFO: update-demo-nautilus-h5bv8 is verified up and running
    STEP: using delete to clean up resources 06/27/23 16:59:03.11
    Jun 27 16:59:03.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 delete --grace-period=0 --force -f -'
    Jun 27 16:59:03.242: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 27 16:59:03.242: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Jun 27 16:59:03.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get rc,svc -l name=update-demo --no-headers'
    Jun 27 16:59:03.409: INFO: stderr: "No resources found in kubectl-5858 namespace.\n"
    Jun 27 16:59:03.409: INFO: stdout: ""
    Jun 27 16:59:03.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-5858 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Jun 27 16:59:03.603: INFO: stderr: ""
    Jun 27 16:59:03.603: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 16:59:03.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5858" for this suite. 06/27/23 16:59:03.628
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:59:03.661
Jun 27 16:59:03.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 16:59:03.664
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:59:03.712
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:59:03.729
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 06/27/23 16:59:03.746
STEP: setting up watch 06/27/23 16:59:03.746
STEP: submitting the pod to kubernetes 06/27/23 16:59:03.866
STEP: verifying the pod is in kubernetes 06/27/23 16:59:03.93
STEP: verifying pod creation was observed 06/27/23 16:59:03.949
Jun 27 16:59:03.949: INFO: Waiting up to 5m0s for pod "pod-submit-remove-0742cb8b-51cc-404e-8736-c36e1aa68b46" in namespace "pods-800" to be "running"
Jun 27 16:59:03.966: INFO: Pod "pod-submit-remove-0742cb8b-51cc-404e-8736-c36e1aa68b46": Phase="Pending", Reason="", readiness=false. Elapsed: 16.917348ms
Jun 27 16:59:05.987: INFO: Pod "pod-submit-remove-0742cb8b-51cc-404e-8736-c36e1aa68b46": Phase="Running", Reason="", readiness=true. Elapsed: 2.037682687s
Jun 27 16:59:05.987: INFO: Pod "pod-submit-remove-0742cb8b-51cc-404e-8736-c36e1aa68b46" satisfied condition "running"
STEP: deleting the pod gracefully 06/27/23 16:59:06.006
STEP: verifying pod deletion was observed 06/27/23 16:59:06.039
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 16:59:09.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-800" for this suite. 06/27/23 16:59:09.801
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":125,"skipped":2648,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.169 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:59:03.661
    Jun 27 16:59:03.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 16:59:03.664
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:59:03.712
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:59:03.729
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 06/27/23 16:59:03.746
    STEP: setting up watch 06/27/23 16:59:03.746
    STEP: submitting the pod to kubernetes 06/27/23 16:59:03.866
    STEP: verifying the pod is in kubernetes 06/27/23 16:59:03.93
    STEP: verifying pod creation was observed 06/27/23 16:59:03.949
    Jun 27 16:59:03.949: INFO: Waiting up to 5m0s for pod "pod-submit-remove-0742cb8b-51cc-404e-8736-c36e1aa68b46" in namespace "pods-800" to be "running"
    Jun 27 16:59:03.966: INFO: Pod "pod-submit-remove-0742cb8b-51cc-404e-8736-c36e1aa68b46": Phase="Pending", Reason="", readiness=false. Elapsed: 16.917348ms
    Jun 27 16:59:05.987: INFO: Pod "pod-submit-remove-0742cb8b-51cc-404e-8736-c36e1aa68b46": Phase="Running", Reason="", readiness=true. Elapsed: 2.037682687s
    Jun 27 16:59:05.987: INFO: Pod "pod-submit-remove-0742cb8b-51cc-404e-8736-c36e1aa68b46" satisfied condition "running"
    STEP: deleting the pod gracefully 06/27/23 16:59:06.006
    STEP: verifying pod deletion was observed 06/27/23 16:59:06.039
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 16:59:09.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-800" for this suite. 06/27/23 16:59:09.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 16:59:09.837
Jun 27 16:59:09.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 16:59:09.839
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:59:09.885
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:59:09.902
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
Jun 27 16:59:09.963: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name cm-test-opt-del-746e0b8a-a759-4026-973e-e6bb7cf60816 06/27/23 16:59:09.963
STEP: Creating configMap with name cm-test-opt-upd-3a85c154-4363-4b97-adf8-8f3111597485 06/27/23 16:59:09.992
STEP: Creating the pod 06/27/23 16:59:10.025
Jun 27 16:59:10.087: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328" in namespace "projected-1983" to be "running and ready"
Jun 27 16:59:10.109: INFO: Pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328": Phase="Pending", Reason="", readiness=false. Elapsed: 21.790587ms
Jun 27 16:59:10.109: INFO: The phase of Pod pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:59:12.128: INFO: Pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040799636s
Jun 27 16:59:12.128: INFO: The phase of Pod pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 16:59:14.129: INFO: Pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328": Phase="Running", Reason="", readiness=true. Elapsed: 4.041604278s
Jun 27 16:59:14.129: INFO: The phase of Pod pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328 is Running (Ready = true)
Jun 27 16:59:14.129: INFO: Pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-746e0b8a-a759-4026-973e-e6bb7cf60816 06/27/23 16:59:14.273
STEP: Updating configmap cm-test-opt-upd-3a85c154-4363-4b97-adf8-8f3111597485 06/27/23 16:59:14.296
STEP: Creating configMap with name cm-test-opt-create-5802a6ab-db15-4a18-9808-1a905436388d 06/27/23 16:59:14.315
STEP: waiting to observe update in volume 06/27/23 16:59:14.337
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 27 17:00:30.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1983" for this suite. 06/27/23 17:00:30.532
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":126,"skipped":2707,"failed":0}
------------------------------
â€¢ [SLOW TEST] [80.722 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 16:59:09.837
    Jun 27 16:59:09.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 16:59:09.839
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 16:59:09.885
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 16:59:09.902
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    Jun 27 16:59:09.963: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name cm-test-opt-del-746e0b8a-a759-4026-973e-e6bb7cf60816 06/27/23 16:59:09.963
    STEP: Creating configMap with name cm-test-opt-upd-3a85c154-4363-4b97-adf8-8f3111597485 06/27/23 16:59:09.992
    STEP: Creating the pod 06/27/23 16:59:10.025
    Jun 27 16:59:10.087: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328" in namespace "projected-1983" to be "running and ready"
    Jun 27 16:59:10.109: INFO: Pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328": Phase="Pending", Reason="", readiness=false. Elapsed: 21.790587ms
    Jun 27 16:59:10.109: INFO: The phase of Pod pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:59:12.128: INFO: Pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328": Phase="Pending", Reason="", readiness=false. Elapsed: 2.040799636s
    Jun 27 16:59:12.128: INFO: The phase of Pod pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 16:59:14.129: INFO: Pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328": Phase="Running", Reason="", readiness=true. Elapsed: 4.041604278s
    Jun 27 16:59:14.129: INFO: The phase of Pod pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328 is Running (Ready = true)
    Jun 27 16:59:14.129: INFO: Pod "pod-projected-configmaps-4aed142b-3dd4-4656-a835-002c38cb0328" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-746e0b8a-a759-4026-973e-e6bb7cf60816 06/27/23 16:59:14.273
    STEP: Updating configmap cm-test-opt-upd-3a85c154-4363-4b97-adf8-8f3111597485 06/27/23 16:59:14.296
    STEP: Creating configMap with name cm-test-opt-create-5802a6ab-db15-4a18-9808-1a905436388d 06/27/23 16:59:14.315
    STEP: waiting to observe update in volume 06/27/23 16:59:14.337
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 27 17:00:30.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1983" for this suite. 06/27/23 17:00:30.532
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:00:30.563
Jun 27 17:00:30.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename endpointslice 06/27/23 17:00:30.566
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:00:30.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:00:30.636
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 06/27/23 17:00:35.956
STEP: referencing matching pods with named port 06/27/23 17:00:40.99
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/27/23 17:00:46.024
STEP: recreating EndpointSlices after they've been deleted 06/27/23 17:00:51.057
Jun 27 17:00:51.208: INFO: EndpointSlice for Service endpointslice-8394/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 27 17:01:01.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-8394" for this suite. 06/27/23 17:01:01.269
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":127,"skipped":2710,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.770 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:00:30.563
    Jun 27 17:00:30.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename endpointslice 06/27/23 17:00:30.566
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:00:30.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:00:30.636
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 06/27/23 17:00:35.956
    STEP: referencing matching pods with named port 06/27/23 17:00:40.99
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 06/27/23 17:00:46.024
    STEP: recreating EndpointSlices after they've been deleted 06/27/23 17:00:51.057
    Jun 27 17:00:51.208: INFO: EndpointSlice for Service endpointslice-8394/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 27 17:01:01.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-8394" for this suite. 06/27/23 17:01:01.269
  << End Captured GinkgoWriter Output
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:01:01.336
Jun 27 17:01:01.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 17:01:01.339
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:01:01.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:01:01.432
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 06/27/23 17:01:01.447
Jun 27 17:01:01.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-8881 api-versions'
Jun 27 17:01:01.583: INFO: stderr: ""
Jun 27 17:01:01.583: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 17:01:01.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8881" for this suite. 06/27/23 17:01:01.61
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":128,"skipped":2710,"failed":0}
------------------------------
â€¢ [0.301 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:01:01.336
    Jun 27 17:01:01.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 17:01:01.339
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:01:01.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:01:01.432
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 06/27/23 17:01:01.447
    Jun 27 17:01:01.447: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-8881 api-versions'
    Jun 27 17:01:01.583: INFO: stderr: ""
    Jun 27 17:01:01.583: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napiserver.openshift.io/v1\napps.openshift.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nauthorization.openshift.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\nbuild.openshift.io/v1\ncertificates.k8s.io/v1\ncloudcredential.openshift.io/v1\nconfig.openshift.io/v1\nconsole.openshift.io/v1\nconsole.openshift.io/v1alpha1\ncontrolplane.operator.openshift.io/v1alpha1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nhelm.openshift.io/v1beta1\nibm.com/v1alpha1\nimage.openshift.io/v1\nimageregistry.operator.openshift.io/v1\ningress.operator.openshift.io/v1\nk8s.cni.cncf.io/v1\nmachineconfiguration.openshift.io/v1\nmetrics.k8s.io/v1beta1\nmigration.k8s.io/v1alpha1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmonitoring.coreos.com/v1beta1\nnetwork.operator.openshift.io/v1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\noauth.openshift.io/v1\noperator.openshift.io/v1\noperator.openshift.io/v1alpha1\noperator.tigera.io/v1\noperators.coreos.com/v1\noperators.coreos.com/v1alpha1\noperators.coreos.com/v1alpha2\noperators.coreos.com/v2\npackages.operators.coreos.com/v1\nperformance.openshift.io/v1\nperformance.openshift.io/v1alpha1\nperformance.openshift.io/v2\npolicy/v1\nproject.openshift.io/v1\nquota.openshift.io/v1\nrbac.authorization.k8s.io/v1\nroute.openshift.io/v1\nsamples.operator.openshift.io/v1\nscheduling.k8s.io/v1\nsecurity.internal.openshift.io/v1\nsecurity.openshift.io/v1\nsnapshot.storage.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\ntemplate.openshift.io/v1\ntuned.openshift.io/v1\nuser.openshift.io/v1\nv1\nwhereabouts.cni.cncf.io/v1alpha1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 17:01:01.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8881" for this suite. 06/27/23 17:01:01.61
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:01:01.641
Jun 27 17:01:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename cronjob 06/27/23 17:01:01.644
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:01:01.697
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:01:01.721
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 06/27/23 17:01:01.74
STEP: Ensuring no jobs are scheduled 06/27/23 17:01:01.759
STEP: Ensuring no job exists by listing jobs explicitly 06/27/23 17:06:01.784
STEP: Removing cronjob 06/27/23 17:06:01.799
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 27 17:06:01.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-305" for this suite. 06/27/23 17:06:01.853
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":129,"skipped":2727,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.244 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:01:01.641
    Jun 27 17:01:01.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename cronjob 06/27/23 17:01:01.644
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:01:01.697
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:01:01.721
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 06/27/23 17:01:01.74
    STEP: Ensuring no jobs are scheduled 06/27/23 17:01:01.759
    STEP: Ensuring no job exists by listing jobs explicitly 06/27/23 17:06:01.784
    STEP: Removing cronjob 06/27/23 17:06:01.799
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 27 17:06:01.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-305" for this suite. 06/27/23 17:06:01.853
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:06:01.89
Jun 27 17:06:01.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:06:01.892
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:01.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:01.959
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Jun 27 17:06:01.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/27/23 17:06:12.814
Jun 27 17:06:12.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 create -f -'
Jun 27 17:06:15.239: INFO: stderr: ""
Jun 27 17:06:15.239: INFO: stdout: "e2e-test-crd-publish-openapi-1168-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 27 17:06:15.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 delete e2e-test-crd-publish-openapi-1168-crds test-foo'
Jun 27 17:06:15.453: INFO: stderr: ""
Jun 27 17:06:15.453: INFO: stdout: "e2e-test-crd-publish-openapi-1168-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Jun 27 17:06:15.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 apply -f -'
Jun 27 17:06:17.610: INFO: stderr: ""
Jun 27 17:06:17.610: INFO: stdout: "e2e-test-crd-publish-openapi-1168-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Jun 27 17:06:17.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 delete e2e-test-crd-publish-openapi-1168-crds test-foo'
Jun 27 17:06:17.822: INFO: stderr: ""
Jun 27 17:06:17.823: INFO: stdout: "e2e-test-crd-publish-openapi-1168-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/27/23 17:06:17.823
Jun 27 17:06:17.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 create -f -'
Jun 27 17:06:18.514: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/27/23 17:06:18.515
Jun 27 17:06:18.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 create -f -'
Jun 27 17:06:19.131: INFO: rc: 1
Jun 27 17:06:19.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 apply -f -'
Jun 27 17:06:19.703: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/27/23 17:06:19.703
Jun 27 17:06:19.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 create -f -'
Jun 27 17:06:20.237: INFO: rc: 1
Jun 27 17:06:20.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 apply -f -'
Jun 27 17:06:20.825: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 06/27/23 17:06:20.825
Jun 27 17:06:20.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds'
Jun 27 17:06:22.445: INFO: stderr: ""
Jun 27 17:06:22.445: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1168-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 06/27/23 17:06:22.445
Jun 27 17:06:22.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds.metadata'
Jun 27 17:06:22.901: INFO: stderr: ""
Jun 27 17:06:22.901: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1168-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Jun 27 17:06:22.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds.spec'
Jun 27 17:06:23.390: INFO: stderr: ""
Jun 27 17:06:23.390: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1168-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Jun 27 17:06:23.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds.spec.bars'
Jun 27 17:06:23.929: INFO: stderr: ""
Jun 27 17:06:23.929: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1168-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/27/23 17:06:23.929
Jun 27 17:06:23.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds.spec.bars2'
Jun 27 17:06:24.436: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:06:34.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-2469" for this suite. 06/27/23 17:06:34.535
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":130,"skipped":2792,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.663 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:06:01.89
    Jun 27 17:06:01.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:06:01.892
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:01.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:01.959
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Jun 27 17:06:01.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 06/27/23 17:06:12.814
    Jun 27 17:06:12.814: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 create -f -'
    Jun 27 17:06:15.239: INFO: stderr: ""
    Jun 27 17:06:15.239: INFO: stdout: "e2e-test-crd-publish-openapi-1168-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun 27 17:06:15.239: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 delete e2e-test-crd-publish-openapi-1168-crds test-foo'
    Jun 27 17:06:15.453: INFO: stderr: ""
    Jun 27 17:06:15.453: INFO: stdout: "e2e-test-crd-publish-openapi-1168-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Jun 27 17:06:15.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 apply -f -'
    Jun 27 17:06:17.610: INFO: stderr: ""
    Jun 27 17:06:17.610: INFO: stdout: "e2e-test-crd-publish-openapi-1168-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Jun 27 17:06:17.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 delete e2e-test-crd-publish-openapi-1168-crds test-foo'
    Jun 27 17:06:17.822: INFO: stderr: ""
    Jun 27 17:06:17.823: INFO: stdout: "e2e-test-crd-publish-openapi-1168-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 06/27/23 17:06:17.823
    Jun 27 17:06:17.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 create -f -'
    Jun 27 17:06:18.514: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 06/27/23 17:06:18.515
    Jun 27 17:06:18.515: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 create -f -'
    Jun 27 17:06:19.131: INFO: rc: 1
    Jun 27 17:06:19.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 apply -f -'
    Jun 27 17:06:19.703: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 06/27/23 17:06:19.703
    Jun 27 17:06:19.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 create -f -'
    Jun 27 17:06:20.237: INFO: rc: 1
    Jun 27 17:06:20.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 --namespace=crd-publish-openapi-2469 apply -f -'
    Jun 27 17:06:20.825: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 06/27/23 17:06:20.825
    Jun 27 17:06:20.826: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds'
    Jun 27 17:06:22.445: INFO: stderr: ""
    Jun 27 17:06:22.445: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1168-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 06/27/23 17:06:22.445
    Jun 27 17:06:22.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds.metadata'
    Jun 27 17:06:22.901: INFO: stderr: ""
    Jun 27 17:06:22.901: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1168-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Jun 27 17:06:22.902: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds.spec'
    Jun 27 17:06:23.390: INFO: stderr: ""
    Jun 27 17:06:23.390: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1168-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Jun 27 17:06:23.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds.spec.bars'
    Jun 27 17:06:23.929: INFO: stderr: ""
    Jun 27 17:06:23.929: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-1168-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 06/27/23 17:06:23.929
    Jun 27 17:06:23.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-2469 explain e2e-test-crd-publish-openapi-1168-crds.spec.bars2'
    Jun 27 17:06:24.436: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:06:34.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-2469" for this suite. 06/27/23 17:06:34.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:06:34.56
Jun 27 17:06:34.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename conformance-tests 06/27/23 17:06:34.563
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:34.597
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:34.607
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 06/27/23 17:06:34.634
Jun 27 17:06:34.635: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Jun 27 17:06:34.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-3591" for this suite. 06/27/23 17:06:34.691
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":131,"skipped":2805,"failed":0}
------------------------------
â€¢ [0.185 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:06:34.56
    Jun 27 17:06:34.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename conformance-tests 06/27/23 17:06:34.563
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:34.597
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:34.607
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 06/27/23 17:06:34.634
    Jun 27 17:06:34.635: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Jun 27 17:06:34.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-3591" for this suite. 06/27/23 17:06:34.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:06:34.748
Jun 27 17:06:34.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 17:06:34.75
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:34.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:34.792
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-3550/configmap-test-e1fd1280-b878-4a3d-84be-5762dde951a0 06/27/23 17:06:34.804
STEP: Creating a pod to test consume configMaps 06/27/23 17:06:34.818
Jun 27 17:06:34.870: INFO: Waiting up to 5m0s for pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244" in namespace "configmap-3550" to be "Succeeded or Failed"
Jun 27 17:06:34.880: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244": Phase="Pending", Reason="", readiness=false. Elapsed: 9.25166ms
Jun 27 17:06:36.893: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022773001s
Jun 27 17:06:38.889: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018774389s
Jun 27 17:06:40.890: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019850096s
STEP: Saw pod success 06/27/23 17:06:40.89
Jun 27 17:06:40.891: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244" satisfied condition "Succeeded or Failed"
Jun 27 17:06:40.900: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244 container env-test: <nil>
STEP: delete the pod 06/27/23 17:06:40.979
Jun 27 17:06:41.007: INFO: Waiting for pod pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244 to disappear
Jun 27 17:06:41.019: INFO: Pod pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 17:06:41.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3550" for this suite. 06/27/23 17:06:41.036
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":132,"skipped":2815,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.307 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:06:34.748
    Jun 27 17:06:34.749: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 17:06:34.75
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:34.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:34.792
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-3550/configmap-test-e1fd1280-b878-4a3d-84be-5762dde951a0 06/27/23 17:06:34.804
    STEP: Creating a pod to test consume configMaps 06/27/23 17:06:34.818
    Jun 27 17:06:34.870: INFO: Waiting up to 5m0s for pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244" in namespace "configmap-3550" to be "Succeeded or Failed"
    Jun 27 17:06:34.880: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244": Phase="Pending", Reason="", readiness=false. Elapsed: 9.25166ms
    Jun 27 17:06:36.893: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022773001s
    Jun 27 17:06:38.889: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244": Phase="Pending", Reason="", readiness=false. Elapsed: 4.018774389s
    Jun 27 17:06:40.890: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019850096s
    STEP: Saw pod success 06/27/23 17:06:40.89
    Jun 27 17:06:40.891: INFO: Pod "pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244" satisfied condition "Succeeded or Failed"
    Jun 27 17:06:40.900: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244 container env-test: <nil>
    STEP: delete the pod 06/27/23 17:06:40.979
    Jun 27 17:06:41.007: INFO: Waiting for pod pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244 to disappear
    Jun 27 17:06:41.019: INFO: Pod pod-configmaps-40bb57e9-87b3-4b2c-9756-684876eb0244 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 17:06:41.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3550" for this suite. 06/27/23 17:06:41.036
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:06:41.066
Jun 27 17:06:41.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 17:06:41.068
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:41.113
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:41.127
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
STEP: creating a Service 06/27/23 17:06:41.184
STEP: watching for the Service to be added 06/27/23 17:06:41.224
Jun 27 17:06:41.231: INFO: Found Service test-service-7dv6z in namespace services-736 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Jun 27 17:06:41.231: INFO: Service test-service-7dv6z created
STEP: Getting /status 06/27/23 17:06:41.231
Jun 27 17:06:41.250: INFO: Service test-service-7dv6z has LoadBalancer: {[]}
STEP: patching the ServiceStatus 06/27/23 17:06:41.25
STEP: watching for the Service to be patched 06/27/23 17:06:41.27
Jun 27 17:06:41.277: INFO: observed Service test-service-7dv6z in namespace services-736 with annotations: map[] & LoadBalancer: {[]}
Jun 27 17:06:41.277: INFO: Found Service test-service-7dv6z in namespace services-736 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Jun 27 17:06:41.277: INFO: Service test-service-7dv6z has service status patched
STEP: updating the ServiceStatus 06/27/23 17:06:41.277
Jun 27 17:06:41.337: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 06/27/23 17:06:41.337
Jun 27 17:06:41.343: INFO: Observed Service test-service-7dv6z in namespace services-736 with annotations: map[] & Conditions: {[]}
Jun 27 17:06:41.343: INFO: Observed event: &Service{ObjectMeta:{test-service-7dv6z  services-736  99ef8e80-ade3-44d6-8382-16a6d0e4f6b3 99927 0 2023-06-27 17:06:41 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-27 17:06:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-27 17:06:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.162.134,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.162.134],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Jun 27 17:06:41.346: INFO: Found Service test-service-7dv6z in namespace services-736 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 27 17:06:41.346: INFO: Service test-service-7dv6z has service status updated
STEP: patching the service 06/27/23 17:06:41.346
STEP: watching for the Service to be patched 06/27/23 17:06:41.366
Jun 27 17:06:41.375: INFO: observed Service test-service-7dv6z in namespace services-736 with labels: map[test-service-static:true]
Jun 27 17:06:41.375: INFO: observed Service test-service-7dv6z in namespace services-736 with labels: map[test-service-static:true]
Jun 27 17:06:41.375: INFO: observed Service test-service-7dv6z in namespace services-736 with labels: map[test-service-static:true]
Jun 27 17:06:41.375: INFO: Found Service test-service-7dv6z in namespace services-736 with labels: map[test-service:patched test-service-static:true]
Jun 27 17:06:41.375: INFO: Service test-service-7dv6z patched
STEP: deleting the service 06/27/23 17:06:41.375
STEP: watching for the Service to be deleted 06/27/23 17:06:41.423
Jun 27 17:06:41.430: INFO: Observed event: ADDED
Jun 27 17:06:41.430: INFO: Observed event: MODIFIED
Jun 27 17:06:41.430: INFO: Observed event: MODIFIED
Jun 27 17:06:41.430: INFO: Observed event: MODIFIED
Jun 27 17:06:41.430: INFO: Found Service test-service-7dv6z in namespace services-736 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Jun 27 17:06:41.430: INFO: Service test-service-7dv6z deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 17:06:41.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-736" for this suite. 06/27/23 17:06:41.483
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":133,"skipped":2826,"failed":0}
------------------------------
â€¢ [0.438 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:06:41.066
    Jun 27 17:06:41.067: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 17:06:41.068
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:41.113
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:41.127
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3394
    STEP: creating a Service 06/27/23 17:06:41.184
    STEP: watching for the Service to be added 06/27/23 17:06:41.224
    Jun 27 17:06:41.231: INFO: Found Service test-service-7dv6z in namespace services-736 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Jun 27 17:06:41.231: INFO: Service test-service-7dv6z created
    STEP: Getting /status 06/27/23 17:06:41.231
    Jun 27 17:06:41.250: INFO: Service test-service-7dv6z has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 06/27/23 17:06:41.25
    STEP: watching for the Service to be patched 06/27/23 17:06:41.27
    Jun 27 17:06:41.277: INFO: observed Service test-service-7dv6z in namespace services-736 with annotations: map[] & LoadBalancer: {[]}
    Jun 27 17:06:41.277: INFO: Found Service test-service-7dv6z in namespace services-736 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Jun 27 17:06:41.277: INFO: Service test-service-7dv6z has service status patched
    STEP: updating the ServiceStatus 06/27/23 17:06:41.277
    Jun 27 17:06:41.337: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 06/27/23 17:06:41.337
    Jun 27 17:06:41.343: INFO: Observed Service test-service-7dv6z in namespace services-736 with annotations: map[] & Conditions: {[]}
    Jun 27 17:06:41.343: INFO: Observed event: &Service{ObjectMeta:{test-service-7dv6z  services-736  99ef8e80-ade3-44d6-8382-16a6d0e4f6b3 99927 0 2023-06-27 17:06:41 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-06-27 17:06:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-06-27 17:06:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:172.21.162.134,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[172.21.162.134],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Jun 27 17:06:41.346: INFO: Found Service test-service-7dv6z in namespace services-736 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 27 17:06:41.346: INFO: Service test-service-7dv6z has service status updated
    STEP: patching the service 06/27/23 17:06:41.346
    STEP: watching for the Service to be patched 06/27/23 17:06:41.366
    Jun 27 17:06:41.375: INFO: observed Service test-service-7dv6z in namespace services-736 with labels: map[test-service-static:true]
    Jun 27 17:06:41.375: INFO: observed Service test-service-7dv6z in namespace services-736 with labels: map[test-service-static:true]
    Jun 27 17:06:41.375: INFO: observed Service test-service-7dv6z in namespace services-736 with labels: map[test-service-static:true]
    Jun 27 17:06:41.375: INFO: Found Service test-service-7dv6z in namespace services-736 with labels: map[test-service:patched test-service-static:true]
    Jun 27 17:06:41.375: INFO: Service test-service-7dv6z patched
    STEP: deleting the service 06/27/23 17:06:41.375
    STEP: watching for the Service to be deleted 06/27/23 17:06:41.423
    Jun 27 17:06:41.430: INFO: Observed event: ADDED
    Jun 27 17:06:41.430: INFO: Observed event: MODIFIED
    Jun 27 17:06:41.430: INFO: Observed event: MODIFIED
    Jun 27 17:06:41.430: INFO: Observed event: MODIFIED
    Jun 27 17:06:41.430: INFO: Found Service test-service-7dv6z in namespace services-736 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Jun 27 17:06:41.430: INFO: Service test-service-7dv6z deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 17:06:41.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-736" for this suite. 06/27/23 17:06:41.483
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:06:41.505
Jun 27 17:06:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 17:06:41.507
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:41.559
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:41.575
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 06/27/23 17:06:41.591
STEP: submitting the pod to kubernetes 06/27/23 17:06:41.592
STEP: verifying QOS class is set on the pod 06/27/23 17:06:41.67
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Jun 27 17:06:41.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-598" for this suite. 06/27/23 17:06:41.733
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":134,"skipped":2840,"failed":0}
------------------------------
â€¢ [0.252 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:06:41.505
    Jun 27 17:06:41.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 17:06:41.507
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:41.559
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:41.575
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 06/27/23 17:06:41.591
    STEP: submitting the pod to kubernetes 06/27/23 17:06:41.592
    STEP: verifying QOS class is set on the pod 06/27/23 17:06:41.67
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Jun 27 17:06:41.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-598" for this suite. 06/27/23 17:06:41.733
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:06:41.758
Jun 27 17:06:41.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 17:06:41.762
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:41.799
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:41.81
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 06/27/23 17:06:41.823
Jun 27 17:06:41.823: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Jun 27 17:06:41.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
Jun 27 17:06:43.828: INFO: stderr: ""
Jun 27 17:06:43.828: INFO: stdout: "service/agnhost-replica created\n"
Jun 27 17:06:43.828: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Jun 27 17:06:43.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
Jun 27 17:06:44.490: INFO: stderr: ""
Jun 27 17:06:44.490: INFO: stdout: "service/agnhost-primary created\n"
Jun 27 17:06:44.490: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Jun 27 17:06:44.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
Jun 27 17:06:46.343: INFO: stderr: ""
Jun 27 17:06:46.343: INFO: stdout: "service/frontend created\n"
Jun 27 17:06:46.343: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Jun 27 17:06:46.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
Jun 27 17:06:46.955: INFO: stderr: ""
Jun 27 17:06:46.955: INFO: stdout: "deployment.apps/frontend created\n"
Jun 27 17:06:46.955: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 27 17:06:46.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
Jun 27 17:06:47.569: INFO: stderr: ""
Jun 27 17:06:47.569: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Jun 27 17:06:47.569: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Jun 27 17:06:47.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
Jun 27 17:06:48.348: INFO: stderr: ""
Jun 27 17:06:48.348: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 06/27/23 17:06:48.348
Jun 27 17:06:48.348: INFO: Waiting for all frontend pods to be Running.
Jun 27 17:06:53.403: INFO: Waiting for frontend to serve content.
Jun 27 17:06:53.444: INFO: Trying to add a new entry to the guestbook.
Jun 27 17:06:53.582: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 06/27/23 17:06:53.669
Jun 27 17:06:53.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
Jun 27 17:06:53.915: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:06:53.915: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 06/27/23 17:06:53.916
Jun 27 17:06:53.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
Jun 27 17:06:54.081: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:06:54.081: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/27/23 17:06:54.081
Jun 27 17:06:54.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
Jun 27 17:06:54.278: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:06:54.278: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/27/23 17:06:54.278
Jun 27 17:06:54.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
Jun 27 17:06:54.444: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:06:54.444: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 06/27/23 17:06:54.445
Jun 27 17:06:54.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
Jun 27 17:06:54.628: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:06:54.628: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 06/27/23 17:06:54.628
Jun 27 17:06:54.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
Jun 27 17:06:54.834: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Jun 27 17:06:54.834: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 17:06:54.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2005" for this suite. 06/27/23 17:06:54.849
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":135,"skipped":2851,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.108 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:06:41.758
    Jun 27 17:06:41.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 17:06:41.762
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:41.799
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:41.81
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 06/27/23 17:06:41.823
    Jun 27 17:06:41.823: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Jun 27 17:06:41.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
    Jun 27 17:06:43.828: INFO: stderr: ""
    Jun 27 17:06:43.828: INFO: stdout: "service/agnhost-replica created\n"
    Jun 27 17:06:43.828: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Jun 27 17:06:43.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
    Jun 27 17:06:44.490: INFO: stderr: ""
    Jun 27 17:06:44.490: INFO: stdout: "service/agnhost-primary created\n"
    Jun 27 17:06:44.490: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Jun 27 17:06:44.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
    Jun 27 17:06:46.343: INFO: stderr: ""
    Jun 27 17:06:46.343: INFO: stdout: "service/frontend created\n"
    Jun 27 17:06:46.343: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Jun 27 17:06:46.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
    Jun 27 17:06:46.955: INFO: stderr: ""
    Jun 27 17:06:46.955: INFO: stdout: "deployment.apps/frontend created\n"
    Jun 27 17:06:46.955: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun 27 17:06:46.955: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
    Jun 27 17:06:47.569: INFO: stderr: ""
    Jun 27 17:06:47.569: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Jun 27 17:06:47.569: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Jun 27 17:06:47.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 create -f -'
    Jun 27 17:06:48.348: INFO: stderr: ""
    Jun 27 17:06:48.348: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 06/27/23 17:06:48.348
    Jun 27 17:06:48.348: INFO: Waiting for all frontend pods to be Running.
    Jun 27 17:06:53.403: INFO: Waiting for frontend to serve content.
    Jun 27 17:06:53.444: INFO: Trying to add a new entry to the guestbook.
    Jun 27 17:06:53.582: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 06/27/23 17:06:53.669
    Jun 27 17:06:53.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
    Jun 27 17:06:53.915: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 27 17:06:53.915: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 06/27/23 17:06:53.916
    Jun 27 17:06:53.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
    Jun 27 17:06:54.081: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 27 17:06:54.081: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/27/23 17:06:54.081
    Jun 27 17:06:54.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
    Jun 27 17:06:54.278: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 27 17:06:54.278: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/27/23 17:06:54.278
    Jun 27 17:06:54.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
    Jun 27 17:06:54.444: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 27 17:06:54.444: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 06/27/23 17:06:54.445
    Jun 27 17:06:54.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
    Jun 27 17:06:54.628: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 27 17:06:54.628: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 06/27/23 17:06:54.628
    Jun 27 17:06:54.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2005 delete --grace-period=0 --force -f -'
    Jun 27 17:06:54.834: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Jun 27 17:06:54.834: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 17:06:54.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2005" for this suite. 06/27/23 17:06:54.849
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:06:54.872
Jun 27 17:06:54.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replication-controller 06/27/23 17:06:54.884
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:54.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:54.935
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Jun 27 17:06:54.944: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/27/23 17:06:55.004
STEP: Checking rc "condition-test" has the desired failure condition set 06/27/23 17:06:55.026
STEP: Scaling down rc "condition-test" to satisfy pod quota 06/27/23 17:06:56.059
Jun 27 17:06:56.118: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 06/27/23 17:06:56.118
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 27 17:06:56.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-7072" for this suite. 06/27/23 17:06:56.167
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":136,"skipped":2863,"failed":0}
------------------------------
â€¢ [1.318 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:06:54.872
    Jun 27 17:06:54.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replication-controller 06/27/23 17:06:54.884
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:54.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:54.935
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Jun 27 17:06:54.944: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 06/27/23 17:06:55.004
    STEP: Checking rc "condition-test" has the desired failure condition set 06/27/23 17:06:55.026
    STEP: Scaling down rc "condition-test" to satisfy pod quota 06/27/23 17:06:56.059
    Jun 27 17:06:56.118: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 06/27/23 17:06:56.118
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 27 17:06:56.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-7072" for this suite. 06/27/23 17:06:56.167
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:06:56.197
Jun 27 17:06:56.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 17:06:56.198
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:56.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:56.266
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 06/27/23 17:06:56.306
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/27/23 17:06:56.316
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/27/23 17:06:56.317
STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/27/23 17:06:56.317
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/27/23 17:06:56.321
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/27/23 17:06:56.321
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/27/23 17:06:56.327
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:06:56.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-772" for this suite. 06/27/23 17:06:56.35
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":137,"skipped":2921,"failed":0}
------------------------------
â€¢ [0.172 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:06:56.197
    Jun 27 17:06:56.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 17:06:56.198
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:56.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:56.266
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 06/27/23 17:06:56.306
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 06/27/23 17:06:56.316
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 06/27/23 17:06:56.317
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 06/27/23 17:06:56.317
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 06/27/23 17:06:56.321
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 06/27/23 17:06:56.321
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 06/27/23 17:06:56.327
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:06:56.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-772" for this suite. 06/27/23 17:06:56.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:06:56.371
Jun 27 17:06:56.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:06:56.378
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:56.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:56.448
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-8f262cec-8e8c-4cec-bad1-72109bb8d5c4 06/27/23 17:06:56.462
STEP: Creating a pod to test consume secrets 06/27/23 17:06:56.478
Jun 27 17:06:56.538: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde" in namespace "projected-152" to be "Succeeded or Failed"
Jun 27 17:06:56.549: INFO: Pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde": Phase="Pending", Reason="", readiness=false. Elapsed: 10.997917ms
Jun 27 17:06:58.564: INFO: Pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02504489s
Jun 27 17:07:00.561: INFO: Pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02282923s
STEP: Saw pod success 06/27/23 17:07:00.561
Jun 27 17:07:00.562: INFO: Pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde" satisfied condition "Succeeded or Failed"
Jun 27 17:07:00.573: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde container projected-secret-volume-test: <nil>
STEP: delete the pod 06/27/23 17:07:00.6
Jun 27 17:07:00.633: INFO: Waiting for pod pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde to disappear
Jun 27 17:07:00.644: INFO: Pod pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 27 17:07:00.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-152" for this suite. 06/27/23 17:07:00.66
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":138,"skipped":2935,"failed":0}
------------------------------
â€¢ [4.308 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:06:56.371
    Jun 27 17:06:56.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:06:56.378
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:06:56.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:06:56.448
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-8f262cec-8e8c-4cec-bad1-72109bb8d5c4 06/27/23 17:06:56.462
    STEP: Creating a pod to test consume secrets 06/27/23 17:06:56.478
    Jun 27 17:06:56.538: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde" in namespace "projected-152" to be "Succeeded or Failed"
    Jun 27 17:06:56.549: INFO: Pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde": Phase="Pending", Reason="", readiness=false. Elapsed: 10.997917ms
    Jun 27 17:06:58.564: INFO: Pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02504489s
    Jun 27 17:07:00.561: INFO: Pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.02282923s
    STEP: Saw pod success 06/27/23 17:07:00.561
    Jun 27 17:07:00.562: INFO: Pod "pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde" satisfied condition "Succeeded or Failed"
    Jun 27 17:07:00.573: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:07:00.6
    Jun 27 17:07:00.633: INFO: Waiting for pod pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde to disappear
    Jun 27 17:07:00.644: INFO: Pod pod-projected-secrets-5ff19be5-ca23-4619-b367-010cc8090fde no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 27 17:07:00.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-152" for this suite. 06/27/23 17:07:00.66
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:07:00.682
Jun 27 17:07:00.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename init-container 06/27/23 17:07:00.685
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:00.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:00.733
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 06/27/23 17:07:00.744
Jun 27 17:07:00.745: INFO: PodSpec: initContainers in spec.initContainers
Jun 27 17:07:52.493: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bbded5c0-2e15-409f-80c2-d930028a492c", GenerateName:"", Namespace:"init-container-2228", SelfLink:"", UID:"dcb74938-8ac3-4916-8b4d-1bf9eb6bb528", ResourceVersion:"101054", Generation:0, CreationTimestamp:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"745282974"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b81a13aac22d09bf15c79ff5c1937db2c3d64f750208fa6781dc80752bc6f4f1", "cni.projectcalico.org/podIP":"172.30.250.208/32", "cni.projectcalico.org/podIPs":"172.30.250.208/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.250.208\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.250.208\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00427e090), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 27, 17, 7, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00427e0c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 27, 17, 7, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00427e0f0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 27, 17, 7, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00427e138), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-frdf5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00d374b40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frdf5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0017ff860), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frdf5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0017ff8c0), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frdf5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0017ff800), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00d43ad88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.113.180.90", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004674ee0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00d43ae40)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00d43ae60)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00d43ae7c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00d43ae80), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00a6f0d30), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.113.180.90", PodIP:"172.30.250.208", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.250.208"}}, StartTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004674fc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004675030)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://ba80a1b17974102b3e26ffaea563b462cd9fe61d77daef800ba72e436a38e43b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00d374bc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00d374ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00d43aef4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 27 17:07:52.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-2228" for this suite. 06/27/23 17:07:52.511
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":139,"skipped":2939,"failed":0}
------------------------------
â€¢ [SLOW TEST] [51.848 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:07:00.682
    Jun 27 17:07:00.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename init-container 06/27/23 17:07:00.685
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:00.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:00.733
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 06/27/23 17:07:00.744
    Jun 27 17:07:00.745: INFO: PodSpec: initContainers in spec.initContainers
    Jun 27 17:07:52.493: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-bbded5c0-2e15-409f-80c2-d930028a492c", GenerateName:"", Namespace:"init-container-2228", SelfLink:"", UID:"dcb74938-8ac3-4916-8b4d-1bf9eb6bb528", ResourceVersion:"101054", Generation:0, CreationTimestamp:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"745282974"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"b81a13aac22d09bf15c79ff5c1937db2c3d64f750208fa6781dc80752bc6f4f1", "cni.projectcalico.org/podIP":"172.30.250.208/32", "cni.projectcalico.org/podIPs":"172.30.250.208/32", "k8s.v1.cni.cncf.io/network-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.250.208\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "k8s.v1.cni.cncf.io/networks-status":"[{\n    \"name\": \"k8s-pod-network\",\n    \"ips\": [\n        \"172.30.250.208\"\n    ],\n    \"default\": true,\n    \"dns\": {}\n}]", "openshift.io/scc":"anyuid"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00427e090), Subresource:""}, v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 27, 17, 7, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00427e0c0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"multus", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 27, 17, 7, 1, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00427e0f0), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.June, 27, 17, 7, 52, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00427e138), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-frdf5", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00d374b40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frdf5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0017ff860), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frdf5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0017ff8c0), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-frdf5", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(0xc0017ff800), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00d43ad88), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"10.113.180.90", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc004674ee0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00d43ae40)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00d43ae60)}, v1.Toleration{Key:"node.kubernetes.io/memory-pressure", Operator:"Exists", Value:"", Effect:"NoSchedule", TolerationSeconds:(*int64)(nil)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00d43ae7c), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00d43ae80), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00a6f0d30), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.113.180.90", PodIP:"172.30.250.208", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.30.250.208"}}, StartTime:time.Date(2023, time.June, 27, 17, 7, 0, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004674fc0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc004675030)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"cri-o://ba80a1b17974102b3e26ffaea563b462cd9fe61d77daef800ba72e436a38e43b", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00d374bc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00d374ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00d43aef4)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 27 17:07:52.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-2228" for this suite. 06/27/23 17:07:52.511
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:07:52.545
Jun 27 17:07:52.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename podtemplate 06/27/23 17:07:52.546
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:52.595
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:52.611
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jun 27 17:07:52.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-9125" for this suite. 06/27/23 17:07:52.74
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":140,"skipped":2991,"failed":0}
------------------------------
â€¢ [0.213 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:07:52.545
    Jun 27 17:07:52.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename podtemplate 06/27/23 17:07:52.546
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:52.595
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:52.611
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jun 27 17:07:52.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-9125" for this suite. 06/27/23 17:07:52.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:07:52.765
Jun 27 17:07:52.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:07:52.768
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:52.806
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:52.82
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-47c71371-bedf-4a35-ac7b-22b64ee063bd 06/27/23 17:07:52.831
STEP: Creating a pod to test consume secrets 06/27/23 17:07:52.843
Jun 27 17:07:52.892: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465" in namespace "projected-4534" to be "Succeeded or Failed"
Jun 27 17:07:52.914: INFO: Pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465": Phase="Pending", Reason="", readiness=false. Elapsed: 21.849794ms
Jun 27 17:07:54.925: INFO: Pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033001999s
Jun 27 17:07:56.926: INFO: Pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033517071s
STEP: Saw pod success 06/27/23 17:07:56.926
Jun 27 17:07:56.926: INFO: Pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465" satisfied condition "Succeeded or Failed"
Jun 27 17:07:56.936: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465 container secret-volume-test: <nil>
STEP: delete the pod 06/27/23 17:07:56.974
Jun 27 17:07:57.008: INFO: Waiting for pod pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465 to disappear
Jun 27 17:07:57.018: INFO: Pod pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 27 17:07:57.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4534" for this suite. 06/27/23 17:07:57.034
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":141,"skipped":3021,"failed":0}
------------------------------
â€¢ [4.288 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:07:52.765
    Jun 27 17:07:52.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:07:52.768
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:52.806
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:52.82
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-47c71371-bedf-4a35-ac7b-22b64ee063bd 06/27/23 17:07:52.831
    STEP: Creating a pod to test consume secrets 06/27/23 17:07:52.843
    Jun 27 17:07:52.892: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465" in namespace "projected-4534" to be "Succeeded or Failed"
    Jun 27 17:07:52.914: INFO: Pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465": Phase="Pending", Reason="", readiness=false. Elapsed: 21.849794ms
    Jun 27 17:07:54.925: INFO: Pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033001999s
    Jun 27 17:07:56.926: INFO: Pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033517071s
    STEP: Saw pod success 06/27/23 17:07:56.926
    Jun 27 17:07:56.926: INFO: Pod "pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465" satisfied condition "Succeeded or Failed"
    Jun 27 17:07:56.936: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465 container secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:07:56.974
    Jun 27 17:07:57.008: INFO: Waiting for pod pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465 to disappear
    Jun 27 17:07:57.018: INFO: Pod pod-projected-secrets-d5d46f95-812d-41d3-8672-25af0f947465 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 27 17:07:57.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4534" for this suite. 06/27/23 17:07:57.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:07:57.06
Jun 27 17:07:57.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename disruption 06/27/23 17:07:57.063
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:57.108
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:57.121
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:07:57.132
Jun 27 17:07:57.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename disruption-2 06/27/23 17:07:57.133
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:57.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:57.183
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 06/27/23 17:07:57.209
STEP: Waiting for the pdb to be processed 06/27/23 17:07:59.261
STEP: Waiting for the pdb to be processed 06/27/23 17:07:59.287
STEP: listing a collection of PDBs across all namespaces 06/27/23 17:07:59.299
STEP: listing a collection of PDBs in namespace disruption-8387 06/27/23 17:07:59.31
STEP: deleting a collection of PDBs 06/27/23 17:07:59.322
STEP: Waiting for the PDB collection to be deleted 06/27/23 17:07:59.363
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Jun 27 17:07:59.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-4425" for this suite. 06/27/23 17:07:59.418
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 27 17:07:59.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8387" for this suite. 06/27/23 17:07:59.456
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":142,"skipped":3035,"failed":0}
------------------------------
â€¢ [2.417 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:07:57.06
    Jun 27 17:07:57.060: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename disruption 06/27/23 17:07:57.063
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:57.108
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:57.121
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:07:57.132
    Jun 27 17:07:57.132: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename disruption-2 06/27/23 17:07:57.133
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:57.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:57.183
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 06/27/23 17:07:57.209
    STEP: Waiting for the pdb to be processed 06/27/23 17:07:59.261
    STEP: Waiting for the pdb to be processed 06/27/23 17:07:59.287
    STEP: listing a collection of PDBs across all namespaces 06/27/23 17:07:59.299
    STEP: listing a collection of PDBs in namespace disruption-8387 06/27/23 17:07:59.31
    STEP: deleting a collection of PDBs 06/27/23 17:07:59.322
    STEP: Waiting for the PDB collection to be deleted 06/27/23 17:07:59.363
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Jun 27 17:07:59.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-4425" for this suite. 06/27/23 17:07:59.418
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 27 17:07:59.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8387" for this suite. 06/27/23 17:07:59.456
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:07:59.488
Jun 27 17:07:59.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename security-context-test 06/27/23 17:07:59.49
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:59.58
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:59.608
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Jun 27 17:07:59.719: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7" in namespace "security-context-test-7074" to be "Succeeded or Failed"
Jun 27 17:07:59.736: INFO: Pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.157384ms
Jun 27 17:08:01.747: INFO: Pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027946332s
Jun 27 17:08:03.747: INFO: Pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028602631s
Jun 27 17:08:03.747: INFO: Pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 27 17:08:03.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7074" for this suite. 06/27/23 17:08:03.765
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":143,"skipped":3046,"failed":0}
------------------------------
â€¢ [4.295 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:07:59.488
    Jun 27 17:07:59.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename security-context-test 06/27/23 17:07:59.49
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:07:59.58
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:07:59.608
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Jun 27 17:07:59.719: INFO: Waiting up to 5m0s for pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7" in namespace "security-context-test-7074" to be "Succeeded or Failed"
    Jun 27 17:07:59.736: INFO: Pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 17.157384ms
    Jun 27 17:08:01.747: INFO: Pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027946332s
    Jun 27 17:08:03.747: INFO: Pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.028602631s
    Jun 27 17:08:03.747: INFO: Pod "busybox-user-65534-a10391eb-89f6-4fd5-8ddf-65c50d4d2ac7" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 27 17:08:03.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7074" for this suite. 06/27/23 17:08:03.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:08:03.793
Jun 27 17:08:03.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename dns 06/27/23 17:08:03.795
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:03.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:03.844
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 06/27/23 17:08:03.861
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
 06/27/23 17:08:03.881
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
 06/27/23 17:08:03.881
STEP: creating a pod to probe DNS 06/27/23 17:08:03.881
STEP: submitting the pod to kubernetes 06/27/23 17:08:03.882
Jun 27 17:08:03.927: INFO: Waiting up to 15m0s for pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429" in namespace "dns-4822" to be "running"
Jun 27 17:08:03.938: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Pending", Reason="", readiness=false. Elapsed: 11.277818ms
Jun 27 17:08:05.954: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027285824s
Jun 27 17:08:07.952: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025161747s
Jun 27 17:08:09.967: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040065554s
Jun 27 17:08:11.951: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Running", Reason="", readiness=true. Elapsed: 8.023496267s
Jun 27 17:08:11.951: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429" satisfied condition "running"
STEP: retrieving the pod 06/27/23 17:08:11.951
STEP: looking for the results for each expected name from probers 06/27/23 17:08:11.961
Jun 27 17:08:12.022: INFO: DNS probes using dns-test-f9d923da-a360-488b-8436-2ac986eb0429 succeeded

STEP: deleting the pod 06/27/23 17:08:12.022
STEP: changing the externalName to bar.example.com 06/27/23 17:08:12.055
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
 06/27/23 17:08:12.102
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
 06/27/23 17:08:12.102
STEP: creating a second pod to probe DNS 06/27/23 17:08:12.102
STEP: submitting the pod to kubernetes 06/27/23 17:08:12.103
Jun 27 17:08:12.133: INFO: Waiting up to 15m0s for pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295" in namespace "dns-4822" to be "running"
Jun 27 17:08:12.146: INFO: Pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295": Phase="Pending", Reason="", readiness=false. Elapsed: 12.598637ms
Jun 27 17:08:14.158: INFO: Pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024522704s
Jun 27 17:08:16.157: INFO: Pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295": Phase="Running", Reason="", readiness=true. Elapsed: 4.023495822s
Jun 27 17:08:16.157: INFO: Pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295" satisfied condition "running"
STEP: retrieving the pod 06/27/23 17:08:16.157
STEP: looking for the results for each expected name from probers 06/27/23 17:08:16.167
Jun 27 17:08:16.222: INFO: DNS probes using dns-test-afa444f8-54b2-4836-b505-ac8f44183295 succeeded

STEP: deleting the pod 06/27/23 17:08:16.222
STEP: changing the service to type=ClusterIP 06/27/23 17:08:16.247
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
 06/27/23 17:08:16.322
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
 06/27/23 17:08:16.322
STEP: creating a third pod to probe DNS 06/27/23 17:08:16.322
STEP: submitting the pod to kubernetes 06/27/23 17:08:16.34
Jun 27 17:08:16.370: INFO: Waiting up to 15m0s for pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548" in namespace "dns-4822" to be "running"
Jun 27 17:08:16.381: INFO: Pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548": Phase="Pending", Reason="", readiness=false. Elapsed: 10.740939ms
Jun 27 17:08:18.394: INFO: Pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023055264s
Jun 27 17:08:20.396: INFO: Pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548": Phase="Running", Reason="", readiness=true. Elapsed: 4.025066685s
Jun 27 17:08:20.396: INFO: Pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548" satisfied condition "running"
STEP: retrieving the pod 06/27/23 17:08:20.396
STEP: looking for the results for each expected name from probers 06/27/23 17:08:20.406
Jun 27 17:08:20.473: INFO: DNS probes using dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548 succeeded

STEP: deleting the pod 06/27/23 17:08:20.473
STEP: deleting the test externalName service 06/27/23 17:08:20.502
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 27 17:08:20.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-4822" for this suite. 06/27/23 17:08:20.575
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":144,"skipped":3066,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.807 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:08:03.793
    Jun 27 17:08:03.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename dns 06/27/23 17:08:03.795
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:03.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:03.844
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 06/27/23 17:08:03.861
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
     06/27/23 17:08:03.881
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
     06/27/23 17:08:03.881
    STEP: creating a pod to probe DNS 06/27/23 17:08:03.881
    STEP: submitting the pod to kubernetes 06/27/23 17:08:03.882
    Jun 27 17:08:03.927: INFO: Waiting up to 15m0s for pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429" in namespace "dns-4822" to be "running"
    Jun 27 17:08:03.938: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Pending", Reason="", readiness=false. Elapsed: 11.277818ms
    Jun 27 17:08:05.954: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027285824s
    Jun 27 17:08:07.952: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025161747s
    Jun 27 17:08:09.967: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Pending", Reason="", readiness=false. Elapsed: 6.040065554s
    Jun 27 17:08:11.951: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429": Phase="Running", Reason="", readiness=true. Elapsed: 8.023496267s
    Jun 27 17:08:11.951: INFO: Pod "dns-test-f9d923da-a360-488b-8436-2ac986eb0429" satisfied condition "running"
    STEP: retrieving the pod 06/27/23 17:08:11.951
    STEP: looking for the results for each expected name from probers 06/27/23 17:08:11.961
    Jun 27 17:08:12.022: INFO: DNS probes using dns-test-f9d923da-a360-488b-8436-2ac986eb0429 succeeded

    STEP: deleting the pod 06/27/23 17:08:12.022
    STEP: changing the externalName to bar.example.com 06/27/23 17:08:12.055
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
     06/27/23 17:08:12.102
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
     06/27/23 17:08:12.102
    STEP: creating a second pod to probe DNS 06/27/23 17:08:12.102
    STEP: submitting the pod to kubernetes 06/27/23 17:08:12.103
    Jun 27 17:08:12.133: INFO: Waiting up to 15m0s for pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295" in namespace "dns-4822" to be "running"
    Jun 27 17:08:12.146: INFO: Pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295": Phase="Pending", Reason="", readiness=false. Elapsed: 12.598637ms
    Jun 27 17:08:14.158: INFO: Pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024522704s
    Jun 27 17:08:16.157: INFO: Pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295": Phase="Running", Reason="", readiness=true. Elapsed: 4.023495822s
    Jun 27 17:08:16.157: INFO: Pod "dns-test-afa444f8-54b2-4836-b505-ac8f44183295" satisfied condition "running"
    STEP: retrieving the pod 06/27/23 17:08:16.157
    STEP: looking for the results for each expected name from probers 06/27/23 17:08:16.167
    Jun 27 17:08:16.222: INFO: DNS probes using dns-test-afa444f8-54b2-4836-b505-ac8f44183295 succeeded

    STEP: deleting the pod 06/27/23 17:08:16.222
    STEP: changing the service to type=ClusterIP 06/27/23 17:08:16.247
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
     06/27/23 17:08:16.322
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-4822.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-4822.svc.cluster.local; sleep 1; done
     06/27/23 17:08:16.322
    STEP: creating a third pod to probe DNS 06/27/23 17:08:16.322
    STEP: submitting the pod to kubernetes 06/27/23 17:08:16.34
    Jun 27 17:08:16.370: INFO: Waiting up to 15m0s for pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548" in namespace "dns-4822" to be "running"
    Jun 27 17:08:16.381: INFO: Pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548": Phase="Pending", Reason="", readiness=false. Elapsed: 10.740939ms
    Jun 27 17:08:18.394: INFO: Pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023055264s
    Jun 27 17:08:20.396: INFO: Pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548": Phase="Running", Reason="", readiness=true. Elapsed: 4.025066685s
    Jun 27 17:08:20.396: INFO: Pod "dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548" satisfied condition "running"
    STEP: retrieving the pod 06/27/23 17:08:20.396
    STEP: looking for the results for each expected name from probers 06/27/23 17:08:20.406
    Jun 27 17:08:20.473: INFO: DNS probes using dns-test-8c5cae37-0bed-44ba-a0f3-dc055a077548 succeeded

    STEP: deleting the pod 06/27/23 17:08:20.473
    STEP: deleting the test externalName service 06/27/23 17:08:20.502
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 27 17:08:20.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-4822" for this suite. 06/27/23 17:08:20.575
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:08:20.606
Jun 27 17:08:20.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:08:20.608
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:20.642
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:20.653
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 06/27/23 17:08:20.666
Jun 27 17:08:20.717: INFO: Waiting up to 5m0s for pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d" in namespace "downward-api-8589" to be "Succeeded or Failed"
Jun 27 17:08:20.730: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.587518ms
Jun 27 17:08:22.742: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024913346s
Jun 27 17:08:24.769: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052249207s
Jun 27 17:08:26.744: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027398853s
STEP: Saw pod success 06/27/23 17:08:26.744
Jun 27 17:08:26.748: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d" satisfied condition "Succeeded or Failed"
Jun 27 17:08:26.758: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d container dapi-container: <nil>
STEP: delete the pod 06/27/23 17:08:26.785
Jun 27 17:08:26.818: INFO: Waiting for pod downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d to disappear
Jun 27 17:08:26.828: INFO: Pod downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 27 17:08:26.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8589" for this suite. 06/27/23 17:08:26.844
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":145,"skipped":3083,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.257 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:08:20.606
    Jun 27 17:08:20.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:08:20.608
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:20.642
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:20.653
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 06/27/23 17:08:20.666
    Jun 27 17:08:20.717: INFO: Waiting up to 5m0s for pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d" in namespace "downward-api-8589" to be "Succeeded or Failed"
    Jun 27 17:08:20.730: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 13.587518ms
    Jun 27 17:08:22.742: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024913346s
    Jun 27 17:08:24.769: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.052249207s
    Jun 27 17:08:26.744: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027398853s
    STEP: Saw pod success 06/27/23 17:08:26.744
    Jun 27 17:08:26.748: INFO: Pod "downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d" satisfied condition "Succeeded or Failed"
    Jun 27 17:08:26.758: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d container dapi-container: <nil>
    STEP: delete the pod 06/27/23 17:08:26.785
    Jun 27 17:08:26.818: INFO: Waiting for pod downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d to disappear
    Jun 27 17:08:26.828: INFO: Pod downward-api-d63d4421-3dec-40e3-b4e6-680d40ed6b4d no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 27 17:08:26.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8589" for this suite. 06/27/23 17:08:26.844
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:08:26.873
Jun 27 17:08:26.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubelet-test 06/27/23 17:08:26.875
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:26.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:26.932
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 27 17:08:31.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-4441" for this suite. 06/27/23 17:08:31.108
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":146,"skipped":3098,"failed":0}
------------------------------
â€¢ [4.282 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:08:26.873
    Jun 27 17:08:26.873: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubelet-test 06/27/23 17:08:26.875
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:26.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:26.932
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 27 17:08:31.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-4441" for this suite. 06/27/23 17:08:31.108
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:08:31.159
Jun 27 17:08:31.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:08:31.162
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:31.258
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:31.268
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:08:31.279
Jun 27 17:08:31.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099" in namespace "projected-9582" to be "Succeeded or Failed"
Jun 27 17:08:31.356: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099": Phase="Pending", Reason="", readiness=false. Elapsed: 13.866162ms
Jun 27 17:08:33.371: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02923074s
Jun 27 17:08:35.373: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03164924s
Jun 27 17:08:37.368: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02656268s
STEP: Saw pod success 06/27/23 17:08:37.368
Jun 27 17:08:37.369: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099" satisfied condition "Succeeded or Failed"
Jun 27 17:08:37.380: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099 container client-container: <nil>
STEP: delete the pod 06/27/23 17:08:37.41
Jun 27 17:08:37.440: INFO: Waiting for pod downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099 to disappear
Jun 27 17:08:37.451: INFO: Pod downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 17:08:37.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9582" for this suite. 06/27/23 17:08:37.467
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":147,"skipped":3139,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.326 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:08:31.159
    Jun 27 17:08:31.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:08:31.162
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:31.258
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:31.268
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:08:31.279
    Jun 27 17:08:31.341: INFO: Waiting up to 5m0s for pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099" in namespace "projected-9582" to be "Succeeded or Failed"
    Jun 27 17:08:31.356: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099": Phase="Pending", Reason="", readiness=false. Elapsed: 13.866162ms
    Jun 27 17:08:33.371: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02923074s
    Jun 27 17:08:35.373: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099": Phase="Pending", Reason="", readiness=false. Elapsed: 4.03164924s
    Jun 27 17:08:37.368: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02656268s
    STEP: Saw pod success 06/27/23 17:08:37.368
    Jun 27 17:08:37.369: INFO: Pod "downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099" satisfied condition "Succeeded or Failed"
    Jun 27 17:08:37.380: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:08:37.41
    Jun 27 17:08:37.440: INFO: Waiting for pod downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099 to disappear
    Jun 27 17:08:37.451: INFO: Pod downwardapi-volume-869a45b3-9cd3-45d1-afd2-7ce3dc208099 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 17:08:37.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9582" for this suite. 06/27/23 17:08:37.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:08:37.496
Jun 27 17:08:37.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:08:37.498
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:37.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:37.567
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:08:37.584
Jun 27 17:08:37.642: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18" in namespace "downward-api-1120" to be "Succeeded or Failed"
Jun 27 17:08:37.655: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18": Phase="Pending", Reason="", readiness=false. Elapsed: 12.874965ms
Jun 27 17:08:39.667: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024682453s
Jun 27 17:08:41.666: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023427369s
Jun 27 17:08:43.668: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025817449s
STEP: Saw pod success 06/27/23 17:08:43.668
Jun 27 17:08:43.668: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18" satisfied condition "Succeeded or Failed"
Jun 27 17:08:43.693: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18 container client-container: <nil>
STEP: delete the pod 06/27/23 17:08:43.739
Jun 27 17:08:43.773: INFO: Waiting for pod downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18 to disappear
Jun 27 17:08:43.783: INFO: Pod downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 17:08:43.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1120" for this suite. 06/27/23 17:08:43.799
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":148,"skipped":3147,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.330 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:08:37.496
    Jun 27 17:08:37.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:08:37.498
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:37.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:37.567
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:08:37.584
    Jun 27 17:08:37.642: INFO: Waiting up to 5m0s for pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18" in namespace "downward-api-1120" to be "Succeeded or Failed"
    Jun 27 17:08:37.655: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18": Phase="Pending", Reason="", readiness=false. Elapsed: 12.874965ms
    Jun 27 17:08:39.667: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024682453s
    Jun 27 17:08:41.666: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.023427369s
    Jun 27 17:08:43.668: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025817449s
    STEP: Saw pod success 06/27/23 17:08:43.668
    Jun 27 17:08:43.668: INFO: Pod "downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18" satisfied condition "Succeeded or Failed"
    Jun 27 17:08:43.693: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:08:43.739
    Jun 27 17:08:43.773: INFO: Waiting for pod downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18 to disappear
    Jun 27 17:08:43.783: INFO: Pod downwardapi-volume-39f8e17e-4b11-49f7-8dee-fa23de3f9c18 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 17:08:43.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1120" for this suite. 06/27/23 17:08:43.799
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:08:43.826
Jun 27 17:08:43.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 17:08:43.828
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:43.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:43.881
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Jun 27 17:08:43.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: creating the pod 06/27/23 17:08:43.893
STEP: submitting the pod to kubernetes 06/27/23 17:08:43.893
Jun 27 17:08:43.947: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549" in namespace "pods-3790" to be "running and ready"
Jun 27 17:08:43.961: INFO: Pod "pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549": Phase="Pending", Reason="", readiness=false. Elapsed: 14.049548ms
Jun 27 17:08:43.961: INFO: The phase of Pod pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:08:45.975: INFO: Pod "pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549": Phase="Running", Reason="", readiness=true. Elapsed: 2.027777816s
Jun 27 17:08:45.975: INFO: The phase of Pod pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549 is Running (Ready = true)
Jun 27 17:08:45.975: INFO: Pod "pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 17:08:46.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3790" for this suite. 06/27/23 17:08:46.237
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":149,"skipped":3148,"failed":0}
------------------------------
â€¢ [2.434 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:08:43.826
    Jun 27 17:08:43.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 17:08:43.828
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:43.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:43.881
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Jun 27 17:08:43.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: creating the pod 06/27/23 17:08:43.893
    STEP: submitting the pod to kubernetes 06/27/23 17:08:43.893
    Jun 27 17:08:43.947: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549" in namespace "pods-3790" to be "running and ready"
    Jun 27 17:08:43.961: INFO: Pod "pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549": Phase="Pending", Reason="", readiness=false. Elapsed: 14.049548ms
    Jun 27 17:08:43.961: INFO: The phase of Pod pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:08:45.975: INFO: Pod "pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549": Phase="Running", Reason="", readiness=true. Elapsed: 2.027777816s
    Jun 27 17:08:45.975: INFO: The phase of Pod pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549 is Running (Ready = true)
    Jun 27 17:08:45.975: INFO: Pod "pod-exec-websocket-1ea30a6d-dcb6-4637-9bbb-2b1dcc4a9549" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 17:08:46.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3790" for this suite. 06/27/23 17:08:46.237
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:08:46.265
Jun 27 17:08:46.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubelet-test 06/27/23 17:08:46.266
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:46.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:46.313
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Jun 27 17:08:46.397: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028" in namespace "kubelet-test-7961" to be "running and ready"
Jun 27 17:08:46.424: INFO: Pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021838ms
Jun 27 17:08:46.424: INFO: The phase of Pod busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:08:48.435: INFO: Pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037244148s
Jun 27 17:08:48.435: INFO: The phase of Pod busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:08:50.435: INFO: Pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028": Phase="Running", Reason="", readiness=true. Elapsed: 4.037552098s
Jun 27 17:08:50.435: INFO: The phase of Pod busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028 is Running (Ready = true)
Jun 27 17:08:50.435: INFO: Pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 27 17:08:50.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7961" for this suite. 06/27/23 17:08:50.497
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":150,"skipped":3194,"failed":0}
------------------------------
â€¢ [4.251 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:08:46.265
    Jun 27 17:08:46.265: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubelet-test 06/27/23 17:08:46.266
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:46.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:46.313
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Jun 27 17:08:46.397: INFO: Waiting up to 5m0s for pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028" in namespace "kubelet-test-7961" to be "running and ready"
    Jun 27 17:08:46.424: INFO: Pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028": Phase="Pending", Reason="", readiness=false. Elapsed: 26.021838ms
    Jun 27 17:08:46.424: INFO: The phase of Pod busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:08:48.435: INFO: Pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037244148s
    Jun 27 17:08:48.435: INFO: The phase of Pod busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:08:50.435: INFO: Pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028": Phase="Running", Reason="", readiness=true. Elapsed: 4.037552098s
    Jun 27 17:08:50.435: INFO: The phase of Pod busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028 is Running (Ready = true)
    Jun 27 17:08:50.435: INFO: Pod "busybox-scheduling-ac34ae01-fe9d-4a89-abab-6492c6732028" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 27 17:08:50.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7961" for this suite. 06/27/23 17:08:50.497
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:08:50.526
Jun 27 17:08:50.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename hostport 06/27/23 17:08:50.528
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:50.565
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:50.585
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/27/23 17:08:50.644
Jun 27 17:08:50.701: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1742" to be "running and ready"
Jun 27 17:08:50.711: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.160141ms
Jun 27 17:08:50.711: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:08:52.725: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023993173s
Jun 27 17:08:52.725: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:08:54.730: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.028988613s
Jun 27 17:08:54.730: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 27 17:08:54.730: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.113.180.90 on the node which pod1 resides and expect scheduled 06/27/23 17:08:54.73
Jun 27 17:08:54.759: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1742" to be "running and ready"
Jun 27 17:08:54.769: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005664ms
Jun 27 17:08:54.769: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:08:56.780: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021065224s
Jun 27 17:08:56.780: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:08:58.787: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.028434274s
Jun 27 17:08:58.795: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 27 17:08:58.795: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.113.180.90 but use UDP protocol on the node which pod2 resides 06/27/23 17:08:58.796
Jun 27 17:08:58.827: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1742" to be "running and ready"
Jun 27 17:08:58.837: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.614682ms
Jun 27 17:08:58.837: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:09:00.874: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047191447s
Jun 27 17:09:00.875: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:09:02.862: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.03439155s
Jun 27 17:09:02.862: INFO: The phase of Pod pod3 is Running (Ready = true)
Jun 27 17:09:02.862: INFO: Pod "pod3" satisfied condition "running and ready"
Jun 27 17:09:02.895: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1742" to be "running and ready"
Jun 27 17:09:02.922: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 26.975316ms
Jun 27 17:09:02.923: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:09:04.933: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037707566s
Jun 27 17:09:04.933: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:09:06.934: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 4.038974934s
Jun 27 17:09:06.935: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Jun 27 17:09:06.935: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/27/23 17:09:06.943
Jun 27 17:09:06.944: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.113.180.90 http://127.0.0.1:54323/hostname] Namespace:hostport-1742 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:09:06.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:09:06.946: INFO: ExecWithOptions: Clientset creation
Jun 27 17:09:06.946: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1742/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.113.180.90+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.113.180.90, port: 54323 06/27/23 17:09:07.172
Jun 27 17:09:07.172: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.113.180.90:54323/hostname] Namespace:hostport-1742 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:09:07.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:09:07.175: INFO: ExecWithOptions: Clientset creation
Jun 27 17:09:07.175: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1742/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.113.180.90%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.113.180.90, port: 54323 UDP 06/27/23 17:09:07.482
Jun 27 17:09:07.482: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.113.180.90 54323] Namespace:hostport-1742 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:09:07.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:09:07.483: INFO: ExecWithOptions: Clientset creation
Jun 27 17:09:07.483: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1742/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.113.180.90+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Jun 27 17:09:12.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-1742" for this suite. 06/27/23 17:09:12.879
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":151,"skipped":3247,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.377 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:08:50.526
    Jun 27 17:08:50.526: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename hostport 06/27/23 17:08:50.528
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:08:50.565
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:08:50.585
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 06/27/23 17:08:50.644
    Jun 27 17:08:50.701: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-1742" to be "running and ready"
    Jun 27 17:08:50.711: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 10.160141ms
    Jun 27 17:08:50.711: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:08:52.725: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023993173s
    Jun 27 17:08:52.725: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:08:54.730: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.028988613s
    Jun 27 17:08:54.730: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 27 17:08:54.730: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.113.180.90 on the node which pod1 resides and expect scheduled 06/27/23 17:08:54.73
    Jun 27 17:08:54.759: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-1742" to be "running and ready"
    Jun 27 17:08:54.769: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 10.005664ms
    Jun 27 17:08:54.769: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:08:56.780: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021065224s
    Jun 27 17:08:56.780: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:08:58.787: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.028434274s
    Jun 27 17:08:58.795: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 27 17:08:58.795: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.113.180.90 but use UDP protocol on the node which pod2 resides 06/27/23 17:08:58.796
    Jun 27 17:08:58.827: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-1742" to be "running and ready"
    Jun 27 17:08:58.837: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.614682ms
    Jun 27 17:08:58.837: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:09:00.874: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047191447s
    Jun 27 17:09:00.875: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:09:02.862: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 4.03439155s
    Jun 27 17:09:02.862: INFO: The phase of Pod pod3 is Running (Ready = true)
    Jun 27 17:09:02.862: INFO: Pod "pod3" satisfied condition "running and ready"
    Jun 27 17:09:02.895: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-1742" to be "running and ready"
    Jun 27 17:09:02.922: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 26.975316ms
    Jun 27 17:09:02.923: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:09:04.933: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037707566s
    Jun 27 17:09:04.933: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:09:06.934: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 4.038974934s
    Jun 27 17:09:06.935: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Jun 27 17:09:06.935: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 06/27/23 17:09:06.943
    Jun 27 17:09:06.944: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.113.180.90 http://127.0.0.1:54323/hostname] Namespace:hostport-1742 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:09:06.944: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:09:06.946: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:09:06.946: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1742/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.113.180.90+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.113.180.90, port: 54323 06/27/23 17:09:07.172
    Jun 27 17:09:07.172: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.113.180.90:54323/hostname] Namespace:hostport-1742 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:09:07.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:09:07.175: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:09:07.175: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1742/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.113.180.90%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.113.180.90, port: 54323 UDP 06/27/23 17:09:07.482
    Jun 27 17:09:07.482: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.113.180.90 54323] Namespace:hostport-1742 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:09:07.482: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:09:07.483: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:09:07.483: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/hostport-1742/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.113.180.90+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Jun 27 17:09:12.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-1742" for this suite. 06/27/23 17:09:12.879
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:09:12.91
Jun 27 17:09:12.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename disruption 06/27/23 17:09:12.912
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:12.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:12.965
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 06/27/23 17:09:12.989
STEP: Updating PodDisruptionBudget status 06/27/23 17:09:15.015
STEP: Waiting for all pods to be running 06/27/23 17:09:15.053
Jun 27 17:09:15.065: INFO: running pods: 0 < 1
Jun 27 17:09:17.079: INFO: running pods: 0 < 1
STEP: locating a running pod 06/27/23 17:09:19.076
STEP: Waiting for the pdb to be processed 06/27/23 17:09:19.111
STEP: Patching PodDisruptionBudget status 06/27/23 17:09:19.132
STEP: Waiting for the pdb to be processed 06/27/23 17:09:19.158
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 27 17:09:19.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1498" for this suite. 06/27/23 17:09:19.196
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":152,"skipped":3258,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.307 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:09:12.91
    Jun 27 17:09:12.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename disruption 06/27/23 17:09:12.912
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:12.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:12.965
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 06/27/23 17:09:12.989
    STEP: Updating PodDisruptionBudget status 06/27/23 17:09:15.015
    STEP: Waiting for all pods to be running 06/27/23 17:09:15.053
    Jun 27 17:09:15.065: INFO: running pods: 0 < 1
    Jun 27 17:09:17.079: INFO: running pods: 0 < 1
    STEP: locating a running pod 06/27/23 17:09:19.076
    STEP: Waiting for the pdb to be processed 06/27/23 17:09:19.111
    STEP: Patching PodDisruptionBudget status 06/27/23 17:09:19.132
    STEP: Waiting for the pdb to be processed 06/27/23 17:09:19.158
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 27 17:09:19.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1498" for this suite. 06/27/23 17:09:19.196
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:09:19.231
Jun 27 17:09:19.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:09:19.233
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:19.268
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:19.29
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-d3fcbed9-3a8c-42ce-baa5-5286e6a63885 06/27/23 17:09:19.3
STEP: Creating a pod to test consume configMaps 06/27/23 17:09:19.316
Jun 27 17:09:19.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1" in namespace "projected-1353" to be "Succeeded or Failed"
Jun 27 17:09:19.379: INFO: Pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.186628ms
Jun 27 17:09:21.393: INFO: Pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03105632s
Jun 27 17:09:23.392: INFO: Pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029992605s
STEP: Saw pod success 06/27/23 17:09:23.392
Jun 27 17:09:23.393: INFO: Pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1" satisfied condition "Succeeded or Failed"
Jun 27 17:09:23.406: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1 container projected-configmap-volume-test: <nil>
STEP: delete the pod 06/27/23 17:09:23.431
Jun 27 17:09:23.465: INFO: Waiting for pod pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1 to disappear
Jun 27 17:09:23.474: INFO: Pod pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 27 17:09:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1353" for this suite. 06/27/23 17:09:23.491
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":153,"skipped":3333,"failed":0}
------------------------------
â€¢ [4.287 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:09:19.231
    Jun 27 17:09:19.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:09:19.233
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:19.268
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:19.29
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-d3fcbed9-3a8c-42ce-baa5-5286e6a63885 06/27/23 17:09:19.3
    STEP: Creating a pod to test consume configMaps 06/27/23 17:09:19.316
    Jun 27 17:09:19.362: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1" in namespace "projected-1353" to be "Succeeded or Failed"
    Jun 27 17:09:19.379: INFO: Pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1": Phase="Pending", Reason="", readiness=false. Elapsed: 17.186628ms
    Jun 27 17:09:21.393: INFO: Pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.03105632s
    Jun 27 17:09:23.392: INFO: Pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029992605s
    STEP: Saw pod success 06/27/23 17:09:23.392
    Jun 27 17:09:23.393: INFO: Pod "pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1" satisfied condition "Succeeded or Failed"
    Jun 27 17:09:23.406: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:09:23.431
    Jun 27 17:09:23.465: INFO: Waiting for pod pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1 to disappear
    Jun 27 17:09:23.474: INFO: Pod pod-projected-configmaps-49e807bb-bddf-4012-81b3-22b8c5fc03c1 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 27 17:09:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1353" for this suite. 06/27/23 17:09:23.491
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:09:23.521
Jun 27 17:09:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 17:09:23.523
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:23.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:23.568
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 17:09:23.634
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:09:24.04
STEP: Deploying the webhook pod 06/27/23 17:09:24.07
STEP: Wait for the deployment to be ready 06/27/23 17:09:24.1
Jun 27 17:09:24.127: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 27 17:09:26.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 9, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 9, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 9, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 9, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 17:09:28.186
STEP: Verifying the service has paired with the endpoint 06/27/23 17:09:28.224
Jun 27 17:09:29.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 06/27/23 17:09:29.235
STEP: Creating a custom resource definition that should be denied by the webhook 06/27/23 17:09:29.295
Jun 27 17:09:29.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:09:29.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8484" for this suite. 06/27/23 17:09:29.396
STEP: Destroying namespace "webhook-8484-markers" for this suite. 06/27/23 17:09:29.42
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":154,"skipped":3348,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.087 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:09:23.521
    Jun 27 17:09:23.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 17:09:23.523
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:23.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:23.568
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 17:09:23.634
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:09:24.04
    STEP: Deploying the webhook pod 06/27/23 17:09:24.07
    STEP: Wait for the deployment to be ready 06/27/23 17:09:24.1
    Jun 27 17:09:24.127: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 27 17:09:26.160: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 9, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 9, 24, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 9, 24, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 9, 24, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 17:09:28.186
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:09:28.224
    Jun 27 17:09:29.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 06/27/23 17:09:29.235
    STEP: Creating a custom resource definition that should be denied by the webhook 06/27/23 17:09:29.295
    Jun 27 17:09:29.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:09:29.358: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8484" for this suite. 06/27/23 17:09:29.396
    STEP: Destroying namespace "webhook-8484-markers" for this suite. 06/27/23 17:09:29.42
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:09:29.608
Jun 27 17:09:29.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename var-expansion 06/27/23 17:09:29.61
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:29.641
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:29.664
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 06/27/23 17:09:29.697
Jun 27 17:09:29.748: INFO: Waiting up to 5m0s for pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03" in namespace "var-expansion-1427" to be "Succeeded or Failed"
Jun 27 17:09:29.795: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03": Phase="Pending", Reason="", readiness=false. Elapsed: 47.070762ms
Jun 27 17:09:31.806: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058444585s
Jun 27 17:09:33.817: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069274783s
Jun 27 17:09:35.812: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063910638s
STEP: Saw pod success 06/27/23 17:09:35.812
Jun 27 17:09:35.812: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03" satisfied condition "Succeeded or Failed"
Jun 27 17:09:35.823: INFO: Trying to get logs from node 10.113.180.90 pod var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03 container dapi-container: <nil>
STEP: delete the pod 06/27/23 17:09:35.857
Jun 27 17:09:35.889: INFO: Waiting for pod var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03 to disappear
Jun 27 17:09:35.902: INFO: Pod var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 27 17:09:35.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1427" for this suite. 06/27/23 17:09:35.933
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":155,"skipped":3354,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.348 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:09:29.608
    Jun 27 17:09:29.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename var-expansion 06/27/23 17:09:29.61
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:29.641
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:29.664
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 06/27/23 17:09:29.697
    Jun 27 17:09:29.748: INFO: Waiting up to 5m0s for pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03" in namespace "var-expansion-1427" to be "Succeeded or Failed"
    Jun 27 17:09:29.795: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03": Phase="Pending", Reason="", readiness=false. Elapsed: 47.070762ms
    Jun 27 17:09:31.806: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03": Phase="Pending", Reason="", readiness=false. Elapsed: 2.058444585s
    Jun 27 17:09:33.817: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03": Phase="Pending", Reason="", readiness=false. Elapsed: 4.069274783s
    Jun 27 17:09:35.812: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.063910638s
    STEP: Saw pod success 06/27/23 17:09:35.812
    Jun 27 17:09:35.812: INFO: Pod "var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03" satisfied condition "Succeeded or Failed"
    Jun 27 17:09:35.823: INFO: Trying to get logs from node 10.113.180.90 pod var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03 container dapi-container: <nil>
    STEP: delete the pod 06/27/23 17:09:35.857
    Jun 27 17:09:35.889: INFO: Waiting for pod var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03 to disappear
    Jun 27 17:09:35.902: INFO: Pod var-expansion-97480160-7bf1-40f4-b8d8-7c6ac4806c03 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 27 17:09:35.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1427" for this suite. 06/27/23 17:09:35.933
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:09:35.958
Jun 27 17:09:35.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename dns 06/27/23 17:09:35.963
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:36
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:36.011
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/27/23 17:09:36.023
Jun 27 17:09:36.072: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6167  ba08e800-8834-4068-8fdd-a24a9985421f 102971 0 2023-06-27 17:09:36 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-06-27 17:09:36 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktcqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktcqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:09:36.073: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6167" to be "running and ready"
Jun 27 17:09:36.090: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 17.539019ms
Jun 27 17:09:36.090: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:09:38.102: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029355615s
Jun 27 17:09:38.102: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:09:40.101: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.028460408s
Jun 27 17:09:40.101: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Jun 27 17:09:40.101: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 06/27/23 17:09:40.102
Jun 27 17:09:40.102: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6167 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:09:40.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:09:40.103: INFO: ExecWithOptions: Clientset creation
Jun 27 17:09:40.103: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-6167/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 06/27/23 17:09:40.397
Jun 27 17:09:40.398: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6167 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:09:40.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:09:40.401: INFO: ExecWithOptions: Clientset creation
Jun 27 17:09:40.401: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-6167/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 27 17:09:40.650: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 27 17:09:40.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6167" for this suite. 06/27/23 17:09:40.704
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":156,"skipped":3354,"failed":0}
------------------------------
â€¢ [4.765 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:09:35.958
    Jun 27 17:09:35.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename dns 06/27/23 17:09:35.963
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:36
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:36.011
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 06/27/23 17:09:36.023
    Jun 27 17:09:36.072: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-6167  ba08e800-8834-4068-8fdd-a24a9985421f 102971 0 2023-06-27 17:09:36 +0000 UTC <nil> <nil> map[] map[openshift.io/scc:anyuid] [] [] [{e2e.test Update v1 2023-06-27 17:09:36 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ktcqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ktcqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c50,c25,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:09:36.073: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-6167" to be "running and ready"
    Jun 27 17:09:36.090: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 17.539019ms
    Jun 27 17:09:36.090: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:09:38.102: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029355615s
    Jun 27 17:09:38.102: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:09:40.101: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 4.028460408s
    Jun 27 17:09:40.101: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Jun 27 17:09:40.101: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 06/27/23 17:09:40.102
    Jun 27 17:09:40.102: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-6167 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:09:40.102: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:09:40.103: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:09:40.103: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-6167/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 06/27/23 17:09:40.397
    Jun 27 17:09:40.398: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-6167 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:09:40.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:09:40.401: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:09:40.401: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/dns-6167/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 27 17:09:40.650: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 27 17:09:40.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6167" for this suite. 06/27/23 17:09:40.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:09:40.731
Jun 27 17:09:40.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename watch 06/27/23 17:09:40.732
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:40.773
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:40.786
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 06/27/23 17:09:40.798
STEP: creating a watch on configmaps with label B 06/27/23 17:09:40.803
STEP: creating a watch on configmaps with label A or B 06/27/23 17:09:40.811
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/27/23 17:09:40.815
Jun 27 17:09:40.833: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103048 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:09:40.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103048 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/27/23 17:09:40.834
Jun 27 17:09:40.860: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103054 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:09:40.860: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103054 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/27/23 17:09:40.86
Jun 27 17:09:40.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103056 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:09:40.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103056 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/27/23 17:09:40.882
Jun 27 17:09:40.899: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103057 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:09:40.899: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103057 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/27/23 17:09:40.899
Jun 27 17:09:40.914: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2625  fcdda00a-2e19-4685-b130-beb0ed4b1de9 103058 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:09:40.914: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2625  fcdda00a-2e19-4685-b130-beb0ed4b1de9 103058 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/27/23 17:09:50.914
Jun 27 17:09:50.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2625  fcdda00a-2e19-4685-b130-beb0ed4b1de9 103231 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:09:50.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2625  fcdda00a-2e19-4685-b130-beb0ed4b1de9 103231 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 27 17:10:00.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2625" for this suite. 06/27/23 17:10:00.952
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":157,"skipped":3386,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.241 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:09:40.731
    Jun 27 17:09:40.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename watch 06/27/23 17:09:40.732
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:09:40.773
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:09:40.786
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 06/27/23 17:09:40.798
    STEP: creating a watch on configmaps with label B 06/27/23 17:09:40.803
    STEP: creating a watch on configmaps with label A or B 06/27/23 17:09:40.811
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 06/27/23 17:09:40.815
    Jun 27 17:09:40.833: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103048 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:09:40.834: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103048 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 06/27/23 17:09:40.834
    Jun 27 17:09:40.860: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103054 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:09:40.860: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103054 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 06/27/23 17:09:40.86
    Jun 27 17:09:40.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103056 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:09:40.882: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103056 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 06/27/23 17:09:40.882
    Jun 27 17:09:40.899: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103057 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:09:40.899: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-2625  ca27a2f8-816e-45d9-87e0-2c8823357c61 103057 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 06/27/23 17:09:40.899
    Jun 27 17:09:40.914: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2625  fcdda00a-2e19-4685-b130-beb0ed4b1de9 103058 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:09:40.914: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2625  fcdda00a-2e19-4685-b130-beb0ed4b1de9 103058 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 06/27/23 17:09:50.914
    Jun 27 17:09:50.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2625  fcdda00a-2e19-4685-b130-beb0ed4b1de9 103231 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:09:50.932: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-2625  fcdda00a-2e19-4685-b130-beb0ed4b1de9 103231 0 2023-06-27 17:09:40 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-06-27 17:09:40 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 27 17:10:00.933: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2625" for this suite. 06/27/23 17:10:00.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:10:00.985
Jun 27 17:10:00.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 17:10:00.987
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:01.033
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:01.044
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/27/23 17:10:01.055
Jun 27 17:10:01.100: INFO: Waiting up to 5m0s for pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7" in namespace "emptydir-1668" to be "Succeeded or Failed"
Jun 27 17:10:01.112: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02579ms
Jun 27 17:10:03.126: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02654759s
Jun 27 17:10:05.126: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025862956s
Jun 27 17:10:07.124: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02391813s
STEP: Saw pod success 06/27/23 17:10:07.124
Jun 27 17:10:07.124: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7" satisfied condition "Succeeded or Failed"
Jun 27 17:10:07.137: INFO: Trying to get logs from node 10.113.180.90 pod pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7 container test-container: <nil>
STEP: delete the pod 06/27/23 17:10:07.187
Jun 27 17:10:07.224: INFO: Waiting for pod pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7 to disappear
Jun 27 17:10:07.235: INFO: Pod pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 17:10:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1668" for this suite. 06/27/23 17:10:07.255
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":158,"skipped":3409,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.289 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:10:00.985
    Jun 27 17:10:00.985: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 17:10:00.987
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:01.033
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:01.044
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/27/23 17:10:01.055
    Jun 27 17:10:01.100: INFO: Waiting up to 5m0s for pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7" in namespace "emptydir-1668" to be "Succeeded or Failed"
    Jun 27 17:10:01.112: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02579ms
    Jun 27 17:10:03.126: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02654759s
    Jun 27 17:10:05.126: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7": Phase="Pending", Reason="", readiness=false. Elapsed: 4.025862956s
    Jun 27 17:10:07.124: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02391813s
    STEP: Saw pod success 06/27/23 17:10:07.124
    Jun 27 17:10:07.124: INFO: Pod "pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7" satisfied condition "Succeeded or Failed"
    Jun 27 17:10:07.137: INFO: Trying to get logs from node 10.113.180.90 pod pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7 container test-container: <nil>
    STEP: delete the pod 06/27/23 17:10:07.187
    Jun 27 17:10:07.224: INFO: Waiting for pod pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7 to disappear
    Jun 27 17:10:07.235: INFO: Pod pod-3f6ba534-a7f1-47e5-9af7-c90cf0d6c1e7 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:10:07.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1668" for this suite. 06/27/23 17:10:07.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:10:07.279
Jun 27 17:10:07.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename job 06/27/23 17:10:07.281
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:07.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:07.325
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 06/27/23 17:10:07.342
STEP: Ensure pods equal to paralellism count is attached to the job 06/27/23 17:10:07.36
STEP: patching /status 06/27/23 17:10:11.373
STEP: updating /status 06/27/23 17:10:11.389
STEP: get /status 06/27/23 17:10:11.452
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 27 17:10:11.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-8893" for this suite. 06/27/23 17:10:11.519
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":159,"skipped":3427,"failed":0}
------------------------------
â€¢ [4.291 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:10:07.279
    Jun 27 17:10:07.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename job 06/27/23 17:10:07.281
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:07.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:07.325
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 06/27/23 17:10:07.342
    STEP: Ensure pods equal to paralellism count is attached to the job 06/27/23 17:10:07.36
    STEP: patching /status 06/27/23 17:10:11.373
    STEP: updating /status 06/27/23 17:10:11.389
    STEP: get /status 06/27/23 17:10:11.452
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 27 17:10:11.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-8893" for this suite. 06/27/23 17:10:11.519
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:10:11.574
Jun 27 17:10:11.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pod-network-test 06/27/23 17:10:11.576
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:11.619
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:11.632
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-4969 06/27/23 17:10:11.647
STEP: creating a selector 06/27/23 17:10:11.647
STEP: Creating the service pods in kubernetes 06/27/23 17:10:11.648
Jun 27 17:10:11.648: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 27 17:10:11.855: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4969" to be "running and ready"
Jun 27 17:10:11.865: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.638444ms
Jun 27 17:10:11.866: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:10:13.880: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024791419s
Jun 27 17:10:13.880: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:10:15.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.02295746s
Jun 27 17:10:15.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:10:17.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.022470995s
Jun 27 17:10:17.877: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:10:19.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023391591s
Jun 27 17:10:19.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:10:21.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023571622s
Jun 27 17:10:21.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:10:23.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022240334s
Jun 27 17:10:23.877: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:10:25.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.023590659s
Jun 27 17:10:25.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:10:27.880: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.024769766s
Jun 27 17:10:27.880: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:10:29.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.023103317s
Jun 27 17:10:29.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:10:31.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023615008s
Jun 27 17:10:31.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:10:33.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.022538603s
Jun 27 17:10:33.877: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 27 17:10:33.877: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 27 17:10:33.894: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4969" to be "running and ready"
Jun 27 17:10:33.904: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.382034ms
Jun 27 17:10:33.904: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 27 17:10:33.904: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 27 17:10:33.914: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4969" to be "running and ready"
Jun 27 17:10:33.924: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.644074ms
Jun 27 17:10:33.924: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 27 17:10:33.924: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/27/23 17:10:33.934
Jun 27 17:10:33.961: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4969" to be "running"
Jun 27 17:10:33.971: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.986357ms
Jun 27 17:10:35.988: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02649075s
Jun 27 17:10:35.988: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 27 17:10:35.998: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 27 17:10:35.998: INFO: Breadth first check of 172.30.106.156 on host 10.113.180.89...
Jun 27 17:10:36.009: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.250.250:9080/dial?request=hostname&protocol=udp&host=172.30.106.156&port=8081&tries=1'] Namespace:pod-network-test-4969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:10:36.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:10:36.010: INFO: ExecWithOptions: Clientset creation
Jun 27 17:10:36.010: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.250.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.106.156%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 27 17:10:36.358: INFO: Waiting for responses: map[]
Jun 27 17:10:36.358: INFO: reached 172.30.106.156 after 0/1 tries
Jun 27 17:10:36.358: INFO: Breadth first check of 172.30.250.253 on host 10.113.180.90...
Jun 27 17:10:36.374: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.250.250:9080/dial?request=hostname&protocol=udp&host=172.30.250.253&port=8081&tries=1'] Namespace:pod-network-test-4969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:10:36.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:10:36.376: INFO: ExecWithOptions: Clientset creation
Jun 27 17:10:36.376: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.250.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.250.253%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 27 17:10:36.595: INFO: Waiting for responses: map[]
Jun 27 17:10:36.595: INFO: reached 172.30.250.253 after 0/1 tries
Jun 27 17:10:36.595: INFO: Breadth first check of 172.30.60.101 on host 10.113.180.96...
Jun 27 17:10:36.606: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.250.250:9080/dial?request=hostname&protocol=udp&host=172.30.60.101&port=8081&tries=1'] Namespace:pod-network-test-4969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:10:36.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:10:36.608: INFO: ExecWithOptions: Clientset creation
Jun 27 17:10:36.608: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.250.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.60.101%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Jun 27 17:10:36.879: INFO: Waiting for responses: map[]
Jun 27 17:10:36.879: INFO: reached 172.30.60.101 after 0/1 tries
Jun 27 17:10:36.879: INFO: Going to retry 0 out of 3 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 27 17:10:36.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4969" for this suite. 06/27/23 17:10:36.897
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":160,"skipped":3446,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.340 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:10:11.574
    Jun 27 17:10:11.574: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pod-network-test 06/27/23 17:10:11.576
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:11.619
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:11.632
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-4969 06/27/23 17:10:11.647
    STEP: creating a selector 06/27/23 17:10:11.647
    STEP: Creating the service pods in kubernetes 06/27/23 17:10:11.648
    Jun 27 17:10:11.648: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 27 17:10:11.855: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4969" to be "running and ready"
    Jun 27 17:10:11.865: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.638444ms
    Jun 27 17:10:11.866: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:10:13.880: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024791419s
    Jun 27 17:10:13.880: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:10:15.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.02295746s
    Jun 27 17:10:15.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:10:17.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.022470995s
    Jun 27 17:10:17.877: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:10:19.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.023391591s
    Jun 27 17:10:19.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:10:21.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.023571622s
    Jun 27 17:10:21.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:10:23.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.022240334s
    Jun 27 17:10:23.877: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:10:25.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.023590659s
    Jun 27 17:10:25.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:10:27.880: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.024769766s
    Jun 27 17:10:27.880: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:10:29.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.023103317s
    Jun 27 17:10:29.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:10:31.878: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023615008s
    Jun 27 17:10:31.878: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:10:33.877: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.022538603s
    Jun 27 17:10:33.877: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 27 17:10:33.877: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 27 17:10:33.894: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4969" to be "running and ready"
    Jun 27 17:10:33.904: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.382034ms
    Jun 27 17:10:33.904: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 27 17:10:33.904: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 27 17:10:33.914: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4969" to be "running and ready"
    Jun 27 17:10:33.924: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 9.644074ms
    Jun 27 17:10:33.924: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 27 17:10:33.924: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/27/23 17:10:33.934
    Jun 27 17:10:33.961: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4969" to be "running"
    Jun 27 17:10:33.971: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.986357ms
    Jun 27 17:10:35.988: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.02649075s
    Jun 27 17:10:35.988: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 27 17:10:35.998: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 27 17:10:35.998: INFO: Breadth first check of 172.30.106.156 on host 10.113.180.89...
    Jun 27 17:10:36.009: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.250.250:9080/dial?request=hostname&protocol=udp&host=172.30.106.156&port=8081&tries=1'] Namespace:pod-network-test-4969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:10:36.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:10:36.010: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:10:36.010: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.250.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.106.156%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 27 17:10:36.358: INFO: Waiting for responses: map[]
    Jun 27 17:10:36.358: INFO: reached 172.30.106.156 after 0/1 tries
    Jun 27 17:10:36.358: INFO: Breadth first check of 172.30.250.253 on host 10.113.180.90...
    Jun 27 17:10:36.374: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.250.250:9080/dial?request=hostname&protocol=udp&host=172.30.250.253&port=8081&tries=1'] Namespace:pod-network-test-4969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:10:36.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:10:36.376: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:10:36.376: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.250.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.250.253%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 27 17:10:36.595: INFO: Waiting for responses: map[]
    Jun 27 17:10:36.595: INFO: reached 172.30.250.253 after 0/1 tries
    Jun 27 17:10:36.595: INFO: Breadth first check of 172.30.60.101 on host 10.113.180.96...
    Jun 27 17:10:36.606: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.30.250.250:9080/dial?request=hostname&protocol=udp&host=172.30.60.101&port=8081&tries=1'] Namespace:pod-network-test-4969 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:10:36.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:10:36.608: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:10:36.608: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4969/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.30.250.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.30.60.101%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Jun 27 17:10:36.879: INFO: Waiting for responses: map[]
    Jun 27 17:10:36.879: INFO: reached 172.30.60.101 after 0/1 tries
    Jun 27 17:10:36.879: INFO: Going to retry 0 out of 3 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 27 17:10:36.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4969" for this suite. 06/27/23 17:10:36.897
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:10:36.923
Jun 27 17:10:36.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename proxy 06/27/23 17:10:36.925
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:36.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:36.987
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 06/27/23 17:10:37.034
STEP: creating replication controller proxy-service-tglvz in namespace proxy-6481 06/27/23 17:10:37.034
I0627 17:10:37.058644      23 runners.go:193] Created replication controller with name: proxy-service-tglvz, namespace: proxy-6481, replica count: 1
I0627 17:10:38.110267      23 runners.go:193] proxy-service-tglvz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
I0627 17:10:39.111412      23 runners.go:193] proxy-service-tglvz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0627 17:10:40.112659      23 runners.go:193] proxy-service-tglvz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 17:10:40.122: INFO: setup took 3.125972911s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/27/23 17:10:40.123
Jun 27 17:10:40.162: INFO: (0) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 38.322717ms)
Jun 27 17:10:40.162: INFO: (0) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 39.159357ms)
Jun 27 17:10:40.163: INFO: (0) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 39.720707ms)
Jun 27 17:10:40.163: INFO: (0) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 39.911687ms)
Jun 27 17:10:40.164: INFO: (0) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 39.425065ms)
Jun 27 17:10:40.165: INFO: (0) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 40.483532ms)
Jun 27 17:10:40.165: INFO: (0) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 40.900537ms)
Jun 27 17:10:40.170: INFO: (0) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 46.100691ms)
Jun 27 17:10:40.170: INFO: (0) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 46.030626ms)
Jun 27 17:10:40.171: INFO: (0) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 46.550291ms)
Jun 27 17:10:40.171: INFO: (0) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 46.427853ms)
Jun 27 17:10:40.178: INFO: (0) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 53.098521ms)
Jun 27 17:10:40.179: INFO: (0) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 54.705193ms)
Jun 27 17:10:40.179: INFO: (0) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 54.70616ms)
Jun 27 17:10:40.179: INFO: (0) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 54.844814ms)
Jun 27 17:10:40.179: INFO: (0) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 55.248626ms)
Jun 27 17:10:40.207: INFO: (1) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 27.727682ms)
Jun 27 17:10:40.207: INFO: (1) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 25.78847ms)
Jun 27 17:10:40.208: INFO: (1) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 26.099969ms)
Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 26.146498ms)
Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 28.102236ms)
Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 27.793756ms)
Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 26.759402ms)
Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 26.500273ms)
Jun 27 17:10:40.210: INFO: (1) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 28.138568ms)
Jun 27 17:10:40.212: INFO: (1) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 30.671959ms)
Jun 27 17:10:40.213: INFO: (1) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 32.128932ms)
Jun 27 17:10:40.213: INFO: (1) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.935654ms)
Jun 27 17:10:40.215: INFO: (1) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 35.228669ms)
Jun 27 17:10:40.216: INFO: (1) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 33.628679ms)
Jun 27 17:10:40.216: INFO: (1) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 33.870141ms)
Jun 27 17:10:40.216: INFO: (1) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 34.091978ms)
Jun 27 17:10:40.234: INFO: (2) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 17.90984ms)
Jun 27 17:10:40.237: INFO: (2) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 19.338022ms)
Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 21.070304ms)
Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 21.508792ms)
Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.528201ms)
Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 21.503186ms)
Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 22.47537ms)
Jun 27 17:10:40.240: INFO: (2) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 22.185171ms)
Jun 27 17:10:40.240: INFO: (2) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 22.76217ms)
Jun 27 17:10:40.240: INFO: (2) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 23.354229ms)
Jun 27 17:10:40.240: INFO: (2) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 23.791668ms)
Jun 27 17:10:40.241: INFO: (2) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 24.171853ms)
Jun 27 17:10:40.245: INFO: (2) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 28.035387ms)
Jun 27 17:10:40.246: INFO: (2) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 28.358473ms)
Jun 27 17:10:40.247: INFO: (2) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 29.717079ms)
Jun 27 17:10:40.247: INFO: (2) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 30.715621ms)
Jun 27 17:10:40.267: INFO: (3) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 19.70078ms)
Jun 27 17:10:40.269: INFO: (3) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 21.275527ms)
Jun 27 17:10:40.269: INFO: (3) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 20.53582ms)
Jun 27 17:10:40.277: INFO: (3) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 27.385264ms)
Jun 27 17:10:40.277: INFO: (3) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 26.922436ms)
Jun 27 17:10:40.277: INFO: (3) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 28.791506ms)
Jun 27 17:10:40.277: INFO: (3) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 26.106857ms)
Jun 27 17:10:40.278: INFO: (3) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 27.514578ms)
Jun 27 17:10:40.278: INFO: (3) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 27.138373ms)
Jun 27 17:10:40.278: INFO: (3) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 27.875253ms)
Jun 27 17:10:40.278: INFO: (3) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.69982ms)
Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 34.346322ms)
Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 33.469132ms)
Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 32.449071ms)
Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 33.794772ms)
Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 32.703265ms)
Jun 27 17:10:40.305: INFO: (4) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 20.896997ms)
Jun 27 17:10:40.305: INFO: (4) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 21.163573ms)
Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 21.677909ms)
Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 21.166716ms)
Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 21.466775ms)
Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.007201ms)
Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 22.475899ms)
Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 21.726486ms)
Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 21.545963ms)
Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 21.049808ms)
Jun 27 17:10:40.313: INFO: (4) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 28.629508ms)
Jun 27 17:10:40.314: INFO: (4) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 29.118377ms)
Jun 27 17:10:40.314: INFO: (4) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.021919ms)
Jun 27 17:10:40.318: INFO: (4) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 33.740495ms)
Jun 27 17:10:40.318: INFO: (4) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 34.430927ms)
Jun 27 17:10:40.319: INFO: (4) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 34.661331ms)
Jun 27 17:10:40.340: INFO: (5) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 21.38111ms)
Jun 27 17:10:40.341: INFO: (5) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 21.938621ms)
Jun 27 17:10:40.341: INFO: (5) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 22.244769ms)
Jun 27 17:10:40.341: INFO: (5) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 22.497799ms)
Jun 27 17:10:40.347: INFO: (5) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 28.372235ms)
Jun 27 17:10:40.348: INFO: (5) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 28.975239ms)
Jun 27 17:10:40.353: INFO: (5) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 34.387893ms)
Jun 27 17:10:40.354: INFO: (5) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 34.901943ms)
Jun 27 17:10:40.354: INFO: (5) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 35.132153ms)
Jun 27 17:10:40.354: INFO: (5) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 35.439658ms)
Jun 27 17:10:40.355: INFO: (5) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 35.499863ms)
Jun 27 17:10:40.357: INFO: (5) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 37.605753ms)
Jun 27 17:10:40.358: INFO: (5) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 38.633986ms)
Jun 27 17:10:40.358: INFO: (5) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 38.438269ms)
Jun 27 17:10:40.363: INFO: (5) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 43.480579ms)
Jun 27 17:10:40.363: INFO: (5) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 43.761554ms)
Jun 27 17:10:40.381: INFO: (6) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 17.575285ms)
Jun 27 17:10:40.392: INFO: (6) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 27.523729ms)
Jun 27 17:10:40.392: INFO: (6) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 27.762771ms)
Jun 27 17:10:40.392: INFO: (6) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 27.920242ms)
Jun 27 17:10:40.397: INFO: (6) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 33.588208ms)
Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 34.237966ms)
Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 33.79629ms)
Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 33.099529ms)
Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 33.242417ms)
Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 33.873183ms)
Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 33.524488ms)
Jun 27 17:10:40.397: INFO: (6) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 33.075125ms)
Jun 27 17:10:40.406: INFO: (6) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 42.253522ms)
Jun 27 17:10:40.406: INFO: (6) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 42.769657ms)
Jun 27 17:10:40.407: INFO: (6) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 43.136642ms)
Jun 27 17:10:40.407: INFO: (6) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 42.644054ms)
Jun 27 17:10:40.426: INFO: (7) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 18.142056ms)
Jun 27 17:10:40.426: INFO: (7) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 18.467925ms)
Jun 27 17:10:40.426: INFO: (7) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 18.430436ms)
Jun 27 17:10:40.430: INFO: (7) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 22.280725ms)
Jun 27 17:10:40.432: INFO: (7) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 23.481391ms)
Jun 27 17:10:40.432: INFO: (7) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 23.672222ms)
Jun 27 17:10:40.432: INFO: (7) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 23.831046ms)
Jun 27 17:10:40.433: INFO: (7) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 25.323399ms)
Jun 27 17:10:40.433: INFO: (7) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 25.208256ms)
Jun 27 17:10:40.433: INFO: (7) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 25.702951ms)
Jun 27 17:10:40.434: INFO: (7) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 26.539244ms)
Jun 27 17:10:40.435: INFO: (7) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 27.074596ms)
Jun 27 17:10:40.437: INFO: (7) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 29.27976ms)
Jun 27 17:10:40.437: INFO: (7) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 29.690844ms)
Jun 27 17:10:40.437: INFO: (7) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 29.612667ms)
Jun 27 17:10:40.438: INFO: (7) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 30.096509ms)
Jun 27 17:10:40.461: INFO: (8) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 23.117062ms)
Jun 27 17:10:40.463: INFO: (8) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 23.299739ms)
Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 24.95491ms)
Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 25.010858ms)
Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 23.939092ms)
Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 25.789682ms)
Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 25.37715ms)
Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 24.974815ms)
Jun 27 17:10:40.465: INFO: (8) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 25.368408ms)
Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 24.892134ms)
Jun 27 17:10:40.468: INFO: (8) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 28.476863ms)
Jun 27 17:10:40.468: INFO: (8) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 28.446348ms)
Jun 27 17:10:40.471: INFO: (8) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 32.492087ms)
Jun 27 17:10:40.472: INFO: (8) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 31.851289ms)
Jun 27 17:10:40.472: INFO: (8) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 33.048746ms)
Jun 27 17:10:40.473: INFO: (8) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 32.526588ms)
Jun 27 17:10:40.501: INFO: (9) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 26.617962ms)
Jun 27 17:10:40.501: INFO: (9) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 26.574472ms)
Jun 27 17:10:40.501: INFO: (9) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 26.33955ms)
Jun 27 17:10:40.501: INFO: (9) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 27.190481ms)
Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 29.239849ms)
Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 29.303944ms)
Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 29.696664ms)
Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 29.99893ms)
Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 28.946893ms)
Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 30.403109ms)
Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.921689ms)
Jun 27 17:10:40.505: INFO: (9) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 29.951627ms)
Jun 27 17:10:40.507: INFO: (9) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 33.345546ms)
Jun 27 17:10:40.510: INFO: (9) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 35.565215ms)
Jun 27 17:10:40.510: INFO: (9) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 37.48067ms)
Jun 27 17:10:40.511: INFO: (9) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 35.961529ms)
Jun 27 17:10:40.527: INFO: (10) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 15.641609ms)
Jun 27 17:10:40.535: INFO: (10) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 24.185089ms)
Jun 27 17:10:40.535: INFO: (10) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 23.91263ms)
Jun 27 17:10:40.535: INFO: (10) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 23.890365ms)
Jun 27 17:10:40.536: INFO: (10) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 24.516644ms)
Jun 27 17:10:40.537: INFO: (10) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 25.992601ms)
Jun 27 17:10:40.538: INFO: (10) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 26.35016ms)
Jun 27 17:10:40.538: INFO: (10) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 26.870651ms)
Jun 27 17:10:40.538: INFO: (10) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 26.838415ms)
Jun 27 17:10:40.538: INFO: (10) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 26.789443ms)
Jun 27 17:10:40.540: INFO: (10) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 28.27136ms)
Jun 27 17:10:40.542: INFO: (10) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.318116ms)
Jun 27 17:10:40.543: INFO: (10) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 31.501454ms)
Jun 27 17:10:40.543: INFO: (10) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 31.778494ms)
Jun 27 17:10:40.543: INFO: (10) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 31.560296ms)
Jun 27 17:10:40.543: INFO: (10) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 32.02897ms)
Jun 27 17:10:40.559: INFO: (11) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 15.960056ms)
Jun 27 17:10:40.565: INFO: (11) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 21.121144ms)
Jun 27 17:10:40.565: INFO: (11) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 20.832675ms)
Jun 27 17:10:40.565: INFO: (11) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 20.944246ms)
Jun 27 17:10:40.566: INFO: (11) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 21.456423ms)
Jun 27 17:10:40.567: INFO: (11) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 22.840265ms)
Jun 27 17:10:40.567: INFO: (11) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 22.243825ms)
Jun 27 17:10:40.567: INFO: (11) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 23.420333ms)
Jun 27 17:10:40.567: INFO: (11) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.974048ms)
Jun 27 17:10:40.574: INFO: (11) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 29.943003ms)
Jun 27 17:10:40.574: INFO: (11) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 30.452838ms)
Jun 27 17:10:40.575: INFO: (11) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 30.827929ms)
Jun 27 17:10:40.576: INFO: (11) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 31.287531ms)
Jun 27 17:10:40.583: INFO: (11) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 39.140775ms)
Jun 27 17:10:40.583: INFO: (11) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 38.103556ms)
Jun 27 17:10:40.583: INFO: (11) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 39.182768ms)
Jun 27 17:10:40.610: INFO: (12) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 26.970162ms)
Jun 27 17:10:40.611: INFO: (12) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 27.80523ms)
Jun 27 17:10:40.640: INFO: (12) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 56.002287ms)
Jun 27 17:10:40.640: INFO: (12) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 57.374757ms)
Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 57.08751ms)
Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 57.157952ms)
Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 57.329892ms)
Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 57.332196ms)
Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 58.490566ms)
Jun 27 17:10:40.642: INFO: (12) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 57.610707ms)
Jun 27 17:10:40.642: INFO: (12) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 58.662719ms)
Jun 27 17:10:40.642: INFO: (12) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 58.26377ms)
Jun 27 17:10:40.642: INFO: (12) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 59.021881ms)
Jun 27 17:10:40.655: INFO: (12) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 71.678125ms)
Jun 27 17:10:40.655: INFO: (12) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 72.064227ms)
Jun 27 17:10:40.656: INFO: (12) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 72.05856ms)
Jun 27 17:10:40.700: INFO: (13) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 40.889925ms)
Jun 27 17:10:40.700: INFO: (13) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 41.93049ms)
Jun 27 17:10:40.700: INFO: (13) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 42.017259ms)
Jun 27 17:10:40.701: INFO: (13) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 41.598496ms)
Jun 27 17:10:40.701: INFO: (13) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 42.329435ms)
Jun 27 17:10:40.702: INFO: (13) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 43.648259ms)
Jun 27 17:10:40.702: INFO: (13) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 42.243211ms)
Jun 27 17:10:40.702: INFO: (13) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 42.521683ms)
Jun 27 17:10:40.703: INFO: (13) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 43.029502ms)
Jun 27 17:10:40.703: INFO: (13) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 43.601251ms)
Jun 27 17:10:40.709: INFO: (13) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 51.792336ms)
Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 53.566388ms)
Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 51.866918ms)
Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 50.799412ms)
Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 50.650318ms)
Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 52.080145ms)
Jun 27 17:10:40.731: INFO: (14) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 19.067139ms)
Jun 27 17:10:40.733: INFO: (14) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 20.941505ms)
Jun 27 17:10:40.733: INFO: (14) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 20.885583ms)
Jun 27 17:10:40.733: INFO: (14) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 20.798216ms)
Jun 27 17:10:40.735: INFO: (14) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 23.00053ms)
Jun 27 17:10:40.735: INFO: (14) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 23.706774ms)
Jun 27 17:10:40.735: INFO: (14) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 24.367463ms)
Jun 27 17:10:40.736: INFO: (14) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 24.886321ms)
Jun 27 17:10:40.736: INFO: (14) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 24.247158ms)
Jun 27 17:10:40.736: INFO: (14) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 24.120831ms)
Jun 27 17:10:40.737: INFO: (14) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 25.948346ms)
Jun 27 17:10:40.739: INFO: (14) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 27.703416ms)
Jun 27 17:10:40.740: INFO: (14) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 29.962542ms)
Jun 27 17:10:40.741: INFO: (14) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 28.920039ms)
Jun 27 17:10:40.741: INFO: (14) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 28.971456ms)
Jun 27 17:10:40.741: INFO: (14) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 30.101286ms)
Jun 27 17:10:40.776: INFO: (15) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 33.78207ms)
Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 56.047304ms)
Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 57.416333ms)
Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 57.311916ms)
Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 59.935189ms)
Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 57.445547ms)
Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 59.403913ms)
Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 58.144203ms)
Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 59.220136ms)
Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 56.882949ms)
Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 58.568219ms)
Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 59.95094ms)
Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 59.521232ms)
Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 57.560714ms)
Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 57.414482ms)
Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 59.043442ms)
Jun 27 17:10:40.834: INFO: (16) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 26.368749ms)
Jun 27 17:10:40.835: INFO: (16) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 27.089713ms)
Jun 27 17:10:40.847: INFO: (16) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 38.408282ms)
Jun 27 17:10:40.848: INFO: (16) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 39.435164ms)
Jun 27 17:10:40.848: INFO: (16) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 40.119708ms)
Jun 27 17:10:40.848: INFO: (16) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 40.260211ms)
Jun 27 17:10:40.848: INFO: (16) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 39.705672ms)
Jun 27 17:10:40.849: INFO: (16) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 39.898992ms)
Jun 27 17:10:40.849: INFO: (16) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 39.854204ms)
Jun 27 17:10:40.849: INFO: (16) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 40.15705ms)
Jun 27 17:10:40.854: INFO: (16) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 45.404061ms)
Jun 27 17:10:40.854: INFO: (16) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 45.874737ms)
Jun 27 17:10:40.854: INFO: (16) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 45.85526ms)
Jun 27 17:10:40.858: INFO: (16) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 49.009115ms)
Jun 27 17:10:40.858: INFO: (16) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 49.273382ms)
Jun 27 17:10:40.858: INFO: (16) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 50.088975ms)
Jun 27 17:10:40.881: INFO: (17) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 22.289605ms)
Jun 27 17:10:40.884: INFO: (17) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 24.155125ms)
Jun 27 17:10:40.884: INFO: (17) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 24.370919ms)
Jun 27 17:10:40.884: INFO: (17) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 25.649463ms)
Jun 27 17:10:40.885: INFO: (17) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 25.430405ms)
Jun 27 17:10:40.885: INFO: (17) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 25.407463ms)
Jun 27 17:10:40.885: INFO: (17) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 25.874947ms)
Jun 27 17:10:40.885: INFO: (17) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 25.818351ms)
Jun 27 17:10:40.886: INFO: (17) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 26.810033ms)
Jun 27 17:10:40.886: INFO: (17) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 26.702728ms)
Jun 27 17:10:40.891: INFO: (17) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 31.831824ms)
Jun 27 17:10:40.891: INFO: (17) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 32.465387ms)
Jun 27 17:10:40.891: INFO: (17) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 31.717532ms)
Jun 27 17:10:40.891: INFO: (17) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 31.298886ms)
Jun 27 17:10:40.893: INFO: (17) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 33.636681ms)
Jun 27 17:10:40.893: INFO: (17) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 33.285304ms)
Jun 27 17:10:40.912: INFO: (18) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 18.799495ms)
Jun 27 17:10:40.915: INFO: (18) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 20.178484ms)
Jun 27 17:10:40.916: INFO: (18) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 21.390187ms)
Jun 27 17:10:40.916: INFO: (18) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 22.216496ms)
Jun 27 17:10:40.916: INFO: (18) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.047288ms)
Jun 27 17:10:40.916: INFO: (18) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 21.732053ms)
Jun 27 17:10:40.917: INFO: (18) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 22.022966ms)
Jun 27 17:10:40.917: INFO: (18) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 22.210006ms)
Jun 27 17:10:40.917: INFO: (18) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 22.344444ms)
Jun 27 17:10:40.918: INFO: (18) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 23.664169ms)
Jun 27 17:10:40.922: INFO: (18) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 26.753565ms)
Jun 27 17:10:40.922: INFO: (18) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 26.940363ms)
Jun 27 17:10:40.923: INFO: (18) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 29.948926ms)
Jun 27 17:10:40.924: INFO: (18) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 30.392716ms)
Jun 27 17:10:40.924: INFO: (18) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 28.909443ms)
Jun 27 17:10:40.924: INFO: (18) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 28.790743ms)
Jun 27 17:10:40.944: INFO: (19) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 20.148186ms)
Jun 27 17:10:40.948: INFO: (19) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.942695ms)
Jun 27 17:10:40.949: INFO: (19) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 23.485844ms)
Jun 27 17:10:40.949: INFO: (19) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 23.378574ms)
Jun 27 17:10:40.955: INFO: (19) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 22.610851ms)
Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 37.788606ms)
Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 37.424606ms)
Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 37.388408ms)
Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 38.385155ms)
Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 38.443228ms)
Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 38.175339ms)
Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 37.861903ms)
Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 39.287673ms)
Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 38.226751ms)
Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 38.010813ms)
Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 39.173032ms)
STEP: deleting ReplicationController proxy-service-tglvz in namespace proxy-6481, will wait for the garbage collector to delete the pods 06/27/23 17:10:40.964
Jun 27 17:10:41.055: INFO: Deleting ReplicationController proxy-service-tglvz took: 24.916968ms
Jun 27 17:10:41.157: INFO: Terminating ReplicationController proxy-service-tglvz pods took: 101.393195ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jun 27 17:10:43.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-6481" for this suite. 06/27/23 17:10:43.877
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":161,"skipped":3458,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.973 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:10:36.923
    Jun 27 17:10:36.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename proxy 06/27/23 17:10:36.925
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:36.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:36.987
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 06/27/23 17:10:37.034
    STEP: creating replication controller proxy-service-tglvz in namespace proxy-6481 06/27/23 17:10:37.034
    I0627 17:10:37.058644      23 runners.go:193] Created replication controller with name: proxy-service-tglvz, namespace: proxy-6481, replica count: 1
    I0627 17:10:38.110267      23 runners.go:193] proxy-service-tglvz Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    I0627 17:10:39.111412      23 runners.go:193] proxy-service-tglvz Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0627 17:10:40.112659      23 runners.go:193] proxy-service-tglvz Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 27 17:10:40.122: INFO: setup took 3.125972911s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 06/27/23 17:10:40.123
    Jun 27 17:10:40.162: INFO: (0) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 38.322717ms)
    Jun 27 17:10:40.162: INFO: (0) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 39.159357ms)
    Jun 27 17:10:40.163: INFO: (0) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 39.720707ms)
    Jun 27 17:10:40.163: INFO: (0) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 39.911687ms)
    Jun 27 17:10:40.164: INFO: (0) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 39.425065ms)
    Jun 27 17:10:40.165: INFO: (0) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 40.483532ms)
    Jun 27 17:10:40.165: INFO: (0) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 40.900537ms)
    Jun 27 17:10:40.170: INFO: (0) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 46.100691ms)
    Jun 27 17:10:40.170: INFO: (0) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 46.030626ms)
    Jun 27 17:10:40.171: INFO: (0) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 46.550291ms)
    Jun 27 17:10:40.171: INFO: (0) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 46.427853ms)
    Jun 27 17:10:40.178: INFO: (0) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 53.098521ms)
    Jun 27 17:10:40.179: INFO: (0) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 54.705193ms)
    Jun 27 17:10:40.179: INFO: (0) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 54.70616ms)
    Jun 27 17:10:40.179: INFO: (0) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 54.844814ms)
    Jun 27 17:10:40.179: INFO: (0) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 55.248626ms)
    Jun 27 17:10:40.207: INFO: (1) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 27.727682ms)
    Jun 27 17:10:40.207: INFO: (1) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 25.78847ms)
    Jun 27 17:10:40.208: INFO: (1) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 26.099969ms)
    Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 26.146498ms)
    Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 28.102236ms)
    Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 27.793756ms)
    Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 26.759402ms)
    Jun 27 17:10:40.209: INFO: (1) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 26.500273ms)
    Jun 27 17:10:40.210: INFO: (1) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 28.138568ms)
    Jun 27 17:10:40.212: INFO: (1) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 30.671959ms)
    Jun 27 17:10:40.213: INFO: (1) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 32.128932ms)
    Jun 27 17:10:40.213: INFO: (1) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.935654ms)
    Jun 27 17:10:40.215: INFO: (1) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 35.228669ms)
    Jun 27 17:10:40.216: INFO: (1) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 33.628679ms)
    Jun 27 17:10:40.216: INFO: (1) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 33.870141ms)
    Jun 27 17:10:40.216: INFO: (1) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 34.091978ms)
    Jun 27 17:10:40.234: INFO: (2) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 17.90984ms)
    Jun 27 17:10:40.237: INFO: (2) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 19.338022ms)
    Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 21.070304ms)
    Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 21.508792ms)
    Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.528201ms)
    Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 21.503186ms)
    Jun 27 17:10:40.239: INFO: (2) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 22.47537ms)
    Jun 27 17:10:40.240: INFO: (2) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 22.185171ms)
    Jun 27 17:10:40.240: INFO: (2) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 22.76217ms)
    Jun 27 17:10:40.240: INFO: (2) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 23.354229ms)
    Jun 27 17:10:40.240: INFO: (2) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 23.791668ms)
    Jun 27 17:10:40.241: INFO: (2) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 24.171853ms)
    Jun 27 17:10:40.245: INFO: (2) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 28.035387ms)
    Jun 27 17:10:40.246: INFO: (2) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 28.358473ms)
    Jun 27 17:10:40.247: INFO: (2) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 29.717079ms)
    Jun 27 17:10:40.247: INFO: (2) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 30.715621ms)
    Jun 27 17:10:40.267: INFO: (3) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 19.70078ms)
    Jun 27 17:10:40.269: INFO: (3) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 21.275527ms)
    Jun 27 17:10:40.269: INFO: (3) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 20.53582ms)
    Jun 27 17:10:40.277: INFO: (3) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 27.385264ms)
    Jun 27 17:10:40.277: INFO: (3) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 26.922436ms)
    Jun 27 17:10:40.277: INFO: (3) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 28.791506ms)
    Jun 27 17:10:40.277: INFO: (3) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 26.106857ms)
    Jun 27 17:10:40.278: INFO: (3) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 27.514578ms)
    Jun 27 17:10:40.278: INFO: (3) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 27.138373ms)
    Jun 27 17:10:40.278: INFO: (3) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 27.875253ms)
    Jun 27 17:10:40.278: INFO: (3) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.69982ms)
    Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 34.346322ms)
    Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 33.469132ms)
    Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 32.449071ms)
    Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 33.794772ms)
    Jun 27 17:10:40.283: INFO: (3) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 32.703265ms)
    Jun 27 17:10:40.305: INFO: (4) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 20.896997ms)
    Jun 27 17:10:40.305: INFO: (4) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 21.163573ms)
    Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 21.677909ms)
    Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 21.166716ms)
    Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 21.466775ms)
    Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.007201ms)
    Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 22.475899ms)
    Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 21.726486ms)
    Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 21.545963ms)
    Jun 27 17:10:40.306: INFO: (4) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 21.049808ms)
    Jun 27 17:10:40.313: INFO: (4) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 28.629508ms)
    Jun 27 17:10:40.314: INFO: (4) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 29.118377ms)
    Jun 27 17:10:40.314: INFO: (4) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.021919ms)
    Jun 27 17:10:40.318: INFO: (4) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 33.740495ms)
    Jun 27 17:10:40.318: INFO: (4) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 34.430927ms)
    Jun 27 17:10:40.319: INFO: (4) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 34.661331ms)
    Jun 27 17:10:40.340: INFO: (5) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 21.38111ms)
    Jun 27 17:10:40.341: INFO: (5) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 21.938621ms)
    Jun 27 17:10:40.341: INFO: (5) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 22.244769ms)
    Jun 27 17:10:40.341: INFO: (5) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 22.497799ms)
    Jun 27 17:10:40.347: INFO: (5) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 28.372235ms)
    Jun 27 17:10:40.348: INFO: (5) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 28.975239ms)
    Jun 27 17:10:40.353: INFO: (5) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 34.387893ms)
    Jun 27 17:10:40.354: INFO: (5) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 34.901943ms)
    Jun 27 17:10:40.354: INFO: (5) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 35.132153ms)
    Jun 27 17:10:40.354: INFO: (5) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 35.439658ms)
    Jun 27 17:10:40.355: INFO: (5) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 35.499863ms)
    Jun 27 17:10:40.357: INFO: (5) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 37.605753ms)
    Jun 27 17:10:40.358: INFO: (5) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 38.633986ms)
    Jun 27 17:10:40.358: INFO: (5) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 38.438269ms)
    Jun 27 17:10:40.363: INFO: (5) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 43.480579ms)
    Jun 27 17:10:40.363: INFO: (5) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 43.761554ms)
    Jun 27 17:10:40.381: INFO: (6) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 17.575285ms)
    Jun 27 17:10:40.392: INFO: (6) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 27.523729ms)
    Jun 27 17:10:40.392: INFO: (6) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 27.762771ms)
    Jun 27 17:10:40.392: INFO: (6) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 27.920242ms)
    Jun 27 17:10:40.397: INFO: (6) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 33.588208ms)
    Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 34.237966ms)
    Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 33.79629ms)
    Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 33.099529ms)
    Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 33.242417ms)
    Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 33.873183ms)
    Jun 27 17:10:40.398: INFO: (6) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 33.524488ms)
    Jun 27 17:10:40.397: INFO: (6) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 33.075125ms)
    Jun 27 17:10:40.406: INFO: (6) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 42.253522ms)
    Jun 27 17:10:40.406: INFO: (6) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 42.769657ms)
    Jun 27 17:10:40.407: INFO: (6) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 43.136642ms)
    Jun 27 17:10:40.407: INFO: (6) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 42.644054ms)
    Jun 27 17:10:40.426: INFO: (7) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 18.142056ms)
    Jun 27 17:10:40.426: INFO: (7) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 18.467925ms)
    Jun 27 17:10:40.426: INFO: (7) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 18.430436ms)
    Jun 27 17:10:40.430: INFO: (7) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 22.280725ms)
    Jun 27 17:10:40.432: INFO: (7) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 23.481391ms)
    Jun 27 17:10:40.432: INFO: (7) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 23.672222ms)
    Jun 27 17:10:40.432: INFO: (7) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 23.831046ms)
    Jun 27 17:10:40.433: INFO: (7) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 25.323399ms)
    Jun 27 17:10:40.433: INFO: (7) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 25.208256ms)
    Jun 27 17:10:40.433: INFO: (7) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 25.702951ms)
    Jun 27 17:10:40.434: INFO: (7) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 26.539244ms)
    Jun 27 17:10:40.435: INFO: (7) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 27.074596ms)
    Jun 27 17:10:40.437: INFO: (7) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 29.27976ms)
    Jun 27 17:10:40.437: INFO: (7) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 29.690844ms)
    Jun 27 17:10:40.437: INFO: (7) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 29.612667ms)
    Jun 27 17:10:40.438: INFO: (7) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 30.096509ms)
    Jun 27 17:10:40.461: INFO: (8) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 23.117062ms)
    Jun 27 17:10:40.463: INFO: (8) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 23.299739ms)
    Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 24.95491ms)
    Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 25.010858ms)
    Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 23.939092ms)
    Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 25.789682ms)
    Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 25.37715ms)
    Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 24.974815ms)
    Jun 27 17:10:40.465: INFO: (8) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 25.368408ms)
    Jun 27 17:10:40.464: INFO: (8) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 24.892134ms)
    Jun 27 17:10:40.468: INFO: (8) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 28.476863ms)
    Jun 27 17:10:40.468: INFO: (8) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 28.446348ms)
    Jun 27 17:10:40.471: INFO: (8) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 32.492087ms)
    Jun 27 17:10:40.472: INFO: (8) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 31.851289ms)
    Jun 27 17:10:40.472: INFO: (8) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 33.048746ms)
    Jun 27 17:10:40.473: INFO: (8) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 32.526588ms)
    Jun 27 17:10:40.501: INFO: (9) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 26.617962ms)
    Jun 27 17:10:40.501: INFO: (9) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 26.574472ms)
    Jun 27 17:10:40.501: INFO: (9) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 26.33955ms)
    Jun 27 17:10:40.501: INFO: (9) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 27.190481ms)
    Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 29.239849ms)
    Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 29.303944ms)
    Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 29.696664ms)
    Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 29.99893ms)
    Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 28.946893ms)
    Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 30.403109ms)
    Jun 27 17:10:40.504: INFO: (9) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.921689ms)
    Jun 27 17:10:40.505: INFO: (9) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 29.951627ms)
    Jun 27 17:10:40.507: INFO: (9) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 33.345546ms)
    Jun 27 17:10:40.510: INFO: (9) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 35.565215ms)
    Jun 27 17:10:40.510: INFO: (9) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 37.48067ms)
    Jun 27 17:10:40.511: INFO: (9) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 35.961529ms)
    Jun 27 17:10:40.527: INFO: (10) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 15.641609ms)
    Jun 27 17:10:40.535: INFO: (10) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 24.185089ms)
    Jun 27 17:10:40.535: INFO: (10) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 23.91263ms)
    Jun 27 17:10:40.535: INFO: (10) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 23.890365ms)
    Jun 27 17:10:40.536: INFO: (10) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 24.516644ms)
    Jun 27 17:10:40.537: INFO: (10) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 25.992601ms)
    Jun 27 17:10:40.538: INFO: (10) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 26.35016ms)
    Jun 27 17:10:40.538: INFO: (10) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 26.870651ms)
    Jun 27 17:10:40.538: INFO: (10) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 26.838415ms)
    Jun 27 17:10:40.538: INFO: (10) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 26.789443ms)
    Jun 27 17:10:40.540: INFO: (10) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 28.27136ms)
    Jun 27 17:10:40.542: INFO: (10) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 30.318116ms)
    Jun 27 17:10:40.543: INFO: (10) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 31.501454ms)
    Jun 27 17:10:40.543: INFO: (10) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 31.778494ms)
    Jun 27 17:10:40.543: INFO: (10) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 31.560296ms)
    Jun 27 17:10:40.543: INFO: (10) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 32.02897ms)
    Jun 27 17:10:40.559: INFO: (11) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 15.960056ms)
    Jun 27 17:10:40.565: INFO: (11) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 21.121144ms)
    Jun 27 17:10:40.565: INFO: (11) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 20.832675ms)
    Jun 27 17:10:40.565: INFO: (11) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 20.944246ms)
    Jun 27 17:10:40.566: INFO: (11) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 21.456423ms)
    Jun 27 17:10:40.567: INFO: (11) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 22.840265ms)
    Jun 27 17:10:40.567: INFO: (11) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 22.243825ms)
    Jun 27 17:10:40.567: INFO: (11) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 23.420333ms)
    Jun 27 17:10:40.567: INFO: (11) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.974048ms)
    Jun 27 17:10:40.574: INFO: (11) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 29.943003ms)
    Jun 27 17:10:40.574: INFO: (11) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 30.452838ms)
    Jun 27 17:10:40.575: INFO: (11) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 30.827929ms)
    Jun 27 17:10:40.576: INFO: (11) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 31.287531ms)
    Jun 27 17:10:40.583: INFO: (11) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 39.140775ms)
    Jun 27 17:10:40.583: INFO: (11) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 38.103556ms)
    Jun 27 17:10:40.583: INFO: (11) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 39.182768ms)
    Jun 27 17:10:40.610: INFO: (12) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 26.970162ms)
    Jun 27 17:10:40.611: INFO: (12) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 27.80523ms)
    Jun 27 17:10:40.640: INFO: (12) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 56.002287ms)
    Jun 27 17:10:40.640: INFO: (12) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 57.374757ms)
    Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 57.08751ms)
    Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 57.157952ms)
    Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 57.329892ms)
    Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 57.332196ms)
    Jun 27 17:10:40.641: INFO: (12) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 58.490566ms)
    Jun 27 17:10:40.642: INFO: (12) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 57.610707ms)
    Jun 27 17:10:40.642: INFO: (12) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 58.662719ms)
    Jun 27 17:10:40.642: INFO: (12) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 58.26377ms)
    Jun 27 17:10:40.642: INFO: (12) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 59.021881ms)
    Jun 27 17:10:40.655: INFO: (12) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 71.678125ms)
    Jun 27 17:10:40.655: INFO: (12) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 72.064227ms)
    Jun 27 17:10:40.656: INFO: (12) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 72.05856ms)
    Jun 27 17:10:40.700: INFO: (13) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 40.889925ms)
    Jun 27 17:10:40.700: INFO: (13) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 41.93049ms)
    Jun 27 17:10:40.700: INFO: (13) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 42.017259ms)
    Jun 27 17:10:40.701: INFO: (13) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 41.598496ms)
    Jun 27 17:10:40.701: INFO: (13) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 42.329435ms)
    Jun 27 17:10:40.702: INFO: (13) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 43.648259ms)
    Jun 27 17:10:40.702: INFO: (13) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 42.243211ms)
    Jun 27 17:10:40.702: INFO: (13) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 42.521683ms)
    Jun 27 17:10:40.703: INFO: (13) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 43.029502ms)
    Jun 27 17:10:40.703: INFO: (13) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 43.601251ms)
    Jun 27 17:10:40.709: INFO: (13) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 51.792336ms)
    Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 53.566388ms)
    Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 51.866918ms)
    Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 50.799412ms)
    Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 50.650318ms)
    Jun 27 17:10:40.710: INFO: (13) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 52.080145ms)
    Jun 27 17:10:40.731: INFO: (14) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 19.067139ms)
    Jun 27 17:10:40.733: INFO: (14) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 20.941505ms)
    Jun 27 17:10:40.733: INFO: (14) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 20.885583ms)
    Jun 27 17:10:40.733: INFO: (14) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 20.798216ms)
    Jun 27 17:10:40.735: INFO: (14) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 23.00053ms)
    Jun 27 17:10:40.735: INFO: (14) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 23.706774ms)
    Jun 27 17:10:40.735: INFO: (14) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 24.367463ms)
    Jun 27 17:10:40.736: INFO: (14) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 24.886321ms)
    Jun 27 17:10:40.736: INFO: (14) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 24.247158ms)
    Jun 27 17:10:40.736: INFO: (14) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 24.120831ms)
    Jun 27 17:10:40.737: INFO: (14) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 25.948346ms)
    Jun 27 17:10:40.739: INFO: (14) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 27.703416ms)
    Jun 27 17:10:40.740: INFO: (14) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 29.962542ms)
    Jun 27 17:10:40.741: INFO: (14) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 28.920039ms)
    Jun 27 17:10:40.741: INFO: (14) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 28.971456ms)
    Jun 27 17:10:40.741: INFO: (14) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 30.101286ms)
    Jun 27 17:10:40.776: INFO: (15) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 33.78207ms)
    Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 56.047304ms)
    Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 57.416333ms)
    Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 57.311916ms)
    Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 59.935189ms)
    Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 57.445547ms)
    Jun 27 17:10:40.801: INFO: (15) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 59.403913ms)
    Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 58.144203ms)
    Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 59.220136ms)
    Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 56.882949ms)
    Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 58.568219ms)
    Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 59.95094ms)
    Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 59.521232ms)
    Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 57.560714ms)
    Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 57.414482ms)
    Jun 27 17:10:40.802: INFO: (15) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 59.043442ms)
    Jun 27 17:10:40.834: INFO: (16) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 26.368749ms)
    Jun 27 17:10:40.835: INFO: (16) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 27.089713ms)
    Jun 27 17:10:40.847: INFO: (16) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 38.408282ms)
    Jun 27 17:10:40.848: INFO: (16) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 39.435164ms)
    Jun 27 17:10:40.848: INFO: (16) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 40.119708ms)
    Jun 27 17:10:40.848: INFO: (16) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 40.260211ms)
    Jun 27 17:10:40.848: INFO: (16) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 39.705672ms)
    Jun 27 17:10:40.849: INFO: (16) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 39.898992ms)
    Jun 27 17:10:40.849: INFO: (16) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 39.854204ms)
    Jun 27 17:10:40.849: INFO: (16) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 40.15705ms)
    Jun 27 17:10:40.854: INFO: (16) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 45.404061ms)
    Jun 27 17:10:40.854: INFO: (16) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 45.874737ms)
    Jun 27 17:10:40.854: INFO: (16) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 45.85526ms)
    Jun 27 17:10:40.858: INFO: (16) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 49.009115ms)
    Jun 27 17:10:40.858: INFO: (16) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 49.273382ms)
    Jun 27 17:10:40.858: INFO: (16) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 50.088975ms)
    Jun 27 17:10:40.881: INFO: (17) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 22.289605ms)
    Jun 27 17:10:40.884: INFO: (17) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 24.155125ms)
    Jun 27 17:10:40.884: INFO: (17) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 24.370919ms)
    Jun 27 17:10:40.884: INFO: (17) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 25.649463ms)
    Jun 27 17:10:40.885: INFO: (17) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 25.430405ms)
    Jun 27 17:10:40.885: INFO: (17) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 25.407463ms)
    Jun 27 17:10:40.885: INFO: (17) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 25.874947ms)
    Jun 27 17:10:40.885: INFO: (17) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 25.818351ms)
    Jun 27 17:10:40.886: INFO: (17) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 26.810033ms)
    Jun 27 17:10:40.886: INFO: (17) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 26.702728ms)
    Jun 27 17:10:40.891: INFO: (17) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 31.831824ms)
    Jun 27 17:10:40.891: INFO: (17) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 32.465387ms)
    Jun 27 17:10:40.891: INFO: (17) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 31.717532ms)
    Jun 27 17:10:40.891: INFO: (17) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 31.298886ms)
    Jun 27 17:10:40.893: INFO: (17) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 33.636681ms)
    Jun 27 17:10:40.893: INFO: (17) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 33.285304ms)
    Jun 27 17:10:40.912: INFO: (18) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 18.799495ms)
    Jun 27 17:10:40.915: INFO: (18) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 20.178484ms)
    Jun 27 17:10:40.916: INFO: (18) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 21.390187ms)
    Jun 27 17:10:40.916: INFO: (18) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 22.216496ms)
    Jun 27 17:10:40.916: INFO: (18) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.047288ms)
    Jun 27 17:10:40.916: INFO: (18) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 21.732053ms)
    Jun 27 17:10:40.917: INFO: (18) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 22.022966ms)
    Jun 27 17:10:40.917: INFO: (18) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 22.210006ms)
    Jun 27 17:10:40.917: INFO: (18) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 22.344444ms)
    Jun 27 17:10:40.918: INFO: (18) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 23.664169ms)
    Jun 27 17:10:40.922: INFO: (18) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 26.753565ms)
    Jun 27 17:10:40.922: INFO: (18) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 26.940363ms)
    Jun 27 17:10:40.923: INFO: (18) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 29.948926ms)
    Jun 27 17:10:40.924: INFO: (18) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 30.392716ms)
    Jun 27 17:10:40.924: INFO: (18) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 28.909443ms)
    Jun 27 17:10:40.924: INFO: (18) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 28.790743ms)
    Jun 27 17:10:40.944: INFO: (19) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:443/proxy/tlsrewritem... (200; 20.148186ms)
    Jun 27 17:10:40.948: INFO: (19) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 21.942695ms)
    Jun 27 17:10:40.949: INFO: (19) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:462/proxy/: tls qux (200; 23.485844ms)
    Jun 27 17:10:40.949: INFO: (19) /api/v1/namespaces/proxy-6481/pods/https:proxy-service-tglvz-x5t6k:460/proxy/: tls baz (200; 23.378574ms)
    Jun 27 17:10:40.955: INFO: (19) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:160/proxy/: foo (200; 22.610851ms)
    Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 37.788606ms)
    Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">test<... (200; 37.424606ms)
    Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/http:proxy-service-tglvz-x5t6k:1080/proxy/rewriteme">... (200; 37.388408ms)
    Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k:162/proxy/: bar (200; 38.385155ms)
    Jun 27 17:10:40.963: INFO: (19) /api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/: <a href="/api/v1/namespaces/proxy-6481/pods/proxy-service-tglvz-x5t6k/proxy/rewriteme">test</a> (200; 38.443228ms)
    Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname2/proxy/: tls qux (200; 38.175339ms)
    Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/https:proxy-service-tglvz:tlsportname1/proxy/: tls baz (200; 37.861903ms)
    Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname1/proxy/: foo (200; 39.287673ms)
    Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/proxy-service-tglvz:portname2/proxy/: bar (200; 38.226751ms)
    Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname1/proxy/: foo (200; 38.010813ms)
    Jun 27 17:10:40.964: INFO: (19) /api/v1/namespaces/proxy-6481/services/http:proxy-service-tglvz:portname2/proxy/: bar (200; 39.173032ms)
    STEP: deleting ReplicationController proxy-service-tglvz in namespace proxy-6481, will wait for the garbage collector to delete the pods 06/27/23 17:10:40.964
    Jun 27 17:10:41.055: INFO: Deleting ReplicationController proxy-service-tglvz took: 24.916968ms
    Jun 27 17:10:41.157: INFO: Terminating ReplicationController proxy-service-tglvz pods took: 101.393195ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jun 27 17:10:43.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-6481" for this suite. 06/27/23 17:10:43.877
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:10:43.902
Jun 27 17:10:43.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:10:43.903
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:43.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:43.947
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-df249b74-02a1-4b6d-b685-8e8a2d56fdaf 06/27/23 17:10:43.956
STEP: Creating a pod to test consume secrets 06/27/23 17:10:43.99
Jun 27 17:10:44.039: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac" in namespace "projected-9069" to be "Succeeded or Failed"
Jun 27 17:10:44.052: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac": Phase="Pending", Reason="", readiness=false. Elapsed: 13.625141ms
Jun 27 17:10:46.063: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024331659s
Jun 27 17:10:48.063: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0249113s
Jun 27 17:10:50.066: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027437957s
STEP: Saw pod success 06/27/23 17:10:50.066
Jun 27 17:10:50.066: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac" satisfied condition "Succeeded or Failed"
Jun 27 17:10:50.081: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac container projected-secret-volume-test: <nil>
STEP: delete the pod 06/27/23 17:10:50.121
Jun 27 17:10:50.161: INFO: Waiting for pod pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac to disappear
Jun 27 17:10:50.189: INFO: Pod pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 27 17:10:50.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9069" for this suite. 06/27/23 17:10:50.203
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":162,"skipped":3497,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.329 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:10:43.902
    Jun 27 17:10:43.902: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:10:43.903
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:43.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:43.947
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-df249b74-02a1-4b6d-b685-8e8a2d56fdaf 06/27/23 17:10:43.956
    STEP: Creating a pod to test consume secrets 06/27/23 17:10:43.99
    Jun 27 17:10:44.039: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac" in namespace "projected-9069" to be "Succeeded or Failed"
    Jun 27 17:10:44.052: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac": Phase="Pending", Reason="", readiness=false. Elapsed: 13.625141ms
    Jun 27 17:10:46.063: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024331659s
    Jun 27 17:10:48.063: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac": Phase="Pending", Reason="", readiness=false. Elapsed: 4.0249113s
    Jun 27 17:10:50.066: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027437957s
    STEP: Saw pod success 06/27/23 17:10:50.066
    Jun 27 17:10:50.066: INFO: Pod "pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac" satisfied condition "Succeeded or Failed"
    Jun 27 17:10:50.081: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:10:50.121
    Jun 27 17:10:50.161: INFO: Waiting for pod pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac to disappear
    Jun 27 17:10:50.189: INFO: Pod pod-projected-secrets-4c2898a8-47c4-4f83-ae11-a13f3d1e2fac no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 27 17:10:50.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9069" for this suite. 06/27/23 17:10:50.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:10:50.232
Jun 27 17:10:50.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename gc 06/27/23 17:10:50.235
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:50.265
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:50.275
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 06/27/23 17:10:50.284
STEP: Wait for the Deployment to create new ReplicaSet 06/27/23 17:10:50.304
STEP: delete the deployment 06/27/23 17:10:50.463
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/27/23 17:10:50.487
STEP: Gathering metrics 06/27/23 17:10:51.054
W0627 17:10:51.074101      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 27 17:10:51.074: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 27 17:10:51.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4548" for this suite. 06/27/23 17:10:51.09
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":163,"skipped":3506,"failed":0}
------------------------------
â€¢ [0.884 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:10:50.232
    Jun 27 17:10:50.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename gc 06/27/23 17:10:50.235
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:50.265
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:50.275
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 06/27/23 17:10:50.284
    STEP: Wait for the Deployment to create new ReplicaSet 06/27/23 17:10:50.304
    STEP: delete the deployment 06/27/23 17:10:50.463
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 06/27/23 17:10:50.487
    STEP: Gathering metrics 06/27/23 17:10:51.054
    W0627 17:10:51.074101      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 27 17:10:51.074: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 27 17:10:51.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4548" for this suite. 06/27/23 17:10:51.09
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:10:51.12
Jun 27 17:10:51.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename svcaccounts 06/27/23 17:10:51.122
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:51.171
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:51.204
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 06/27/23 17:10:51.213
STEP: watching for the ServiceAccount to be added 06/27/23 17:10:51.242
STEP: patching the ServiceAccount 06/27/23 17:10:51.246
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/27/23 17:10:51.267
STEP: deleting the ServiceAccount 06/27/23 17:10:51.292
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 27 17:10:51.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6143" for this suite. 06/27/23 17:10:51.409
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":164,"skipped":3515,"failed":0}
------------------------------
â€¢ [0.317 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:10:51.12
    Jun 27 17:10:51.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename svcaccounts 06/27/23 17:10:51.122
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:51.171
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:51.204
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 06/27/23 17:10:51.213
    STEP: watching for the ServiceAccount to be added 06/27/23 17:10:51.242
    STEP: patching the ServiceAccount 06/27/23 17:10:51.246
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 06/27/23 17:10:51.267
    STEP: deleting the ServiceAccount 06/27/23 17:10:51.292
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 27 17:10:51.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6143" for this suite. 06/27/23 17:10:51.409
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:10:51.443
Jun 27 17:10:51.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 17:10:51.444
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:51.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:51.49
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 06/27/23 17:10:51.501
Jun 27 17:10:51.558: INFO: created test-pod-1
Jun 27 17:10:51.609: INFO: created test-pod-2
Jun 27 17:10:51.636: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 06/27/23 17:10:51.636
Jun 27 17:10:51.636: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1927' to be running and ready
Jun 27 17:10:51.673: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 27 17:10:51.673: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 27 17:10:51.673: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 27 17:10:51.673: INFO: 0 / 3 pods in namespace 'pods-1927' are running and ready (0 seconds elapsed)
Jun 27 17:10:51.673: INFO: expected 0 pod replicas in namespace 'pods-1927', 0 are Running and Ready.
Jun 27 17:10:51.673: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Jun 27 17:10:51.673: INFO: test-pod-1  10.113.180.90  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
Jun 27 17:10:51.673: INFO: test-pod-2  10.113.180.90  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
Jun 27 17:10:51.673: INFO: test-pod-3  10.113.180.90  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
Jun 27 17:10:51.673: INFO: 
Jun 27 17:10:53.709: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 27 17:10:53.709: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 27 17:10:53.709: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Jun 27 17:10:53.709: INFO: 0 / 3 pods in namespace 'pods-1927' are running and ready (2 seconds elapsed)
Jun 27 17:10:53.709: INFO: expected 0 pod replicas in namespace 'pods-1927', 0 are Running and Ready.
Jun 27 17:10:53.709: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
Jun 27 17:10:53.709: INFO: test-pod-1  10.113.180.90  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
Jun 27 17:10:53.709: INFO: test-pod-2  10.113.180.90  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
Jun 27 17:10:53.709: INFO: test-pod-3  10.113.180.90  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
Jun 27 17:10:53.709: INFO: 
Jun 27 17:10:55.708: INFO: 3 / 3 pods in namespace 'pods-1927' are running and ready (4 seconds elapsed)
Jun 27 17:10:55.708: INFO: expected 0 pod replicas in namespace 'pods-1927', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 06/27/23 17:10:55.771
Jun 27 17:10:55.782: INFO: Pod quantity 3 is different from expected quantity 0
Jun 27 17:10:56.800: INFO: Pod quantity 3 is different from expected quantity 0
Jun 27 17:10:57.795: INFO: Pod quantity 3 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 17:10:58.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1927" for this suite. 06/27/23 17:10:58.832
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":165,"skipped":3537,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.419 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:10:51.443
    Jun 27 17:10:51.443: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 17:10:51.444
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:51.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:51.49
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 06/27/23 17:10:51.501
    Jun 27 17:10:51.558: INFO: created test-pod-1
    Jun 27 17:10:51.609: INFO: created test-pod-2
    Jun 27 17:10:51.636: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 06/27/23 17:10:51.636
    Jun 27 17:10:51.636: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-1927' to be running and ready
    Jun 27 17:10:51.673: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 27 17:10:51.673: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 27 17:10:51.673: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 27 17:10:51.673: INFO: 0 / 3 pods in namespace 'pods-1927' are running and ready (0 seconds elapsed)
    Jun 27 17:10:51.673: INFO: expected 0 pod replicas in namespace 'pods-1927', 0 are Running and Ready.
    Jun 27 17:10:51.673: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    Jun 27 17:10:51.673: INFO: test-pod-1  10.113.180.90  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
    Jun 27 17:10:51.673: INFO: test-pod-2  10.113.180.90  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
    Jun 27 17:10:51.673: INFO: test-pod-3  10.113.180.90  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
    Jun 27 17:10:51.673: INFO: 
    Jun 27 17:10:53.709: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 27 17:10:53.709: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 27 17:10:53.709: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Jun 27 17:10:53.709: INFO: 0 / 3 pods in namespace 'pods-1927' are running and ready (2 seconds elapsed)
    Jun 27 17:10:53.709: INFO: expected 0 pod replicas in namespace 'pods-1927', 0 are Running and Ready.
    Jun 27 17:10:53.709: INFO: POD         NODE           PHASE    GRACE  CONDITIONS
    Jun 27 17:10:53.709: INFO: test-pod-1  10.113.180.90  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
    Jun 27 17:10:53.709: INFO: test-pod-2  10.113.180.90  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
    Jun 27 17:10:53.709: INFO: test-pod-3  10.113.180.90  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 17:10:51 +0000 UTC  }]
    Jun 27 17:10:53.709: INFO: 
    Jun 27 17:10:55.708: INFO: 3 / 3 pods in namespace 'pods-1927' are running and ready (4 seconds elapsed)
    Jun 27 17:10:55.708: INFO: expected 0 pod replicas in namespace 'pods-1927', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 06/27/23 17:10:55.771
    Jun 27 17:10:55.782: INFO: Pod quantity 3 is different from expected quantity 0
    Jun 27 17:10:56.800: INFO: Pod quantity 3 is different from expected quantity 0
    Jun 27 17:10:57.795: INFO: Pod quantity 3 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 17:10:58.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1927" for this suite. 06/27/23 17:10:58.832
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:10:58.862
Jun 27 17:10:58.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 17:10:58.871
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:58.92
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:58.93
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 17:10:58.982
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:10:59.326
STEP: Deploying the webhook pod 06/27/23 17:10:59.358
STEP: Wait for the deployment to be ready 06/27/23 17:10:59.413
Jun 27 17:10:59.457: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/27/23 17:11:01.491
STEP: Verifying the service has paired with the endpoint 06/27/23 17:11:01.522
Jun 27 17:11:02.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/27/23 17:11:02.534
STEP: Registering slow webhook via the AdmissionRegistration API 06/27/23 17:11:02.535
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/27/23 17:11:02.63
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/27/23 17:11:03.657
STEP: Registering slow webhook via the AdmissionRegistration API 06/27/23 17:11:03.658
STEP: Having no error when timeout is longer than webhook latency 06/27/23 17:11:04.755
STEP: Registering slow webhook via the AdmissionRegistration API 06/27/23 17:11:04.755
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/27/23 17:11:09.885
STEP: Registering slow webhook via the AdmissionRegistration API 06/27/23 17:11:09.886
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:11:14.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8142" for this suite. 06/27/23 17:11:15
STEP: Destroying namespace "webhook-8142-markers" for this suite. 06/27/23 17:11:15.018
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":166,"skipped":3541,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.300 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:10:58.862
    Jun 27 17:10:58.863: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 17:10:58.871
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:10:58.92
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:10:58.93
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 17:10:58.982
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:10:59.326
    STEP: Deploying the webhook pod 06/27/23 17:10:59.358
    STEP: Wait for the deployment to be ready 06/27/23 17:10:59.413
    Jun 27 17:10:59.457: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/27/23 17:11:01.491
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:11:01.522
    Jun 27 17:11:02.524: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 06/27/23 17:11:02.534
    STEP: Registering slow webhook via the AdmissionRegistration API 06/27/23 17:11:02.535
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 06/27/23 17:11:02.63
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 06/27/23 17:11:03.657
    STEP: Registering slow webhook via the AdmissionRegistration API 06/27/23 17:11:03.658
    STEP: Having no error when timeout is longer than webhook latency 06/27/23 17:11:04.755
    STEP: Registering slow webhook via the AdmissionRegistration API 06/27/23 17:11:04.755
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 06/27/23 17:11:09.885
    STEP: Registering slow webhook via the AdmissionRegistration API 06/27/23 17:11:09.886
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:11:14.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8142" for this suite. 06/27/23 17:11:15
    STEP: Destroying namespace "webhook-8142-markers" for this suite. 06/27/23 17:11:15.018
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:11:15.163
Jun 27 17:11:15.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 17:11:15.165
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:15.2
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:15.212
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-811 06/27/23 17:11:15.229
STEP: changing the ExternalName service to type=NodePort 06/27/23 17:11:15.273
STEP: creating replication controller externalname-service in namespace services-811 06/27/23 17:11:15.347
I0627 17:11:15.365858      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-811, replica count: 2
I0627 17:11:18.416734      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 17:11:18.416: INFO: Creating new exec pod
Jun 27 17:11:18.460: INFO: Waiting up to 5m0s for pod "execpod8qnbq" in namespace "services-811" to be "running"
Jun 27 17:11:18.474: INFO: Pod "execpod8qnbq": Phase="Pending", Reason="", readiness=false. Elapsed: 13.829362ms
Jun 27 17:11:20.485: INFO: Pod "execpod8qnbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.0248506s
Jun 27 17:11:20.486: INFO: Pod "execpod8qnbq" satisfied condition "running"
Jun 27 17:11:21.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 27 17:11:21.957: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 27 17:11:21.957: INFO: stdout: ""
Jun 27 17:11:22.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 27 17:11:23.328: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 27 17:11:23.328: INFO: stdout: "externalname-service-7nn7z"
Jun 27 17:11:23.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.207.168 80'
Jun 27 17:11:23.766: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.207.168 80\nConnection to 172.21.207.168 80 port [tcp/http] succeeded!\n"
Jun 27 17:11:23.766: INFO: stdout: "externalname-service-7nn7z"
Jun 27 17:11:23.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31991'
Jun 27 17:11:24.199: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31991\nConnection to 10.113.180.89 31991 port [tcp/*] succeeded!\n"
Jun 27 17:11:24.199: INFO: stdout: ""
Jun 27 17:11:25.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31991'
Jun 27 17:11:25.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31991\nConnection to 10.113.180.89 31991 port [tcp/*] succeeded!\n"
Jun 27 17:11:25.630: INFO: stdout: ""
Jun 27 17:11:26.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31991'
Jun 27 17:11:26.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31991\nConnection to 10.113.180.89 31991 port [tcp/*] succeeded!\n"
Jun 27 17:11:26.767: INFO: stdout: "externalname-service-g74b8"
Jun 27 17:11:26.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.96 31991'
Jun 27 17:11:27.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.96 31991\nConnection to 10.113.180.96 31991 port [tcp/*] succeeded!\n"
Jun 27 17:11:27.234: INFO: stdout: "externalname-service-g74b8"
Jun 27 17:11:27.234: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 17:11:27.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-811" for this suite. 06/27/23 17:11:27.341
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":167,"skipped":3549,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.210 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:11:15.163
    Jun 27 17:11:15.164: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 17:11:15.165
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:15.2
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:15.212
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-811 06/27/23 17:11:15.229
    STEP: changing the ExternalName service to type=NodePort 06/27/23 17:11:15.273
    STEP: creating replication controller externalname-service in namespace services-811 06/27/23 17:11:15.347
    I0627 17:11:15.365858      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-811, replica count: 2
    I0627 17:11:18.416734      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 27 17:11:18.416: INFO: Creating new exec pod
    Jun 27 17:11:18.460: INFO: Waiting up to 5m0s for pod "execpod8qnbq" in namespace "services-811" to be "running"
    Jun 27 17:11:18.474: INFO: Pod "execpod8qnbq": Phase="Pending", Reason="", readiness=false. Elapsed: 13.829362ms
    Jun 27 17:11:20.485: INFO: Pod "execpod8qnbq": Phase="Running", Reason="", readiness=true. Elapsed: 2.0248506s
    Jun 27 17:11:20.486: INFO: Pod "execpod8qnbq" satisfied condition "running"
    Jun 27 17:11:21.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 27 17:11:21.957: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 27 17:11:21.957: INFO: stdout: ""
    Jun 27 17:11:22.958: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 27 17:11:23.328: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 27 17:11:23.328: INFO: stdout: "externalname-service-7nn7z"
    Jun 27 17:11:23.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.207.168 80'
    Jun 27 17:11:23.766: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.207.168 80\nConnection to 172.21.207.168 80 port [tcp/http] succeeded!\n"
    Jun 27 17:11:23.766: INFO: stdout: "externalname-service-7nn7z"
    Jun 27 17:11:23.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31991'
    Jun 27 17:11:24.199: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31991\nConnection to 10.113.180.89 31991 port [tcp/*] succeeded!\n"
    Jun 27 17:11:24.199: INFO: stdout: ""
    Jun 27 17:11:25.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31991'
    Jun 27 17:11:25.630: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31991\nConnection to 10.113.180.89 31991 port [tcp/*] succeeded!\n"
    Jun 27 17:11:25.630: INFO: stdout: ""
    Jun 27 17:11:26.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.89 31991'
    Jun 27 17:11:26.767: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.89 31991\nConnection to 10.113.180.89 31991 port [tcp/*] succeeded!\n"
    Jun 27 17:11:26.767: INFO: stdout: "externalname-service-g74b8"
    Jun 27 17:11:26.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-811 exec execpod8qnbq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.113.180.96 31991'
    Jun 27 17:11:27.234: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.113.180.96 31991\nConnection to 10.113.180.96 31991 port [tcp/*] succeeded!\n"
    Jun 27 17:11:27.234: INFO: stdout: "externalname-service-g74b8"
    Jun 27 17:11:27.234: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 17:11:27.323: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-811" for this suite. 06/27/23 17:11:27.341
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:11:27.375
Jun 27 17:11:27.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 17:11:27.378
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:27.417
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:27.431
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 17:11:27.5
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:11:28.109
STEP: Deploying the webhook pod 06/27/23 17:11:28.139
STEP: Wait for the deployment to be ready 06/27/23 17:11:28.173
Jun 27 17:11:28.197: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 27 17:11:30.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 11, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 11, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 11, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 11, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 17:11:32.248
STEP: Verifying the service has paired with the endpoint 06/27/23 17:11:32.283
Jun 27 17:11:33.289: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 06/27/23 17:11:33.311
STEP: create a pod that should be denied by the webhook 06/27/23 17:11:33.386
STEP: create a pod that causes the webhook to hang 06/27/23 17:11:33.464
STEP: create a configmap that should be denied by the webhook 06/27/23 17:11:43.499
STEP: create a configmap that should be admitted by the webhook 06/27/23 17:11:43.55
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/27/23 17:11:43.586
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/27/23 17:11:43.618
STEP: create a namespace that bypass the webhook 06/27/23 17:11:43.64
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/27/23 17:11:43.662
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:11:43.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9517" for this suite. 06/27/23 17:11:43.81
STEP: Destroying namespace "webhook-9517-markers" for this suite. 06/27/23 17:11:43.831
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":168,"skipped":3560,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.643 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:11:27.375
    Jun 27 17:11:27.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 17:11:27.378
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:27.417
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:27.431
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 17:11:27.5
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:11:28.109
    STEP: Deploying the webhook pod 06/27/23 17:11:28.139
    STEP: Wait for the deployment to be ready 06/27/23 17:11:28.173
    Jun 27 17:11:28.197: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 27 17:11:30.233: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 11, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 11, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 11, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 11, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 17:11:32.248
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:11:32.283
    Jun 27 17:11:33.289: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 06/27/23 17:11:33.311
    STEP: create a pod that should be denied by the webhook 06/27/23 17:11:33.386
    STEP: create a pod that causes the webhook to hang 06/27/23 17:11:33.464
    STEP: create a configmap that should be denied by the webhook 06/27/23 17:11:43.499
    STEP: create a configmap that should be admitted by the webhook 06/27/23 17:11:43.55
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 06/27/23 17:11:43.586
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 06/27/23 17:11:43.618
    STEP: create a namespace that bypass the webhook 06/27/23 17:11:43.64
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 06/27/23 17:11:43.662
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:11:43.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9517" for this suite. 06/27/23 17:11:43.81
    STEP: Destroying namespace "webhook-9517-markers" for this suite. 06/27/23 17:11:43.831
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:11:44.019
Jun 27 17:11:44.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename init-container 06/27/23 17:11:44.021
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:44.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:44.07
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 06/27/23 17:11:44.083
Jun 27 17:11:44.083: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 27 17:11:48.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-5868" for this suite. 06/27/23 17:11:48.437
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":169,"skipped":3569,"failed":0}
------------------------------
â€¢ [4.436 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:11:44.019
    Jun 27 17:11:44.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename init-container 06/27/23 17:11:44.021
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:44.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:44.07
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 06/27/23 17:11:44.083
    Jun 27 17:11:44.083: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 27 17:11:48.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-5868" for this suite. 06/27/23 17:11:48.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:11:48.463
Jun 27 17:11:48.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 17:11:48.465
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:48.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:48.521
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 06/27/23 17:11:48.567
Jun 27 17:11:48.630: INFO: Waiting up to 5m0s for pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3" in namespace "emptydir-6113" to be "Succeeded or Failed"
Jun 27 17:11:48.639: INFO: Pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.004734ms
Jun 27 17:11:50.649: INFO: Pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018658918s
Jun 27 17:11:52.666: INFO: Pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035996437s
STEP: Saw pod success 06/27/23 17:11:52.667
Jun 27 17:11:52.668: INFO: Pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3" satisfied condition "Succeeded or Failed"
Jun 27 17:11:52.680: INFO: Trying to get logs from node 10.113.180.90 pod pod-1e321399-0025-4ad4-9f9f-651e820306f3 container test-container: <nil>
STEP: delete the pod 06/27/23 17:11:52.74
Jun 27 17:11:52.823: INFO: Waiting for pod pod-1e321399-0025-4ad4-9f9f-651e820306f3 to disappear
Jun 27 17:11:52.832: INFO: Pod pod-1e321399-0025-4ad4-9f9f-651e820306f3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 17:11:52.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6113" for this suite. 06/27/23 17:11:52.866
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":170,"skipped":3584,"failed":0}
------------------------------
â€¢ [4.432 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:11:48.463
    Jun 27 17:11:48.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 17:11:48.465
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:48.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:48.521
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 06/27/23 17:11:48.567
    Jun 27 17:11:48.630: INFO: Waiting up to 5m0s for pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3" in namespace "emptydir-6113" to be "Succeeded or Failed"
    Jun 27 17:11:48.639: INFO: Pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3": Phase="Pending", Reason="", readiness=false. Elapsed: 9.004734ms
    Jun 27 17:11:50.649: INFO: Pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.018658918s
    Jun 27 17:11:52.666: INFO: Pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.035996437s
    STEP: Saw pod success 06/27/23 17:11:52.667
    Jun 27 17:11:52.668: INFO: Pod "pod-1e321399-0025-4ad4-9f9f-651e820306f3" satisfied condition "Succeeded or Failed"
    Jun 27 17:11:52.680: INFO: Trying to get logs from node 10.113.180.90 pod pod-1e321399-0025-4ad4-9f9f-651e820306f3 container test-container: <nil>
    STEP: delete the pod 06/27/23 17:11:52.74
    Jun 27 17:11:52.823: INFO: Waiting for pod pod-1e321399-0025-4ad4-9f9f-651e820306f3 to disappear
    Jun 27 17:11:52.832: INFO: Pod pod-1e321399-0025-4ad4-9f9f-651e820306f3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:11:52.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6113" for this suite. 06/27/23 17:11:52.866
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:11:52.897
Jun 27 17:11:52.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename server-version 06/27/23 17:11:52.899
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:52.953
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:52.964
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 06/27/23 17:11:52.976
STEP: Confirm major version 06/27/23 17:11:52.982
Jun 27 17:11:52.982: INFO: Major version: 1
STEP: Confirm minor version 06/27/23 17:11:52.982
Jun 27 17:11:52.982: INFO: cleanMinorVersion: 25
Jun 27 17:11:52.982: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Jun 27 17:11:52.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8399" for this suite. 06/27/23 17:11:52.997
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":171,"skipped":3611,"failed":0}
------------------------------
â€¢ [0.125 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:11:52.897
    Jun 27 17:11:52.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename server-version 06/27/23 17:11:52.899
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:52.953
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:52.964
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 06/27/23 17:11:52.976
    STEP: Confirm major version 06/27/23 17:11:52.982
    Jun 27 17:11:52.982: INFO: Major version: 1
    STEP: Confirm minor version 06/27/23 17:11:52.982
    Jun 27 17:11:52.982: INFO: cleanMinorVersion: 25
    Jun 27 17:11:52.982: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Jun 27 17:11:52.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-8399" for this suite. 06/27/23 17:11:52.997
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:11:53.023
Jun 27 17:11:53.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename svcaccounts 06/27/23 17:11:53.025
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:53.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:53.068
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Jun 27 17:11:53.168: INFO: created pod
Jun 27 17:11:53.168: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2494" to be "Succeeded or Failed"
Jun 27 17:11:53.194: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 26.110588ms
Jun 27 17:11:55.208: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039792476s
Jun 27 17:11:57.213: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044303403s
Jun 27 17:11:59.206: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037348752s
STEP: Saw pod success 06/27/23 17:11:59.206
Jun 27 17:11:59.206: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Jun 27 17:12:29.207: INFO: polling logs
Jun 27 17:12:29.239: INFO: Pod logs: 
I0627 17:11:54.666408       1 log.go:195] OK: Got token
I0627 17:11:54.666603       1 log.go:195] validating with in-cluster discovery
I0627 17:11:54.667145       1 log.go:195] OK: got issuer https://kubernetes.default.svc
I0627 17:11:54.667203       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2494:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687886513, NotBefore:1687885913, IssuedAt:1687885913, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2494", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d63db1b8-4e8d-44da-ab00-a1c8893072b8"}}}
I0627 17:11:54.698672       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
I0627 17:11:54.729255       1 log.go:195] OK: Validated signature on JWT
I0627 17:11:54.729570       1 log.go:195] OK: Got valid claims from token!
I0627 17:11:54.729687       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2494:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687886513, NotBefore:1687885913, IssuedAt:1687885913, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2494", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d63db1b8-4e8d-44da-ab00-a1c8893072b8"}}}

Jun 27 17:12:29.239: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 27 17:12:29.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2494" for this suite. 06/27/23 17:12:29.284
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":172,"skipped":3611,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.286 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:11:53.023
    Jun 27 17:11:53.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename svcaccounts 06/27/23 17:11:53.025
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:11:53.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:11:53.068
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Jun 27 17:11:53.168: INFO: created pod
    Jun 27 17:11:53.168: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-2494" to be "Succeeded or Failed"
    Jun 27 17:11:53.194: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 26.110588ms
    Jun 27 17:11:55.208: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039792476s
    Jun 27 17:11:57.213: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 4.044303403s
    Jun 27 17:11:59.206: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.037348752s
    STEP: Saw pod success 06/27/23 17:11:59.206
    Jun 27 17:11:59.206: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Jun 27 17:12:29.207: INFO: polling logs
    Jun 27 17:12:29.239: INFO: Pod logs: 
    I0627 17:11:54.666408       1 log.go:195] OK: Got token
    I0627 17:11:54.666603       1 log.go:195] validating with in-cluster discovery
    I0627 17:11:54.667145       1 log.go:195] OK: got issuer https://kubernetes.default.svc
    I0627 17:11:54.667203       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2494:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687886513, NotBefore:1687885913, IssuedAt:1687885913, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2494", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d63db1b8-4e8d-44da-ab00-a1c8893072b8"}}}
    I0627 17:11:54.698672       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc
    I0627 17:11:54.729255       1 log.go:195] OK: Validated signature on JWT
    I0627 17:11:54.729570       1 log.go:195] OK: Got valid claims from token!
    I0627 17:11:54.729687       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc", Subject:"system:serviceaccount:svcaccounts-2494:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1687886513, NotBefore:1687885913, IssuedAt:1687885913, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-2494", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"d63db1b8-4e8d-44da-ab00-a1c8893072b8"}}}

    Jun 27 17:12:29.239: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 27 17:12:29.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2494" for this suite. 06/27/23 17:12:29.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:12:29.312
Jun 27 17:12:29.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 17:12:29.314
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:12:29.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:12:29.357
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
STEP: creating an Endpoint 06/27/23 17:12:29.384
STEP: waiting for available Endpoint 06/27/23 17:12:29.401
STEP: listing all Endpoints 06/27/23 17:12:29.406
STEP: updating the Endpoint 06/27/23 17:12:29.437
STEP: fetching the Endpoint 06/27/23 17:12:29.456
STEP: patching the Endpoint 06/27/23 17:12:29.465
STEP: fetching the Endpoint 06/27/23 17:12:29.486
STEP: deleting the Endpoint by Collection 06/27/23 17:12:29.496
STEP: waiting for Endpoint deletion 06/27/23 17:12:29.523
STEP: fetching the Endpoint 06/27/23 17:12:29.527
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 17:12:29.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1316" for this suite. 06/27/23 17:12:29.551
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":173,"skipped":3638,"failed":0}
------------------------------
â€¢ [0.257 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:12:29.312
    Jun 27 17:12:29.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 17:12:29.314
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:12:29.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:12:29.357
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3210
    STEP: creating an Endpoint 06/27/23 17:12:29.384
    STEP: waiting for available Endpoint 06/27/23 17:12:29.401
    STEP: listing all Endpoints 06/27/23 17:12:29.406
    STEP: updating the Endpoint 06/27/23 17:12:29.437
    STEP: fetching the Endpoint 06/27/23 17:12:29.456
    STEP: patching the Endpoint 06/27/23 17:12:29.465
    STEP: fetching the Endpoint 06/27/23 17:12:29.486
    STEP: deleting the Endpoint by Collection 06/27/23 17:12:29.496
    STEP: waiting for Endpoint deletion 06/27/23 17:12:29.523
    STEP: fetching the Endpoint 06/27/23 17:12:29.527
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 17:12:29.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1316" for this suite. 06/27/23 17:12:29.551
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:12:29.569
Jun 27 17:12:29.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-probe 06/27/23 17:12:29.57
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:12:29.651
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:12:29.66
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b in namespace container-probe-7217 06/27/23 17:12:29.675
Jun 27 17:12:29.730: INFO: Waiting up to 5m0s for pod "liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b" in namespace "container-probe-7217" to be "not pending"
Jun 27 17:12:29.746: INFO: Pod "liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.218156ms
Jun 27 17:12:31.758: INFO: Pod "liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b": Phase="Running", Reason="", readiness=true. Elapsed: 2.028116535s
Jun 27 17:12:31.758: INFO: Pod "liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b" satisfied condition "not pending"
Jun 27 17:12:31.758: INFO: Started pod liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b in namespace container-probe-7217
STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 17:12:31.758
Jun 27 17:12:31.768: INFO: Initial restart count of pod liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b is 0
Jun 27 17:12:51.897: INFO: Restart count of pod container-probe-7217/liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b is now 1 (20.128134264s elapsed)
STEP: deleting the pod 06/27/23 17:12:51.897
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 27 17:12:51.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7217" for this suite. 06/27/23 17:12:51.946
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":174,"skipped":3645,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.396 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:12:29.569
    Jun 27 17:12:29.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-probe 06/27/23 17:12:29.57
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:12:29.651
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:12:29.66
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b in namespace container-probe-7217 06/27/23 17:12:29.675
    Jun 27 17:12:29.730: INFO: Waiting up to 5m0s for pod "liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b" in namespace "container-probe-7217" to be "not pending"
    Jun 27 17:12:29.746: INFO: Pod "liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b": Phase="Pending", Reason="", readiness=false. Elapsed: 16.218156ms
    Jun 27 17:12:31.758: INFO: Pod "liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b": Phase="Running", Reason="", readiness=true. Elapsed: 2.028116535s
    Jun 27 17:12:31.758: INFO: Pod "liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b" satisfied condition "not pending"
    Jun 27 17:12:31.758: INFO: Started pod liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b in namespace container-probe-7217
    STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 17:12:31.758
    Jun 27 17:12:31.768: INFO: Initial restart count of pod liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b is 0
    Jun 27 17:12:51.897: INFO: Restart count of pod container-probe-7217/liveness-9c1f0c1c-96c1-4178-9d31-4d73a137602b is now 1 (20.128134264s elapsed)
    STEP: deleting the pod 06/27/23 17:12:51.897
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 27 17:12:51.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7217" for this suite. 06/27/23 17:12:51.946
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:12:51.972
Jun 27 17:12:51.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 17:12:51.975
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:12:52.018
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:12:52.029
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/27/23 17:12:52.041
Jun 27 17:12:52.085: INFO: Waiting up to 5m0s for pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8" in namespace "emptydir-6586" to be "Succeeded or Failed"
Jun 27 17:12:52.098: INFO: Pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.559964ms
Jun 27 17:12:54.113: INFO: Pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028190651s
Jun 27 17:12:56.111: INFO: Pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026302741s
STEP: Saw pod success 06/27/23 17:12:56.111
Jun 27 17:12:56.112: INFO: Pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8" satisfied condition "Succeeded or Failed"
Jun 27 17:12:56.121: INFO: Trying to get logs from node 10.113.180.90 pod pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8 container test-container: <nil>
STEP: delete the pod 06/27/23 17:12:56.164
Jun 27 17:12:56.188: INFO: Waiting for pod pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8 to disappear
Jun 27 17:12:56.197: INFO: Pod pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 17:12:56.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6586" for this suite. 06/27/23 17:12:56.213
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":175,"skipped":3686,"failed":0}
------------------------------
â€¢ [4.258 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:12:51.972
    Jun 27 17:12:51.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 17:12:51.975
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:12:52.018
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:12:52.029
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/27/23 17:12:52.041
    Jun 27 17:12:52.085: INFO: Waiting up to 5m0s for pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8" in namespace "emptydir-6586" to be "Succeeded or Failed"
    Jun 27 17:12:52.098: INFO: Pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8": Phase="Pending", Reason="", readiness=false. Elapsed: 12.559964ms
    Jun 27 17:12:54.113: INFO: Pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028190651s
    Jun 27 17:12:56.111: INFO: Pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.026302741s
    STEP: Saw pod success 06/27/23 17:12:56.111
    Jun 27 17:12:56.112: INFO: Pod "pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8" satisfied condition "Succeeded or Failed"
    Jun 27 17:12:56.121: INFO: Trying to get logs from node 10.113.180.90 pod pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8 container test-container: <nil>
    STEP: delete the pod 06/27/23 17:12:56.164
    Jun 27 17:12:56.188: INFO: Waiting for pod pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8 to disappear
    Jun 27 17:12:56.197: INFO: Pod pod-992c41a6-ced3-43f7-b3ff-6dbaff4d45b8 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:12:56.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6586" for this suite. 06/27/23 17:12:56.213
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:12:56.241
Jun 27 17:12:56.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 17:12:56.242
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:12:56.281
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:12:56.291
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 06/27/23 17:12:56.301
Jun 27 17:12:56.353: INFO: Waiting up to 5m0s for pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27" in namespace "emptydir-9223" to be "Succeeded or Failed"
Jun 27 17:12:56.364: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27": Phase="Pending", Reason="", readiness=false. Elapsed: 10.452027ms
Jun 27 17:12:58.375: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021881728s
Jun 27 17:13:00.374: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020313262s
Jun 27 17:13:02.382: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028226595s
STEP: Saw pod success 06/27/23 17:13:02.382
Jun 27 17:13:02.382: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27" satisfied condition "Succeeded or Failed"
Jun 27 17:13:02.392: INFO: Trying to get logs from node 10.113.180.90 pod pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27 container test-container: <nil>
STEP: delete the pod 06/27/23 17:13:02.422
Jun 27 17:13:02.452: INFO: Waiting for pod pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27 to disappear
Jun 27 17:13:02.464: INFO: Pod pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 17:13:02.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-9223" for this suite. 06/27/23 17:13:02.5
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":176,"skipped":3731,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.278 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:12:56.241
    Jun 27 17:12:56.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 17:12:56.242
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:12:56.281
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:12:56.291
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 06/27/23 17:12:56.301
    Jun 27 17:12:56.353: INFO: Waiting up to 5m0s for pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27" in namespace "emptydir-9223" to be "Succeeded or Failed"
    Jun 27 17:12:56.364: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27": Phase="Pending", Reason="", readiness=false. Elapsed: 10.452027ms
    Jun 27 17:12:58.375: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021881728s
    Jun 27 17:13:00.374: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020313262s
    Jun 27 17:13:02.382: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.028226595s
    STEP: Saw pod success 06/27/23 17:13:02.382
    Jun 27 17:13:02.382: INFO: Pod "pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27" satisfied condition "Succeeded or Failed"
    Jun 27 17:13:02.392: INFO: Trying to get logs from node 10.113.180.90 pod pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27 container test-container: <nil>
    STEP: delete the pod 06/27/23 17:13:02.422
    Jun 27 17:13:02.452: INFO: Waiting for pod pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27 to disappear
    Jun 27 17:13:02.464: INFO: Pod pod-e920da71-53a0-4cbf-8bc3-d45816ad2c27 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:13:02.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-9223" for this suite. 06/27/23 17:13:02.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:13:02.519
Jun 27 17:13:02.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 17:13:02.521
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:02.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:02.567
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/27/23 17:13:02.577
Jun 27 17:13:02.626: INFO: Waiting up to 5m0s for pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3" in namespace "emptydir-4760" to be "Succeeded or Failed"
Jun 27 17:13:02.642: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.396558ms
Jun 27 17:13:04.654: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027415651s
Jun 27 17:13:06.653: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026679898s
Jun 27 17:13:08.653: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026136152s
STEP: Saw pod success 06/27/23 17:13:08.653
Jun 27 17:13:08.653: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3" satisfied condition "Succeeded or Failed"
Jun 27 17:13:08.663: INFO: Trying to get logs from node 10.113.180.90 pod pod-5d69b726-f69a-4906-b78f-d43101014bb3 container test-container: <nil>
STEP: delete the pod 06/27/23 17:13:08.718
Jun 27 17:13:08.742: INFO: Waiting for pod pod-5d69b726-f69a-4906-b78f-d43101014bb3 to disappear
Jun 27 17:13:08.751: INFO: Pod pod-5d69b726-f69a-4906-b78f-d43101014bb3 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 17:13:08.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-4760" for this suite. 06/27/23 17:13:08.78
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":177,"skipped":3739,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.279 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:13:02.519
    Jun 27 17:13:02.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 17:13:02.521
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:02.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:02.567
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/27/23 17:13:02.577
    Jun 27 17:13:02.626: INFO: Waiting up to 5m0s for pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3" in namespace "emptydir-4760" to be "Succeeded or Failed"
    Jun 27 17:13:02.642: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 15.396558ms
    Jun 27 17:13:04.654: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027415651s
    Jun 27 17:13:06.653: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3": Phase="Pending", Reason="", readiness=false. Elapsed: 4.026679898s
    Jun 27 17:13:08.653: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.026136152s
    STEP: Saw pod success 06/27/23 17:13:08.653
    Jun 27 17:13:08.653: INFO: Pod "pod-5d69b726-f69a-4906-b78f-d43101014bb3" satisfied condition "Succeeded or Failed"
    Jun 27 17:13:08.663: INFO: Trying to get logs from node 10.113.180.90 pod pod-5d69b726-f69a-4906-b78f-d43101014bb3 container test-container: <nil>
    STEP: delete the pod 06/27/23 17:13:08.718
    Jun 27 17:13:08.742: INFO: Waiting for pod pod-5d69b726-f69a-4906-b78f-d43101014bb3 to disappear
    Jun 27 17:13:08.751: INFO: Pod pod-5d69b726-f69a-4906-b78f-d43101014bb3 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:13:08.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-4760" for this suite. 06/27/23 17:13:08.78
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:13:08.8
Jun 27 17:13:08.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 17:13:08.803
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:08.833
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:08.842
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 17:13:08.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6959" for this suite. 06/27/23 17:13:08.988
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":178,"skipped":3751,"failed":0}
------------------------------
â€¢ [0.206 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:13:08.8
    Jun 27 17:13:08.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 17:13:08.803
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:08.833
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:08.842
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 17:13:08.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6959" for this suite. 06/27/23 17:13:08.988
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:13:09.008
Jun 27 17:13:09.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename deployment 06/27/23 17:13:09.01
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:09.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:09.059
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Jun 27 17:13:09.074: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Jun 27 17:13:09.095: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 27 17:13:14.107: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/27/23 17:13:14.107
Jun 27 17:13:14.107: INFO: Creating deployment "test-rolling-update-deployment"
Jun 27 17:13:14.120: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Jun 27 17:13:14.140: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Jun 27 17:13:16.163: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Jun 27 17:13:16.172: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 27 17:13:16.203: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2757  f04323ea-dc4c-485d-b350-40d1d61e3463 106294 1 2023-06-27 17:13:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-27 17:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:13:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002721338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-27 17:13:14 +0000 UTC,LastTransitionTime:2023-06-27 17:13:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-06-27 17:13:16 +0000 UTC,LastTransitionTime:2023-06-27 17:13:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 27 17:13:16.213: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2757  445ad726-f3e5-454c-bf7d-a9b055197897 106284 1 2023-06-27 17:13:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f04323ea-dc4c-485d-b350-40d1d61e3463 0xc002721827 0xc002721828}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f04323ea-dc4c-485d-b350-40d1d61e3463\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027218d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 27 17:13:16.213: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Jun 27 17:13:16.213: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2757  5225e075-53ff-4f2a-98b1-cbcbe01d80df 106293 2 2023-06-27 17:13:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f04323ea-dc4c-485d-b350-40d1d61e3463 0xc0027216e7 0xc0027216e8}] [] [{e2e.test Update apps/v1 2023-06-27 17:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f04323ea-dc4c-485d-b350-40d1d61e3463\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0027217b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 27 17:13:16.224: INFO: Pod "test-rolling-update-deployment-78f575d8ff-h6z6m" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-h6z6m test-rolling-update-deployment-78f575d8ff- deployment-2757  78d89cda-a41b-498e-9fc7-50bf3ecfbfb1 106283 0 2023-06-27 17:13:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:3f451794fad9df478478b01d9ca0603f07f9e7eb547b0c54492e99e945af5176 cni.projectcalico.org/podIP:172.30.250.196/32 cni.projectcalico.org/podIPs:172.30.250.196/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.196"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.196"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 445ad726-f3e5-454c-bf7d-a9b055197897 0xc0094dc5d7 0xc0094dc5d8}] [] [{kube-controller-manager Update v1 2023-06-27 17:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"445ad726-f3e5-454c-bf7d-a9b055197897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r7m2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r7m2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c53,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-f29r9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:13:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:13:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:13:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:13:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.196,StartTime:2023-06-27 17:13:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:13:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://4a535cfcb500b4514412d9c6a0c70dadaf1c0636a45997f9b2c416c83b835357,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 27 17:13:16.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2757" for this suite. 06/27/23 17:13:16.243
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":179,"skipped":3758,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.254 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:13:09.008
    Jun 27 17:13:09.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename deployment 06/27/23 17:13:09.01
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:09.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:09.059
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Jun 27 17:13:09.074: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Jun 27 17:13:09.095: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 27 17:13:14.107: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/27/23 17:13:14.107
    Jun 27 17:13:14.107: INFO: Creating deployment "test-rolling-update-deployment"
    Jun 27 17:13:14.120: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Jun 27 17:13:14.140: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Jun 27 17:13:16.163: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Jun 27 17:13:16.172: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 27 17:13:16.203: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2757  f04323ea-dc4c-485d-b350-40d1d61e3463 106294 1 2023-06-27 17:13:14 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-06-27 17:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:13:16 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002721338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-06-27 17:13:14 +0000 UTC,LastTransitionTime:2023-06-27 17:13:14 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-06-27 17:13:16 +0000 UTC,LastTransitionTime:2023-06-27 17:13:14 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 27 17:13:16.213: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-2757  445ad726-f3e5-454c-bf7d-a9b055197897 106284 1 2023-06-27 17:13:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment f04323ea-dc4c-485d-b350-40d1d61e3463 0xc002721827 0xc002721828}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f04323ea-dc4c-485d-b350-40d1d61e3463\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0027218d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 17:13:16.213: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Jun 27 17:13:16.213: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2757  5225e075-53ff-4f2a-98b1-cbcbe01d80df 106293 2 2023-06-27 17:13:09 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment f04323ea-dc4c-485d-b350-40d1d61e3463 0xc0027216e7 0xc0027216e8}] [] [{e2e.test Update apps/v1 2023-06-27 17:13:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f04323ea-dc4c-485d-b350-40d1d61e3463\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0027217b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 17:13:16.224: INFO: Pod "test-rolling-update-deployment-78f575d8ff-h6z6m" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-h6z6m test-rolling-update-deployment-78f575d8ff- deployment-2757  78d89cda-a41b-498e-9fc7-50bf3ecfbfb1 106283 0 2023-06-27 17:13:14 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[cni.projectcalico.org/containerID:3f451794fad9df478478b01d9ca0603f07f9e7eb547b0c54492e99e945af5176 cni.projectcalico.org/podIP:172.30.250.196/32 cni.projectcalico.org/podIPs:172.30.250.196/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.196"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.196"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff 445ad726-f3e5-454c-bf7d-a9b055197897 0xc0094dc5d7 0xc0094dc5d8}] [] [{kube-controller-manager Update v1 2023-06-27 17:13:14 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"445ad726-f3e5-454c-bf7d-a9b055197897\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {multus Update v1 2023-06-27 17:13:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5r7m2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5r7m2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c53,c2,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-f29r9,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:13:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:13:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:13:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:13:14 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.196,StartTime:2023-06-27 17:13:14 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:13:15 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787,ContainerID:cri-o://4a535cfcb500b4514412d9c6a0c70dadaf1c0636a45997f9b2c416c83b835357,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.196,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 27 17:13:16.224: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2757" for this suite. 06/27/23 17:13:16.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:13:16.27
Jun 27 17:13:16.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:13:16.273
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:16.31
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:16.321
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-6e5cb098-97a9-4baf-8dde-6ff172106d7b 06/27/23 17:13:16.341
STEP: Creating a pod to test consume secrets 06/27/23 17:13:16.356
Jun 27 17:13:16.399: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4" in namespace "projected-8549" to be "Succeeded or Failed"
Jun 27 17:13:16.409: INFO: Pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.59159ms
Jun 27 17:13:18.422: INFO: Pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022077863s
Jun 27 17:13:20.421: INFO: Pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021521904s
STEP: Saw pod success 06/27/23 17:13:20.421
Jun 27 17:13:20.421: INFO: Pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4" satisfied condition "Succeeded or Failed"
Jun 27 17:13:20.432: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/27/23 17:13:20.459
Jun 27 17:13:20.487: INFO: Waiting for pod pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4 to disappear
Jun 27 17:13:20.503: INFO: Pod pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 27 17:13:20.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8549" for this suite. 06/27/23 17:13:20.521
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":180,"skipped":3776,"failed":0}
------------------------------
â€¢ [4.268 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:13:16.27
    Jun 27 17:13:16.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:13:16.273
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:16.31
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:16.321
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-6e5cb098-97a9-4baf-8dde-6ff172106d7b 06/27/23 17:13:16.341
    STEP: Creating a pod to test consume secrets 06/27/23 17:13:16.356
    Jun 27 17:13:16.399: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4" in namespace "projected-8549" to be "Succeeded or Failed"
    Jun 27 17:13:16.409: INFO: Pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4": Phase="Pending", Reason="", readiness=false. Elapsed: 9.59159ms
    Jun 27 17:13:18.422: INFO: Pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022077863s
    Jun 27 17:13:20.421: INFO: Pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.021521904s
    STEP: Saw pod success 06/27/23 17:13:20.421
    Jun 27 17:13:20.421: INFO: Pod "pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4" satisfied condition "Succeeded or Failed"
    Jun 27 17:13:20.432: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:13:20.459
    Jun 27 17:13:20.487: INFO: Waiting for pod pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4 to disappear
    Jun 27 17:13:20.503: INFO: Pod pod-projected-secrets-7a99d8d7-234b-4f23-9da0-e5adea3103f4 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 27 17:13:20.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8549" for this suite. 06/27/23 17:13:20.521
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:13:20.543
Jun 27 17:13:20.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename tables 06/27/23 17:13:20.545
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:20.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:20.589
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Jun 27 17:13:20.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-3924" for this suite. 06/27/23 17:13:20.63
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":181,"skipped":3778,"failed":0}
------------------------------
â€¢ [0.128 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:13:20.543
    Jun 27 17:13:20.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename tables 06/27/23 17:13:20.545
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:20.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:20.589
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Jun 27 17:13:20.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-3924" for this suite. 06/27/23 17:13:20.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:13:20.676
Jun 27 17:13:20.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 17:13:20.677
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:20.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:20.77
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-3bc720a6-81d9-482b-944d-d3bcf81b0228 06/27/23 17:13:20.785
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 17:13:20.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1140" for this suite. 06/27/23 17:13:20.805
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":182,"skipped":3794,"failed":0}
------------------------------
â€¢ [0.150 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:13:20.676
    Jun 27 17:13:20.676: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 17:13:20.677
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:20.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:20.77
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-3bc720a6-81d9-482b-944d-d3bcf81b0228 06/27/23 17:13:20.785
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 17:13:20.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1140" for this suite. 06/27/23 17:13:20.805
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:13:20.826
Jun 27 17:13:20.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replication-controller 06/27/23 17:13:20.828
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:20.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:20.875
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 06/27/23 17:13:20.888
STEP: When the matched label of one of its pods change 06/27/23 17:13:20.916
Jun 27 17:13:20.959: INFO: Pod name pod-release: Found 0 pods out of 1
Jun 27 17:13:25.977: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 06/27/23 17:13:26.027
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 27 17:13:26.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-4151" for this suite. 06/27/23 17:13:26.062
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":183,"skipped":3796,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.256 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:13:20.826
    Jun 27 17:13:20.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replication-controller 06/27/23 17:13:20.828
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:20.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:20.875
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 06/27/23 17:13:20.888
    STEP: When the matched label of one of its pods change 06/27/23 17:13:20.916
    Jun 27 17:13:20.959: INFO: Pod name pod-release: Found 0 pods out of 1
    Jun 27 17:13:25.977: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/27/23 17:13:26.027
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 27 17:13:26.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-4151" for this suite. 06/27/23 17:13:26.062
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:13:26.083
Jun 27 17:13:26.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename namespaces 06/27/23 17:13:26.085
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:26.123
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:26.133
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 06/27/23 17:13:26.145
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:26.185
STEP: Creating a pod in the namespace 06/27/23 17:13:26.197
STEP: Waiting for the pod to have running status 06/27/23 17:13:26.251
Jun 27 17:13:26.252: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5710" to be "running"
Jun 27 17:13:26.281: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 28.969447ms
Jun 27 17:13:28.294: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.042292672s
Jun 27 17:13:28.294: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 06/27/23 17:13:28.294
STEP: Waiting for the namespace to be removed. 06/27/23 17:13:28.318
STEP: Recreating the namespace 06/27/23 17:13:42.338
STEP: Verifying there are no pods in the namespace 06/27/23 17:13:42.374
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:13:42.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7717" for this suite. 06/27/23 17:13:42.443
STEP: Destroying namespace "nsdeletetest-5710" for this suite. 06/27/23 17:13:42.484
Jun 27 17:13:42.495: INFO: Namespace nsdeletetest-5710 was already deleted
STEP: Destroying namespace "nsdeletetest-3595" for this suite. 06/27/23 17:13:42.495
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":184,"skipped":3803,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.434 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:13:26.083
    Jun 27 17:13:26.084: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename namespaces 06/27/23 17:13:26.085
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:26.123
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:26.133
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 06/27/23 17:13:26.145
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:26.185
    STEP: Creating a pod in the namespace 06/27/23 17:13:26.197
    STEP: Waiting for the pod to have running status 06/27/23 17:13:26.251
    Jun 27 17:13:26.252: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-5710" to be "running"
    Jun 27 17:13:26.281: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 28.969447ms
    Jun 27 17:13:28.294: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.042292672s
    Jun 27 17:13:28.294: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 06/27/23 17:13:28.294
    STEP: Waiting for the namespace to be removed. 06/27/23 17:13:28.318
    STEP: Recreating the namespace 06/27/23 17:13:42.338
    STEP: Verifying there are no pods in the namespace 06/27/23 17:13:42.374
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:13:42.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7717" for this suite. 06/27/23 17:13:42.443
    STEP: Destroying namespace "nsdeletetest-5710" for this suite. 06/27/23 17:13:42.484
    Jun 27 17:13:42.495: INFO: Namespace nsdeletetest-5710 was already deleted
    STEP: Destroying namespace "nsdeletetest-3595" for this suite. 06/27/23 17:13:42.495
  << End Captured GinkgoWriter Output
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:13:42.518
Jun 27 17:13:42.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-pred 06/27/23 17:13:42.521
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:42.556
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:42.57
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 27 17:13:42.584: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 27 17:13:42.613: INFO: Waiting for terminating namespaces to be deleted...
Jun 27 17:13:42.636: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.89 before test
Jun 27 17:13:42.691: INFO: calico-node-vplt6 from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.691: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 17:13:42.691: INFO: calico-typha-657df678d7-65kdh from calico-system started at 2023-06-27 13:54:48 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.691: INFO: 	Container calico-typha ready: true, restart count 0
Jun 27 17:13:42.691: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.692: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
Jun 27 17:13:42.692: INFO: ibm-keepalived-watcher-9rf8d from kube-system started at 2023-06-27 13:53:32 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.692: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 17:13:42.692: INFO: ibm-master-proxy-static-10.113.180.89 from kube-system started at 2023-06-27 13:53:30 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.692: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 17:13:42.692: INFO: 	Container pause ready: true, restart count 0
Jun 27 17:13:42.692: INFO: ibmcloud-block-storage-driver-5ml2s from kube-system started at 2023-06-27 13:53:43 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.692: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 17:13:42.692: INFO: vpn-546f6bf578-7ks4f from kube-system started at 2023-06-27 14:06:39 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.692: INFO: 	Container vpn ready: true, restart count 0
Jun 27 17:13:42.692: INFO: tuned-mwm8w from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.692: INFO: 	Container tuned ready: true, restart count 0
Jun 27 17:13:42.692: INFO: csi-snapshot-controller-5694c47cbb-9pvhd from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.692: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 27 17:13:42.693: INFO: csi-snapshot-webhook-5d7cc7f6cb-lwgl9 from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.693: INFO: 	Container webhook ready: true, restart count 0
Jun 27 17:13:42.693: INFO: console-6c8658586b-jlcfh from openshift-console started at 2023-06-27 14:03:39 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.693: INFO: 	Container console ready: true, restart count 0
Jun 27 17:13:42.693: INFO: downloads-57bd479866-2fpqm from openshift-console started at 2023-06-27 13:56:16 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.693: INFO: 	Container download-server ready: true, restart count 0
Jun 27 17:13:42.693: INFO: dns-default-z76hf from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.693: INFO: 	Container dns ready: true, restart count 0
Jun 27 17:13:42.693: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.693: INFO: node-resolver-cwltw from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.693: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 17:13:42.693: INFO: image-registry-7f546fc5bb-n7w67 from openshift-image-registry started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.693: INFO: 	Container registry ready: true, restart count 0
Jun 27 17:13:42.693: INFO: node-ca-d4q6s from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.693: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 17:13:42.694: INFO: registry-pvc-permissions-4bk2b from openshift-image-registry started at 2023-06-27 14:02:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.694: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 27 17:13:42.694: INFO: ingress-canary-bmwtq from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.694: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 17:13:42.694: INFO: router-default-7f97cd5c5f-m5pcx from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.694: INFO: 	Container router ready: true, restart count 0
Jun 27 17:13:42.694: INFO: openshift-kube-proxy-b4224 from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.694: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 17:13:42.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.694: INFO: certified-operators-799hx from openshift-marketplace started at 2023-06-27 16:44:15 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.694: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 17:13:42.694: INFO: community-operators-9jptm from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.694: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 17:13:42.694: INFO: redhat-marketplace-qzk6b from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.694: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 17:13:42.694: INFO: redhat-operators-jqjb9 from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.694: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 17:13:42.695: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-06-27 14:02:16 +0000 UTC (6 container statuses recorded)
Jun 27 17:13:42.695: INFO: 	Container alertmanager ready: true, restart count 1
Jun 27 17:13:42.695: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 27 17:13:42.695: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jun 27 17:13:42.695: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 17:13:42.695: INFO: kube-state-metrics-685444f59b-5dlq4 from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 27 17:13:42.695: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 27 17:13:42.695: INFO: node-exporter-lqlws from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.695: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 17:13:42.695: INFO: openshift-state-metrics-6d79d8c586-9mgzr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 27 17:13:42.695: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 27 17:13:42.695: INFO: prometheus-adapter-648f68fcc-h27rt from openshift-monitoring started at 2023-06-27 14:01:39 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.696: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 27 17:13:42.696: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-06-27 14:01:59 +0000 UTC (6 container statuses recorded)
Jun 27 17:13:42.696: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 17:13:42.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.696: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 27 17:13:42.696: INFO: 	Container prometheus ready: true, restart count 0
Jun 27 17:13:42.696: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 27 17:13:42.696: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 27 17:13:42.696: INFO: prometheus-operator-68dfcc5c8-kn6tn from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.696: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 27 17:13:42.696: INFO: prometheus-operator-admission-webhook-6c667b594b-bds22 from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.696: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jun 27 17:13:42.696: INFO: telemeter-client-7dc8fdddc8-kpccw from openshift-monitoring started at 2023-06-27 14:00:33 +0000 UTC (3 container statuses recorded)
Jun 27 17:13:42.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.697: INFO: 	Container reload ready: true, restart count 0
Jun 27 17:13:42.697: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 27 17:13:42.697: INFO: thanos-querier-754f675f77-6nf9l from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
Jun 27 17:13:42.697: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.697: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jun 27 17:13:42.697: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 27 17:13:42.697: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 27 17:13:42.697: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 17:13:42.697: INFO: 	Container thanos-query ready: true, restart count 0
Jun 27 17:13:42.697: INFO: multus-7zznf from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.697: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 17:13:42.697: INFO: multus-additional-cni-plugins-pn2hb from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.697: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 17:13:42.697: INFO: multus-admission-controller-754d449d79-strmv from openshift-multus started at 2023-06-27 16:44:18 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.697: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.697: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 27 17:13:42.697: INFO: network-metrics-daemon-p8dh4 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.698: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 17:13:42.698: INFO: network-check-target-7qghz from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.698: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 17:13:42.698: INFO: packageserver-8d78bf5dd-q8ct5 from openshift-operator-lifecycle-manager started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.698: INFO: 	Container packageserver ready: true, restart count 0
Jun 27 17:13:42.698: INFO: sonobuoy from sonobuoy started at 2023-06-27 16:21:42 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.698: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 27 17:13:42.698: INFO: sonobuoy-e2e-job-0e516cd149fa4871 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.698: INFO: 	Container e2e ready: true, restart count 0
Jun 27 17:13:42.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 17:13:42.698: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 17:13:42.698: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 27 17:13:42.698: INFO: tigera-operator-687c49f5c8-qcfmm from tigera-operator started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.698: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 27 17:13:42.698: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.90 before test
Jun 27 17:13:42.731: INFO: calico-node-6j4xp from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 17:13:42.731: INFO: ibm-keepalived-watcher-bpqvm from kube-system started at 2023-06-27 13:53:26 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 17:13:42.731: INFO: ibm-master-proxy-static-10.113.180.90 from kube-system started at 2023-06-27 13:53:22 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 17:13:42.731: INFO: 	Container pause ready: true, restart count 0
Jun 27 17:13:42.731: INFO: ibmcloud-block-storage-driver-jb9wr from kube-system started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 17:13:42.731: INFO: tuned-md8mr from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container tuned ready: true, restart count 0
Jun 27 17:13:42.731: INFO: dns-default-q6q9l from openshift-dns started at 2023-06-27 16:44:33 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container dns ready: true, restart count 0
Jun 27 17:13:42.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.731: INFO: node-resolver-dtvx7 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 17:13:42.731: INFO: node-ca-8pgp8 from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 17:13:42.731: INFO: ingress-canary-hfzkr from openshift-ingress-canary started at 2023-06-27 16:44:33 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 17:13:42.731: INFO: openshift-kube-proxy-bf9cj from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 17:13:42.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.731: INFO: node-exporter-5vgpm from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.731: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 17:13:42.731: INFO: multus-additional-cni-plugins-qszdk from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 17:13:42.731: INFO: multus-skrpb from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 17:13:42.731: INFO: network-metrics-daemon-4jmw9 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.731: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 17:13:42.731: INFO: network-check-target-zmtm2 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 17:13:42.731: INFO: collect-profiles-28131405-8w5hd from openshift-operator-lifecycle-manager started at 2023-06-27 16:45:00 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 27 17:13:42.731: INFO: collect-profiles-28131420-v25h5 from openshift-operator-lifecycle-manager started at 2023-06-27 17:00:00 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 27 17:13:42.731: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 17:13:42.731: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 27 17:13:42.731: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.96 before test
Jun 27 17:13:42.778: INFO: calico-kube-controllers-79f474fb8-m45qw from calico-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.778: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 27 17:13:42.778: INFO: calico-node-z28vk from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.778: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 17:13:42.778: INFO: calico-typha-657df678d7-4ch2t from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.778: INFO: 	Container calico-typha ready: true, restart count 0
Jun 27 17:13:42.778: INFO: managed-storage-validation-webhooks-665764b6b4-r77pv from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.778: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 17:13:42.778: INFO: managed-storage-validation-webhooks-665764b6b4-v4g8s from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.778: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 17:13:42.778: INFO: managed-storage-validation-webhooks-665764b6b4-vhqr7 from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.778: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 17:13:42.778: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-tnr88 from ibm-system started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.778: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
Jun 27 17:13:42.778: INFO: ibm-file-plugin-84cc66cdb-97nzr from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.778: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 27 17:13:42.778: INFO: ibm-keepalived-watcher-h8p9q from kube-system started at 2023-06-27 13:53:48 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 17:13:42.779: INFO: ibm-master-proxy-static-10.113.180.96 from kube-system started at 2023-06-27 13:53:45 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 17:13:42.779: INFO: 	Container pause ready: true, restart count 0
Jun 27 17:13:42.779: INFO: ibm-storage-metrics-agent-746d76fb56-nvvw9 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Jun 27 17:13:42.779: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Jun 27 17:13:42.779: INFO: ibm-storage-watcher-676b4ddbc6-dz54r from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 27 17:13:42.779: INFO: ibmcloud-block-storage-driver-pkhxd from kube-system started at 2023-06-27 13:53:53 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 17:13:42.779: INFO: ibmcloud-block-storage-plugin-5f677b8577-8f7s7 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 27 17:13:42.779: INFO: cluster-node-tuning-operator-848d57c5bb-7ln5t from openshift-cluster-node-tuning-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 27 17:13:42.779: INFO: tuned-kk4gn from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container tuned ready: true, restart count 0
Jun 27 17:13:42.779: INFO: cluster-samples-operator-769ddfc8cc-88z62 from openshift-cluster-samples-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 27 17:13:42.779: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 27 17:13:42.779: INFO: cluster-storage-operator-5fc995c76b-jhmzr from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 27 17:13:42.779: INFO: csi-snapshot-controller-5694c47cbb-qfvbs from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 27 17:13:42.779: INFO: csi-snapshot-controller-operator-64ff6856c8-l29px from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 27 17:13:42.779: INFO: csi-snapshot-webhook-5d7cc7f6cb-gzt6x from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container webhook ready: true, restart count 0
Jun 27 17:13:42.779: INFO: console-operator-5694fff656-5wp7b from openshift-console-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container console-operator ready: true, restart count 1
Jun 27 17:13:42.779: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Jun 27 17:13:42.779: INFO: console-6c8658586b-rc55z from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.779: INFO: 	Container console ready: true, restart count 0
Jun 27 17:13:42.780: INFO: downloads-57bd479866-8bj2l from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container download-server ready: true, restart count 0
Jun 27 17:13:42.780: INFO: dns-operator-b64d4b4c7-5p5hs from openshift-dns-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container dns-operator ready: true, restart count 0
Jun 27 17:13:42.780: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.780: INFO: dns-default-q8pl2 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container dns ready: true, restart count 0
Jun 27 17:13:42.780: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.780: INFO: node-resolver-nbx64 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 17:13:42.780: INFO: cluster-image-registry-operator-55bdbc85cd-vpldl from openshift-image-registry started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 27 17:13:42.780: INFO: node-ca-tbfkf from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 17:13:42.780: INFO: ingress-canary-mtqfl from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 17:13:42.780: INFO: ingress-operator-5c457b57b4-67rvh from openshift-ingress-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 27 17:13:42.780: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.780: INFO: router-default-7f97cd5c5f-n56jj from openshift-ingress started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container router ready: true, restart count 0
Jun 27 17:13:42.780: INFO: insights-operator-8f4d6c949-n2nlt from openshift-insights started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container insights-operator ready: true, restart count 1
Jun 27 17:13:42.780: INFO: openshift-kube-proxy-mpv6g from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 17:13:42.780: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.780: INFO: kube-storage-version-migrator-operator-59f4c7c696-jq8gm from openshift-kube-storage-version-migrator-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 27 17:13:42.780: INFO: migrator-6795cdbdb7-qsfqv from openshift-kube-storage-version-migrator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container migrator ready: true, restart count 0
Jun 27 17:13:42.780: INFO: marketplace-operator-57bd87cfb8-5hhm8 from openshift-marketplace started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 27 17:13:42.780: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-06-27 16:44:14 +0000 UTC (6 container statuses recorded)
Jun 27 17:13:42.780: INFO: 	Container alertmanager ready: true, restart count 1
Jun 27 17:13:42.781: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: cluster-monitoring-operator-644d445879-l5jj8 from openshift-monitoring started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.781: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: node-exporter-l5dxr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 17:13:42.781: INFO: prometheus-adapter-648f68fcc-mpvkt from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.781: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 27 17:13:42.781: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-06-27 16:44:13 +0000 UTC (6 container statuses recorded)
Jun 27 17:13:42.781: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container prometheus ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 27 17:13:42.781: INFO: prometheus-operator-admission-webhook-6c667b594b-c9dw4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.781: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jun 27 17:13:42.781: INFO: thanos-querier-754f675f77-gs6k4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (6 container statuses recorded)
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container thanos-query ready: true, restart count 0
Jun 27 17:13:42.781: INFO: multus-additional-cni-plugins-xqsgs from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.781: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 17:13:42.781: INFO: multus-admission-controller-754d449d79-6cbgx from openshift-multus started at 2023-06-27 16:44:27 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.781: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 27 17:13:42.781: INFO: multus-kx7qm from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.781: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 17:13:42.782: INFO: network-metrics-daemon-cbrq2 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:13:42.782: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 17:13:42.782: INFO: network-check-source-6b8766f86f-s8s7j from openshift-network-diagnostics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 27 17:13:42.782: INFO: network-check-target-6lv98 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 17:13:42.782: INFO: network-operator-ffb9884c5-25r42 from openshift-network-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container network-operator ready: true, restart count 0
Jun 27 17:13:42.782: INFO: catalog-operator-58898485cd-qjmms from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 27 17:13:42.782: INFO: olm-operator-7959cfcd9f-dt77g from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container olm-operator ready: true, restart count 0
Jun 27 17:13:42.782: INFO: package-server-manager-f7b6c466f-9bbzw from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container package-server-manager ready: true, restart count 0
Jun 27 17:13:42.782: INFO: packageserver-8d78bf5dd-r9g9t from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container packageserver ready: true, restart count 0
Jun 27 17:13:42.782: INFO: metrics-6d46d44d8f-jbnwq from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container metrics ready: true, restart count 3
Jun 27 17:13:42.782: INFO: push-gateway-5465b544cc-f8v8q from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container push-gateway ready: true, restart count 0
Jun 27 17:13:42.782: INFO: service-ca-operator-74dd8bfc8-cl4bp from openshift-service-ca-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 27 17:13:42.782: INFO: service-ca-6f86485857-drj9g from openshift-service-ca started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container service-ca-controller ready: false, restart count 0
Jun 27 17:13:42.782: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 17:13:42.782: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 17:13:42.782: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/27/23 17:13:42.782
Jun 27 17:13:42.825: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5057" to be "running"
Jun 27 17:13:42.835: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.16286ms
Jun 27 17:13:44.846: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021518102s
Jun 27 17:13:46.847: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.022421994s
Jun 27 17:13:46.847: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/27/23 17:13:46.858
STEP: Trying to apply a random label on the found node. 06/27/23 17:13:46.907
STEP: verifying the node has the label kubernetes.io/e2e-475eb746-5f89-4364-9ae2-535ea306403f 95 06/27/23 17:13:46.931
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/27/23 17:13:46.945
Jun 27 17:13:46.978: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5057" to be "not pending"
Jun 27 17:13:46.988: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.965374ms
Jun 27 17:13:49.003: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02454307s
Jun 27 17:13:50.999: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.020151542s
Jun 27 17:13:50.999: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.113.180.90 on the node which pod4 resides and expect not scheduled 06/27/23 17:13:50.999
Jun 27 17:13:51.033: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5057" to be "not pending"
Jun 27 17:13:51.043: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.187852ms
Jun 27 17:13:53.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022021233s
Jun 27 17:13:55.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020385697s
Jun 27 17:13:57.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021561893s
Jun 27 17:13:59.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022048225s
Jun 27 17:14:01.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02106027s
Jun 27 17:14:03.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02157031s
Jun 27 17:14:05.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.020797831s
Jun 27 17:14:07.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024934719s
Jun 27 17:14:09.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022512087s
Jun 27 17:14:11.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020393968s
Jun 27 17:14:13.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.022902481s
Jun 27 17:14:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.021732431s
Jun 27 17:14:17.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023299834s
Jun 27 17:14:19.070: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.036537779s
Jun 27 17:14:21.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.019788407s
Jun 27 17:14:23.060: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.026549021s
Jun 27 17:14:25.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.022385778s
Jun 27 17:14:27.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.023667317s
Jun 27 17:14:29.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.022368771s
Jun 27 17:14:31.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022104538s
Jun 27 17:14:33.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.022351317s
Jun 27 17:14:35.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.022760179s
Jun 27 17:14:37.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.021292097s
Jun 27 17:14:39.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024736873s
Jun 27 17:14:41.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.025017522s
Jun 27 17:14:43.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021547966s
Jun 27 17:14:45.061: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.02757182s
Jun 27 17:14:47.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021541852s
Jun 27 17:14:49.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.02125782s
Jun 27 17:14:51.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.022195974s
Jun 27 17:14:53.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.02038205s
Jun 27 17:14:55.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023891567s
Jun 27 17:14:57.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.025029612s
Jun 27 17:14:59.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.022401983s
Jun 27 17:15:01.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022158525s
Jun 27 17:15:03.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.021317517s
Jun 27 17:15:05.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.020052262s
Jun 27 17:15:07.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.021440663s
Jun 27 17:15:09.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.02587109s
Jun 27 17:15:11.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.021755397s
Jun 27 17:15:13.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.021809817s
Jun 27 17:15:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.021540926s
Jun 27 17:15:17.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.02304684s
Jun 27 17:15:19.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.022008195s
Jun 27 17:15:21.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021514461s
Jun 27 17:15:23.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.021493498s
Jun 27 17:15:25.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021257981s
Jun 27 17:15:27.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.021768522s
Jun 27 17:15:29.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025342128s
Jun 27 17:15:31.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.021242652s
Jun 27 17:15:33.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.021230794s
Jun 27 17:15:35.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.022563012s
Jun 27 17:15:37.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.02405078s
Jun 27 17:15:39.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.021293162s
Jun 27 17:15:41.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.046911626s
Jun 27 17:15:43.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020894002s
Jun 27 17:15:45.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.02172298s
Jun 27 17:15:47.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022548565s
Jun 27 17:15:49.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.022427312s
Jun 27 17:15:51.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022391798s
Jun 27 17:15:53.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.023111799s
Jun 27 17:15:55.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.022651681s
Jun 27 17:15:57.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.0230394s
Jun 27 17:15:59.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.021301624s
Jun 27 17:16:01.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.029134468s
Jun 27 17:16:03.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.022475939s
Jun 27 17:16:05.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.022139777s
Jun 27 17:16:07.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.022531488s
Jun 27 17:16:09.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.021337691s
Jun 27 17:16:11.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.020801444s
Jun 27 17:16:13.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.02193758s
Jun 27 17:16:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.021796098s
Jun 27 17:16:17.085: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.052150607s
Jun 27 17:16:19.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.020765987s
Jun 27 17:16:21.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.022796661s
Jun 27 17:16:23.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.021545822s
Jun 27 17:16:25.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.021848928s
Jun 27 17:16:27.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.022392539s
Jun 27 17:16:29.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.026255242s
Jun 27 17:16:31.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.022208037s
Jun 27 17:16:33.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.026179983s
Jun 27 17:16:35.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.022289784s
Jun 27 17:16:37.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.021476683s
Jun 27 17:16:39.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.022555821s
Jun 27 17:16:41.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.021035487s
Jun 27 17:16:43.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.030015778s
Jun 27 17:16:45.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.021916919s
Jun 27 17:16:47.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.021837448s
Jun 27 17:16:49.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.024278406s
Jun 27 17:16:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.020901956s
Jun 27 17:16:53.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.021834556s
Jun 27 17:16:55.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.022498288s
Jun 27 17:16:57.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.022212471s
Jun 27 17:16:59.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.020807066s
Jun 27 17:17:01.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.022170578s
Jun 27 17:17:03.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.021362027s
Jun 27 17:17:05.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.024275349s
Jun 27 17:17:07.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.021139835s
Jun 27 17:17:09.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.023994845s
Jun 27 17:17:11.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.021405004s
Jun 27 17:17:13.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.021555335s
Jun 27 17:17:15.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.025083364s
Jun 27 17:17:17.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.020816702s
Jun 27 17:17:19.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.023359784s
Jun 27 17:17:21.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.019858979s
Jun 27 17:17:23.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.022035651s
Jun 27 17:17:25.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.032738512s
Jun 27 17:17:27.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.025156331s
Jun 27 17:17:29.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.022445421s
Jun 27 17:17:31.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.020956712s
Jun 27 17:17:33.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.021864132s
Jun 27 17:17:35.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.022162848s
Jun 27 17:17:37.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.02242758s
Jun 27 17:17:39.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.022757514s
Jun 27 17:17:41.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.028496042s
Jun 27 17:17:43.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.021785283s
Jun 27 17:17:45.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.022394311s
Jun 27 17:17:47.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.020821401s
Jun 27 17:17:49.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.026299999s
Jun 27 17:17:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.020689195s
Jun 27 17:17:53.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.020262865s
Jun 27 17:17:55.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.022144831s
Jun 27 17:17:57.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.025897846s
Jun 27 17:17:59.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.021868017s
Jun 27 17:18:01.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.021502434s
Jun 27 17:18:03.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.026247884s
Jun 27 17:18:05.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.020438851s
Jun 27 17:18:07.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.022292577s
Jun 27 17:18:09.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.023168579s
Jun 27 17:18:11.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.020682762s
Jun 27 17:18:13.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.023097117s
Jun 27 17:18:15.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.020593691s
Jun 27 17:18:17.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.022250678s
Jun 27 17:18:19.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.022597965s
Jun 27 17:18:21.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.020918984s
Jun 27 17:18:23.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.022927541s
Jun 27 17:18:25.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.02187936s
Jun 27 17:18:27.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.021663644s
Jun 27 17:18:29.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.022041197s
Jun 27 17:18:31.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.020877957s
Jun 27 17:18:33.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.021183702s
Jun 27 17:18:35.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.023061803s
Jun 27 17:18:37.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.023910866s
Jun 27 17:18:39.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.021414499s
Jun 27 17:18:41.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.020991397s
Jun 27 17:18:43.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.022016919s
Jun 27 17:18:45.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.022394464s
Jun 27 17:18:47.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.023226387s
Jun 27 17:18:49.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.030114121s
Jun 27 17:18:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020804926s
Jun 27 17:18:51.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.030483318s
STEP: removing the label kubernetes.io/e2e-475eb746-5f89-4364-9ae2-535ea306403f off the node 10.113.180.90 06/27/23 17:18:51.064
STEP: verifying the node doesn't have the label kubernetes.io/e2e-475eb746-5f89-4364-9ae2-535ea306403f 06/27/23 17:18:51.102
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:18:51.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5057" for this suite. 06/27/23 17:18:51.135
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":185,"skipped":3803,"failed":0}
------------------------------
â€¢ [SLOW TEST] [308.636 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:13:42.518
    Jun 27 17:13:42.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-pred 06/27/23 17:13:42.521
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:13:42.556
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:13:42.57
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 27 17:13:42.584: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 27 17:13:42.613: INFO: Waiting for terminating namespaces to be deleted...
    Jun 27 17:13:42.636: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.89 before test
    Jun 27 17:13:42.691: INFO: calico-node-vplt6 from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.691: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 17:13:42.691: INFO: calico-typha-657df678d7-65kdh from calico-system started at 2023-06-27 13:54:48 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.691: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 27 17:13:42.691: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.692: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
    Jun 27 17:13:42.692: INFO: ibm-keepalived-watcher-9rf8d from kube-system started at 2023-06-27 13:53:32 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.692: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 17:13:42.692: INFO: ibm-master-proxy-static-10.113.180.89 from kube-system started at 2023-06-27 13:53:30 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.692: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 17:13:42.692: INFO: 	Container pause ready: true, restart count 0
    Jun 27 17:13:42.692: INFO: ibmcloud-block-storage-driver-5ml2s from kube-system started at 2023-06-27 13:53:43 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.692: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 17:13:42.692: INFO: vpn-546f6bf578-7ks4f from kube-system started at 2023-06-27 14:06:39 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.692: INFO: 	Container vpn ready: true, restart count 0
    Jun 27 17:13:42.692: INFO: tuned-mwm8w from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.692: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 17:13:42.692: INFO: csi-snapshot-controller-5694c47cbb-9pvhd from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.692: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 27 17:13:42.693: INFO: csi-snapshot-webhook-5d7cc7f6cb-lwgl9 from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.693: INFO: 	Container webhook ready: true, restart count 0
    Jun 27 17:13:42.693: INFO: console-6c8658586b-jlcfh from openshift-console started at 2023-06-27 14:03:39 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.693: INFO: 	Container console ready: true, restart count 0
    Jun 27 17:13:42.693: INFO: downloads-57bd479866-2fpqm from openshift-console started at 2023-06-27 13:56:16 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.693: INFO: 	Container download-server ready: true, restart count 0
    Jun 27 17:13:42.693: INFO: dns-default-z76hf from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.693: INFO: 	Container dns ready: true, restart count 0
    Jun 27 17:13:42.693: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.693: INFO: node-resolver-cwltw from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.693: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 17:13:42.693: INFO: image-registry-7f546fc5bb-n7w67 from openshift-image-registry started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.693: INFO: 	Container registry ready: true, restart count 0
    Jun 27 17:13:42.693: INFO: node-ca-d4q6s from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.693: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 17:13:42.694: INFO: registry-pvc-permissions-4bk2b from openshift-image-registry started at 2023-06-27 14:02:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.694: INFO: 	Container pvc-permissions ready: false, restart count 0
    Jun 27 17:13:42.694: INFO: ingress-canary-bmwtq from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.694: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 17:13:42.694: INFO: router-default-7f97cd5c5f-m5pcx from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.694: INFO: 	Container router ready: true, restart count 0
    Jun 27 17:13:42.694: INFO: openshift-kube-proxy-b4224 from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.694: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 17:13:42.694: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.694: INFO: certified-operators-799hx from openshift-marketplace started at 2023-06-27 16:44:15 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.694: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 17:13:42.694: INFO: community-operators-9jptm from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.694: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 17:13:42.694: INFO: redhat-marketplace-qzk6b from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.694: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 17:13:42.694: INFO: redhat-operators-jqjb9 from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.694: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-06-27 14:02:16 +0000 UTC (6 container statuses recorded)
    Jun 27 17:13:42.695: INFO: 	Container alertmanager ready: true, restart count 1
    Jun 27 17:13:42.695: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: kube-state-metrics-685444f59b-5dlq4 from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
    Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: node-exporter-lqlws from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: openshift-state-metrics-6d79d8c586-9mgzr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
    Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jun 27 17:13:42.695: INFO: prometheus-adapter-648f68fcc-h27rt from openshift-monitoring started at 2023-06-27 14:01:39 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.696: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-06-27 14:01:59 +0000 UTC (6 container statuses recorded)
    Jun 27 17:13:42.696: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: 	Container prometheus ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: prometheus-operator-68dfcc5c8-kn6tn from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: prometheus-operator-admission-webhook-6c667b594b-bds22 from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.696: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jun 27 17:13:42.696: INFO: telemeter-client-7dc8fdddc8-kpccw from openshift-monitoring started at 2023-06-27 14:00:33 +0000 UTC (3 container statuses recorded)
    Jun 27 17:13:42.696: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: 	Container reload ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: 	Container telemeter-client ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: thanos-querier-754f675f77-6nf9l from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
    Jun 27 17:13:42.697: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: 	Container thanos-query ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: multus-7zznf from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.697: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: multus-additional-cni-plugins-pn2hb from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.697: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: multus-admission-controller-754d449d79-strmv from openshift-multus started at 2023-06-27 16:44:18 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.697: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Jun 27 17:13:42.697: INFO: network-metrics-daemon-p8dh4 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.698: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: network-check-target-7qghz from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.698: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: packageserver-8d78bf5dd-q8ct5 from openshift-operator-lifecycle-manager started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.698: INFO: 	Container packageserver ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: sonobuoy from sonobuoy started at 2023-06-27 16:21:42 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.698: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: sonobuoy-e2e-job-0e516cd149fa4871 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.698: INFO: 	Container e2e ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.698: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: tigera-operator-687c49f5c8-qcfmm from tigera-operator started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.698: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun 27 17:13:42.698: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.90 before test
    Jun 27 17:13:42.731: INFO: calico-node-6j4xp from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: ibm-keepalived-watcher-bpqvm from kube-system started at 2023-06-27 13:53:26 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: ibm-master-proxy-static-10.113.180.90 from kube-system started at 2023-06-27 13:53:22 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: 	Container pause ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: ibmcloud-block-storage-driver-jb9wr from kube-system started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: tuned-md8mr from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: dns-default-q6q9l from openshift-dns started at 2023-06-27 16:44:33 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container dns ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: node-resolver-dtvx7 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: node-ca-8pgp8 from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: ingress-canary-hfzkr from openshift-ingress-canary started at 2023-06-27 16:44:33 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: openshift-kube-proxy-bf9cj from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: node-exporter-5vgpm from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: multus-additional-cni-plugins-qszdk from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: multus-skrpb from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: network-metrics-daemon-4jmw9 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: network-check-target-zmtm2 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: collect-profiles-28131405-8w5hd from openshift-operator-lifecycle-manager started at 2023-06-27 16:45:00 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container collect-profiles ready: false, restart count 0
    Jun 27 17:13:42.731: INFO: collect-profiles-28131420-v25h5 from openshift-operator-lifecycle-manager started at 2023-06-27 17:00:00 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container collect-profiles ready: false, restart count 0
    Jun 27 17:13:42.731: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.731: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 27 17:13:42.731: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.96 before test
    Jun 27 17:13:42.778: INFO: calico-kube-controllers-79f474fb8-m45qw from calico-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.778: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun 27 17:13:42.778: INFO: calico-node-z28vk from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.778: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 17:13:42.778: INFO: calico-typha-657df678d7-4ch2t from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.778: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 27 17:13:42.778: INFO: managed-storage-validation-webhooks-665764b6b4-r77pv from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.778: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 17:13:42.778: INFO: managed-storage-validation-webhooks-665764b6b4-v4g8s from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.778: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 17:13:42.778: INFO: managed-storage-validation-webhooks-665764b6b4-vhqr7 from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.778: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 17:13:42.778: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-tnr88 from ibm-system started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.778: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
    Jun 27 17:13:42.778: INFO: ibm-file-plugin-84cc66cdb-97nzr from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.778: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jun 27 17:13:42.778: INFO: ibm-keepalived-watcher-h8p9q from kube-system started at 2023-06-27 13:53:48 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: ibm-master-proxy-static-10.113.180.96 from kube-system started at 2023-06-27 13:53:45 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: 	Container pause ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: ibm-storage-metrics-agent-746d76fb56-nvvw9 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: ibm-storage-watcher-676b4ddbc6-dz54r from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: ibmcloud-block-storage-driver-pkhxd from kube-system started at 2023-06-27 13:53:53 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: ibmcloud-block-storage-plugin-5f677b8577-8f7s7 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: cluster-node-tuning-operator-848d57c5bb-7ln5t from openshift-cluster-node-tuning-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: tuned-kk4gn from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: cluster-samples-operator-769ddfc8cc-88z62 from openshift-cluster-samples-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: cluster-storage-operator-5fc995c76b-jhmzr from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Jun 27 17:13:42.779: INFO: csi-snapshot-controller-5694c47cbb-qfvbs from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: csi-snapshot-controller-operator-64ff6856c8-l29px from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: csi-snapshot-webhook-5d7cc7f6cb-gzt6x from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container webhook ready: true, restart count 0
    Jun 27 17:13:42.779: INFO: console-operator-5694fff656-5wp7b from openshift-console-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container console-operator ready: true, restart count 1
    Jun 27 17:13:42.779: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Jun 27 17:13:42.779: INFO: console-6c8658586b-rc55z from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.779: INFO: 	Container console ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: downloads-57bd479866-8bj2l from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container download-server ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: dns-operator-b64d4b4c7-5p5hs from openshift-dns-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container dns-operator ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: dns-default-q8pl2 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container dns ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: node-resolver-nbx64 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: cluster-image-registry-operator-55bdbc85cd-vpldl from openshift-image-registry started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: node-ca-tbfkf from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: ingress-canary-mtqfl from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: ingress-operator-5c457b57b4-67rvh from openshift-ingress-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container ingress-operator ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: router-default-7f97cd5c5f-n56jj from openshift-ingress started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container router ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: insights-operator-8f4d6c949-n2nlt from openshift-insights started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container insights-operator ready: true, restart count 1
    Jun 27 17:13:42.780: INFO: openshift-kube-proxy-mpv6g from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: kube-storage-version-migrator-operator-59f4c7c696-jq8gm from openshift-kube-storage-version-migrator-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Jun 27 17:13:42.780: INFO: migrator-6795cdbdb7-qsfqv from openshift-kube-storage-version-migrator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container migrator ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: marketplace-operator-57bd87cfb8-5hhm8 from openshift-marketplace started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container marketplace-operator ready: true, restart count 0
    Jun 27 17:13:42.780: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-06-27 16:44:14 +0000 UTC (6 container statuses recorded)
    Jun 27 17:13:42.780: INFO: 	Container alertmanager ready: true, restart count 1
    Jun 27 17:13:42.781: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: cluster-monitoring-operator-644d445879-l5jj8 from openshift-monitoring started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.781: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: node-exporter-l5dxr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: prometheus-adapter-648f68fcc-mpvkt from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.781: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-06-27 16:44:13 +0000 UTC (6 container statuses recorded)
    Jun 27 17:13:42.781: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container prometheus ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: prometheus-operator-admission-webhook-6c667b594b-c9dw4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.781: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: thanos-querier-754f675f77-gs6k4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (6 container statuses recorded)
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container thanos-query ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: multus-additional-cni-plugins-xqsgs from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.781: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: multus-admission-controller-754d449d79-6cbgx from openshift-multus started at 2023-06-27 16:44:27 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.781: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Jun 27 17:13:42.781: INFO: multus-kx7qm from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.781: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: network-metrics-daemon-cbrq2 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: network-check-source-6b8766f86f-s8s7j from openshift-network-diagnostics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container check-endpoints ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: network-check-target-6lv98 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: network-operator-ffb9884c5-25r42 from openshift-network-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container network-operator ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: catalog-operator-58898485cd-qjmms from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container catalog-operator ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: olm-operator-7959cfcd9f-dt77g from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container olm-operator ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: package-server-manager-f7b6c466f-9bbzw from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container package-server-manager ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: packageserver-8d78bf5dd-r9g9t from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container packageserver ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: metrics-6d46d44d8f-jbnwq from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container metrics ready: true, restart count 3
    Jun 27 17:13:42.782: INFO: push-gateway-5465b544cc-f8v8q from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container push-gateway ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: service-ca-operator-74dd8bfc8-cl4bp from openshift-service-ca-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container service-ca-operator ready: true, restart count 1
    Jun 27 17:13:42.782: INFO: service-ca-6f86485857-drj9g from openshift-service-ca started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container service-ca-controller ready: false, restart count 0
    Jun 27 17:13:42.782: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 17:13:42.782: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 17:13:42.782: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/27/23 17:13:42.782
    Jun 27 17:13:42.825: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-5057" to be "running"
    Jun 27 17:13:42.835: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 10.16286ms
    Jun 27 17:13:44.846: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021518102s
    Jun 27 17:13:46.847: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 4.022421994s
    Jun 27 17:13:46.847: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/27/23 17:13:46.858
    STEP: Trying to apply a random label on the found node. 06/27/23 17:13:46.907
    STEP: verifying the node has the label kubernetes.io/e2e-475eb746-5f89-4364-9ae2-535ea306403f 95 06/27/23 17:13:46.931
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 06/27/23 17:13:46.945
    Jun 27 17:13:46.978: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-5057" to be "not pending"
    Jun 27 17:13:46.988: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 8.965374ms
    Jun 27 17:13:49.003: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02454307s
    Jun 27 17:13:50.999: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 4.020151542s
    Jun 27 17:13:50.999: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.113.180.90 on the node which pod4 resides and expect not scheduled 06/27/23 17:13:50.999
    Jun 27 17:13:51.033: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-5057" to be "not pending"
    Jun 27 17:13:51.043: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.187852ms
    Jun 27 17:13:53.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022021233s
    Jun 27 17:13:55.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020385697s
    Jun 27 17:13:57.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.021561893s
    Jun 27 17:13:59.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.022048225s
    Jun 27 17:14:01.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.02106027s
    Jun 27 17:14:03.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.02157031s
    Jun 27 17:14:05.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.020797831s
    Jun 27 17:14:07.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.024934719s
    Jun 27 17:14:09.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.022512087s
    Jun 27 17:14:11.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.020393968s
    Jun 27 17:14:13.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.022902481s
    Jun 27 17:14:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.021732431s
    Jun 27 17:14:17.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.023299834s
    Jun 27 17:14:19.070: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.036537779s
    Jun 27 17:14:21.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.019788407s
    Jun 27 17:14:23.060: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.026549021s
    Jun 27 17:14:25.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.022385778s
    Jun 27 17:14:27.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.023667317s
    Jun 27 17:14:29.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.022368771s
    Jun 27 17:14:31.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.022104538s
    Jun 27 17:14:33.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.022351317s
    Jun 27 17:14:35.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.022760179s
    Jun 27 17:14:37.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.021292097s
    Jun 27 17:14:39.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024736873s
    Jun 27 17:14:41.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.025017522s
    Jun 27 17:14:43.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.021547966s
    Jun 27 17:14:45.061: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.02757182s
    Jun 27 17:14:47.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.021541852s
    Jun 27 17:14:49.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.02125782s
    Jun 27 17:14:51.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.022195974s
    Jun 27 17:14:53.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.02038205s
    Jun 27 17:14:55.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.023891567s
    Jun 27 17:14:57.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.025029612s
    Jun 27 17:14:59.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.022401983s
    Jun 27 17:15:01.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.022158525s
    Jun 27 17:15:03.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.021317517s
    Jun 27 17:15:05.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.020052262s
    Jun 27 17:15:07.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.021440663s
    Jun 27 17:15:09.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.02587109s
    Jun 27 17:15:11.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.021755397s
    Jun 27 17:15:13.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.021809817s
    Jun 27 17:15:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.021540926s
    Jun 27 17:15:17.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.02304684s
    Jun 27 17:15:19.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.022008195s
    Jun 27 17:15:21.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.021514461s
    Jun 27 17:15:23.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.021493498s
    Jun 27 17:15:25.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.021257981s
    Jun 27 17:15:27.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.021768522s
    Jun 27 17:15:29.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025342128s
    Jun 27 17:15:31.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.021242652s
    Jun 27 17:15:33.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.021230794s
    Jun 27 17:15:35.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.022563012s
    Jun 27 17:15:37.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.02405078s
    Jun 27 17:15:39.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.021293162s
    Jun 27 17:15:41.080: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.046911626s
    Jun 27 17:15:43.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.020894002s
    Jun 27 17:15:45.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.02172298s
    Jun 27 17:15:47.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.022548565s
    Jun 27 17:15:49.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.022427312s
    Jun 27 17:15:51.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.022391798s
    Jun 27 17:15:53.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.023111799s
    Jun 27 17:15:55.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.022651681s
    Jun 27 17:15:57.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.0230394s
    Jun 27 17:15:59.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.021301624s
    Jun 27 17:16:01.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.029134468s
    Jun 27 17:16:03.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.022475939s
    Jun 27 17:16:05.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.022139777s
    Jun 27 17:16:07.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.022531488s
    Jun 27 17:16:09.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.021337691s
    Jun 27 17:16:11.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.020801444s
    Jun 27 17:16:13.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.02193758s
    Jun 27 17:16:15.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.021796098s
    Jun 27 17:16:17.085: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.052150607s
    Jun 27 17:16:19.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.020765987s
    Jun 27 17:16:21.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.022796661s
    Jun 27 17:16:23.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.021545822s
    Jun 27 17:16:25.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.021848928s
    Jun 27 17:16:27.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.022392539s
    Jun 27 17:16:29.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.026255242s
    Jun 27 17:16:31.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.022208037s
    Jun 27 17:16:33.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.026179983s
    Jun 27 17:16:35.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.022289784s
    Jun 27 17:16:37.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.021476683s
    Jun 27 17:16:39.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.022555821s
    Jun 27 17:16:41.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.021035487s
    Jun 27 17:16:43.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.030015778s
    Jun 27 17:16:45.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.021916919s
    Jun 27 17:16:47.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.021837448s
    Jun 27 17:16:49.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.024278406s
    Jun 27 17:16:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.020901956s
    Jun 27 17:16:53.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.021834556s
    Jun 27 17:16:55.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.022498288s
    Jun 27 17:16:57.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.022212471s
    Jun 27 17:16:59.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.020807066s
    Jun 27 17:17:01.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.022170578s
    Jun 27 17:17:03.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.021362027s
    Jun 27 17:17:05.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.024275349s
    Jun 27 17:17:07.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.021139835s
    Jun 27 17:17:09.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.023994845s
    Jun 27 17:17:11.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.021405004s
    Jun 27 17:17:13.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.021555335s
    Jun 27 17:17:15.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.025083364s
    Jun 27 17:17:17.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.020816702s
    Jun 27 17:17:19.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.023359784s
    Jun 27 17:17:21.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.019858979s
    Jun 27 17:17:23.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.022035651s
    Jun 27 17:17:25.066: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.032738512s
    Jun 27 17:17:27.058: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.025156331s
    Jun 27 17:17:29.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.022445421s
    Jun 27 17:17:31.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.020956712s
    Jun 27 17:17:33.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.021864132s
    Jun 27 17:17:35.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.022162848s
    Jun 27 17:17:37.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.02242758s
    Jun 27 17:17:39.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.022757514s
    Jun 27 17:17:41.062: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.028496042s
    Jun 27 17:17:43.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.021785283s
    Jun 27 17:17:45.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.022394311s
    Jun 27 17:17:47.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.020821401s
    Jun 27 17:17:49.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.026299999s
    Jun 27 17:17:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.020689195s
    Jun 27 17:17:53.053: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.020262865s
    Jun 27 17:17:55.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.022144831s
    Jun 27 17:17:57.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.025897846s
    Jun 27 17:17:59.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.021868017s
    Jun 27 17:18:01.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.021502434s
    Jun 27 17:18:03.059: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.026247884s
    Jun 27 17:18:05.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.020438851s
    Jun 27 17:18:07.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.022292577s
    Jun 27 17:18:09.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.023168579s
    Jun 27 17:18:11.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.020682762s
    Jun 27 17:18:13.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.023097117s
    Jun 27 17:18:15.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.020593691s
    Jun 27 17:18:17.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.022250678s
    Jun 27 17:18:19.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.022597965s
    Jun 27 17:18:21.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.020918984s
    Jun 27 17:18:23.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.022927541s
    Jun 27 17:18:25.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.02187936s
    Jun 27 17:18:27.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.021663644s
    Jun 27 17:18:29.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.022041197s
    Jun 27 17:18:31.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.020877957s
    Jun 27 17:18:33.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.021183702s
    Jun 27 17:18:35.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.023061803s
    Jun 27 17:18:37.057: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.023910866s
    Jun 27 17:18:39.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.021414499s
    Jun 27 17:18:41.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.020991397s
    Jun 27 17:18:43.055: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.022016919s
    Jun 27 17:18:45.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.022394464s
    Jun 27 17:18:47.056: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.023226387s
    Jun 27 17:18:49.063: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.030114121s
    Jun 27 17:18:51.054: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.020804926s
    Jun 27 17:18:51.064: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.030483318s
    STEP: removing the label kubernetes.io/e2e-475eb746-5f89-4364-9ae2-535ea306403f off the node 10.113.180.90 06/27/23 17:18:51.064
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-475eb746-5f89-4364-9ae2-535ea306403f 06/27/23 17:18:51.102
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:18:51.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5057" for this suite. 06/27/23 17:18:51.135
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:18:51.162
Jun 27 17:18:51.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 17:18:51.164
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:18:51.199
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:18:51.214
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-2563/configmap-test-bfe0428a-bd33-490a-8390-1873778912f1 06/27/23 17:18:51.225
STEP: Creating a pod to test consume configMaps 06/27/23 17:18:51.238
Jun 27 17:18:51.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d" in namespace "configmap-2563" to be "Succeeded or Failed"
Jun 27 17:18:51.300: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033728ms
Jun 27 17:18:53.312: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022180596s
Jun 27 17:18:55.345: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055266095s
Jun 27 17:18:57.311: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020738077s
STEP: Saw pod success 06/27/23 17:18:57.311
Jun 27 17:18:57.312: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d" satisfied condition "Succeeded or Failed"
Jun 27 17:18:57.321: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d container env-test: <nil>
STEP: delete the pod 06/27/23 17:18:57.393
Jun 27 17:18:57.425: INFO: Waiting for pod pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d to disappear
Jun 27 17:18:57.439: INFO: Pod pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 17:18:57.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2563" for this suite. 06/27/23 17:18:57.462
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":186,"skipped":3832,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.317 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:18:51.162
    Jun 27 17:18:51.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 17:18:51.164
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:18:51.199
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:18:51.214
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-2563/configmap-test-bfe0428a-bd33-490a-8390-1873778912f1 06/27/23 17:18:51.225
    STEP: Creating a pod to test consume configMaps 06/27/23 17:18:51.238
    Jun 27 17:18:51.289: INFO: Waiting up to 5m0s for pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d" in namespace "configmap-2563" to be "Succeeded or Failed"
    Jun 27 17:18:51.300: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d": Phase="Pending", Reason="", readiness=false. Elapsed: 10.033728ms
    Jun 27 17:18:53.312: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022180596s
    Jun 27 17:18:55.345: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.055266095s
    Jun 27 17:18:57.311: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020738077s
    STEP: Saw pod success 06/27/23 17:18:57.311
    Jun 27 17:18:57.312: INFO: Pod "pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d" satisfied condition "Succeeded or Failed"
    Jun 27 17:18:57.321: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d container env-test: <nil>
    STEP: delete the pod 06/27/23 17:18:57.393
    Jun 27 17:18:57.425: INFO: Waiting for pod pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d to disappear
    Jun 27 17:18:57.439: INFO: Pod pod-configmaps-51d3fb15-fc9a-4865-baf3-dce8c35a552d no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 17:18:57.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2563" for this suite. 06/27/23 17:18:57.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:18:57.48
Jun 27 17:18:57.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename namespaces 06/27/23 17:18:57.484
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:18:57.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:18:57.538
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 06/27/23 17:18:57.548
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:18:57.584
STEP: Creating a service in the namespace 06/27/23 17:18:57.599
STEP: Deleting the namespace 06/27/23 17:18:57.647
STEP: Waiting for the namespace to be removed. 06/27/23 17:18:57.673
STEP: Recreating the namespace 06/27/23 17:19:04.727
STEP: Verifying there is no service in the namespace 06/27/23 17:19:04.773
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:19:04.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-6845" for this suite. 06/27/23 17:19:04.81
STEP: Destroying namespace "nsdeletetest-573" for this suite. 06/27/23 17:19:04.827
Jun 27 17:19:04.845: INFO: Namespace nsdeletetest-573 was already deleted
STEP: Destroying namespace "nsdeletetest-6458" for this suite. 06/27/23 17:19:04.845
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":187,"skipped":3839,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.386 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:18:57.48
    Jun 27 17:18:57.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename namespaces 06/27/23 17:18:57.484
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:18:57.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:18:57.538
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 06/27/23 17:18:57.548
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:18:57.584
    STEP: Creating a service in the namespace 06/27/23 17:18:57.599
    STEP: Deleting the namespace 06/27/23 17:18:57.647
    STEP: Waiting for the namespace to be removed. 06/27/23 17:18:57.673
    STEP: Recreating the namespace 06/27/23 17:19:04.727
    STEP: Verifying there is no service in the namespace 06/27/23 17:19:04.773
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:19:04.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-6845" for this suite. 06/27/23 17:19:04.81
    STEP: Destroying namespace "nsdeletetest-573" for this suite. 06/27/23 17:19:04.827
    Jun 27 17:19:04.845: INFO: Namespace nsdeletetest-573 was already deleted
    STEP: Destroying namespace "nsdeletetest-6458" for this suite. 06/27/23 17:19:04.845
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:19:04.87
Jun 27 17:19:04.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename watch 06/27/23 17:19:04.872
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:04.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:04.92
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 06/27/23 17:19:04.933
STEP: creating a new configmap 06/27/23 17:19:04.945
STEP: modifying the configmap once 06/27/23 17:19:04.958
STEP: closing the watch once it receives two notifications 06/27/23 17:19:04.987
Jun 27 17:19:04.987: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6261  48f9f257-8244-4964-b758-138c86d3a12a 108820 0 2023-06-27 17:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-27 17:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:19:04.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6261  48f9f257-8244-4964-b758-138c86d3a12a 108825 0 2023-06-27 17:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-27 17:19:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 06/27/23 17:19:04.988
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/27/23 17:19:05.028
STEP: deleting the configmap 06/27/23 17:19:05.033
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/27/23 17:19:05.053
Jun 27 17:19:05.053: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6261  48f9f257-8244-4964-b758-138c86d3a12a 108830 0 2023-06-27 17:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:19:05.054: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6261  48f9f257-8244-4964-b758-138c86d3a12a 108832 0 2023-06-27 17:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 27 17:19:05.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6261" for this suite. 06/27/23 17:19:05.068
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":188,"skipped":3859,"failed":0}
------------------------------
â€¢ [0.216 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:19:04.87
    Jun 27 17:19:04.871: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename watch 06/27/23 17:19:04.872
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:04.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:04.92
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 06/27/23 17:19:04.933
    STEP: creating a new configmap 06/27/23 17:19:04.945
    STEP: modifying the configmap once 06/27/23 17:19:04.958
    STEP: closing the watch once it receives two notifications 06/27/23 17:19:04.987
    Jun 27 17:19:04.987: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6261  48f9f257-8244-4964-b758-138c86d3a12a 108820 0 2023-06-27 17:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-27 17:19:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:19:04.988: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6261  48f9f257-8244-4964-b758-138c86d3a12a 108825 0 2023-06-27 17:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-27 17:19:04 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 06/27/23 17:19:04.988
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 06/27/23 17:19:05.028
    STEP: deleting the configmap 06/27/23 17:19:05.033
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 06/27/23 17:19:05.053
    Jun 27 17:19:05.053: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6261  48f9f257-8244-4964-b758-138c86d3a12a 108830 0 2023-06-27 17:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:19:05.054: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6261  48f9f257-8244-4964-b758-138c86d3a12a 108832 0 2023-06-27 17:19:04 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 27 17:19:05.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6261" for this suite. 06/27/23 17:19:05.068
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:19:05.09
Jun 27 17:19:05.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename deployment 06/27/23 17:19:05.098
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:05.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:05.151
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Jun 27 17:19:05.162: INFO: Creating deployment "webserver-deployment"
Jun 27 17:19:05.180: INFO: Waiting for observed generation 1
Jun 27 17:19:07.208: INFO: Waiting for all required pods to come up
Jun 27 17:19:07.224: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 06/27/23 17:19:07.224
Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-62s87" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-lnmzp" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mn6p2" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rdkdh" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b8qjz" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4868m" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6mhc5" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.225: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qw89q" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.225: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-h2q6w" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5z8wn" in namespace "deployment-104" to be "running"
Jun 27 17:19:07.237: INFO: Pod "webserver-deployment-845c8977d9-lnmzp": Phase="Pending", Reason="", readiness=false. Elapsed: 12.378689ms
Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-rdkdh": Phase="Pending", Reason="", readiness=false. Elapsed: 21.400506ms
Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-5z8wn": Phase="Pending", Reason="", readiness=false. Elapsed: 20.845873ms
Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-qw89q": Phase="Pending", Reason="", readiness=false. Elapsed: 21.559836ms
Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-4868m": Phase="Pending", Reason="", readiness=false. Elapsed: 21.740684ms
Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-h2q6w": Phase="Pending", Reason="", readiness=false. Elapsed: 21.535009ms
Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-62s87": Phase="Pending", Reason="", readiness=false. Elapsed: 22.194812ms
Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-b8qjz": Phase="Pending", Reason="", readiness=false. Elapsed: 21.988755ms
Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-mn6p2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.127435ms
Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-6mhc5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.862921ms
Jun 27 17:19:09.247: INFO: Pod "webserver-deployment-845c8977d9-lnmzp": Phase="Running", Reason="", readiness=true. Elapsed: 2.023103158s
Jun 27 17:19:09.247: INFO: Pod "webserver-deployment-845c8977d9-lnmzp" satisfied condition "running"
Jun 27 17:19:09.257: INFO: Pod "webserver-deployment-845c8977d9-qw89q": Phase="Running", Reason="", readiness=true. Elapsed: 2.032001339s
Jun 27 17:19:09.257: INFO: Pod "webserver-deployment-845c8977d9-qw89q" satisfied condition "running"
Jun 27 17:19:09.261: INFO: Pod "webserver-deployment-845c8977d9-mn6p2": Phase="Running", Reason="", readiness=true. Elapsed: 2.036624226s
Jun 27 17:19:09.261: INFO: Pod "webserver-deployment-845c8977d9-mn6p2" satisfied condition "running"
Jun 27 17:19:09.263: INFO: Pod "webserver-deployment-845c8977d9-rdkdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.039124688s
Jun 27 17:19:09.263: INFO: Pod "webserver-deployment-845c8977d9-rdkdh" satisfied condition "running"
Jun 27 17:19:09.264: INFO: Pod "webserver-deployment-845c8977d9-62s87": Phase="Running", Reason="", readiness=true. Elapsed: 2.039509753s
Jun 27 17:19:09.265: INFO: Pod "webserver-deployment-845c8977d9-62s87" satisfied condition "running"
Jun 27 17:19:09.264: INFO: Pod "webserver-deployment-845c8977d9-h2q6w": Phase="Running", Reason="", readiness=true. Elapsed: 2.039321528s
Jun 27 17:19:09.265: INFO: Pod "webserver-deployment-845c8977d9-h2q6w" satisfied condition "running"
Jun 27 17:19:09.265: INFO: Pod "webserver-deployment-845c8977d9-b8qjz": Phase="Running", Reason="", readiness=true. Elapsed: 2.040950582s
Jun 27 17:19:09.265: INFO: Pod "webserver-deployment-845c8977d9-b8qjz" satisfied condition "running"
Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-4868m": Phase="Running", Reason="", readiness=true. Elapsed: 2.041325082s
Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-4868m" satisfied condition "running"
Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-6mhc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.041516136s
Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-5z8wn": Phase="Running", Reason="", readiness=true. Elapsed: 2.041367964s
Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-5z8wn" satisfied condition "running"
Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-6mhc5" satisfied condition "running"
Jun 27 17:19:09.266: INFO: Waiting for deployment "webserver-deployment" to complete
Jun 27 17:19:09.285: INFO: Updating deployment "webserver-deployment" with a non-existent image
Jun 27 17:19:09.361: INFO: Updating deployment webserver-deployment
Jun 27 17:19:09.361: INFO: Waiting for observed generation 2
Jun 27 17:19:11.381: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Jun 27 17:19:11.392: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Jun 27 17:19:11.401: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 27 17:19:11.464: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Jun 27 17:19:11.464: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Jun 27 17:19:11.477: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Jun 27 17:19:11.501: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Jun 27 17:19:11.501: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Jun 27 17:19:11.525: INFO: Updating deployment webserver-deployment
Jun 27 17:19:11.525: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Jun 27 17:19:11.557: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Jun 27 17:19:13.584: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 27 17:19:13.610: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-104  ac02836c-2838-4c3e-8561-541603222710 109278 3 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d627398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-27 17:19:11 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-06-27 17:19:11 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Jun 27 17:19:13.624: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-104  2f49d3a5-0331-4889-9364-051a1465e1eb 109272 3 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ac02836c-2838-4c3e-8561-541603222710 0xc00d6277b7 0xc00d6277b8}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac02836c-2838-4c3e-8561-541603222710\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d627858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 27 17:19:13.624: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Jun 27 17:19:13.632: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-104  c50172c5-7a5b-4910-8bd6-65345d1b2b2d 109264 3 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ac02836c-2838-4c3e-8561-541603222710 0xc00d6278b7 0xc00d6278b8}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac02836c-2838-4c3e-8561-541603222710\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d627948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Jun 27 17:19:13.657: INFO: Pod "webserver-deployment-69b7448995-2mhlg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-2mhlg webserver-deployment-69b7448995- deployment-104  a5a839c6-c094-4206-85d2-9bb669804428 109151 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:23e80e960dacd103800ef524d3b23457053d9022fc2fe307b2444cc76f65495d cni.projectcalico.org/podIP:172.30.60.125/32 cni.projectcalico.org/podIPs:172.30.60.125/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.125"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.125"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00d627e27 0xc00d627e28}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hr7gh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hr7gh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.658: INFO: Pod "webserver-deployment-69b7448995-6m62s" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-6m62s webserver-deployment-69b7448995- deployment-104  d1a9516b-fd15-4c19-ba4c-13f8aeda9fe0 109355 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:960e2f25b99fc8247c061962de09e2f05242633f8ad15b75bf017c84218d84de cni.projectcalico.org/podIP:172.30.250.208/32 cni.projectcalico.org/podIPs:172.30.250.208/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.208"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.208"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00029afe7 0xc00029afe8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzmlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzmlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.208,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.658: INFO: Pod "webserver-deployment-69b7448995-7nnzg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-7nnzg webserver-deployment-69b7448995- deployment-104  0ea87fe3-9459-4236-bceb-f7a27b035e4d 109386 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:dd08efc575a50194a777e8ffd8d21424801c0588d8ec0d15c8256910141cc8b4 cni.projectcalico.org/podIP:172.30.250.199/32 cni.projectcalico.org/podIPs:172.30.250.199/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.199"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.199"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00018cce7 0xc00018cce8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khgdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khgdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.658: INFO: Pod "webserver-deployment-69b7448995-82lvd" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-82lvd webserver-deployment-69b7448995- deployment-104  f05d02b6-c871-4355-8cdb-bf71af6f8e81 109359 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:869f70e785df37f1e938bdaf4416881ce4539fed5055a9d374114c2bedb60ed2 cni.projectcalico.org/podIP:172.30.250.220/32 cni.projectcalico.org/podIPs:172.30.250.220/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.220"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.220"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00133fbc7 0xc00133fbc8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6qsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6qsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.220,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
denied: requested access to the resource is denied
unauthorized: authentication required
,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.659: INFO: Pod "webserver-deployment-69b7448995-bmpwf" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-bmpwf webserver-deployment-69b7448995- deployment-104  ad73f12f-13d1-41a2-aa10-77158da574fc 109174 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:13ec4b17f1b61af388b893d591953f069bd11fe970ca210984b48f10d5618308 cni.projectcalico.org/podIP:172.30.106.141/32 cni.projectcalico.org/podIPs:172.30.106.141/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.141"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.141"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00133ff97 0xc00133ff98}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwlxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwlxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.659: INFO: Pod "webserver-deployment-69b7448995-fz6p7" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-fz6p7 webserver-deployment-69b7448995- deployment-104  9b1227b8-5062-48c7-86e5-7435189d0a7b 109280 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0005d5ac7 0xc0005d5ac8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pg8gz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pg8gz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.659: INFO: Pod "webserver-deployment-69b7448995-h9grz" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-h9grz webserver-deployment-69b7448995- deployment-104  8e7e3f81-99ad-4ddd-861c-1a7904155d0f 109149 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:952f3f0ed9c4b1565f518b169af9c88a88ed2a1ffbb05303fd89d606e474cfad cni.projectcalico.org/podIP:172.30.106.161/32 cni.projectcalico.org/podIPs:172.30.106.161/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.161"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.161"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d81b7 0xc0095d81b8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4blq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4blq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.660: INFO: Pod "webserver-deployment-69b7448995-kml9w" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kml9w webserver-deployment-69b7448995- deployment-104  867751c1-24ba-480e-a188-d530d09bf8fa 109405 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:2c7cdd19741bfab244a4a55881abfb9c762b0efca6bf4c4928dba8ecfd18178f cni.projectcalico.org/podIP:172.30.106.176/32 cni.projectcalico.org/podIPs:172.30.106.176/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.176"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.176"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8427 0xc0095d8428}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2g7s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2g7s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.660: INFO: Pod "webserver-deployment-69b7448995-nrjn9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-nrjn9 webserver-deployment-69b7448995- deployment-104  08f5ef78-5529-4b12-8443-7fb761f1bdd8 109351 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1783d9cf21be02423cf679fa9f07b3f219caedd57711c2cf7d7932cc43493d78 cni.projectcalico.org/podIP:172.30.60.119/32 cni.projectcalico.org/podIPs:172.30.60.119/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.119"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.119"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d86b7 0xc0095d86b8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95rvh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95rvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.660: INFO: Pod "webserver-deployment-69b7448995-nwtz9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-nwtz9 webserver-deployment-69b7448995- deployment-104  0c24af53-02d0-48fc-990e-7c523200193b 109377 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e009d04efc9f4a50dafacc56f2a3d8849c70778f8a2d843eb8ee9c5197712ae7 cni.projectcalico.org/podIP:172.30.106.160/32 cni.projectcalico.org/podIPs:172.30.106.160/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.160"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.160"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8927 0xc0095d8928}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2srtd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2srtd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.661: INFO: Pod "webserver-deployment-69b7448995-r6w5l" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-r6w5l webserver-deployment-69b7448995- deployment-104  7ba1355e-84fe-43a7-b911-acd568009969 109288 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8b97 0xc0095d8b98}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7kvs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7kvs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.661: INFO: Pod "webserver-deployment-69b7448995-szd4c" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-szd4c webserver-deployment-69b7448995- deployment-104  1e8964e1-ea04-4175-8642-ecbd62023fae 109252 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8dc7 0xc0095d8dc8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dxxch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dxxch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.661: INFO: Pod "webserver-deployment-69b7448995-wlgh9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-wlgh9 webserver-deployment-69b7448995- deployment-104  47faef12-ae1c-4c38-a1a6-d2b94ccb02df 109398 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:eb40d59700b639fa8daf5b502b13ee44e415ca504d479a096a4e4dffdbd856eb cni.projectcalico.org/podIP:172.30.250.201/32 cni.projectcalico.org/podIPs:172.30.250.201/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.201"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.201"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8ff7 0xc0095d8ff8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qkzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qkzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.661: INFO: Pod "webserver-deployment-845c8977d9-4868m" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-4868m webserver-deployment-845c8977d9- deployment-104  3e33780f-9ba4-4550-9821-857a8bd260e0 109017 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:929576102f41c6fd0dfbce37f469dda2892a9542f0af2d38ed9c19721f7c382c cni.projectcalico.org/podIP:172.30.60.109/32 cni.projectcalico.org/podIPs:172.30.60.109/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.109"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.109"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d9287 0xc0095d9288}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ktjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ktjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.109,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://41dca4cfbfb43d5a152419cd7bed434741ddd62adee9708a07d0b5371fc73146,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.662: INFO: Pod "webserver-deployment-845c8977d9-5z8wn" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-5z8wn webserver-deployment-845c8977d9- deployment-104  da09b2ab-c16d-450e-896b-80b06153e2a6 109022 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:bd3154dfdba17734125e9a3da4149b40495a24aebe5e94ce8bea6f3aa9c63499 cni.projectcalico.org/podIP:172.30.106.166/32 cni.projectcalico.org/podIPs:172.30.106.166/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.166"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.166"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d94f7 0xc0095d94f8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tp2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tp2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:172.30.106.166,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ab9728a3b61fa07693f5539b4070773e22ad8275678f97b77b8c48aae01abe5c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.166,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.662: INFO: Pod "webserver-deployment-845c8977d9-62s87" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-62s87 webserver-deployment-845c8977d9- deployment-104  72af7e03-c5fe-4d24-86c2-19ffc06a2b1d 109013 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:42206ecdd9dc964fd93f3f69a2067621030cece24adedff153d25215de6add4a cni.projectcalico.org/podIP:172.30.60.123/32 cni.projectcalico.org/podIPs:172.30.60.123/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.123"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.123"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d9787 0xc0095d9788}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nnvvx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nnvvx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.123,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://54a931ec01e69f9458e85f72a0562ef6240064c7a541e4068ce853231df7689f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.663: INFO: Pod "webserver-deployment-845c8977d9-9tsvq" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-9tsvq webserver-deployment-845c8977d9- deployment-104  1e472311-699e-41b4-b3b3-065fd5a7bc0c 109368 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d921ab2830aabd1c1b2b9fd659ecaf1acc5e94ee3aeffbb117d82f0c5eb1fe2f cni.projectcalico.org/podIP:172.30.250.245/32 cni.projectcalico.org/podIPs:172.30.250.245/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.245"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.245"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d99f7 0xc0095d99f8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fdk7f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fdk7f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.663: INFO: Pod "webserver-deployment-845c8977d9-b47pr" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b47pr webserver-deployment-845c8977d9- deployment-104  1afb15be-81e5-420f-a53e-ff7557770793 109239 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d9c47 0xc0095d9c48}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t778w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t778w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.663: INFO: Pod "webserver-deployment-845c8977d9-b8qjz" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-b8qjz webserver-deployment-845c8977d9- deployment-104  141b0ed3-53a5-4c4e-b9ff-25f6d405f74e 109048 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2642902aa1550f29db9085eb8ff2f99894c6c7d84dec1feba74b2b782fc4e05b cni.projectcalico.org/podIP:172.30.60.120/32 cni.projectcalico.org/podIPs:172.30.60.120/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.120"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.120"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d9e77 0xc0095d9e78}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plxqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plxqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.120,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://70924d14f999b16f50002ae5606a0f4d6865e8a46f65aa903de4d7e3423330c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.663: INFO: Pod "webserver-deployment-845c8977d9-c4mj9" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-c4mj9 webserver-deployment-845c8977d9- deployment-104  e6ab4a9a-7577-4410-ae6c-fddcdf7c6e63 109299 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d060f7 0xc003d060f8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94z5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94z5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.664: INFO: Pod "webserver-deployment-845c8977d9-cqtqk" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-cqtqk webserver-deployment-845c8977d9- deployment-104  48dede5e-64fd-4d91-87e1-a2d045d935c2 109375 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3dd07f50cde7b2415745430f7d70b03e69338f76b89ff1c09064c3068e5ed6cc cni.projectcalico.org/podIP:172.30.60.75/32 cni.projectcalico.org/podIPs:172.30.60.75/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.75"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.75"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d06367 0xc003d06368}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmlcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmlcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.664: INFO: Pod "webserver-deployment-845c8977d9-drmz8" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-drmz8 webserver-deployment-845c8977d9- deployment-104  9949925a-fc86-4429-a47d-2dd35bfaa5a5 109290 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d065b7 0xc003d065b8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkfc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkfc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.664: INFO: Pod "webserver-deployment-845c8977d9-f6dvj" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-f6dvj webserver-deployment-845c8977d9- deployment-104  b67ed788-86ec-45e9-8ee3-2440959b2770 109419 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e26876880aaeb736e3ac106987a25f1ae82b056c04c421b82e89b6621cf11850 cni.projectcalico.org/podIP:172.30.250.209/32 cni.projectcalico.org/podIPs:172.30.250.209/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.209"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.209"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d067c7 0xc003d067c8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkxfh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkxfh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.668: INFO: Pod "webserver-deployment-845c8977d9-gmtfd" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gmtfd webserver-deployment-845c8977d9- deployment-104  ff1b97ad-b8ad-43f9-b4b8-68a2367c9132 109342 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:fd28853a51f0028666e98a5850ccbc41acfa6cf2daa952bb84cbb7450a49fe2a cni.projectcalico.org/podIP:172.30.250.218/32 cni.projectcalico.org/podIPs:172.30.250.218/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.218"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.218"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d06a17 0xc003d06a18}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fzvmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fzvmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.668: INFO: Pod "webserver-deployment-845c8977d9-h2q6w" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-h2q6w webserver-deployment-845c8977d9- deployment-104  93c57dde-f804-455b-b70d-1e3204af2445 109035 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5fd78f4c2d1dfcfedcd8fb05df01b478d921e5f63b9bbd18dcb7ad9ae358eda0 cni.projectcalico.org/podIP:172.30.250.216/32 cni.projectcalico.org/podIPs:172.30.250.216/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.216"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.216"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d06c77 0xc003d06c78}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twvkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twvkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.216,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://fd85ca4faafe42076c625ab58d1d2a21e8a6113013bf9ac3dba24aa7f2308aa7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.669: INFO: Pod "webserver-deployment-845c8977d9-k4fkj" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-k4fkj webserver-deployment-845c8977d9- deployment-104  38310f72-a557-4a18-a84b-fefe6e6e17ca 109258 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d06ee7 0xc003d06ee8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ph6zj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ph6zj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.672: INFO: Pod "webserver-deployment-845c8977d9-lnmzp" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lnmzp webserver-deployment-845c8977d9- deployment-104  66f99ada-4433-4000-94c0-d5ef2803a792 109024 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f20c8df303cb259c78be0a21633a3d1f92ca27425d05104ba71d2c5a592d3a0d cni.projectcalico.org/podIP:172.30.106.159/32 cni.projectcalico.org/podIPs:172.30.106.159/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.159"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.159"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d070f7 0xc003d070f8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lq7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lq7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:172.30.106.159,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://42e1d9a3dbc115522a82405944f71bc52317715e30b946b8099fb53939b984fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.672: INFO: Pod "webserver-deployment-845c8977d9-mn6p2" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-mn6p2 webserver-deployment-845c8977d9- deployment-104  9480c468-4c95-4ceb-8913-a095b974eff8 109031 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1eedf708ee910c9e618b250e1fc6191cc06dfa5fb90ffbead825a6e13193ffc9 cni.projectcalico.org/podIP:172.30.106.173/32 cni.projectcalico.org/podIPs:172.30.106.173/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.173"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.173"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07367 0xc003d07368}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5bjx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bjx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:172.30.106.173,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://566f400d10b29afd70fc7f5e6b1e5668cb263ddb7d00c42fbc20ce79337cddff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.672: INFO: Pod "webserver-deployment-845c8977d9-qw89q" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-qw89q webserver-deployment-845c8977d9- deployment-104  4e58830a-3d22-4a13-a72f-b329636981cc 109038 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4db218fe96fc5b238aa0b4ed2b069511925b5a769aa01c985914fcc5becebb6e cni.projectcalico.org/podIP:172.30.250.211/32 cni.projectcalico.org/podIPs:172.30.250.211/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.211"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.211"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d075d7 0xc003d075d8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9whz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9whz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.211,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d04086e5a12002c2660583b86845c174749e313711e6ede4a6e3a1e393b95fc6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.673: INFO: Pod "webserver-deployment-845c8977d9-rzmms" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rzmms webserver-deployment-845c8977d9- deployment-104  842de420-cdea-452d-98aa-ccf6ca48e8bf 109416 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9ecd7aae0655cdd056845e6fc7a3c7ec636c02f9a61ef580662d7892318e6689 cni.projectcalico.org/podIP:172.30.106.162/32 cni.projectcalico.org/podIPs:172.30.106.162/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.162"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.162"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07857 0xc003d07858}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zj45b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zj45b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.673: INFO: Pod "webserver-deployment-845c8977d9-t9w6k" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-t9w6k webserver-deployment-845c8977d9- deployment-104  60dd64ec-ab23-433c-9509-a2527d42dfbb 109414 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4f740d3dd18130e794544b87bb1fecf33ab72b35ca2e4eba6e7e7b7c175d63ed cni.projectcalico.org/podIP:172.30.60.108/32 cni.projectcalico.org/podIPs:172.30.60.108/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07ac7 0xc003d07ac8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6td4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6td4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.673: INFO: Pod "webserver-deployment-845c8977d9-wq8d6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wq8d6 webserver-deployment-845c8977d9- deployment-104  288c78ad-d1c2-4c13-89c9-5736580b4bb5 109338 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0476ef7dd4d053806b8473f1621d2c2f33db1460095f6715fc941a0280edfa0e cni.projectcalico.org/podIP:172.30.106.157/32 cni.projectcalico.org/podIPs:172.30.106.157/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.157"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.157"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07cf7 0xc003d07cf8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bm2vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bm2vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:19:13.674: INFO: Pod "webserver-deployment-845c8977d9-wrsvp" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-wrsvp webserver-deployment-845c8977d9- deployment-104  aa964780-5c64-4e99-8744-299777fe25c7 109350 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:09b2f82351861267485713337311756d2ef099a3c34560bc6a968490457add10 cni.projectcalico.org/podIP:172.30.106.164/32 cni.projectcalico.org/podIPs:172.30.106.164/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.164"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.164"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07f57 0xc003d07f58}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xd4hj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xd4hj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 27 17:19:13.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-104" for this suite. 06/27/23 17:19:13.694
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":189,"skipped":3861,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.637 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:19:05.09
    Jun 27 17:19:05.096: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename deployment 06/27/23 17:19:05.098
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:05.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:05.151
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Jun 27 17:19:05.162: INFO: Creating deployment "webserver-deployment"
    Jun 27 17:19:05.180: INFO: Waiting for observed generation 1
    Jun 27 17:19:07.208: INFO: Waiting for all required pods to come up
    Jun 27 17:19:07.224: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 06/27/23 17:19:07.224
    Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-62s87" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-lnmzp" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-mn6p2" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rdkdh" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-b8qjz" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4868m" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-6mhc5" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.225: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-qw89q" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.225: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-h2q6w" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.224: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-5z8wn" in namespace "deployment-104" to be "running"
    Jun 27 17:19:07.237: INFO: Pod "webserver-deployment-845c8977d9-lnmzp": Phase="Pending", Reason="", readiness=false. Elapsed: 12.378689ms
    Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-rdkdh": Phase="Pending", Reason="", readiness=false. Elapsed: 21.400506ms
    Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-5z8wn": Phase="Pending", Reason="", readiness=false. Elapsed: 20.845873ms
    Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-qw89q": Phase="Pending", Reason="", readiness=false. Elapsed: 21.559836ms
    Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-4868m": Phase="Pending", Reason="", readiness=false. Elapsed: 21.740684ms
    Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-h2q6w": Phase="Pending", Reason="", readiness=false. Elapsed: 21.535009ms
    Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-62s87": Phase="Pending", Reason="", readiness=false. Elapsed: 22.194812ms
    Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-b8qjz": Phase="Pending", Reason="", readiness=false. Elapsed: 21.988755ms
    Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-mn6p2": Phase="Pending", Reason="", readiness=false. Elapsed: 22.127435ms
    Jun 27 17:19:07.246: INFO: Pod "webserver-deployment-845c8977d9-6mhc5": Phase="Pending", Reason="", readiness=false. Elapsed: 21.862921ms
    Jun 27 17:19:09.247: INFO: Pod "webserver-deployment-845c8977d9-lnmzp": Phase="Running", Reason="", readiness=true. Elapsed: 2.023103158s
    Jun 27 17:19:09.247: INFO: Pod "webserver-deployment-845c8977d9-lnmzp" satisfied condition "running"
    Jun 27 17:19:09.257: INFO: Pod "webserver-deployment-845c8977d9-qw89q": Phase="Running", Reason="", readiness=true. Elapsed: 2.032001339s
    Jun 27 17:19:09.257: INFO: Pod "webserver-deployment-845c8977d9-qw89q" satisfied condition "running"
    Jun 27 17:19:09.261: INFO: Pod "webserver-deployment-845c8977d9-mn6p2": Phase="Running", Reason="", readiness=true. Elapsed: 2.036624226s
    Jun 27 17:19:09.261: INFO: Pod "webserver-deployment-845c8977d9-mn6p2" satisfied condition "running"
    Jun 27 17:19:09.263: INFO: Pod "webserver-deployment-845c8977d9-rdkdh": Phase="Running", Reason="", readiness=true. Elapsed: 2.039124688s
    Jun 27 17:19:09.263: INFO: Pod "webserver-deployment-845c8977d9-rdkdh" satisfied condition "running"
    Jun 27 17:19:09.264: INFO: Pod "webserver-deployment-845c8977d9-62s87": Phase="Running", Reason="", readiness=true. Elapsed: 2.039509753s
    Jun 27 17:19:09.265: INFO: Pod "webserver-deployment-845c8977d9-62s87" satisfied condition "running"
    Jun 27 17:19:09.264: INFO: Pod "webserver-deployment-845c8977d9-h2q6w": Phase="Running", Reason="", readiness=true. Elapsed: 2.039321528s
    Jun 27 17:19:09.265: INFO: Pod "webserver-deployment-845c8977d9-h2q6w" satisfied condition "running"
    Jun 27 17:19:09.265: INFO: Pod "webserver-deployment-845c8977d9-b8qjz": Phase="Running", Reason="", readiness=true. Elapsed: 2.040950582s
    Jun 27 17:19:09.265: INFO: Pod "webserver-deployment-845c8977d9-b8qjz" satisfied condition "running"
    Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-4868m": Phase="Running", Reason="", readiness=true. Elapsed: 2.041325082s
    Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-4868m" satisfied condition "running"
    Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-6mhc5": Phase="Running", Reason="", readiness=true. Elapsed: 2.041516136s
    Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-5z8wn": Phase="Running", Reason="", readiness=true. Elapsed: 2.041367964s
    Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-5z8wn" satisfied condition "running"
    Jun 27 17:19:09.266: INFO: Pod "webserver-deployment-845c8977d9-6mhc5" satisfied condition "running"
    Jun 27 17:19:09.266: INFO: Waiting for deployment "webserver-deployment" to complete
    Jun 27 17:19:09.285: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Jun 27 17:19:09.361: INFO: Updating deployment webserver-deployment
    Jun 27 17:19:09.361: INFO: Waiting for observed generation 2
    Jun 27 17:19:11.381: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Jun 27 17:19:11.392: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Jun 27 17:19:11.401: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun 27 17:19:11.464: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Jun 27 17:19:11.464: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Jun 27 17:19:11.477: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Jun 27 17:19:11.501: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Jun 27 17:19:11.501: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Jun 27 17:19:11.525: INFO: Updating deployment webserver-deployment
    Jun 27 17:19:11.525: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Jun 27 17:19:11.557: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Jun 27 17:19:13.584: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 27 17:19:13.610: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-104  ac02836c-2838-4c3e-8561-541603222710 109278 3 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d627398 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-27 17:19:11 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-06-27 17:19:11 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Jun 27 17:19:13.624: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-104  2f49d3a5-0331-4889-9364-051a1465e1eb 109272 3 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment ac02836c-2838-4c3e-8561-541603222710 0xc00d6277b7 0xc00d6277b8}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac02836c-2838-4c3e-8561-541603222710\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d627858 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 17:19:13.624: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Jun 27 17:19:13.632: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-104  c50172c5-7a5b-4910-8bd6-65345d1b2b2d 109264 3 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment ac02836c-2838-4c3e-8561-541603222710 0xc00d6278b7 0xc00d6278b8}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ac02836c-2838-4c3e-8561-541603222710\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00d627948 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 17:19:13.657: INFO: Pod "webserver-deployment-69b7448995-2mhlg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-2mhlg webserver-deployment-69b7448995- deployment-104  a5a839c6-c094-4206-85d2-9bb669804428 109151 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:23e80e960dacd103800ef524d3b23457053d9022fc2fe307b2444cc76f65495d cni.projectcalico.org/podIP:172.30.60.125/32 cni.projectcalico.org/podIPs:172.30.60.125/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.125"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.125"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00d627e27 0xc00d627e28}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hr7gh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hr7gh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.658: INFO: Pod "webserver-deployment-69b7448995-6m62s" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-6m62s webserver-deployment-69b7448995- deployment-104  d1a9516b-fd15-4c19-ba4c-13f8aeda9fe0 109355 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:960e2f25b99fc8247c061962de09e2f05242633f8ad15b75bf017c84218d84de cni.projectcalico.org/podIP:172.30.250.208/32 cni.projectcalico.org/podIPs:172.30.250.208/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.208"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.208"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00029afe7 0xc00029afe8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.208\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kzmlp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kzmlp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.208,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.208,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.658: INFO: Pod "webserver-deployment-69b7448995-7nnzg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-7nnzg webserver-deployment-69b7448995- deployment-104  0ea87fe3-9459-4236-bceb-f7a27b035e4d 109386 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:dd08efc575a50194a777e8ffd8d21424801c0588d8ec0d15c8256910141cc8b4 cni.projectcalico.org/podIP:172.30.250.199/32 cni.projectcalico.org/podIPs:172.30.250.199/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.199"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.199"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00018cce7 0xc00018cce8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-khgdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-khgdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.658: INFO: Pod "webserver-deployment-69b7448995-82lvd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-82lvd webserver-deployment-69b7448995- deployment-104  f05d02b6-c871-4355-8cdb-bf71af6f8e81 109359 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:869f70e785df37f1e938bdaf4416881ce4539fed5055a9d374114c2bedb60ed2 cni.projectcalico.org/podIP:172.30.250.220/32 cni.projectcalico.org/podIPs:172.30.250.220/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.220"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.220"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00133fbc7 0xc00133fbc8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.220\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k6qsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k6qsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.220,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = reading manifest 404 in docker.io/library/webserver: errors:
    denied: requested access to the resource is denied
    unauthorized: authentication required
    ,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.220,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.659: INFO: Pod "webserver-deployment-69b7448995-bmpwf" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-bmpwf webserver-deployment-69b7448995- deployment-104  ad73f12f-13d1-41a2-aa10-77158da574fc 109174 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:13ec4b17f1b61af388b893d591953f069bd11fe970ca210984b48f10d5618308 cni.projectcalico.org/podIP:172.30.106.141/32 cni.projectcalico.org/podIPs:172.30.106.141/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.141"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.141"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc00133ff97 0xc00133ff98}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pwlxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pwlxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.659: INFO: Pod "webserver-deployment-69b7448995-fz6p7" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-fz6p7 webserver-deployment-69b7448995- deployment-104  9b1227b8-5062-48c7-86e5-7435189d0a7b 109280 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0005d5ac7 0xc0005d5ac8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pg8gz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pg8gz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.659: INFO: Pod "webserver-deployment-69b7448995-h9grz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-h9grz webserver-deployment-69b7448995- deployment-104  8e7e3f81-99ad-4ddd-861c-1a7904155d0f 109149 0 2023-06-27 17:19:09 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:952f3f0ed9c4b1565f518b169af9c88a88ed2a1ffbb05303fd89d606e474cfad cni.projectcalico.org/podIP:172.30.106.161/32 cni.projectcalico.org/podIPs:172.30.106.161/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.161"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.161"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d81b7 0xc0095d81b8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4blq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4blq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.660: INFO: Pod "webserver-deployment-69b7448995-kml9w" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kml9w webserver-deployment-69b7448995- deployment-104  867751c1-24ba-480e-a188-d530d09bf8fa 109405 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:2c7cdd19741bfab244a4a55881abfb9c762b0efca6bf4c4928dba8ecfd18178f cni.projectcalico.org/podIP:172.30.106.176/32 cni.projectcalico.org/podIPs:172.30.106.176/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.176"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.176"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8427 0xc0095d8428}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2g7s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2g7s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.660: INFO: Pod "webserver-deployment-69b7448995-nrjn9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-nrjn9 webserver-deployment-69b7448995- deployment-104  08f5ef78-5529-4b12-8443-7fb761f1bdd8 109351 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:1783d9cf21be02423cf679fa9f07b3f219caedd57711c2cf7d7932cc43493d78 cni.projectcalico.org/podIP:172.30.60.119/32 cni.projectcalico.org/podIPs:172.30.60.119/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.119"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.119"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d86b7 0xc0095d86b8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-95rvh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-95rvh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.660: INFO: Pod "webserver-deployment-69b7448995-nwtz9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-nwtz9 webserver-deployment-69b7448995- deployment-104  0c24af53-02d0-48fc-990e-7c523200193b 109377 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:e009d04efc9f4a50dafacc56f2a3d8849c70778f8a2d843eb8ee9c5197712ae7 cni.projectcalico.org/podIP:172.30.106.160/32 cni.projectcalico.org/podIPs:172.30.106.160/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.160"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.160"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8927 0xc0095d8928}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2srtd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2srtd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.661: INFO: Pod "webserver-deployment-69b7448995-r6w5l" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-r6w5l webserver-deployment-69b7448995- deployment-104  7ba1355e-84fe-43a7-b911-acd568009969 109288 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8b97 0xc0095d8b98}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7kvs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7kvs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.661: INFO: Pod "webserver-deployment-69b7448995-szd4c" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-szd4c webserver-deployment-69b7448995- deployment-104  1e8964e1-ea04-4175-8642-ecbd62023fae 109252 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8dc7 0xc0095d8dc8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dxxch,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dxxch,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.661: INFO: Pod "webserver-deployment-69b7448995-wlgh9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-wlgh9 webserver-deployment-69b7448995- deployment-104  47faef12-ae1c-4c38-a1a6-d2b94ccb02df 109398 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[cni.projectcalico.org/containerID:eb40d59700b639fa8daf5b502b13ee44e415ca504d479a096a4e4dffdbd856eb cni.projectcalico.org/podIP:172.30.250.201/32 cni.projectcalico.org/podIPs:172.30.250.201/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.201"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.201"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 2f49d3a5-0331-4889-9364-051a1465e1eb 0xc0095d8ff7 0xc0095d8ff8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2f49d3a5-0331-4889-9364-051a1465e1eb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4qkzn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4qkzn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.661: INFO: Pod "webserver-deployment-845c8977d9-4868m" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-4868m webserver-deployment-845c8977d9- deployment-104  3e33780f-9ba4-4550-9821-857a8bd260e0 109017 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:929576102f41c6fd0dfbce37f469dda2892a9542f0af2d38ed9c19721f7c382c cni.projectcalico.org/podIP:172.30.60.109/32 cni.projectcalico.org/podIPs:172.30.60.109/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.109"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.109"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d9287 0xc0095d9288}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.109\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5ktjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5ktjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.109,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://41dca4cfbfb43d5a152419cd7bed434741ddd62adee9708a07d0b5371fc73146,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.109,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.662: INFO: Pod "webserver-deployment-845c8977d9-5z8wn" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-5z8wn webserver-deployment-845c8977d9- deployment-104  da09b2ab-c16d-450e-896b-80b06153e2a6 109022 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:bd3154dfdba17734125e9a3da4149b40495a24aebe5e94ce8bea6f3aa9c63499 cni.projectcalico.org/podIP:172.30.106.166/32 cni.projectcalico.org/podIPs:172.30.106.166/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.166"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.166"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d94f7 0xc0095d94f8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.166\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4tp2b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4tp2b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:172.30.106.166,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://ab9728a3b61fa07693f5539b4070773e22ad8275678f97b77b8c48aae01abe5c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.166,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.662: INFO: Pod "webserver-deployment-845c8977d9-62s87" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-62s87 webserver-deployment-845c8977d9- deployment-104  72af7e03-c5fe-4d24-86c2-19ffc06a2b1d 109013 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:42206ecdd9dc964fd93f3f69a2067621030cece24adedff153d25215de6add4a cni.projectcalico.org/podIP:172.30.60.123/32 cni.projectcalico.org/podIPs:172.30.60.123/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.123"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.123"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d9787 0xc0095d9788}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nnvvx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nnvvx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.123,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://54a931ec01e69f9458e85f72a0562ef6240064c7a541e4068ce853231df7689f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.123,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.663: INFO: Pod "webserver-deployment-845c8977d9-9tsvq" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-9tsvq webserver-deployment-845c8977d9- deployment-104  1e472311-699e-41b4-b3b3-065fd5a7bc0c 109368 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:d921ab2830aabd1c1b2b9fd659ecaf1acc5e94ee3aeffbb117d82f0c5eb1fe2f cni.projectcalico.org/podIP:172.30.250.245/32 cni.projectcalico.org/podIPs:172.30.250.245/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.245"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.245"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d99f7 0xc0095d99f8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fdk7f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fdk7f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.663: INFO: Pod "webserver-deployment-845c8977d9-b47pr" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b47pr webserver-deployment-845c8977d9- deployment-104  1afb15be-81e5-420f-a53e-ff7557770793 109239 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d9c47 0xc0095d9c48}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t778w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t778w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.663: INFO: Pod "webserver-deployment-845c8977d9-b8qjz" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-b8qjz webserver-deployment-845c8977d9- deployment-104  141b0ed3-53a5-4c4e-b9ff-25f6d405f74e 109048 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:2642902aa1550f29db9085eb8ff2f99894c6c7d84dec1feba74b2b782fc4e05b cni.projectcalico.org/podIP:172.30.60.120/32 cni.projectcalico.org/podIPs:172.30.60.120/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.120"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.120"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc0095d9e77 0xc0095d9e78}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.120\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plxqw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plxqw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.120,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://70924d14f999b16f50002ae5606a0f4d6865e8a46f65aa903de4d7e3423330c8,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.120,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.663: INFO: Pod "webserver-deployment-845c8977d9-c4mj9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-c4mj9 webserver-deployment-845c8977d9- deployment-104  e6ab4a9a-7577-4410-ae6c-fddcdf7c6e63 109299 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d060f7 0xc003d060f8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-94z5q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-94z5q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.664: INFO: Pod "webserver-deployment-845c8977d9-cqtqk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-cqtqk webserver-deployment-845c8977d9- deployment-104  48dede5e-64fd-4d91-87e1-a2d045d935c2 109375 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:3dd07f50cde7b2415745430f7d70b03e69338f76b89ff1c09064c3068e5ed6cc cni.projectcalico.org/podIP:172.30.60.75/32 cni.projectcalico.org/podIPs:172.30.60.75/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.75"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.75"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d06367 0xc003d06368}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmlcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmlcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.664: INFO: Pod "webserver-deployment-845c8977d9-drmz8" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-drmz8 webserver-deployment-845c8977d9- deployment-104  9949925a-fc86-4429-a47d-2dd35bfaa5a5 109290 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d065b7 0xc003d065b8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nkfc5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nkfc5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.664: INFO: Pod "webserver-deployment-845c8977d9-f6dvj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-f6dvj webserver-deployment-845c8977d9- deployment-104  b67ed788-86ec-45e9-8ee3-2440959b2770 109419 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:e26876880aaeb736e3ac106987a25f1ae82b056c04c421b82e89b6621cf11850 cni.projectcalico.org/podIP:172.30.250.209/32 cni.projectcalico.org/podIPs:172.30.250.209/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.209"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.209"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d067c7 0xc003d067c8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkxfh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkxfh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.668: INFO: Pod "webserver-deployment-845c8977d9-gmtfd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gmtfd webserver-deployment-845c8977d9- deployment-104  ff1b97ad-b8ad-43f9-b4b8-68a2367c9132 109342 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:fd28853a51f0028666e98a5850ccbc41acfa6cf2daa952bb84cbb7450a49fe2a cni.projectcalico.org/podIP:172.30.250.218/32 cni.projectcalico.org/podIPs:172.30.250.218/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.218"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.218"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d06a17 0xc003d06a18}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fzvmh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fzvmh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.668: INFO: Pod "webserver-deployment-845c8977d9-h2q6w" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-h2q6w webserver-deployment-845c8977d9- deployment-104  93c57dde-f804-455b-b70d-1e3204af2445 109035 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:5fd78f4c2d1dfcfedcd8fb05df01b478d921e5f63b9bbd18dcb7ad9ae358eda0 cni.projectcalico.org/podIP:172.30.250.216/32 cni.projectcalico.org/podIPs:172.30.250.216/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.216"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.216"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d06c77 0xc003d06c78}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-twvkd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-twvkd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.216,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://fd85ca4faafe42076c625ab58d1d2a21e8a6113013bf9ac3dba24aa7f2308aa7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.669: INFO: Pod "webserver-deployment-845c8977d9-k4fkj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-k4fkj webserver-deployment-845c8977d9- deployment-104  38310f72-a557-4a18-a84b-fefe6e6e17ca 109258 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d06ee7 0xc003d06ee8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ph6zj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ph6zj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.672: INFO: Pod "webserver-deployment-845c8977d9-lnmzp" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lnmzp webserver-deployment-845c8977d9- deployment-104  66f99ada-4433-4000-94c0-d5ef2803a792 109024 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:f20c8df303cb259c78be0a21633a3d1f92ca27425d05104ba71d2c5a592d3a0d cni.projectcalico.org/podIP:172.30.106.159/32 cni.projectcalico.org/podIPs:172.30.106.159/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.159"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.159"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d070f7 0xc003d070f8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.159\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8lq7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8lq7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:172.30.106.159,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://42e1d9a3dbc115522a82405944f71bc52317715e30b946b8099fb53939b984fd,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.159,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.672: INFO: Pod "webserver-deployment-845c8977d9-mn6p2" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-mn6p2 webserver-deployment-845c8977d9- deployment-104  9480c468-4c95-4ceb-8913-a095b974eff8 109031 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:1eedf708ee910c9e618b250e1fc6191cc06dfa5fb90ffbead825a6e13193ffc9 cni.projectcalico.org/podIP:172.30.106.173/32 cni.projectcalico.org/podIPs:172.30.106.173/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.173"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.173"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07367 0xc003d07368}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.173\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5bjx8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bjx8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:172.30.106.173,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://566f400d10b29afd70fc7f5e6b1e5668cb263ddb7d00c42fbc20ce79337cddff,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.173,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.672: INFO: Pod "webserver-deployment-845c8977d9-qw89q" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-qw89q webserver-deployment-845c8977d9- deployment-104  4e58830a-3d22-4a13-a72f-b329636981cc 109038 0 2023-06-27 17:19:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4db218fe96fc5b238aa0b4ed2b069511925b5a769aa01c985914fcc5becebb6e cni.projectcalico.org/podIP:172.30.250.211/32 cni.projectcalico.org/podIPs:172.30.250.211/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.211"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.211"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d075d7 0xc003d075d8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:19:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.211\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9whz2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9whz2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.211,StartTime:2023-06-27 17:19:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:19:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d04086e5a12002c2660583b86845c174749e313711e6ede4a6e3a1e393b95fc6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.211,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.673: INFO: Pod "webserver-deployment-845c8977d9-rzmms" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rzmms webserver-deployment-845c8977d9- deployment-104  842de420-cdea-452d-98aa-ccf6ca48e8bf 109416 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:9ecd7aae0655cdd056845e6fc7a3c7ec636c02f9a61ef580662d7892318e6689 cni.projectcalico.org/podIP:172.30.106.162/32 cni.projectcalico.org/podIPs:172.30.106.162/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.162"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.162"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07857 0xc003d07858}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zj45b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zj45b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.673: INFO: Pod "webserver-deployment-845c8977d9-t9w6k" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-t9w6k webserver-deployment-845c8977d9- deployment-104  60dd64ec-ab23-433c-9509-a2527d42dfbb 109414 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:4f740d3dd18130e794544b87bb1fecf33ab72b35ca2e4eba6e7e7b7c175d63ed cni.projectcalico.org/podIP:172.30.60.108/32 cni.projectcalico.org/podIPs:172.30.60.108/32 openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07ac7 0xc003d07ac8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s6td4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s6td4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.673: INFO: Pod "webserver-deployment-845c8977d9-wq8d6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wq8d6 webserver-deployment-845c8977d9- deployment-104  288c78ad-d1c2-4c13-89c9-5736580b4bb5 109338 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:0476ef7dd4d053806b8473f1621d2c2f33db1460095f6715fc941a0280edfa0e cni.projectcalico.org/podIP:172.30.106.157/32 cni.projectcalico.org/podIPs:172.30.106.157/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.157"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.157"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07cf7 0xc003d07cf8}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bm2vj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bm2vj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:19:13.674: INFO: Pod "webserver-deployment-845c8977d9-wrsvp" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-wrsvp webserver-deployment-845c8977d9- deployment-104  aa964780-5c64-4e99-8744-299777fe25c7 109350 0 2023-06-27 17:19:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:09b2f82351861267485713337311756d2ef099a3c34560bc6a968490457add10 cni.projectcalico.org/podIP:172.30.106.164/32 cni.projectcalico.org/podIPs:172.30.106.164/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.164"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.164"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 c50172c5-7a5b-4910-8bd6-65345d1b2b2d 0xc003d07f57 0xc003d07f58}] [] [{kube-controller-manager Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c50172c5-7a5b-4910-8bd6-65345d1b2b2d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:19:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:19:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xd4hj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xd4hj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c54,c19,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kjtjb,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:19:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:,StartTime:2023-06-27 17:19:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 27 17:19:13.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-104" for this suite. 06/27/23 17:19:13.694
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:19:13.732
Jun 27 17:19:13.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename security-context 06/27/23 17:19:13.734
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:13.767
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:13.791
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/27/23 17:19:13.802
Jun 27 17:19:13.850: INFO: Waiting up to 5m0s for pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0" in namespace "security-context-4253" to be "Succeeded or Failed"
Jun 27 17:19:13.861: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.939899ms
Jun 27 17:19:15.872: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022115863s
Jun 27 17:19:17.885: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034362246s
Jun 27 17:19:19.873: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022945606s
STEP: Saw pod success 06/27/23 17:19:19.873
Jun 27 17:19:19.874: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0" satisfied condition "Succeeded or Failed"
Jun 27 17:19:19.882: INFO: Trying to get logs from node 10.113.180.90 pod security-context-c73189ea-809d-45bf-9197-0ce4fff648b0 container test-container: <nil>
STEP: delete the pod 06/27/23 17:19:19.906
Jun 27 17:19:19.928: INFO: Waiting for pod security-context-c73189ea-809d-45bf-9197-0ce4fff648b0 to disappear
Jun 27 17:19:19.937: INFO: Pod security-context-c73189ea-809d-45bf-9197-0ce4fff648b0 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 27 17:19:19.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-4253" for this suite. 06/27/23 17:19:19.956
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":190,"skipped":3880,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.249 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:19:13.732
    Jun 27 17:19:13.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename security-context 06/27/23 17:19:13.734
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:13.767
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:13.791
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/27/23 17:19:13.802
    Jun 27 17:19:13.850: INFO: Waiting up to 5m0s for pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0" in namespace "security-context-4253" to be "Succeeded or Failed"
    Jun 27 17:19:13.861: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0": Phase="Pending", Reason="", readiness=false. Elapsed: 10.939899ms
    Jun 27 17:19:15.872: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022115863s
    Jun 27 17:19:17.885: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034362246s
    Jun 27 17:19:19.873: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022945606s
    STEP: Saw pod success 06/27/23 17:19:19.873
    Jun 27 17:19:19.874: INFO: Pod "security-context-c73189ea-809d-45bf-9197-0ce4fff648b0" satisfied condition "Succeeded or Failed"
    Jun 27 17:19:19.882: INFO: Trying to get logs from node 10.113.180.90 pod security-context-c73189ea-809d-45bf-9197-0ce4fff648b0 container test-container: <nil>
    STEP: delete the pod 06/27/23 17:19:19.906
    Jun 27 17:19:19.928: INFO: Waiting for pod security-context-c73189ea-809d-45bf-9197-0ce4fff648b0 to disappear
    Jun 27 17:19:19.937: INFO: Pod security-context-c73189ea-809d-45bf-9197-0ce4fff648b0 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 27 17:19:19.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-4253" for this suite. 06/27/23 17:19:19.956
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:19:19.982
Jun 27 17:19:19.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 17:19:19.986
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:20.025
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:20.036
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-3a1ceb33-801b-431f-9579-c7be5415bf77 06/27/23 17:19:20.095
STEP: Creating a pod to test consume secrets 06/27/23 17:19:20.115
Jun 27 17:19:20.169: INFO: Waiting up to 5m0s for pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9" in namespace "secrets-8896" to be "Succeeded or Failed"
Jun 27 17:19:20.183: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.741211ms
Jun 27 17:19:22.196: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027166732s
Jun 27 17:19:24.197: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02774859s
Jun 27 17:19:26.197: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028225045s
Jun 27 17:19:28.196: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026478808s
STEP: Saw pod success 06/27/23 17:19:28.196
Jun 27 17:19:28.196: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9" satisfied condition "Succeeded or Failed"
Jun 27 17:19:28.206: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9 container secret-volume-test: <nil>
STEP: delete the pod 06/27/23 17:19:28.233
Jun 27 17:19:28.257: INFO: Waiting for pod pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9 to disappear
Jun 27 17:19:28.266: INFO: Pod pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 27 17:19:28.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8896" for this suite. 06/27/23 17:19:28.282
STEP: Destroying namespace "secret-namespace-2806" for this suite. 06/27/23 17:19:28.301
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":191,"skipped":3882,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.339 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:19:19.982
    Jun 27 17:19:19.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 17:19:19.986
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:20.025
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:20.036
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-3a1ceb33-801b-431f-9579-c7be5415bf77 06/27/23 17:19:20.095
    STEP: Creating a pod to test consume secrets 06/27/23 17:19:20.115
    Jun 27 17:19:20.169: INFO: Waiting up to 5m0s for pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9" in namespace "secrets-8896" to be "Succeeded or Failed"
    Jun 27 17:19:20.183: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Pending", Reason="", readiness=false. Elapsed: 13.741211ms
    Jun 27 17:19:22.196: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027166732s
    Jun 27 17:19:24.197: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02774859s
    Jun 27 17:19:26.197: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.028225045s
    Jun 27 17:19:28.196: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.026478808s
    STEP: Saw pod success 06/27/23 17:19:28.196
    Jun 27 17:19:28.196: INFO: Pod "pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9" satisfied condition "Succeeded or Failed"
    Jun 27 17:19:28.206: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9 container secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:19:28.233
    Jun 27 17:19:28.257: INFO: Waiting for pod pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9 to disappear
    Jun 27 17:19:28.266: INFO: Pod pod-secrets-3b9257a0-6a26-449a-913b-efab61c7dee9 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 17:19:28.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8896" for this suite. 06/27/23 17:19:28.282
    STEP: Destroying namespace "secret-namespace-2806" for this suite. 06/27/23 17:19:28.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:19:28.338
Jun 27 17:19:28.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 17:19:28.339
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:28.385
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:28.394
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 17:19:28.447
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:19:28.714
STEP: Deploying the webhook pod 06/27/23 17:19:28.753
STEP: Wait for the deployment to be ready 06/27/23 17:19:28.812
Jun 27 17:19:28.836: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 27 17:19:30.873: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 19, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 19, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 19, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 19, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 17:19:32.885
STEP: Verifying the service has paired with the endpoint 06/27/23 17:19:32.917
Jun 27 17:19:33.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/27/23 17:19:33.929
STEP: create a pod that should be updated by the webhook 06/27/23 17:19:34.014
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:19:34.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9316" for this suite. 06/27/23 17:19:34.266
STEP: Destroying namespace "webhook-9316-markers" for this suite. 06/27/23 17:19:34.292
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":192,"skipped":3900,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.190 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:19:28.338
    Jun 27 17:19:28.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 17:19:28.339
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:28.385
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:28.394
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 17:19:28.447
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:19:28.714
    STEP: Deploying the webhook pod 06/27/23 17:19:28.753
    STEP: Wait for the deployment to be ready 06/27/23 17:19:28.812
    Jun 27 17:19:28.836: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 27 17:19:30.873: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 19, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 19, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 19, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 19, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 17:19:32.885
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:19:32.917
    Jun 27 17:19:33.918: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 06/27/23 17:19:33.929
    STEP: create a pod that should be updated by the webhook 06/27/23 17:19:34.014
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:19:34.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9316" for this suite. 06/27/23 17:19:34.266
    STEP: Destroying namespace "webhook-9316-markers" for this suite. 06/27/23 17:19:34.292
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:19:34.529
Jun 27 17:19:34.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 17:19:34.531
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:34.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:34.599
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 06/27/23 17:19:34.632
Jun 27 17:19:34.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-7084 cluster-info'
Jun 27 17:19:34.823: INFO: stderr: ""
Jun 27 17:19:34.823: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 17:19:34.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7084" for this suite. 06/27/23 17:19:34.855
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":193,"skipped":3906,"failed":0}
------------------------------
â€¢ [0.377 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:19:34.529
    Jun 27 17:19:34.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 17:19:34.531
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:34.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:34.599
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 06/27/23 17:19:34.632
    Jun 27 17:19:34.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-7084 cluster-info'
    Jun 27 17:19:34.823: INFO: stderr: ""
    Jun 27 17:19:34.823: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://172.21.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 17:19:34.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7084" for this suite. 06/27/23 17:19:34.855
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:19:34.906
Jun 27 17:19:34.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename statefulset 06/27/23 17:19:34.907
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:34.955
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:34.972
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-423 06/27/23 17:19:34.98
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 06/27/23 17:19:35.004
STEP: Creating pod with conflicting port in namespace statefulset-423 06/27/23 17:19:35.02
STEP: Waiting until pod test-pod will start running in namespace statefulset-423 06/27/23 17:19:35.06
Jun 27 17:19:35.060: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-423" to be "running"
Jun 27 17:19:35.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.756277ms
Jun 27 17:19:37.087: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026478707s
Jun 27 17:19:39.084: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.023472133s
Jun 27 17:19:39.084: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-423 06/27/23 17:19:39.084
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-423 06/27/23 17:19:39.101
Jun 27 17:19:39.161: INFO: Observed stateful pod in namespace: statefulset-423, name: ss-0, uid: f007f90e-c739-43b9-84ad-e11322850228, status phase: Pending. Waiting for statefulset controller to delete.
Jun 27 17:19:39.215: INFO: Observed stateful pod in namespace: statefulset-423, name: ss-0, uid: f007f90e-c739-43b9-84ad-e11322850228, status phase: Failed. Waiting for statefulset controller to delete.
Jun 27 17:19:39.231: INFO: Observed stateful pod in namespace: statefulset-423, name: ss-0, uid: f007f90e-c739-43b9-84ad-e11322850228, status phase: Failed. Waiting for statefulset controller to delete.
Jun 27 17:19:39.244: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-423
STEP: Removing pod with conflicting port in namespace statefulset-423 06/27/23 17:19:39.244
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-423 and will be in running state 06/27/23 17:19:39.281
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 27 17:19:45.364: INFO: Deleting all statefulset in ns statefulset-423
Jun 27 17:19:45.377: INFO: Scaling statefulset ss to 0
Jun 27 17:19:55.453: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:19:55.468: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 27 17:19:55.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-423" for this suite. 06/27/23 17:19:55.555
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":194,"skipped":3909,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.671 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:19:34.906
    Jun 27 17:19:34.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename statefulset 06/27/23 17:19:34.907
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:34.955
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:34.972
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-423 06/27/23 17:19:34.98
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 06/27/23 17:19:35.004
    STEP: Creating pod with conflicting port in namespace statefulset-423 06/27/23 17:19:35.02
    STEP: Waiting until pod test-pod will start running in namespace statefulset-423 06/27/23 17:19:35.06
    Jun 27 17:19:35.060: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-423" to be "running"
    Jun 27 17:19:35.073: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 12.756277ms
    Jun 27 17:19:37.087: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026478707s
    Jun 27 17:19:39.084: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=false. Elapsed: 4.023472133s
    Jun 27 17:19:39.084: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-423 06/27/23 17:19:39.084
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-423 06/27/23 17:19:39.101
    Jun 27 17:19:39.161: INFO: Observed stateful pod in namespace: statefulset-423, name: ss-0, uid: f007f90e-c739-43b9-84ad-e11322850228, status phase: Pending. Waiting for statefulset controller to delete.
    Jun 27 17:19:39.215: INFO: Observed stateful pod in namespace: statefulset-423, name: ss-0, uid: f007f90e-c739-43b9-84ad-e11322850228, status phase: Failed. Waiting for statefulset controller to delete.
    Jun 27 17:19:39.231: INFO: Observed stateful pod in namespace: statefulset-423, name: ss-0, uid: f007f90e-c739-43b9-84ad-e11322850228, status phase: Failed. Waiting for statefulset controller to delete.
    Jun 27 17:19:39.244: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-423
    STEP: Removing pod with conflicting port in namespace statefulset-423 06/27/23 17:19:39.244
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-423 and will be in running state 06/27/23 17:19:39.281
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 27 17:19:45.364: INFO: Deleting all statefulset in ns statefulset-423
    Jun 27 17:19:45.377: INFO: Scaling statefulset ss to 0
    Jun 27 17:19:55.453: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 17:19:55.468: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 27 17:19:55.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-423" for this suite. 06/27/23 17:19:55.555
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:19:55.581
Jun 27 17:19:55.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 17:19:55.584
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:55.623
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:55.633
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 06/27/23 17:19:55.645
Jun 27 17:19:55.789: INFO: Waiting up to 5m0s for pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0" in namespace "emptydir-1657" to be "Succeeded or Failed"
Jun 27 17:19:55.807: INFO: Pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.327548ms
Jun 27 17:19:57.818: INFO: Pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029376159s
Jun 27 17:19:59.819: INFO: Pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030061087s
STEP: Saw pod success 06/27/23 17:19:59.819
Jun 27 17:19:59.819: INFO: Pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0" satisfied condition "Succeeded or Failed"
Jun 27 17:19:59.829: INFO: Trying to get logs from node 10.113.180.90 pod pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0 container test-container: <nil>
STEP: delete the pod 06/27/23 17:19:59.856
Jun 27 17:19:59.880: INFO: Waiting for pod pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0 to disappear
Jun 27 17:19:59.889: INFO: Pod pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 17:19:59.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1657" for this suite. 06/27/23 17:19:59.908
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":195,"skipped":3915,"failed":0}
------------------------------
â€¢ [4.345 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:19:55.581
    Jun 27 17:19:55.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 17:19:55.584
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:55.623
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:55.633
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/27/23 17:19:55.645
    Jun 27 17:19:55.789: INFO: Waiting up to 5m0s for pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0" in namespace "emptydir-1657" to be "Succeeded or Failed"
    Jun 27 17:19:55.807: INFO: Pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 18.327548ms
    Jun 27 17:19:57.818: INFO: Pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029376159s
    Jun 27 17:19:59.819: INFO: Pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.030061087s
    STEP: Saw pod success 06/27/23 17:19:59.819
    Jun 27 17:19:59.819: INFO: Pod "pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0" satisfied condition "Succeeded or Failed"
    Jun 27 17:19:59.829: INFO: Trying to get logs from node 10.113.180.90 pod pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0 container test-container: <nil>
    STEP: delete the pod 06/27/23 17:19:59.856
    Jun 27 17:19:59.880: INFO: Waiting for pod pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0 to disappear
    Jun 27 17:19:59.889: INFO: Pod pod-12b0c1a9-720e-4f5c-bd0f-4d1d3749a5c0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:19:59.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1657" for this suite. 06/27/23 17:19:59.908
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:19:59.935
Jun 27 17:19:59.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 17:19:59.938
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:59.981
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:59.993
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 06/27/23 17:20:00.01
STEP: listing secrets in all namespaces to ensure that there are more than zero 06/27/23 17:20:00.025
STEP: patching the secret 06/27/23 17:20:00.157
STEP: deleting the secret using a LabelSelector 06/27/23 17:20:00.183
STEP: listing secrets in all namespaces, searching for label name and value in patch 06/27/23 17:20:00.205
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 27 17:20:00.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-927" for this suite. 06/27/23 17:20:00.348
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":196,"skipped":3927,"failed":0}
------------------------------
â€¢ [0.431 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:19:59.935
    Jun 27 17:19:59.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 17:19:59.938
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:19:59.981
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:19:59.993
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 06/27/23 17:20:00.01
    STEP: listing secrets in all namespaces to ensure that there are more than zero 06/27/23 17:20:00.025
    STEP: patching the secret 06/27/23 17:20:00.157
    STEP: deleting the secret using a LabelSelector 06/27/23 17:20:00.183
    STEP: listing secrets in all namespaces, searching for label name and value in patch 06/27/23 17:20:00.205
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 17:20:00.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-927" for this suite. 06/27/23 17:20:00.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:20:00.371
Jun 27 17:20:00.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replicaset 06/27/23 17:20:00.373
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:20:00.427
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:20:00.438
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/27/23 17:20:00.449
Jun 27 17:20:00.502: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-698" to be "running and ready"
Jun 27 17:20:00.514: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 11.761144ms
Jun 27 17:20:00.514: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:20:02.529: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.026538699s
Jun 27 17:20:02.529: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Jun 27 17:20:02.529: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 06/27/23 17:20:02.538
STEP: Then the orphan pod is adopted 06/27/23 17:20:02.551
STEP: When the matched label of one of its pods change 06/27/23 17:20:03.576
Jun 27 17:20:03.586: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 06/27/23 17:20:03.627
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 27 17:20:04.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-698" for this suite. 06/27/23 17:20:04.722
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":197,"skipped":3949,"failed":0}
------------------------------
â€¢ [4.371 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:20:00.371
    Jun 27 17:20:00.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replicaset 06/27/23 17:20:00.373
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:20:00.427
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:20:00.438
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 06/27/23 17:20:00.449
    Jun 27 17:20:00.502: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-698" to be "running and ready"
    Jun 27 17:20:00.514: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 11.761144ms
    Jun 27 17:20:00.514: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:20:02.529: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.026538699s
    Jun 27 17:20:02.529: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Jun 27 17:20:02.529: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 06/27/23 17:20:02.538
    STEP: Then the orphan pod is adopted 06/27/23 17:20:02.551
    STEP: When the matched label of one of its pods change 06/27/23 17:20:03.576
    Jun 27 17:20:03.586: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 06/27/23 17:20:03.627
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 27 17:20:04.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-698" for this suite. 06/27/23 17:20:04.722
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:20:04.742
Jun 27 17:20:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename statefulset 06/27/23 17:20:04.744
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:20:04.783
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:20:04.792
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6957 06/27/23 17:20:04.804
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 06/27/23 17:20:04.824
STEP: Creating stateful set ss in namespace statefulset-6957 06/27/23 17:20:04.837
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6957 06/27/23 17:20:04.856
Jun 27 17:20:04.868: INFO: Found 0 stateful pods, waiting for 1
Jun 27 17:20:14.879: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/27/23 17:20:14.879
Jun 27 17:20:14.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 17:20:15.300: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 17:20:15.300: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 17:20:15.300: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 27 17:20:15.310: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Jun 27 17:20:25.337: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 17:20:25.337: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:20:25.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999996133s
Jun 27 17:20:26.405: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.987903304s
Jun 27 17:20:27.415: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.977149894s
Jun 27 17:20:28.428: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966936079s
Jun 27 17:20:29.440: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.953662298s
Jun 27 17:20:30.451: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.941561811s
Jun 27 17:20:31.461: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.931167007s
Jun 27 17:20:32.471: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.920482045s
Jun 27 17:20:33.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.910222933s
Jun 27 17:20:34.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 899.373171ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6957 06/27/23 17:20:35.501
Jun 27 17:20:35.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 27 17:20:35.862: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 27 17:20:35.862: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 27 17:20:35.862: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 27 17:20:35.873: INFO: Found 1 stateful pods, waiting for 3
Jun 27 17:20:45.887: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:20:45.887: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:20:45.888: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 06/27/23 17:20:45.888
STEP: Scale down will halt with unhealthy stateful pod 06/27/23 17:20:45.888
Jun 27 17:20:45.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 17:20:46.298: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 17:20:46.298: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 17:20:46.298: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 27 17:20:46.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 17:20:46.729: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 17:20:46.729: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 17:20:46.729: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 27 17:20:46.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 17:20:47.166: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 17:20:47.166: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 17:20:47.166: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 27 17:20:47.166: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:20:47.181: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Jun 27 17:20:57.215: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 17:20:57.215: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 17:20:57.215: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Jun 27 17:20:57.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996774s
Jun 27 17:20:58.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98645504s
Jun 27 17:20:59.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973525011s
Jun 27 17:21:00.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960415954s
Jun 27 17:21:01.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.946310256s
Jun 27 17:21:02.333: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.932408383s
Jun 27 17:21:03.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.919261982s
Jun 27 17:21:04.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.906385366s
Jun 27 17:21:05.386: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.890720547s
Jun 27 17:21:06.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 865.321448ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6957 06/27/23 17:21:07.4
Jun 27 17:21:07.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 27 17:21:07.892: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 27 17:21:07.892: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 27 17:21:07.892: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 27 17:21:07.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 27 17:21:08.266: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 27 17:21:08.266: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 27 17:21:08.266: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 27 17:21:08.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 27 17:21:08.685: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 27 17:21:08.685: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 27 17:21:08.685: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 27 17:21:08.685: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 06/27/23 17:21:18.744
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 27 17:21:18.744: INFO: Deleting all statefulset in ns statefulset-6957
Jun 27 17:21:18.759: INFO: Scaling statefulset ss to 0
Jun 27 17:21:18.797: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:21:18.832: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 27 17:21:18.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6957" for this suite. 06/27/23 17:21:18.903
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":198,"skipped":3954,"failed":0}
------------------------------
â€¢ [SLOW TEST] [74.180 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:20:04.742
    Jun 27 17:20:04.743: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename statefulset 06/27/23 17:20:04.744
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:20:04.783
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:20:04.792
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6957 06/27/23 17:20:04.804
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 06/27/23 17:20:04.824
    STEP: Creating stateful set ss in namespace statefulset-6957 06/27/23 17:20:04.837
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6957 06/27/23 17:20:04.856
    Jun 27 17:20:04.868: INFO: Found 0 stateful pods, waiting for 1
    Jun 27 17:20:14.879: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 06/27/23 17:20:14.879
    Jun 27 17:20:14.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 17:20:15.300: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 17:20:15.300: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 17:20:15.300: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 27 17:20:15.310: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Jun 27 17:20:25.337: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 27 17:20:25.337: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 17:20:25.394: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999996133s
    Jun 27 17:20:26.405: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.987903304s
    Jun 27 17:20:27.415: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.977149894s
    Jun 27 17:20:28.428: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.966936079s
    Jun 27 17:20:29.440: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.953662298s
    Jun 27 17:20:30.451: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.941561811s
    Jun 27 17:20:31.461: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.931167007s
    Jun 27 17:20:32.471: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.920482045s
    Jun 27 17:20:33.482: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.910222933s
    Jun 27 17:20:34.500: INFO: Verifying statefulset ss doesn't scale past 1 for another 899.373171ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6957 06/27/23 17:20:35.501
    Jun 27 17:20:35.518: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 27 17:20:35.862: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 27 17:20:35.862: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 27 17:20:35.862: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 27 17:20:35.873: INFO: Found 1 stateful pods, waiting for 3
    Jun 27 17:20:45.887: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 17:20:45.887: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 17:20:45.888: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 06/27/23 17:20:45.888
    STEP: Scale down will halt with unhealthy stateful pod 06/27/23 17:20:45.888
    Jun 27 17:20:45.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 17:20:46.298: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 17:20:46.298: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 17:20:46.298: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 27 17:20:46.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 17:20:46.729: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 17:20:46.729: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 17:20:46.729: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 27 17:20:46.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 17:20:47.166: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 17:20:47.166: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 17:20:47.166: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 27 17:20:47.166: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 17:20:47.181: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Jun 27 17:20:57.215: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Jun 27 17:20:57.215: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Jun 27 17:20:57.215: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Jun 27 17:20:57.266: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999996774s
    Jun 27 17:20:58.279: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.98645504s
    Jun 27 17:20:59.292: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973525011s
    Jun 27 17:21:00.305: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.960415954s
    Jun 27 17:21:01.319: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.946310256s
    Jun 27 17:21:02.333: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.932408383s
    Jun 27 17:21:03.345: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.919261982s
    Jun 27 17:21:04.358: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.906385366s
    Jun 27 17:21:05.386: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.890720547s
    Jun 27 17:21:06.400: INFO: Verifying statefulset ss doesn't scale past 3 for another 865.321448ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6957 06/27/23 17:21:07.4
    Jun 27 17:21:07.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 27 17:21:07.892: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 27 17:21:07.892: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 27 17:21:07.892: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 27 17:21:07.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 27 17:21:08.266: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 27 17:21:08.266: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 27 17:21:08.266: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 27 17:21:08.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-6957 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 27 17:21:08.685: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 27 17:21:08.685: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 27 17:21:08.685: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 27 17:21:08.685: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 06/27/23 17:21:18.744
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 27 17:21:18.744: INFO: Deleting all statefulset in ns statefulset-6957
    Jun 27 17:21:18.759: INFO: Scaling statefulset ss to 0
    Jun 27 17:21:18.797: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 17:21:18.832: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 27 17:21:18.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6957" for this suite. 06/27/23 17:21:18.903
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:21:18.925
Jun 27 17:21:18.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:21:18.928
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:21:18.96
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:21:18.972
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:21:18.988
Jun 27 17:21:19.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202" in namespace "downward-api-7227" to be "Succeeded or Failed"
Jun 27 17:21:19.051: INFO: Pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202": Phase="Pending", Reason="", readiness=false. Elapsed: 21.152582ms
Jun 27 17:21:21.062: INFO: Pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032337801s
Jun 27 17:21:23.066: INFO: Pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036754706s
STEP: Saw pod success 06/27/23 17:21:23.066
Jun 27 17:21:23.067: INFO: Pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202" satisfied condition "Succeeded or Failed"
Jun 27 17:21:23.077: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202 container client-container: <nil>
STEP: delete the pod 06/27/23 17:21:23.104
Jun 27 17:21:23.132: INFO: Waiting for pod downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202 to disappear
Jun 27 17:21:23.142: INFO: Pod downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 17:21:23.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7227" for this suite. 06/27/23 17:21:23.157
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":199,"skipped":3962,"failed":0}
------------------------------
â€¢ [4.249 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:21:18.925
    Jun 27 17:21:18.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:21:18.928
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:21:18.96
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:21:18.972
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:21:18.988
    Jun 27 17:21:19.029: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202" in namespace "downward-api-7227" to be "Succeeded or Failed"
    Jun 27 17:21:19.051: INFO: Pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202": Phase="Pending", Reason="", readiness=false. Elapsed: 21.152582ms
    Jun 27 17:21:21.062: INFO: Pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032337801s
    Jun 27 17:21:23.066: INFO: Pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.036754706s
    STEP: Saw pod success 06/27/23 17:21:23.066
    Jun 27 17:21:23.067: INFO: Pod "downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202" satisfied condition "Succeeded or Failed"
    Jun 27 17:21:23.077: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:21:23.104
    Jun 27 17:21:23.132: INFO: Waiting for pod downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202 to disappear
    Jun 27 17:21:23.142: INFO: Pod downwardapi-volume-d8b9bfaa-d604-4aea-94f6-c50c7dbc7202 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 17:21:23.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7227" for this suite. 06/27/23 17:21:23.157
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:21:23.181
Jun 27 17:21:23.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename events 06/27/23 17:21:23.184
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:21:23.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:21:23.234
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 06/27/23 17:21:23.249
STEP: listing all events in all namespaces 06/27/23 17:21:23.26
STEP: patching the test event 06/27/23 17:21:23.33
STEP: fetching the test event 06/27/23 17:21:23.349
STEP: updating the test event 06/27/23 17:21:23.359
STEP: getting the test event 06/27/23 17:21:23.387
STEP: deleting the test event 06/27/23 17:21:23.397
STEP: listing all events in all namespaces 06/27/23 17:21:23.418
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Jun 27 17:21:23.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-6498" for this suite. 06/27/23 17:21:23.493
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":200,"skipped":3988,"failed":0}
------------------------------
â€¢ [0.330 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:21:23.181
    Jun 27 17:21:23.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename events 06/27/23 17:21:23.184
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:21:23.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:21:23.234
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 06/27/23 17:21:23.249
    STEP: listing all events in all namespaces 06/27/23 17:21:23.26
    STEP: patching the test event 06/27/23 17:21:23.33
    STEP: fetching the test event 06/27/23 17:21:23.349
    STEP: updating the test event 06/27/23 17:21:23.359
    STEP: getting the test event 06/27/23 17:21:23.387
    STEP: deleting the test event 06/27/23 17:21:23.397
    STEP: listing all events in all namespaces 06/27/23 17:21:23.418
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Jun 27 17:21:23.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-6498" for this suite. 06/27/23 17:21:23.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:21:23.516
Jun 27 17:21:23.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename var-expansion 06/27/23 17:21:23.519
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:21:23.557
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:21:23.569
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 06/27/23 17:21:23.579
Jun 27 17:21:23.629: INFO: Waiting up to 2m0s for pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" in namespace "var-expansion-7074" to be "running"
Jun 27 17:21:23.643: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 13.151772ms
Jun 27 17:21:25.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024982006s
Jun 27 17:21:27.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024371674s
Jun 27 17:21:29.662: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0318043s
Jun 27 17:21:31.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025210788s
Jun 27 17:21:33.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024128064s
Jun 27 17:21:35.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023739259s
Jun 27 17:21:37.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 14.023913655s
Jun 27 17:21:39.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 16.026365372s
Jun 27 17:21:41.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 18.023884381s
Jun 27 17:21:43.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026248543s
Jun 27 17:21:45.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 22.026115969s
Jun 27 17:21:47.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026936808s
Jun 27 17:21:49.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 26.026053877s
Jun 27 17:21:51.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 28.02560826s
Jun 27 17:21:53.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02683224s
Jun 27 17:21:55.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 32.026083764s
Jun 27 17:21:57.668: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 34.038575254s
Jun 27 17:21:59.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 36.026168444s
Jun 27 17:22:01.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02661473s
Jun 27 17:22:03.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025975053s
Jun 27 17:22:05.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 42.025666015s
Jun 27 17:22:07.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025448588s
Jun 27 17:22:09.661: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030904219s
Jun 27 17:22:11.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024115618s
Jun 27 17:22:13.658: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 50.027726511s
Jun 27 17:22:15.663: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 52.033295123s
Jun 27 17:22:17.661: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 54.031158685s
Jun 27 17:22:19.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 56.025239082s
Jun 27 17:22:21.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 58.025797477s
Jun 27 17:22:23.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.025177585s
Jun 27 17:22:25.670: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.04016948s
Jun 27 17:22:27.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.024281903s
Jun 27 17:22:29.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.024579935s
Jun 27 17:22:31.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024294325s
Jun 27 17:22:33.658: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.028079177s
Jun 27 17:22:35.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.024894648s
Jun 27 17:22:37.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.025975272s
Jun 27 17:22:39.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.025846467s
Jun 27 17:22:41.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025863106s
Jun 27 17:22:43.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.027248148s
Jun 27 17:22:45.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.024859929s
Jun 27 17:22:47.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.026933147s
Jun 27 17:22:49.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.025474706s
Jun 27 17:22:51.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.026069703s
Jun 27 17:22:53.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025278112s
Jun 27 17:22:55.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.024964816s
Jun 27 17:22:57.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024473442s
Jun 27 17:22:59.662: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.032627515s
Jun 27 17:23:01.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025907723s
Jun 27 17:23:03.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.025342391s
Jun 27 17:23:05.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.024843249s
Jun 27 17:23:07.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025728324s
Jun 27 17:23:09.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.025669201s
Jun 27 17:23:11.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.025437284s
Jun 27 17:23:13.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.025679018s
Jun 27 17:23:15.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.026947806s
Jun 27 17:23:17.658: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.027727327s
Jun 27 17:23:19.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025610104s
Jun 27 17:23:21.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026650477s
Jun 27 17:23:23.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.026334309s
Jun 27 17:23:23.667: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.036850576s
STEP: updating the pod 06/27/23 17:23:23.667
Jun 27 17:23:24.242: INFO: Successfully updated pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14"
STEP: waiting for pod running 06/27/23 17:23:24.242
Jun 27 17:23:24.242: INFO: Waiting up to 2m0s for pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" in namespace "var-expansion-7074" to be "running"
Jun 27 17:23:24.253: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.408283ms
Jun 27 17:23:26.265: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Running", Reason="", readiness=true. Elapsed: 2.023098886s
Jun 27 17:23:26.265: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" satisfied condition "running"
STEP: deleting the pod gracefully 06/27/23 17:23:26.265
Jun 27 17:23:26.266: INFO: Deleting pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" in namespace "var-expansion-7074"
Jun 27 17:23:26.283: INFO: Wait up to 5m0s for pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 27 17:23:58.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-7074" for this suite. 06/27/23 17:23:58.325
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":201,"skipped":4000,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.826 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:21:23.516
    Jun 27 17:21:23.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename var-expansion 06/27/23 17:21:23.519
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:21:23.557
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:21:23.569
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 06/27/23 17:21:23.579
    Jun 27 17:21:23.629: INFO: Waiting up to 2m0s for pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" in namespace "var-expansion-7074" to be "running"
    Jun 27 17:21:23.643: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 13.151772ms
    Jun 27 17:21:25.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024982006s
    Jun 27 17:21:27.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024371674s
    Jun 27 17:21:29.662: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 6.0318043s
    Jun 27 17:21:31.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 8.025210788s
    Jun 27 17:21:33.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.024128064s
    Jun 27 17:21:35.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 12.023739259s
    Jun 27 17:21:37.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 14.023913655s
    Jun 27 17:21:39.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 16.026365372s
    Jun 27 17:21:41.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 18.023884381s
    Jun 27 17:21:43.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 20.026248543s
    Jun 27 17:21:45.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 22.026115969s
    Jun 27 17:21:47.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 24.026936808s
    Jun 27 17:21:49.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 26.026053877s
    Jun 27 17:21:51.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 28.02560826s
    Jun 27 17:21:53.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 30.02683224s
    Jun 27 17:21:55.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 32.026083764s
    Jun 27 17:21:57.668: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 34.038575254s
    Jun 27 17:21:59.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 36.026168444s
    Jun 27 17:22:01.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 38.02661473s
    Jun 27 17:22:03.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 40.025975053s
    Jun 27 17:22:05.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 42.025666015s
    Jun 27 17:22:07.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 44.025448588s
    Jun 27 17:22:09.661: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 46.030904219s
    Jun 27 17:22:11.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 48.024115618s
    Jun 27 17:22:13.658: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 50.027726511s
    Jun 27 17:22:15.663: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 52.033295123s
    Jun 27 17:22:17.661: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 54.031158685s
    Jun 27 17:22:19.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 56.025239082s
    Jun 27 17:22:21.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 58.025797477s
    Jun 27 17:22:23.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.025177585s
    Jun 27 17:22:25.670: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.04016948s
    Jun 27 17:22:27.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.024281903s
    Jun 27 17:22:29.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.024579935s
    Jun 27 17:22:31.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.024294325s
    Jun 27 17:22:33.658: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.028079177s
    Jun 27 17:22:35.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.024894648s
    Jun 27 17:22:37.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.025975272s
    Jun 27 17:22:39.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.025846467s
    Jun 27 17:22:41.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.025863106s
    Jun 27 17:22:43.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.027248148s
    Jun 27 17:22:45.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.024859929s
    Jun 27 17:22:47.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.026933147s
    Jun 27 17:22:49.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.025474706s
    Jun 27 17:22:51.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.026069703s
    Jun 27 17:22:53.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.025278112s
    Jun 27 17:22:55.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.024964816s
    Jun 27 17:22:57.654: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.024473442s
    Jun 27 17:22:59.662: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.032627515s
    Jun 27 17:23:01.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.025907723s
    Jun 27 17:23:03.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.025342391s
    Jun 27 17:23:05.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.024843249s
    Jun 27 17:23:07.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.025728324s
    Jun 27 17:23:09.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.025669201s
    Jun 27 17:23:11.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.025437284s
    Jun 27 17:23:13.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.025679018s
    Jun 27 17:23:15.657: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.026947806s
    Jun 27 17:23:17.658: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.027727327s
    Jun 27 17:23:19.655: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.025610104s
    Jun 27 17:23:21.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.026650477s
    Jun 27 17:23:23.656: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.026334309s
    Jun 27 17:23:23.667: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.036850576s
    STEP: updating the pod 06/27/23 17:23:23.667
    Jun 27 17:23:24.242: INFO: Successfully updated pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14"
    STEP: waiting for pod running 06/27/23 17:23:24.242
    Jun 27 17:23:24.242: INFO: Waiting up to 2m0s for pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" in namespace "var-expansion-7074" to be "running"
    Jun 27 17:23:24.253: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Pending", Reason="", readiness=false. Elapsed: 10.408283ms
    Jun 27 17:23:26.265: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14": Phase="Running", Reason="", readiness=true. Elapsed: 2.023098886s
    Jun 27 17:23:26.265: INFO: Pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" satisfied condition "running"
    STEP: deleting the pod gracefully 06/27/23 17:23:26.265
    Jun 27 17:23:26.266: INFO: Deleting pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" in namespace "var-expansion-7074"
    Jun 27 17:23:26.283: INFO: Wait up to 5m0s for pod "var-expansion-99b35ca3-644e-4940-8322-65f79be04e14" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 27 17:23:58.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-7074" for this suite. 06/27/23 17:23:58.325
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:23:58.344
Jun 27 17:23:58.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:23:58.346
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:23:58.383
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:23:58.393
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 06/27/23 17:23:58.404
Jun 27 17:23:58.476: INFO: Waiting up to 5m0s for pod "downward-api-6ef35355-1217-4914-82d2-11281588e602" in namespace "downward-api-8199" to be "Succeeded or Failed"
Jun 27 17:23:58.488: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602": Phase="Pending", Reason="", readiness=false. Elapsed: 11.608586ms
Jun 27 17:24:00.500: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023779046s
Jun 27 17:24:02.499: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022836528s
Jun 27 17:24:04.507: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030604303s
STEP: Saw pod success 06/27/23 17:24:04.507
Jun 27 17:24:04.507: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602" satisfied condition "Succeeded or Failed"
Jun 27 17:24:04.517: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-6ef35355-1217-4914-82d2-11281588e602 container dapi-container: <nil>
STEP: delete the pod 06/27/23 17:24:04.618
Jun 27 17:24:04.675: INFO: Waiting for pod downward-api-6ef35355-1217-4914-82d2-11281588e602 to disappear
Jun 27 17:24:04.684: INFO: Pod downward-api-6ef35355-1217-4914-82d2-11281588e602 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 27 17:24:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8199" for this suite. 06/27/23 17:24:04.704
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":202,"skipped":4013,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.390 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:23:58.344
    Jun 27 17:23:58.344: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:23:58.346
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:23:58.383
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:23:58.393
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 06/27/23 17:23:58.404
    Jun 27 17:23:58.476: INFO: Waiting up to 5m0s for pod "downward-api-6ef35355-1217-4914-82d2-11281588e602" in namespace "downward-api-8199" to be "Succeeded or Failed"
    Jun 27 17:23:58.488: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602": Phase="Pending", Reason="", readiness=false. Elapsed: 11.608586ms
    Jun 27 17:24:00.500: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023779046s
    Jun 27 17:24:02.499: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022836528s
    Jun 27 17:24:04.507: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.030604303s
    STEP: Saw pod success 06/27/23 17:24:04.507
    Jun 27 17:24:04.507: INFO: Pod "downward-api-6ef35355-1217-4914-82d2-11281588e602" satisfied condition "Succeeded or Failed"
    Jun 27 17:24:04.517: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-6ef35355-1217-4914-82d2-11281588e602 container dapi-container: <nil>
    STEP: delete the pod 06/27/23 17:24:04.618
    Jun 27 17:24:04.675: INFO: Waiting for pod downward-api-6ef35355-1217-4914-82d2-11281588e602 to disappear
    Jun 27 17:24:04.684: INFO: Pod downward-api-6ef35355-1217-4914-82d2-11281588e602 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 27 17:24:04.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8199" for this suite. 06/27/23 17:24:04.704
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:24:04.735
Jun 27 17:24:04.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 17:24:04.735
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:24:04.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:24:04.795
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 06/27/23 17:24:04.81
Jun 27 17:24:04.857: INFO: Waiting up to 5m0s for pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb" in namespace "emptydir-8264" to be "Succeeded or Failed"
Jun 27 17:24:04.877: INFO: Pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.595498ms
Jun 27 17:24:06.889: INFO: Pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031013768s
Jun 27 17:24:08.887: INFO: Pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029853201s
STEP: Saw pod success 06/27/23 17:24:08.887
Jun 27 17:24:08.888: INFO: Pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb" satisfied condition "Succeeded or Failed"
Jun 27 17:24:08.898: INFO: Trying to get logs from node 10.113.180.90 pod pod-d6e170f4-9e5d-47d4-a488-0466604685bb container test-container: <nil>
STEP: delete the pod 06/27/23 17:24:08.931
Jun 27 17:24:08.957: INFO: Waiting for pod pod-d6e170f4-9e5d-47d4-a488-0466604685bb to disappear
Jun 27 17:24:08.966: INFO: Pod pod-d6e170f4-9e5d-47d4-a488-0466604685bb no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 17:24:08.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8264" for this suite. 06/27/23 17:24:08.985
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":203,"skipped":4045,"failed":0}
------------------------------
â€¢ [4.284 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:24:04.735
    Jun 27 17:24:04.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 17:24:04.735
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:24:04.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:24:04.795
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 06/27/23 17:24:04.81
    Jun 27 17:24:04.857: INFO: Waiting up to 5m0s for pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb" in namespace "emptydir-8264" to be "Succeeded or Failed"
    Jun 27 17:24:04.877: INFO: Pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.595498ms
    Jun 27 17:24:06.889: INFO: Pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031013768s
    Jun 27 17:24:08.887: INFO: Pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029853201s
    STEP: Saw pod success 06/27/23 17:24:08.887
    Jun 27 17:24:08.888: INFO: Pod "pod-d6e170f4-9e5d-47d4-a488-0466604685bb" satisfied condition "Succeeded or Failed"
    Jun 27 17:24:08.898: INFO: Trying to get logs from node 10.113.180.90 pod pod-d6e170f4-9e5d-47d4-a488-0466604685bb container test-container: <nil>
    STEP: delete the pod 06/27/23 17:24:08.931
    Jun 27 17:24:08.957: INFO: Waiting for pod pod-d6e170f4-9e5d-47d4-a488-0466604685bb to disappear
    Jun 27 17:24:08.966: INFO: Pod pod-d6e170f4-9e5d-47d4-a488-0466604685bb no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:24:08.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8264" for this suite. 06/27/23 17:24:08.985
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:24:09.021
Jun 27 17:24:09.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename gc 06/27/23 17:24:09.023
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:24:09.085
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:24:09.109
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 06/27/23 17:24:09.15
STEP: delete the rc 06/27/23 17:24:14.192
STEP: wait for the rc to be deleted 06/27/23 17:24:14.219
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/27/23 17:24:19.238
STEP: Gathering metrics 06/27/23 17:24:49.272
W0627 17:24:49.291711      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 27 17:24:49.291: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 27 17:24:49.291: INFO: Deleting pod "simpletest.rc-29qmv" in namespace "gc-4259"
Jun 27 17:24:49.340: INFO: Deleting pod "simpletest.rc-2kkwl" in namespace "gc-4259"
Jun 27 17:24:49.369: INFO: Deleting pod "simpletest.rc-2kxkw" in namespace "gc-4259"
Jun 27 17:24:49.404: INFO: Deleting pod "simpletest.rc-2sxzv" in namespace "gc-4259"
Jun 27 17:24:49.443: INFO: Deleting pod "simpletest.rc-42c47" in namespace "gc-4259"
Jun 27 17:24:49.471: INFO: Deleting pod "simpletest.rc-4hgzq" in namespace "gc-4259"
Jun 27 17:24:49.523: INFO: Deleting pod "simpletest.rc-5dlw8" in namespace "gc-4259"
Jun 27 17:24:49.553: INFO: Deleting pod "simpletest.rc-5fphf" in namespace "gc-4259"
Jun 27 17:24:49.588: INFO: Deleting pod "simpletest.rc-5q59p" in namespace "gc-4259"
Jun 27 17:24:49.613: INFO: Deleting pod "simpletest.rc-5tpdv" in namespace "gc-4259"
Jun 27 17:24:49.654: INFO: Deleting pod "simpletest.rc-5v6dm" in namespace "gc-4259"
Jun 27 17:24:49.690: INFO: Deleting pod "simpletest.rc-6fz7p" in namespace "gc-4259"
Jun 27 17:24:49.724: INFO: Deleting pod "simpletest.rc-6hkdb" in namespace "gc-4259"
Jun 27 17:24:49.747: INFO: Deleting pod "simpletest.rc-6jdmn" in namespace "gc-4259"
Jun 27 17:24:49.775: INFO: Deleting pod "simpletest.rc-726ld" in namespace "gc-4259"
Jun 27 17:24:49.806: INFO: Deleting pod "simpletest.rc-74lln" in namespace "gc-4259"
Jun 27 17:24:49.836: INFO: Deleting pod "simpletest.rc-88b6m" in namespace "gc-4259"
Jun 27 17:24:49.868: INFO: Deleting pod "simpletest.rc-8bcrc" in namespace "gc-4259"
Jun 27 17:24:49.905: INFO: Deleting pod "simpletest.rc-8bv9v" in namespace "gc-4259"
Jun 27 17:24:49.942: INFO: Deleting pod "simpletest.rc-8lzdc" in namespace "gc-4259"
Jun 27 17:24:50.003: INFO: Deleting pod "simpletest.rc-8zmdq" in namespace "gc-4259"
Jun 27 17:24:50.046: INFO: Deleting pod "simpletest.rc-9748j" in namespace "gc-4259"
Jun 27 17:24:50.091: INFO: Deleting pod "simpletest.rc-9w46z" in namespace "gc-4259"
Jun 27 17:24:50.122: INFO: Deleting pod "simpletest.rc-9zdjp" in namespace "gc-4259"
Jun 27 17:24:50.152: INFO: Deleting pod "simpletest.rc-b6v4z" in namespace "gc-4259"
Jun 27 17:24:50.206: INFO: Deleting pod "simpletest.rc-b8gx2" in namespace "gc-4259"
Jun 27 17:24:50.248: INFO: Deleting pod "simpletest.rc-b8x7r" in namespace "gc-4259"
Jun 27 17:24:50.305: INFO: Deleting pod "simpletest.rc-b9t8j" in namespace "gc-4259"
Jun 27 17:24:50.334: INFO: Deleting pod "simpletest.rc-bj6nj" in namespace "gc-4259"
Jun 27 17:24:50.371: INFO: Deleting pod "simpletest.rc-bkgns" in namespace "gc-4259"
Jun 27 17:24:50.398: INFO: Deleting pod "simpletest.rc-bmfm4" in namespace "gc-4259"
Jun 27 17:24:50.424: INFO: Deleting pod "simpletest.rc-bmkpz" in namespace "gc-4259"
Jun 27 17:24:50.458: INFO: Deleting pod "simpletest.rc-c4pr2" in namespace "gc-4259"
Jun 27 17:24:50.497: INFO: Deleting pod "simpletest.rc-cdr4h" in namespace "gc-4259"
Jun 27 17:24:50.531: INFO: Deleting pod "simpletest.rc-ckgh9" in namespace "gc-4259"
Jun 27 17:24:50.564: INFO: Deleting pod "simpletest.rc-cp529" in namespace "gc-4259"
Jun 27 17:24:50.618: INFO: Deleting pod "simpletest.rc-cxx2b" in namespace "gc-4259"
Jun 27 17:24:50.653: INFO: Deleting pod "simpletest.rc-d9qrj" in namespace "gc-4259"
Jun 27 17:24:50.743: INFO: Deleting pod "simpletest.rc-ddmpq" in namespace "gc-4259"
Jun 27 17:24:50.771: INFO: Deleting pod "simpletest.rc-dqvzx" in namespace "gc-4259"
Jun 27 17:24:50.812: INFO: Deleting pod "simpletest.rc-dtcpx" in namespace "gc-4259"
Jun 27 17:24:50.852: INFO: Deleting pod "simpletest.rc-dznb7" in namespace "gc-4259"
Jun 27 17:24:50.897: INFO: Deleting pod "simpletest.rc-fcdq6" in namespace "gc-4259"
Jun 27 17:24:50.931: INFO: Deleting pod "simpletest.rc-gd2ct" in namespace "gc-4259"
Jun 27 17:24:50.962: INFO: Deleting pod "simpletest.rc-gd95v" in namespace "gc-4259"
Jun 27 17:24:50.996: INFO: Deleting pod "simpletest.rc-ghblb" in namespace "gc-4259"
Jun 27 17:24:51.019: INFO: Deleting pod "simpletest.rc-gnzqh" in namespace "gc-4259"
Jun 27 17:24:51.080: INFO: Deleting pod "simpletest.rc-gpkc7" in namespace "gc-4259"
Jun 27 17:24:51.109: INFO: Deleting pod "simpletest.rc-gsk4j" in namespace "gc-4259"
Jun 27 17:24:51.131: INFO: Deleting pod "simpletest.rc-h4tbw" in namespace "gc-4259"
Jun 27 17:24:51.176: INFO: Deleting pod "simpletest.rc-h6g96" in namespace "gc-4259"
Jun 27 17:24:51.241: INFO: Deleting pod "simpletest.rc-h99gz" in namespace "gc-4259"
Jun 27 17:24:51.284: INFO: Deleting pod "simpletest.rc-hrsgn" in namespace "gc-4259"
Jun 27 17:24:51.392: INFO: Deleting pod "simpletest.rc-jrk2c" in namespace "gc-4259"
Jun 27 17:24:51.547: INFO: Deleting pod "simpletest.rc-jzqbs" in namespace "gc-4259"
Jun 27 17:24:51.609: INFO: Deleting pod "simpletest.rc-jzvjx" in namespace "gc-4259"
Jun 27 17:24:51.682: INFO: Deleting pod "simpletest.rc-k76dm" in namespace "gc-4259"
Jun 27 17:24:51.844: INFO: Deleting pod "simpletest.rc-kvh6w" in namespace "gc-4259"
Jun 27 17:24:52.206: INFO: Deleting pod "simpletest.rc-l4gdb" in namespace "gc-4259"
Jun 27 17:24:52.308: INFO: Deleting pod "simpletest.rc-l8qjv" in namespace "gc-4259"
Jun 27 17:24:52.438: INFO: Deleting pod "simpletest.rc-lnc7x" in namespace "gc-4259"
Jun 27 17:24:52.472: INFO: Deleting pod "simpletest.rc-lzbkp" in namespace "gc-4259"
Jun 27 17:24:52.515: INFO: Deleting pod "simpletest.rc-m2t42" in namespace "gc-4259"
Jun 27 17:24:52.573: INFO: Deleting pod "simpletest.rc-mnbcn" in namespace "gc-4259"
Jun 27 17:24:52.608: INFO: Deleting pod "simpletest.rc-n62td" in namespace "gc-4259"
Jun 27 17:24:52.635: INFO: Deleting pod "simpletest.rc-nf45p" in namespace "gc-4259"
Jun 27 17:24:52.829: INFO: Deleting pod "simpletest.rc-ng55x" in namespace "gc-4259"
Jun 27 17:24:53.125: INFO: Deleting pod "simpletest.rc-nkg29" in namespace "gc-4259"
Jun 27 17:24:53.180: INFO: Deleting pod "simpletest.rc-nvgvk" in namespace "gc-4259"
Jun 27 17:24:53.207: INFO: Deleting pod "simpletest.rc-p5xql" in namespace "gc-4259"
Jun 27 17:24:53.238: INFO: Deleting pod "simpletest.rc-p9sq7" in namespace "gc-4259"
Jun 27 17:24:53.279: INFO: Deleting pod "simpletest.rc-pd8gr" in namespace "gc-4259"
Jun 27 17:24:53.312: INFO: Deleting pod "simpletest.rc-qbx7r" in namespace "gc-4259"
Jun 27 17:24:53.337: INFO: Deleting pod "simpletest.rc-qczdc" in namespace "gc-4259"
Jun 27 17:24:53.361: INFO: Deleting pod "simpletest.rc-qj6rn" in namespace "gc-4259"
Jun 27 17:24:53.393: INFO: Deleting pod "simpletest.rc-ql5fc" in namespace "gc-4259"
Jun 27 17:24:53.449: INFO: Deleting pod "simpletest.rc-qxk2t" in namespace "gc-4259"
Jun 27 17:24:53.482: INFO: Deleting pod "simpletest.rc-rcwwf" in namespace "gc-4259"
Jun 27 17:24:53.538: INFO: Deleting pod "simpletest.rc-rrlm5" in namespace "gc-4259"
Jun 27 17:24:53.597: INFO: Deleting pod "simpletest.rc-rwwxb" in namespace "gc-4259"
Jun 27 17:24:53.654: INFO: Deleting pod "simpletest.rc-skzht" in namespace "gc-4259"
Jun 27 17:24:53.778: INFO: Deleting pod "simpletest.rc-slnth" in namespace "gc-4259"
Jun 27 17:24:53.831: INFO: Deleting pod "simpletest.rc-v8zmm" in namespace "gc-4259"
Jun 27 17:24:53.923: INFO: Deleting pod "simpletest.rc-vwdls" in namespace "gc-4259"
Jun 27 17:24:54.020: INFO: Deleting pod "simpletest.rc-vwmv6" in namespace "gc-4259"
Jun 27 17:24:54.053: INFO: Deleting pod "simpletest.rc-vzjq9" in namespace "gc-4259"
Jun 27 17:24:54.101: INFO: Deleting pod "simpletest.rc-w766s" in namespace "gc-4259"
Jun 27 17:24:54.136: INFO: Deleting pod "simpletest.rc-wb9x7" in namespace "gc-4259"
Jun 27 17:24:54.293: INFO: Deleting pod "simpletest.rc-wbhcr" in namespace "gc-4259"
Jun 27 17:24:54.429: INFO: Deleting pod "simpletest.rc-wjbgs" in namespace "gc-4259"
Jun 27 17:24:54.529: INFO: Deleting pod "simpletest.rc-wltxf" in namespace "gc-4259"
Jun 27 17:24:54.564: INFO: Deleting pod "simpletest.rc-wp5kp" in namespace "gc-4259"
Jun 27 17:24:54.676: INFO: Deleting pod "simpletest.rc-wsg2s" in namespace "gc-4259"
Jun 27 17:24:54.716: INFO: Deleting pod "simpletest.rc-wz696" in namespace "gc-4259"
Jun 27 17:24:54.742: INFO: Deleting pod "simpletest.rc-xdmw9" in namespace "gc-4259"
Jun 27 17:24:54.786: INFO: Deleting pod "simpletest.rc-xwr4x" in namespace "gc-4259"
Jun 27 17:24:54.845: INFO: Deleting pod "simpletest.rc-xxthf" in namespace "gc-4259"
Jun 27 17:24:54.873: INFO: Deleting pod "simpletest.rc-z6kg7" in namespace "gc-4259"
Jun 27 17:24:54.902: INFO: Deleting pod "simpletest.rc-zdqc5" in namespace "gc-4259"
Jun 27 17:24:54.957: INFO: Deleting pod "simpletest.rc-zmjxl" in namespace "gc-4259"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 27 17:24:55.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4259" for this suite. 06/27/23 17:24:55.086
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":204,"skipped":4058,"failed":0}
------------------------------
â€¢ [SLOW TEST] [46.086 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:24:09.021
    Jun 27 17:24:09.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename gc 06/27/23 17:24:09.023
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:24:09.085
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:24:09.109
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 06/27/23 17:24:09.15
    STEP: delete the rc 06/27/23 17:24:14.192
    STEP: wait for the rc to be deleted 06/27/23 17:24:14.219
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 06/27/23 17:24:19.238
    STEP: Gathering metrics 06/27/23 17:24:49.272
    W0627 17:24:49.291711      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 27 17:24:49.291: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun 27 17:24:49.291: INFO: Deleting pod "simpletest.rc-29qmv" in namespace "gc-4259"
    Jun 27 17:24:49.340: INFO: Deleting pod "simpletest.rc-2kkwl" in namespace "gc-4259"
    Jun 27 17:24:49.369: INFO: Deleting pod "simpletest.rc-2kxkw" in namespace "gc-4259"
    Jun 27 17:24:49.404: INFO: Deleting pod "simpletest.rc-2sxzv" in namespace "gc-4259"
    Jun 27 17:24:49.443: INFO: Deleting pod "simpletest.rc-42c47" in namespace "gc-4259"
    Jun 27 17:24:49.471: INFO: Deleting pod "simpletest.rc-4hgzq" in namespace "gc-4259"
    Jun 27 17:24:49.523: INFO: Deleting pod "simpletest.rc-5dlw8" in namespace "gc-4259"
    Jun 27 17:24:49.553: INFO: Deleting pod "simpletest.rc-5fphf" in namespace "gc-4259"
    Jun 27 17:24:49.588: INFO: Deleting pod "simpletest.rc-5q59p" in namespace "gc-4259"
    Jun 27 17:24:49.613: INFO: Deleting pod "simpletest.rc-5tpdv" in namespace "gc-4259"
    Jun 27 17:24:49.654: INFO: Deleting pod "simpletest.rc-5v6dm" in namespace "gc-4259"
    Jun 27 17:24:49.690: INFO: Deleting pod "simpletest.rc-6fz7p" in namespace "gc-4259"
    Jun 27 17:24:49.724: INFO: Deleting pod "simpletest.rc-6hkdb" in namespace "gc-4259"
    Jun 27 17:24:49.747: INFO: Deleting pod "simpletest.rc-6jdmn" in namespace "gc-4259"
    Jun 27 17:24:49.775: INFO: Deleting pod "simpletest.rc-726ld" in namespace "gc-4259"
    Jun 27 17:24:49.806: INFO: Deleting pod "simpletest.rc-74lln" in namespace "gc-4259"
    Jun 27 17:24:49.836: INFO: Deleting pod "simpletest.rc-88b6m" in namespace "gc-4259"
    Jun 27 17:24:49.868: INFO: Deleting pod "simpletest.rc-8bcrc" in namespace "gc-4259"
    Jun 27 17:24:49.905: INFO: Deleting pod "simpletest.rc-8bv9v" in namespace "gc-4259"
    Jun 27 17:24:49.942: INFO: Deleting pod "simpletest.rc-8lzdc" in namespace "gc-4259"
    Jun 27 17:24:50.003: INFO: Deleting pod "simpletest.rc-8zmdq" in namespace "gc-4259"
    Jun 27 17:24:50.046: INFO: Deleting pod "simpletest.rc-9748j" in namespace "gc-4259"
    Jun 27 17:24:50.091: INFO: Deleting pod "simpletest.rc-9w46z" in namespace "gc-4259"
    Jun 27 17:24:50.122: INFO: Deleting pod "simpletest.rc-9zdjp" in namespace "gc-4259"
    Jun 27 17:24:50.152: INFO: Deleting pod "simpletest.rc-b6v4z" in namespace "gc-4259"
    Jun 27 17:24:50.206: INFO: Deleting pod "simpletest.rc-b8gx2" in namespace "gc-4259"
    Jun 27 17:24:50.248: INFO: Deleting pod "simpletest.rc-b8x7r" in namespace "gc-4259"
    Jun 27 17:24:50.305: INFO: Deleting pod "simpletest.rc-b9t8j" in namespace "gc-4259"
    Jun 27 17:24:50.334: INFO: Deleting pod "simpletest.rc-bj6nj" in namespace "gc-4259"
    Jun 27 17:24:50.371: INFO: Deleting pod "simpletest.rc-bkgns" in namespace "gc-4259"
    Jun 27 17:24:50.398: INFO: Deleting pod "simpletest.rc-bmfm4" in namespace "gc-4259"
    Jun 27 17:24:50.424: INFO: Deleting pod "simpletest.rc-bmkpz" in namespace "gc-4259"
    Jun 27 17:24:50.458: INFO: Deleting pod "simpletest.rc-c4pr2" in namespace "gc-4259"
    Jun 27 17:24:50.497: INFO: Deleting pod "simpletest.rc-cdr4h" in namespace "gc-4259"
    Jun 27 17:24:50.531: INFO: Deleting pod "simpletest.rc-ckgh9" in namespace "gc-4259"
    Jun 27 17:24:50.564: INFO: Deleting pod "simpletest.rc-cp529" in namespace "gc-4259"
    Jun 27 17:24:50.618: INFO: Deleting pod "simpletest.rc-cxx2b" in namespace "gc-4259"
    Jun 27 17:24:50.653: INFO: Deleting pod "simpletest.rc-d9qrj" in namespace "gc-4259"
    Jun 27 17:24:50.743: INFO: Deleting pod "simpletest.rc-ddmpq" in namespace "gc-4259"
    Jun 27 17:24:50.771: INFO: Deleting pod "simpletest.rc-dqvzx" in namespace "gc-4259"
    Jun 27 17:24:50.812: INFO: Deleting pod "simpletest.rc-dtcpx" in namespace "gc-4259"
    Jun 27 17:24:50.852: INFO: Deleting pod "simpletest.rc-dznb7" in namespace "gc-4259"
    Jun 27 17:24:50.897: INFO: Deleting pod "simpletest.rc-fcdq6" in namespace "gc-4259"
    Jun 27 17:24:50.931: INFO: Deleting pod "simpletest.rc-gd2ct" in namespace "gc-4259"
    Jun 27 17:24:50.962: INFO: Deleting pod "simpletest.rc-gd95v" in namespace "gc-4259"
    Jun 27 17:24:50.996: INFO: Deleting pod "simpletest.rc-ghblb" in namespace "gc-4259"
    Jun 27 17:24:51.019: INFO: Deleting pod "simpletest.rc-gnzqh" in namespace "gc-4259"
    Jun 27 17:24:51.080: INFO: Deleting pod "simpletest.rc-gpkc7" in namespace "gc-4259"
    Jun 27 17:24:51.109: INFO: Deleting pod "simpletest.rc-gsk4j" in namespace "gc-4259"
    Jun 27 17:24:51.131: INFO: Deleting pod "simpletest.rc-h4tbw" in namespace "gc-4259"
    Jun 27 17:24:51.176: INFO: Deleting pod "simpletest.rc-h6g96" in namespace "gc-4259"
    Jun 27 17:24:51.241: INFO: Deleting pod "simpletest.rc-h99gz" in namespace "gc-4259"
    Jun 27 17:24:51.284: INFO: Deleting pod "simpletest.rc-hrsgn" in namespace "gc-4259"
    Jun 27 17:24:51.392: INFO: Deleting pod "simpletest.rc-jrk2c" in namespace "gc-4259"
    Jun 27 17:24:51.547: INFO: Deleting pod "simpletest.rc-jzqbs" in namespace "gc-4259"
    Jun 27 17:24:51.609: INFO: Deleting pod "simpletest.rc-jzvjx" in namespace "gc-4259"
    Jun 27 17:24:51.682: INFO: Deleting pod "simpletest.rc-k76dm" in namespace "gc-4259"
    Jun 27 17:24:51.844: INFO: Deleting pod "simpletest.rc-kvh6w" in namespace "gc-4259"
    Jun 27 17:24:52.206: INFO: Deleting pod "simpletest.rc-l4gdb" in namespace "gc-4259"
    Jun 27 17:24:52.308: INFO: Deleting pod "simpletest.rc-l8qjv" in namespace "gc-4259"
    Jun 27 17:24:52.438: INFO: Deleting pod "simpletest.rc-lnc7x" in namespace "gc-4259"
    Jun 27 17:24:52.472: INFO: Deleting pod "simpletest.rc-lzbkp" in namespace "gc-4259"
    Jun 27 17:24:52.515: INFO: Deleting pod "simpletest.rc-m2t42" in namespace "gc-4259"
    Jun 27 17:24:52.573: INFO: Deleting pod "simpletest.rc-mnbcn" in namespace "gc-4259"
    Jun 27 17:24:52.608: INFO: Deleting pod "simpletest.rc-n62td" in namespace "gc-4259"
    Jun 27 17:24:52.635: INFO: Deleting pod "simpletest.rc-nf45p" in namespace "gc-4259"
    Jun 27 17:24:52.829: INFO: Deleting pod "simpletest.rc-ng55x" in namespace "gc-4259"
    Jun 27 17:24:53.125: INFO: Deleting pod "simpletest.rc-nkg29" in namespace "gc-4259"
    Jun 27 17:24:53.180: INFO: Deleting pod "simpletest.rc-nvgvk" in namespace "gc-4259"
    Jun 27 17:24:53.207: INFO: Deleting pod "simpletest.rc-p5xql" in namespace "gc-4259"
    Jun 27 17:24:53.238: INFO: Deleting pod "simpletest.rc-p9sq7" in namespace "gc-4259"
    Jun 27 17:24:53.279: INFO: Deleting pod "simpletest.rc-pd8gr" in namespace "gc-4259"
    Jun 27 17:24:53.312: INFO: Deleting pod "simpletest.rc-qbx7r" in namespace "gc-4259"
    Jun 27 17:24:53.337: INFO: Deleting pod "simpletest.rc-qczdc" in namespace "gc-4259"
    Jun 27 17:24:53.361: INFO: Deleting pod "simpletest.rc-qj6rn" in namespace "gc-4259"
    Jun 27 17:24:53.393: INFO: Deleting pod "simpletest.rc-ql5fc" in namespace "gc-4259"
    Jun 27 17:24:53.449: INFO: Deleting pod "simpletest.rc-qxk2t" in namespace "gc-4259"
    Jun 27 17:24:53.482: INFO: Deleting pod "simpletest.rc-rcwwf" in namespace "gc-4259"
    Jun 27 17:24:53.538: INFO: Deleting pod "simpletest.rc-rrlm5" in namespace "gc-4259"
    Jun 27 17:24:53.597: INFO: Deleting pod "simpletest.rc-rwwxb" in namespace "gc-4259"
    Jun 27 17:24:53.654: INFO: Deleting pod "simpletest.rc-skzht" in namespace "gc-4259"
    Jun 27 17:24:53.778: INFO: Deleting pod "simpletest.rc-slnth" in namespace "gc-4259"
    Jun 27 17:24:53.831: INFO: Deleting pod "simpletest.rc-v8zmm" in namespace "gc-4259"
    Jun 27 17:24:53.923: INFO: Deleting pod "simpletest.rc-vwdls" in namespace "gc-4259"
    Jun 27 17:24:54.020: INFO: Deleting pod "simpletest.rc-vwmv6" in namespace "gc-4259"
    Jun 27 17:24:54.053: INFO: Deleting pod "simpletest.rc-vzjq9" in namespace "gc-4259"
    Jun 27 17:24:54.101: INFO: Deleting pod "simpletest.rc-w766s" in namespace "gc-4259"
    Jun 27 17:24:54.136: INFO: Deleting pod "simpletest.rc-wb9x7" in namespace "gc-4259"
    Jun 27 17:24:54.293: INFO: Deleting pod "simpletest.rc-wbhcr" in namespace "gc-4259"
    Jun 27 17:24:54.429: INFO: Deleting pod "simpletest.rc-wjbgs" in namespace "gc-4259"
    Jun 27 17:24:54.529: INFO: Deleting pod "simpletest.rc-wltxf" in namespace "gc-4259"
    Jun 27 17:24:54.564: INFO: Deleting pod "simpletest.rc-wp5kp" in namespace "gc-4259"
    Jun 27 17:24:54.676: INFO: Deleting pod "simpletest.rc-wsg2s" in namespace "gc-4259"
    Jun 27 17:24:54.716: INFO: Deleting pod "simpletest.rc-wz696" in namespace "gc-4259"
    Jun 27 17:24:54.742: INFO: Deleting pod "simpletest.rc-xdmw9" in namespace "gc-4259"
    Jun 27 17:24:54.786: INFO: Deleting pod "simpletest.rc-xwr4x" in namespace "gc-4259"
    Jun 27 17:24:54.845: INFO: Deleting pod "simpletest.rc-xxthf" in namespace "gc-4259"
    Jun 27 17:24:54.873: INFO: Deleting pod "simpletest.rc-z6kg7" in namespace "gc-4259"
    Jun 27 17:24:54.902: INFO: Deleting pod "simpletest.rc-zdqc5" in namespace "gc-4259"
    Jun 27 17:24:54.957: INFO: Deleting pod "simpletest.rc-zmjxl" in namespace "gc-4259"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 27 17:24:55.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4259" for this suite. 06/27/23 17:24:55.086
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:24:55.109
Jun 27 17:24:55.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-runtime 06/27/23 17:24:55.112
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:24:55.151
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:24:55.164
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/27/23 17:24:55.24
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/27/23 17:25:15.634
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/27/23 17:25:15.667
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/27/23 17:25:15.687
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/27/23 17:25:15.687
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/27/23 17:25:15.77
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/27/23 17:25:19.838
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/27/23 17:25:21.875
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/27/23 17:25:21.895
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/27/23 17:25:21.896
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/27/23 17:25:21.967
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/27/23 17:25:22.992
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/27/23 17:25:27.053
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/27/23 17:25:27.074
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/27/23 17:25:27.074
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 27 17:25:27.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-9120" for this suite. 06/27/23 17:25:27.155
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":205,"skipped":4072,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.070 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:24:55.109
    Jun 27 17:24:55.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-runtime 06/27/23 17:24:55.112
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:24:55.151
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:24:55.164
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 06/27/23 17:24:55.24
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 06/27/23 17:25:15.634
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 06/27/23 17:25:15.667
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 06/27/23 17:25:15.687
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 06/27/23 17:25:15.687
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 06/27/23 17:25:15.77
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 06/27/23 17:25:19.838
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 06/27/23 17:25:21.875
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 06/27/23 17:25:21.895
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 06/27/23 17:25:21.896
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 06/27/23 17:25:21.967
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 06/27/23 17:25:22.992
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 06/27/23 17:25:27.053
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 06/27/23 17:25:27.074
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 06/27/23 17:25:27.074
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 27 17:25:27.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-9120" for this suite. 06/27/23 17:25:27.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:25:27.18
Jun 27 17:25:27.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename job 06/27/23 17:25:27.184
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:27.244
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:27.257
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 06/27/23 17:25:27.274
STEP: Ensuring job reaches completions 06/27/23 17:25:27.289
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 27 17:25:41.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-715" for this suite. 06/27/23 17:25:41.315
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":206,"skipped":4078,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.160 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:25:27.18
    Jun 27 17:25:27.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename job 06/27/23 17:25:27.184
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:27.244
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:27.257
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 06/27/23 17:25:27.274
    STEP: Ensuring job reaches completions 06/27/23 17:25:27.289
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 27 17:25:41.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-715" for this suite. 06/27/23 17:25:41.315
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:25:41.343
Jun 27 17:25:41.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename security-context-test 06/27/23 17:25:41.345
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:41.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:41.391
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Jun 27 17:25:41.451: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0" in namespace "security-context-test-7715" to be "Succeeded or Failed"
Jun 27 17:25:41.464: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.748778ms
Jun 27 17:25:43.476: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024572819s
Jun 27 17:25:45.474: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022796031s
Jun 27 17:25:47.474: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022827759s
Jun 27 17:25:47.475: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 27 17:25:47.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7715" for this suite. 06/27/23 17:25:47.563
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":207,"skipped":4081,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.244 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:25:41.343
    Jun 27 17:25:41.343: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename security-context-test 06/27/23 17:25:41.345
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:41.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:41.391
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Jun 27 17:25:41.451: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0" in namespace "security-context-test-7715" to be "Succeeded or Failed"
    Jun 27 17:25:41.464: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 12.748778ms
    Jun 27 17:25:43.476: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024572819s
    Jun 27 17:25:45.474: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022796031s
    Jun 27 17:25:47.474: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022827759s
    Jun 27 17:25:47.475: INFO: Pod "alpine-nnp-false-cd6630e4-cee7-4221-81c9-4d5d4efe6fd0" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 27 17:25:47.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7715" for this suite. 06/27/23 17:25:47.563
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:25:47.608
Jun 27 17:25:47.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:25:47.61
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:47.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:47.682
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:25:47.697
Jun 27 17:25:47.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1" in namespace "projected-142" to be "Succeeded or Failed"
Jun 27 17:25:47.779: INFO: Pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.714272ms
Jun 27 17:25:49.789: INFO: Pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032073869s
Jun 27 17:25:51.790: INFO: Pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033008645s
STEP: Saw pod success 06/27/23 17:25:51.79
Jun 27 17:25:51.791: INFO: Pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1" satisfied condition "Succeeded or Failed"
Jun 27 17:25:51.802: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1 container client-container: <nil>
STEP: delete the pod 06/27/23 17:25:51.828
Jun 27 17:25:51.860: INFO: Waiting for pod downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1 to disappear
Jun 27 17:25:51.869: INFO: Pod downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 17:25:51.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-142" for this suite. 06/27/23 17:25:51.886
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":208,"skipped":4131,"failed":0}
------------------------------
â€¢ [4.297 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:25:47.608
    Jun 27 17:25:47.608: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:25:47.61
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:47.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:47.682
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:25:47.697
    Jun 27 17:25:47.757: INFO: Waiting up to 5m0s for pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1" in namespace "projected-142" to be "Succeeded or Failed"
    Jun 27 17:25:47.779: INFO: Pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 21.714272ms
    Jun 27 17:25:49.789: INFO: Pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.032073869s
    Jun 27 17:25:51.790: INFO: Pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.033008645s
    STEP: Saw pod success 06/27/23 17:25:51.79
    Jun 27 17:25:51.791: INFO: Pod "downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1" satisfied condition "Succeeded or Failed"
    Jun 27 17:25:51.802: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:25:51.828
    Jun 27 17:25:51.860: INFO: Waiting for pod downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1 to disappear
    Jun 27 17:25:51.869: INFO: Pod downwardapi-volume-72e978ad-3b09-4aba-a9a9-0a9864039ca1 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 17:25:51.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-142" for this suite. 06/27/23 17:25:51.886
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:25:51.906
Jun 27 17:25:51.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename containers 06/27/23 17:25:51.909
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:51.954
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:51.966
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 06/27/23 17:25:51.977
Jun 27 17:25:52.042: INFO: Waiting up to 5m0s for pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f" in namespace "containers-8604" to be "Succeeded or Failed"
Jun 27 17:25:52.053: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.598803ms
Jun 27 17:25:54.064: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022157709s
Jun 27 17:25:56.088: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046870673s
Jun 27 17:25:58.067: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025822678s
STEP: Saw pod success 06/27/23 17:25:58.067
Jun 27 17:25:58.068: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f" satisfied condition "Succeeded or Failed"
Jun 27 17:25:58.078: INFO: Trying to get logs from node 10.113.180.90 pod client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f container agnhost-container: <nil>
STEP: delete the pod 06/27/23 17:25:58.106
Jun 27 17:25:58.142: INFO: Waiting for pod client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f to disappear
Jun 27 17:25:58.155: INFO: Pod client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 27 17:25:58.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8604" for this suite. 06/27/23 17:25:58.176
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":209,"skipped":4133,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.288 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:25:51.906
    Jun 27 17:25:51.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename containers 06/27/23 17:25:51.909
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:51.954
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:51.966
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 06/27/23 17:25:51.977
    Jun 27 17:25:52.042: INFO: Waiting up to 5m0s for pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f" in namespace "containers-8604" to be "Succeeded or Failed"
    Jun 27 17:25:52.053: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 11.598803ms
    Jun 27 17:25:54.064: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022157709s
    Jun 27 17:25:56.088: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.046870673s
    Jun 27 17:25:58.067: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.025822678s
    STEP: Saw pod success 06/27/23 17:25:58.067
    Jun 27 17:25:58.068: INFO: Pod "client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f" satisfied condition "Succeeded or Failed"
    Jun 27 17:25:58.078: INFO: Trying to get logs from node 10.113.180.90 pod client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 17:25:58.106
    Jun 27 17:25:58.142: INFO: Waiting for pod client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f to disappear
    Jun 27 17:25:58.155: INFO: Pod client-containers-c81becc5-0ee8-4604-a171-84ce1f973e3f no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 27 17:25:58.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8604" for this suite. 06/27/23 17:25:58.176
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:25:58.202
Jun 27 17:25:58.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename daemonsets 06/27/23 17:25:58.204
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:58.246
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:58.261
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Jun 27 17:25:58.350: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 17:25:58.363
Jun 27 17:25:58.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:25:58.384: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:25:59.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:25:59.417: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:26:00.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:26:00.415: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:26:01.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 27 17:26:01.430: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Update daemon pods image. 06/27/23 17:26:01.468
STEP: Check that daemon pods images are updated. 06/27/23 17:26:01.496
Jun 27 17:26:01.510: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:01.510: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:02.536: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:02.536: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:03.541: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:03.541: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:04.536: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:04.537: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:05.544: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:05.544: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:05.544: INFO: Pod daemon-set-wk5br is not available
Jun 27 17:26:06.540: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:06.540: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:06.540: INFO: Pod daemon-set-wk5br is not available
Jun 27 17:26:07.551: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:08.540: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:09.537: INFO: Pod daemon-set-6dll7 is not available
Jun 27 17:26:09.537: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:10.538: INFO: Pod daemon-set-6dll7 is not available
Jun 27 17:26:10.538: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Jun 27 17:26:12.537: INFO: Pod daemon-set-c9hzr is not available
STEP: Check that daemon pods are still running on every node of the cluster. 06/27/23 17:26:12.552
Jun 27 17:26:12.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:26:12.577: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:26:13.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:26:13.603: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:26:14.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 27 17:26:14.606: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/27/23 17:26:14.657
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5974, will wait for the garbage collector to delete the pods 06/27/23 17:26:14.657
Jun 27 17:26:14.737: INFO: Deleting DaemonSet.extensions daemon-set took: 20.404305ms
Jun 27 17:26:14.839: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.222215ms
Jun 27 17:26:18.254: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:26:18.255: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 27 17:26:18.264: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"116835"},"items":null}

Jun 27 17:26:18.273: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"116835"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:26:18.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-5974" for this suite. 06/27/23 17:26:18.338
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":210,"skipped":4136,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.159 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:25:58.202
    Jun 27 17:25:58.202: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename daemonsets 06/27/23 17:25:58.204
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:25:58.246
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:25:58.261
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Jun 27 17:25:58.350: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 17:25:58.363
    Jun 27 17:25:58.384: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:25:58.384: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:25:59.417: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:25:59.417: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:26:00.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:26:00.415: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:26:01.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 27 17:26:01.430: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Update daemon pods image. 06/27/23 17:26:01.468
    STEP: Check that daemon pods images are updated. 06/27/23 17:26:01.496
    Jun 27 17:26:01.510: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:01.510: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:02.536: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:02.536: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:03.541: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:03.541: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:04.536: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:04.537: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:05.544: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:05.544: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:05.544: INFO: Pod daemon-set-wk5br is not available
    Jun 27 17:26:06.540: INFO: Wrong image for pod: daemon-set-5d7mt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:06.540: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:06.540: INFO: Pod daemon-set-wk5br is not available
    Jun 27 17:26:07.551: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:08.540: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:09.537: INFO: Pod daemon-set-6dll7 is not available
    Jun 27 17:26:09.537: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:10.538: INFO: Pod daemon-set-6dll7 is not available
    Jun 27 17:26:10.538: INFO: Wrong image for pod: daemon-set-c8rvr. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Jun 27 17:26:12.537: INFO: Pod daemon-set-c9hzr is not available
    STEP: Check that daemon pods are still running on every node of the cluster. 06/27/23 17:26:12.552
    Jun 27 17:26:12.577: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:26:12.577: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:26:13.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:26:13.603: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:26:14.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 27 17:26:14.606: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/27/23 17:26:14.657
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5974, will wait for the garbage collector to delete the pods 06/27/23 17:26:14.657
    Jun 27 17:26:14.737: INFO: Deleting DaemonSet.extensions daemon-set took: 20.404305ms
    Jun 27 17:26:14.839: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.222215ms
    Jun 27 17:26:18.254: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:26:18.255: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 27 17:26:18.264: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"116835"},"items":null}

    Jun 27 17:26:18.273: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"116835"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:26:18.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-5974" for this suite. 06/27/23 17:26:18.338
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:26:18.366
Jun 27 17:26:18.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename statefulset 06/27/23 17:26:18.368
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:26:18.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:26:18.418
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2955 06/27/23 17:26:18.429
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 06/27/23 17:26:18.451
Jun 27 17:26:18.485: INFO: Found 0 stateful pods, waiting for 3
Jun 27 17:26:28.504: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:26:28.504: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:26:28.505: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:26:28.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-2955 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 17:26:29.104: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 17:26:29.104: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 17:26:29.104: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/27/23 17:26:29.137
Jun 27 17:26:29.180: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/27/23 17:26:29.18
STEP: Updating Pods in reverse ordinal order 06/27/23 17:26:39.236
Jun 27 17:26:39.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-2955 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 27 17:26:39.678: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 27 17:26:39.678: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 27 17:26:39.678: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Jun 27 17:26:59.765: INFO: Waiting for StatefulSet statefulset-2955/ss2 to complete update
Jun 27 17:26:59.765: INFO: Waiting for Pod statefulset-2955/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Rolling back to a previous revision 06/27/23 17:27:09.799
Jun 27 17:27:09.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-2955 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Jun 27 17:27:10.181: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Jun 27 17:27:10.181: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Jun 27 17:27:10.181: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Jun 27 17:27:20.297: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 06/27/23 17:27:30.352
Jun 27 17:27:30.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-2955 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Jun 27 17:27:30.910: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Jun 27 17:27:30.910: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Jun 27 17:27:30.910: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 27 17:27:50.987: INFO: Deleting all statefulset in ns statefulset-2955
Jun 27 17:27:50.999: INFO: Scaling statefulset ss2 to 0
Jun 27 17:28:01.061: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:28:01.074: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 27 17:28:01.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2955" for this suite. 06/27/23 17:28:01.139
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":211,"skipped":4137,"failed":0}
------------------------------
â€¢ [SLOW TEST] [102.797 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:26:18.366
    Jun 27 17:26:18.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename statefulset 06/27/23 17:26:18.368
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:26:18.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:26:18.418
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2955 06/27/23 17:26:18.429
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 06/27/23 17:26:18.451
    Jun 27 17:26:18.485: INFO: Found 0 stateful pods, waiting for 3
    Jun 27 17:26:28.504: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 17:26:28.504: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 17:26:28.505: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 17:26:28.565: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-2955 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 17:26:29.104: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 17:26:29.104: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 17:26:29.104: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/27/23 17:26:29.137
    Jun 27 17:26:29.180: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/27/23 17:26:29.18
    STEP: Updating Pods in reverse ordinal order 06/27/23 17:26:39.236
    Jun 27 17:26:39.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-2955 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 27 17:26:39.678: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 27 17:26:39.678: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 27 17:26:39.678: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Jun 27 17:26:59.765: INFO: Waiting for StatefulSet statefulset-2955/ss2 to complete update
    Jun 27 17:26:59.765: INFO: Waiting for Pod statefulset-2955/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Rolling back to a previous revision 06/27/23 17:27:09.799
    Jun 27 17:27:09.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-2955 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Jun 27 17:27:10.181: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Jun 27 17:27:10.181: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Jun 27 17:27:10.181: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Jun 27 17:27:20.297: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 06/27/23 17:27:30.352
    Jun 27 17:27:30.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=statefulset-2955 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Jun 27 17:27:30.910: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Jun 27 17:27:30.910: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Jun 27 17:27:30.910: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 27 17:27:50.987: INFO: Deleting all statefulset in ns statefulset-2955
    Jun 27 17:27:50.999: INFO: Scaling statefulset ss2 to 0
    Jun 27 17:28:01.061: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 17:28:01.074: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 27 17:28:01.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2955" for this suite. 06/27/23 17:28:01.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:28:01.172
Jun 27 17:28:01.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replicaset 06/27/23 17:28:01.175
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:28:01.234
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:28:01.257
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Jun 27 17:28:01.324: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 27 17:28:06.336: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/27/23 17:28:06.336
STEP: Scaling up "test-rs" replicaset  06/27/23 17:28:06.336
Jun 27 17:28:06.360: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 06/27/23 17:28:06.36
W0627 17:28:06.373429      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun 27 17:28:06.378: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 1, AvailableReplicas 1
Jun 27 17:28:06.443: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 1, AvailableReplicas 1
Jun 27 17:28:06.482: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 1, AvailableReplicas 1
Jun 27 17:28:06.495: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 1, AvailableReplicas 1
Jun 27 17:28:09.039: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 2, AvailableReplicas 2
Jun 27 17:28:09.226: INFO: observed Replicaset test-rs in namespace replicaset-2717 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 27 17:28:09.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-2717" for this suite. 06/27/23 17:28:09.243
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":212,"skipped":4153,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.088 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:28:01.172
    Jun 27 17:28:01.173: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replicaset 06/27/23 17:28:01.175
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:28:01.234
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:28:01.257
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Jun 27 17:28:01.324: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 27 17:28:06.336: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/27/23 17:28:06.336
    STEP: Scaling up "test-rs" replicaset  06/27/23 17:28:06.336
    Jun 27 17:28:06.360: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 06/27/23 17:28:06.36
    W0627 17:28:06.373429      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun 27 17:28:06.378: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 1, AvailableReplicas 1
    Jun 27 17:28:06.443: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 1, AvailableReplicas 1
    Jun 27 17:28:06.482: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 1, AvailableReplicas 1
    Jun 27 17:28:06.495: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 1, AvailableReplicas 1
    Jun 27 17:28:09.039: INFO: observed ReplicaSet test-rs in namespace replicaset-2717 with ReadyReplicas 2, AvailableReplicas 2
    Jun 27 17:28:09.226: INFO: observed Replicaset test-rs in namespace replicaset-2717 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 27 17:28:09.227: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-2717" for this suite. 06/27/23 17:28:09.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:28:09.273
Jun 27 17:28:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replicaset 06/27/23 17:28:09.275
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:28:09.307
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:28:09.32
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/27/23 17:28:09.331
Jun 27 17:28:09.387: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 27 17:28:14.409: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/27/23 17:28:14.409
STEP: getting scale subresource 06/27/23 17:28:14.41
STEP: updating a scale subresource 06/27/23 17:28:14.423
STEP: verifying the replicaset Spec.Replicas was modified 06/27/23 17:28:14.436
STEP: Patch a scale subresource 06/27/23 17:28:14.448
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 27 17:28:14.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3003" for this suite. 06/27/23 17:28:14.503
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":213,"skipped":4221,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.260 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:28:09.273
    Jun 27 17:28:09.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replicaset 06/27/23 17:28:09.275
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:28:09.307
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:28:09.32
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 06/27/23 17:28:09.331
    Jun 27 17:28:09.387: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 27 17:28:14.409: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/27/23 17:28:14.409
    STEP: getting scale subresource 06/27/23 17:28:14.41
    STEP: updating a scale subresource 06/27/23 17:28:14.423
    STEP: verifying the replicaset Spec.Replicas was modified 06/27/23 17:28:14.436
    STEP: Patch a scale subresource 06/27/23 17:28:14.448
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 27 17:28:14.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3003" for this suite. 06/27/23 17:28:14.503
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:28:14.54
Jun 27 17:28:14.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename subpath 06/27/23 17:28:14.543
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:28:14.581
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:28:14.592
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/27/23 17:28:14.601
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-lgc2 06/27/23 17:28:14.636
STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:28:14.636
Jun 27 17:28:14.697: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lgc2" in namespace "subpath-5230" to be "Succeeded or Failed"
Jun 27 17:28:14.714: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.757168ms
Jun 27 17:28:16.740: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.043208347s
Jun 27 17:28:18.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 4.027485804s
Jun 27 17:28:20.728: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 6.030700218s
Jun 27 17:28:22.726: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 8.028766752s
Jun 27 17:28:24.729: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 10.031607605s
Jun 27 17:28:26.725: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 12.028434843s
Jun 27 17:28:28.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 14.02744058s
Jun 27 17:28:30.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 16.027515028s
Jun 27 17:28:32.725: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.028270577s
Jun 27 17:28:34.749: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 20.052494114s
Jun 27 17:28:36.725: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=false. Elapsed: 22.027815853s
Jun 27 17:28:38.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.027104013s
STEP: Saw pod success 06/27/23 17:28:38.724
Jun 27 17:28:38.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2" satisfied condition "Succeeded or Failed"
Jun 27 17:28:38.733: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-downwardapi-lgc2 container test-container-subpath-downwardapi-lgc2: <nil>
STEP: delete the pod 06/27/23 17:28:38.803
Jun 27 17:28:38.835: INFO: Waiting for pod pod-subpath-test-downwardapi-lgc2 to disappear
Jun 27 17:28:38.845: INFO: Pod pod-subpath-test-downwardapi-lgc2 no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-lgc2 06/27/23 17:28:38.845
Jun 27 17:28:38.845: INFO: Deleting pod "pod-subpath-test-downwardapi-lgc2" in namespace "subpath-5230"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 27 17:28:38.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5230" for this suite. 06/27/23 17:28:38.873
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":214,"skipped":4233,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.353 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:28:14.54
    Jun 27 17:28:14.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename subpath 06/27/23 17:28:14.543
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:28:14.581
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:28:14.592
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/27/23 17:28:14.601
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-lgc2 06/27/23 17:28:14.636
    STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:28:14.636
    Jun 27 17:28:14.697: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-lgc2" in namespace "subpath-5230" to be "Succeeded or Failed"
    Jun 27 17:28:14.714: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Pending", Reason="", readiness=false. Elapsed: 16.757168ms
    Jun 27 17:28:16.740: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 2.043208347s
    Jun 27 17:28:18.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 4.027485804s
    Jun 27 17:28:20.728: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 6.030700218s
    Jun 27 17:28:22.726: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 8.028766752s
    Jun 27 17:28:24.729: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 10.031607605s
    Jun 27 17:28:26.725: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 12.028434843s
    Jun 27 17:28:28.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 14.02744058s
    Jun 27 17:28:30.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 16.027515028s
    Jun 27 17:28:32.725: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 18.028270577s
    Jun 27 17:28:34.749: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=true. Elapsed: 20.052494114s
    Jun 27 17:28:36.725: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Running", Reason="", readiness=false. Elapsed: 22.027815853s
    Jun 27 17:28:38.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.027104013s
    STEP: Saw pod success 06/27/23 17:28:38.724
    Jun 27 17:28:38.724: INFO: Pod "pod-subpath-test-downwardapi-lgc2" satisfied condition "Succeeded or Failed"
    Jun 27 17:28:38.733: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-downwardapi-lgc2 container test-container-subpath-downwardapi-lgc2: <nil>
    STEP: delete the pod 06/27/23 17:28:38.803
    Jun 27 17:28:38.835: INFO: Waiting for pod pod-subpath-test-downwardapi-lgc2 to disappear
    Jun 27 17:28:38.845: INFO: Pod pod-subpath-test-downwardapi-lgc2 no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-lgc2 06/27/23 17:28:38.845
    Jun 27 17:28:38.845: INFO: Deleting pod "pod-subpath-test-downwardapi-lgc2" in namespace "subpath-5230"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 27 17:28:38.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5230" for this suite. 06/27/23 17:28:38.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:28:38.895
Jun 27 17:28:38.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-probe 06/27/23 17:28:38.897
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:28:38.935
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:28:38.95
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f in namespace container-probe-9678 06/27/23 17:28:38.97
Jun 27 17:28:39.040: INFO: Waiting up to 5m0s for pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f" in namespace "container-probe-9678" to be "not pending"
Jun 27 17:28:39.050: INFO: Pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.671151ms
Jun 27 17:28:41.061: INFO: Pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020373332s
Jun 27 17:28:43.064: INFO: Pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f": Phase="Running", Reason="", readiness=true. Elapsed: 4.024038479s
Jun 27 17:28:43.065: INFO: Pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f" satisfied condition "not pending"
Jun 27 17:28:43.065: INFO: Started pod busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f in namespace container-probe-9678
STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 17:28:43.065
Jun 27 17:28:43.076: INFO: Initial restart count of pod busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f is 0
Jun 27 17:29:31.446: INFO: Restart count of pod container-probe-9678/busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f is now 1 (48.370061232s elapsed)
STEP: deleting the pod 06/27/23 17:29:31.446
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 27 17:29:31.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9678" for this suite. 06/27/23 17:29:31.497
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":215,"skipped":4244,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.624 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:28:38.895
    Jun 27 17:28:38.895: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-probe 06/27/23 17:28:38.897
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:28:38.935
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:28:38.95
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f in namespace container-probe-9678 06/27/23 17:28:38.97
    Jun 27 17:28:39.040: INFO: Waiting up to 5m0s for pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f" in namespace "container-probe-9678" to be "not pending"
    Jun 27 17:28:39.050: INFO: Pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f": Phase="Pending", Reason="", readiness=false. Elapsed: 9.671151ms
    Jun 27 17:28:41.061: INFO: Pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020373332s
    Jun 27 17:28:43.064: INFO: Pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f": Phase="Running", Reason="", readiness=true. Elapsed: 4.024038479s
    Jun 27 17:28:43.065: INFO: Pod "busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f" satisfied condition "not pending"
    Jun 27 17:28:43.065: INFO: Started pod busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f in namespace container-probe-9678
    STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 17:28:43.065
    Jun 27 17:28:43.076: INFO: Initial restart count of pod busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f is 0
    Jun 27 17:29:31.446: INFO: Restart count of pod container-probe-9678/busybox-f59e4226-a838-4ed2-a150-b6bbf2e0fd3f is now 1 (48.370061232s elapsed)
    STEP: deleting the pod 06/27/23 17:29:31.446
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 27 17:29:31.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9678" for this suite. 06/27/23 17:29:31.497
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:29:31.519
Jun 27 17:29:31.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 17:29:31.522
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:29:31.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:29:31.576
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-1722 06/27/23 17:29:31.587
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[] 06/27/23 17:29:31.626
Jun 27 17:29:31.672: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-1722 06/27/23 17:29:31.672
Jun 27 17:29:31.708: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1722" to be "running and ready"
Jun 27 17:29:31.719: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.640795ms
Jun 27 17:29:31.719: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:29:33.730: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020401285s
Jun 27 17:29:33.730: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:29:35.731: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.020644093s
Jun 27 17:29:35.731: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 27 17:29:35.731: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[pod1:[80]] 06/27/23 17:29:35.74
Jun 27 17:29:35.777: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 06/27/23 17:29:35.777
Jun 27 17:29:35.777: INFO: Creating new exec pod
Jun 27 17:29:35.809: INFO: Waiting up to 5m0s for pod "execpoddmnvd" in namespace "services-1722" to be "running"
Jun 27 17:29:35.819: INFO: Pod "execpoddmnvd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.388631ms
Jun 27 17:29:37.833: INFO: Pod "execpoddmnvd": Phase="Running", Reason="", readiness=true. Elapsed: 2.023873989s
Jun 27 17:29:37.833: INFO: Pod "execpoddmnvd" satisfied condition "running"
Jun 27 17:29:38.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 27 17:29:39.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 27 17:29:39.222: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 17:29:39.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.13.161 80'
Jun 27 17:29:39.590: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.13.161 80\nConnection to 172.21.13.161 80 port [tcp/http] succeeded!\n"
Jun 27 17:29:39.590: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-1722 06/27/23 17:29:39.59
Jun 27 17:29:39.618: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1722" to be "running and ready"
Jun 27 17:29:39.628: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.54608ms
Jun 27 17:29:39.628: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:29:41.639: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020464709s
Jun 27 17:29:41.639: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:29:43.641: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.022256941s
Jun 27 17:29:43.641: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 27 17:29:43.641: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[pod1:[80] pod2:[80]] 06/27/23 17:29:43.65
Jun 27 17:29:43.702: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 06/27/23 17:29:43.702
Jun 27 17:29:44.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 27 17:29:45.089: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 27 17:29:45.089: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 17:29:45.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.13.161 80'
Jun 27 17:29:45.472: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.13.161 80\nConnection to 172.21.13.161 80 port [tcp/http] succeeded!\n"
Jun 27 17:29:45.472: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-1722 06/27/23 17:29:45.472
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[pod2:[80]] 06/27/23 17:29:45.504
Jun 27 17:29:45.591: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 06/27/23 17:29:45.591
Jun 27 17:29:46.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Jun 27 17:29:47.068: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Jun 27 17:29:47.068: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 17:29:47.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.13.161 80'
Jun 27 17:29:47.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.13.161 80\nConnection to 172.21.13.161 80 port [tcp/http] succeeded!\n"
Jun 27 17:29:47.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-1722 06/27/23 17:29:47.524
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[] 06/27/23 17:29:47.56
Jun 27 17:29:47.588: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 17:29:47.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1722" for this suite. 06/27/23 17:29:47.659
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":216,"skipped":4244,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.157 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:29:31.519
    Jun 27 17:29:31.520: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 17:29:31.522
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:29:31.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:29:31.576
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-1722 06/27/23 17:29:31.587
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[] 06/27/23 17:29:31.626
    Jun 27 17:29:31.672: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-1722 06/27/23 17:29:31.672
    Jun 27 17:29:31.708: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-1722" to be "running and ready"
    Jun 27 17:29:31.719: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 8.640795ms
    Jun 27 17:29:31.719: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:29:33.730: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020401285s
    Jun 27 17:29:33.730: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:29:35.731: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.020644093s
    Jun 27 17:29:35.731: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 27 17:29:35.731: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[pod1:[80]] 06/27/23 17:29:35.74
    Jun 27 17:29:35.777: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 06/27/23 17:29:35.777
    Jun 27 17:29:35.777: INFO: Creating new exec pod
    Jun 27 17:29:35.809: INFO: Waiting up to 5m0s for pod "execpoddmnvd" in namespace "services-1722" to be "running"
    Jun 27 17:29:35.819: INFO: Pod "execpoddmnvd": Phase="Pending", Reason="", readiness=false. Elapsed: 9.388631ms
    Jun 27 17:29:37.833: INFO: Pod "execpoddmnvd": Phase="Running", Reason="", readiness=true. Elapsed: 2.023873989s
    Jun 27 17:29:37.833: INFO: Pod "execpoddmnvd" satisfied condition "running"
    Jun 27 17:29:38.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jun 27 17:29:39.222: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 27 17:29:39.222: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 17:29:39.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.13.161 80'
    Jun 27 17:29:39.590: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.13.161 80\nConnection to 172.21.13.161 80 port [tcp/http] succeeded!\n"
    Jun 27 17:29:39.590: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-1722 06/27/23 17:29:39.59
    Jun 27 17:29:39.618: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-1722" to be "running and ready"
    Jun 27 17:29:39.628: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 9.54608ms
    Jun 27 17:29:39.628: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:29:41.639: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020464709s
    Jun 27 17:29:41.639: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:29:43.641: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 4.022256941s
    Jun 27 17:29:43.641: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 27 17:29:43.641: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[pod1:[80] pod2:[80]] 06/27/23 17:29:43.65
    Jun 27 17:29:43.702: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 06/27/23 17:29:43.702
    Jun 27 17:29:44.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jun 27 17:29:45.089: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 27 17:29:45.089: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 17:29:45.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.13.161 80'
    Jun 27 17:29:45.472: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.13.161 80\nConnection to 172.21.13.161 80 port [tcp/http] succeeded!\n"
    Jun 27 17:29:45.472: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-1722 06/27/23 17:29:45.472
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[pod2:[80]] 06/27/23 17:29:45.504
    Jun 27 17:29:45.591: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 06/27/23 17:29:45.591
    Jun 27 17:29:46.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Jun 27 17:29:47.068: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Jun 27 17:29:47.068: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 17:29:47.068: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1722 exec execpoddmnvd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.13.161 80'
    Jun 27 17:29:47.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.13.161 80\nConnection to 172.21.13.161 80 port [tcp/http] succeeded!\n"
    Jun 27 17:29:47.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-1722 06/27/23 17:29:47.524
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1722 to expose endpoints map[] 06/27/23 17:29:47.56
    Jun 27 17:29:47.588: INFO: successfully validated that service endpoint-test2 in namespace services-1722 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 17:29:47.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1722" for this suite. 06/27/23 17:29:47.659
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:29:47.679
Jun 27 17:29:47.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 17:29:47.681
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:29:47.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:29:47.745
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 06/27/23 17:29:47.764
Jun 27 17:29:47.854: INFO: Waiting up to 5m0s for pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52" in namespace "emptydir-656" to be "Succeeded or Failed"
Jun 27 17:29:47.869: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52": Phase="Pending", Reason="", readiness=false. Elapsed: 14.690964ms
Jun 27 17:29:49.879: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025179722s
Jun 27 17:29:51.889: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034290467s
Jun 27 17:29:53.885: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031163974s
STEP: Saw pod success 06/27/23 17:29:53.885
Jun 27 17:29:53.886: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52" satisfied condition "Succeeded or Failed"
Jun 27 17:29:53.894: INFO: Trying to get logs from node 10.113.180.90 pod pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52 container test-container: <nil>
STEP: delete the pod 06/27/23 17:29:53.93
Jun 27 17:29:53.966: INFO: Waiting for pod pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52 to disappear
Jun 27 17:29:53.985: INFO: Pod pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 17:29:53.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-656" for this suite. 06/27/23 17:29:54.004
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":217,"skipped":4258,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.343 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:29:47.679
    Jun 27 17:29:47.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 17:29:47.681
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:29:47.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:29:47.745
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 06/27/23 17:29:47.764
    Jun 27 17:29:47.854: INFO: Waiting up to 5m0s for pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52" in namespace "emptydir-656" to be "Succeeded or Failed"
    Jun 27 17:29:47.869: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52": Phase="Pending", Reason="", readiness=false. Elapsed: 14.690964ms
    Jun 27 17:29:49.879: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025179722s
    Jun 27 17:29:51.889: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034290467s
    Jun 27 17:29:53.885: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031163974s
    STEP: Saw pod success 06/27/23 17:29:53.885
    Jun 27 17:29:53.886: INFO: Pod "pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52" satisfied condition "Succeeded or Failed"
    Jun 27 17:29:53.894: INFO: Trying to get logs from node 10.113.180.90 pod pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52 container test-container: <nil>
    STEP: delete the pod 06/27/23 17:29:53.93
    Jun 27 17:29:53.966: INFO: Waiting for pod pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52 to disappear
    Jun 27 17:29:53.985: INFO: Pod pod-da475bc8-cf29-4d4c-b1a6-58e0bd0b8f52 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:29:53.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-656" for this suite. 06/27/23 17:29:54.004
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:29:54.022
Jun 27 17:29:54.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename subpath 06/27/23 17:29:54.024
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:29:54.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:29:54.08
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/27/23 17:29:54.091
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-bp52 06/27/23 17:29:54.122
STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:29:54.123
Jun 27 17:29:54.203: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bp52" in namespace "subpath-6297" to be "Succeeded or Failed"
Jun 27 17:29:54.220: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Pending", Reason="", readiness=false. Elapsed: 17.010416ms
Jun 27 17:29:56.230: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 2.027415131s
Jun 27 17:29:58.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 4.028106312s
Jun 27 17:30:00.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 6.028161074s
Jun 27 17:30:02.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 8.028073542s
Jun 27 17:30:04.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 10.028053365s
Jun 27 17:30:06.229: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 12.026318021s
Jun 27 17:30:08.230: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 14.027019467s
Jun 27 17:30:10.232: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 16.029152808s
Jun 27 17:30:12.230: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 18.027534963s
Jun 27 17:30:14.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 20.028453298s
Jun 27 17:30:16.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=false. Elapsed: 22.028667917s
Jun 27 17:30:18.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028558939s
STEP: Saw pod success 06/27/23 17:30:18.231
Jun 27 17:30:18.231: INFO: Pod "pod-subpath-test-configmap-bp52" satisfied condition "Succeeded or Failed"
Jun 27 17:30:18.241: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-configmap-bp52 container test-container-subpath-configmap-bp52: <nil>
STEP: delete the pod 06/27/23 17:30:18.276
Jun 27 17:30:18.302: INFO: Waiting for pod pod-subpath-test-configmap-bp52 to disappear
Jun 27 17:30:18.312: INFO: Pod pod-subpath-test-configmap-bp52 no longer exists
STEP: Deleting pod pod-subpath-test-configmap-bp52 06/27/23 17:30:18.312
Jun 27 17:30:18.312: INFO: Deleting pod "pod-subpath-test-configmap-bp52" in namespace "subpath-6297"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 27 17:30:18.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6297" for this suite. 06/27/23 17:30:18.338
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":218,"skipped":4259,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.334 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:29:54.022
    Jun 27 17:29:54.022: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename subpath 06/27/23 17:29:54.024
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:29:54.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:29:54.08
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/27/23 17:29:54.091
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-bp52 06/27/23 17:29:54.122
    STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:29:54.123
    Jun 27 17:29:54.203: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-bp52" in namespace "subpath-6297" to be "Succeeded or Failed"
    Jun 27 17:29:54.220: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Pending", Reason="", readiness=false. Elapsed: 17.010416ms
    Jun 27 17:29:56.230: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 2.027415131s
    Jun 27 17:29:58.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 4.028106312s
    Jun 27 17:30:00.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 6.028161074s
    Jun 27 17:30:02.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 8.028073542s
    Jun 27 17:30:04.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 10.028053365s
    Jun 27 17:30:06.229: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 12.026318021s
    Jun 27 17:30:08.230: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 14.027019467s
    Jun 27 17:30:10.232: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 16.029152808s
    Jun 27 17:30:12.230: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 18.027534963s
    Jun 27 17:30:14.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=true. Elapsed: 20.028453298s
    Jun 27 17:30:16.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Running", Reason="", readiness=false. Elapsed: 22.028667917s
    Jun 27 17:30:18.231: INFO: Pod "pod-subpath-test-configmap-bp52": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.028558939s
    STEP: Saw pod success 06/27/23 17:30:18.231
    Jun 27 17:30:18.231: INFO: Pod "pod-subpath-test-configmap-bp52" satisfied condition "Succeeded or Failed"
    Jun 27 17:30:18.241: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-configmap-bp52 container test-container-subpath-configmap-bp52: <nil>
    STEP: delete the pod 06/27/23 17:30:18.276
    Jun 27 17:30:18.302: INFO: Waiting for pod pod-subpath-test-configmap-bp52 to disappear
    Jun 27 17:30:18.312: INFO: Pod pod-subpath-test-configmap-bp52 no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-bp52 06/27/23 17:30:18.312
    Jun 27 17:30:18.312: INFO: Deleting pod "pod-subpath-test-configmap-bp52" in namespace "subpath-6297"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 27 17:30:18.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6297" for this suite. 06/27/23 17:30:18.338
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:30:18.359
Jun 27 17:30:18.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename runtimeclass 06/27/23 17:30:18.361
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:30:18.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:30:18.41
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Jun 27 17:30:18.505: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4836 to be scheduled
Jun 27 17:30:18.514: INFO: 1 pods are not scheduled: [runtimeclass-4836/test-runtimeclass-runtimeclass-4836-preconfigured-handler-z8jcj(830106ae-9427-49bb-a683-13c0609e36a6)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 27 17:30:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-4836" for this suite. 06/27/23 17:30:20.56
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":219,"skipped":4272,"failed":0}
------------------------------
â€¢ [2.227 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:30:18.359
    Jun 27 17:30:18.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename runtimeclass 06/27/23 17:30:18.361
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:30:18.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:30:18.41
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Jun 27 17:30:18.505: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-4836 to be scheduled
    Jun 27 17:30:18.514: INFO: 1 pods are not scheduled: [runtimeclass-4836/test-runtimeclass-runtimeclass-4836-preconfigured-handler-z8jcj(830106ae-9427-49bb-a683-13c0609e36a6)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 27 17:30:20.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-4836" for this suite. 06/27/23 17:30:20.56
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:30:20.592
Jun 27 17:30:20.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename statefulset 06/27/23 17:30:20.598
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:30:20.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:30:20.648
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-6535 06/27/23 17:30:20.66
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 06/27/23 17:30:20.68
Jun 27 17:30:20.720: INFO: Found 0 stateful pods, waiting for 3
Jun 27 17:30:30.734: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:30:30.734: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:30:30.735: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/27/23 17:30:30.77
Jun 27 17:30:30.812: INFO: Updating stateful set ss2
STEP: Creating a new revision 06/27/23 17:30:30.812
STEP: Not applying an update when the partition is greater than the number of replicas 06/27/23 17:30:40.868
STEP: Performing a canary update 06/27/23 17:30:40.868
Jun 27 17:30:40.909: INFO: Updating stateful set ss2
Jun 27 17:30:40.944: INFO: Waiting for Pod statefulset-6535/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 06/27/23 17:30:50.981
Jun 27 17:30:51.088: INFO: Found 2 stateful pods, waiting for 3
Jun 27 17:31:01.102: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:31:01.102: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Jun 27 17:31:01.102: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 06/27/23 17:31:01.128
Jun 27 17:31:01.171: INFO: Updating stateful set ss2
Jun 27 17:31:01.200: INFO: Waiting for Pod statefulset-6535/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Jun 27 17:31:11.299: INFO: Updating stateful set ss2
Jun 27 17:31:11.349: INFO: Waiting for StatefulSet statefulset-6535/ss2 to complete update
Jun 27 17:31:11.349: INFO: Waiting for Pod statefulset-6535/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 27 17:31:21.376: INFO: Deleting all statefulset in ns statefulset-6535
Jun 27 17:31:21.389: INFO: Scaling statefulset ss2 to 0
Jun 27 17:31:31.442: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:31:31.455: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 27 17:31:31.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-6535" for this suite. 06/27/23 17:31:31.525
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":220,"skipped":4275,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.958 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:30:20.592
    Jun 27 17:30:20.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename statefulset 06/27/23 17:30:20.598
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:30:20.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:30:20.648
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-6535 06/27/23 17:30:20.66
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 06/27/23 17:30:20.68
    Jun 27 17:30:20.720: INFO: Found 0 stateful pods, waiting for 3
    Jun 27 17:30:30.734: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 17:30:30.734: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 17:30:30.735: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 06/27/23 17:30:30.77
    Jun 27 17:30:30.812: INFO: Updating stateful set ss2
    STEP: Creating a new revision 06/27/23 17:30:30.812
    STEP: Not applying an update when the partition is greater than the number of replicas 06/27/23 17:30:40.868
    STEP: Performing a canary update 06/27/23 17:30:40.868
    Jun 27 17:30:40.909: INFO: Updating stateful set ss2
    Jun 27 17:30:40.944: INFO: Waiting for Pod statefulset-6535/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 06/27/23 17:30:50.981
    Jun 27 17:30:51.088: INFO: Found 2 stateful pods, waiting for 3
    Jun 27 17:31:01.102: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 17:31:01.102: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Jun 27 17:31:01.102: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 06/27/23 17:31:01.128
    Jun 27 17:31:01.171: INFO: Updating stateful set ss2
    Jun 27 17:31:01.200: INFO: Waiting for Pod statefulset-6535/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Jun 27 17:31:11.299: INFO: Updating stateful set ss2
    Jun 27 17:31:11.349: INFO: Waiting for StatefulSet statefulset-6535/ss2 to complete update
    Jun 27 17:31:11.349: INFO: Waiting for Pod statefulset-6535/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 27 17:31:21.376: INFO: Deleting all statefulset in ns statefulset-6535
    Jun 27 17:31:21.389: INFO: Scaling statefulset ss2 to 0
    Jun 27 17:31:31.442: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 17:31:31.455: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 27 17:31:31.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-6535" for this suite. 06/27/23 17:31:31.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:31:31.551
Jun 27 17:31:31.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename statefulset 06/27/23 17:31:31.554
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:31:31.59
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:31:31.601
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-5241 06/27/23 17:31:31.611
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-5241 06/27/23 17:31:31.662
Jun 27 17:31:31.695: INFO: Found 0 stateful pods, waiting for 1
Jun 27 17:31:41.706: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 06/27/23 17:31:41.729
STEP: Getting /status 06/27/23 17:31:41.75
Jun 27 17:31:41.766: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 06/27/23 17:31:41.766
Jun 27 17:31:41.797: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 06/27/23 17:31:41.797
Jun 27 17:31:41.804: INFO: Observed &StatefulSet event: ADDED
Jun 27 17:31:41.804: INFO: Found Statefulset ss in namespace statefulset-5241 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 27 17:31:41.804: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 06/27/23 17:31:41.804
Jun 27 17:31:41.804: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 27 17:31:41.821: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 06/27/23 17:31:41.822
Jun 27 17:31:41.827: INFO: Observed &StatefulSet event: ADDED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 27 17:31:41.827: INFO: Deleting all statefulset in ns statefulset-5241
Jun 27 17:31:41.840: INFO: Scaling statefulset ss to 0
Jun 27 17:31:51.896: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:31:51.910: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 27 17:31:51.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-5241" for this suite. 06/27/23 17:31:51.975
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":221,"skipped":4284,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.442 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:31:31.551
    Jun 27 17:31:31.552: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename statefulset 06/27/23 17:31:31.554
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:31:31.59
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:31:31.601
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-5241 06/27/23 17:31:31.611
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-5241 06/27/23 17:31:31.662
    Jun 27 17:31:31.695: INFO: Found 0 stateful pods, waiting for 1
    Jun 27 17:31:41.706: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 06/27/23 17:31:41.729
    STEP: Getting /status 06/27/23 17:31:41.75
    Jun 27 17:31:41.766: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 06/27/23 17:31:41.766
    Jun 27 17:31:41.797: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 06/27/23 17:31:41.797
    Jun 27 17:31:41.804: INFO: Observed &StatefulSet event: ADDED
    Jun 27 17:31:41.804: INFO: Found Statefulset ss in namespace statefulset-5241 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 27 17:31:41.804: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 06/27/23 17:31:41.804
    Jun 27 17:31:41.804: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 27 17:31:41.821: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 06/27/23 17:31:41.822
    Jun 27 17:31:41.827: INFO: Observed &StatefulSet event: ADDED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 27 17:31:41.827: INFO: Deleting all statefulset in ns statefulset-5241
    Jun 27 17:31:41.840: INFO: Scaling statefulset ss to 0
    Jun 27 17:31:51.896: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 17:31:51.910: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 27 17:31:51.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-5241" for this suite. 06/27/23 17:31:51.975
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:31:52.003
Jun 27 17:31:52.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 17:31:52.006
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:31:52.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:31:52.053
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-03045948-aa49-48a0-b1f1-c63d203486ba 06/27/23 17:31:52.064
STEP: Creating a pod to test consume configMaps 06/27/23 17:31:52.078
Jun 27 17:31:52.157: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387" in namespace "configmap-348" to be "Succeeded or Failed"
Jun 27 17:31:52.166: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387": Phase="Pending", Reason="", readiness=false. Elapsed: 8.570534ms
Jun 27 17:31:54.180: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022014489s
Jun 27 17:31:56.177: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019315404s
Jun 27 17:31:58.178: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020912621s
STEP: Saw pod success 06/27/23 17:31:58.178
Jun 27 17:31:58.179: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387" satisfied condition "Succeeded or Failed"
Jun 27 17:31:58.190: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387 container agnhost-container: <nil>
STEP: delete the pod 06/27/23 17:31:58.326
Jun 27 17:31:58.362: INFO: Waiting for pod pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387 to disappear
Jun 27 17:31:58.375: INFO: Pod pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 17:31:58.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-348" for this suite. 06/27/23 17:31:58.395
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":222,"skipped":4288,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.407 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:31:52.003
    Jun 27 17:31:52.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 17:31:52.006
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:31:52.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:31:52.053
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-03045948-aa49-48a0-b1f1-c63d203486ba 06/27/23 17:31:52.064
    STEP: Creating a pod to test consume configMaps 06/27/23 17:31:52.078
    Jun 27 17:31:52.157: INFO: Waiting up to 5m0s for pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387" in namespace "configmap-348" to be "Succeeded or Failed"
    Jun 27 17:31:52.166: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387": Phase="Pending", Reason="", readiness=false. Elapsed: 8.570534ms
    Jun 27 17:31:54.180: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022014489s
    Jun 27 17:31:56.177: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387": Phase="Pending", Reason="", readiness=false. Elapsed: 4.019315404s
    Jun 27 17:31:58.178: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.020912621s
    STEP: Saw pod success 06/27/23 17:31:58.178
    Jun 27 17:31:58.179: INFO: Pod "pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387" satisfied condition "Succeeded or Failed"
    Jun 27 17:31:58.190: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387 container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 17:31:58.326
    Jun 27 17:31:58.362: INFO: Waiting for pod pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387 to disappear
    Jun 27 17:31:58.375: INFO: Pod pod-configmaps-b6dfdf50-cfb3-4482-bc1d-6ce18af76387 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 17:31:58.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-348" for this suite. 06/27/23 17:31:58.395
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:31:58.417
Jun 27 17:31:58.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pod-network-test 06/27/23 17:31:58.418
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:31:58.451
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:31:58.462
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-8195 06/27/23 17:31:58.47
STEP: creating a selector 06/27/23 17:31:58.471
STEP: Creating the service pods in kubernetes 06/27/23 17:31:58.471
Jun 27 17:31:58.471: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 27 17:31:58.732: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8195" to be "running and ready"
Jun 27 17:31:58.774: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.583629ms
Jun 27 17:31:58.774: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:32:00.785: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053363356s
Jun 27 17:32:00.785: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:32:02.792: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.060309275s
Jun 27 17:32:02.792: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:32:04.792: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.060691524s
Jun 27 17:32:04.792: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:32:06.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.054230514s
Jun 27 17:32:06.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:32:08.798: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.066811842s
Jun 27 17:32:08.799: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:32:10.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.055640241s
Jun 27 17:32:10.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:32:12.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.05591208s
Jun 27 17:32:12.788: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:32:14.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.052954428s
Jun 27 17:32:14.785: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:32:16.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.05464719s
Jun 27 17:32:16.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:32:18.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.054534292s
Jun 27 17:32:18.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:32:20.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.053384244s
Jun 27 17:32:20.785: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 27 17:32:20.785: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 27 17:32:20.794: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8195" to be "running and ready"
Jun 27 17:32:20.804: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.808658ms
Jun 27 17:32:20.804: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 27 17:32:20.804: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 27 17:32:20.814: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8195" to be "running and ready"
Jun 27 17:32:20.833: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 18.789911ms
Jun 27 17:32:20.833: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 27 17:32:20.833: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/27/23 17:32:20.842
Jun 27 17:32:20.899: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8195" to be "running"
Jun 27 17:32:20.909: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.758964ms
Jun 27 17:32:22.920: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021557204s
Jun 27 17:32:22.921: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 27 17:32:22.934: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8195" to be "running"
Jun 27 17:32:22.946: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.098639ms
Jun 27 17:32:22.946: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun 27 17:32:22.957: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 27 17:32:22.958: INFO: Going to poll 172.30.106.191 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 27 17:32:22.968: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.106.191:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8195 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:32:22.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:32:22.971: INFO: ExecWithOptions: Clientset creation
Jun 27 17:32:22.972: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8195/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.106.191%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 27 17:32:23.296: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 27 17:32:23.296: INFO: Going to poll 172.30.250.234 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 27 17:32:23.306: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.250.234:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8195 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:32:23.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:32:23.307: INFO: ExecWithOptions: Clientset creation
Jun 27 17:32:23.307: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8195/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.250.234%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 27 17:32:23.610: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 27 17:32:23.610: INFO: Going to poll 172.30.60.150 on port 8083 at least 0 times, with a maximum of 39 tries before failing
Jun 27 17:32:23.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.60.150:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8195 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:32:23.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:32:23.621: INFO: ExecWithOptions: Clientset creation
Jun 27 17:32:23.621: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8195/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.60.150%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 27 17:32:23.942: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 27 17:32:23.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8195" for this suite. 06/27/23 17:32:23.96
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":223,"skipped":4326,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.593 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:31:58.417
    Jun 27 17:31:58.417: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pod-network-test 06/27/23 17:31:58.418
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:31:58.451
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:31:58.462
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-8195 06/27/23 17:31:58.47
    STEP: creating a selector 06/27/23 17:31:58.471
    STEP: Creating the service pods in kubernetes 06/27/23 17:31:58.471
    Jun 27 17:31:58.471: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 27 17:31:58.732: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8195" to be "running and ready"
    Jun 27 17:31:58.774: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 42.583629ms
    Jun 27 17:31:58.774: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:32:00.785: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.053363356s
    Jun 27 17:32:00.785: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:32:02.792: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.060309275s
    Jun 27 17:32:02.792: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:32:04.792: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.060691524s
    Jun 27 17:32:04.792: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:32:06.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.054230514s
    Jun 27 17:32:06.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:32:08.798: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.066811842s
    Jun 27 17:32:08.799: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:32:10.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.055640241s
    Jun 27 17:32:10.787: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:32:12.787: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.05591208s
    Jun 27 17:32:12.788: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:32:14.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.052954428s
    Jun 27 17:32:14.785: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:32:16.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.05464719s
    Jun 27 17:32:16.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:32:18.786: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.054534292s
    Jun 27 17:32:18.786: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:32:20.785: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.053384244s
    Jun 27 17:32:20.785: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 27 17:32:20.785: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 27 17:32:20.794: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8195" to be "running and ready"
    Jun 27 17:32:20.804: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 9.808658ms
    Jun 27 17:32:20.804: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 27 17:32:20.804: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 27 17:32:20.814: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-8195" to be "running and ready"
    Jun 27 17:32:20.833: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 18.789911ms
    Jun 27 17:32:20.833: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 27 17:32:20.833: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/27/23 17:32:20.842
    Jun 27 17:32:20.899: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8195" to be "running"
    Jun 27 17:32:20.909: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.758964ms
    Jun 27 17:32:22.920: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.021557204s
    Jun 27 17:32:22.921: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 27 17:32:22.934: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-8195" to be "running"
    Jun 27 17:32:22.946: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.098639ms
    Jun 27 17:32:22.946: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun 27 17:32:22.957: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 27 17:32:22.958: INFO: Going to poll 172.30.106.191 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 27 17:32:22.968: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.106.191:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8195 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:32:22.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:32:22.971: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:32:22.972: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8195/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.106.191%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 27 17:32:23.296: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun 27 17:32:23.296: INFO: Going to poll 172.30.250.234 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 27 17:32:23.306: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.250.234:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8195 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:32:23.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:32:23.307: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:32:23.307: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8195/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.250.234%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 27 17:32:23.610: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun 27 17:32:23.610: INFO: Going to poll 172.30.60.150 on port 8083 at least 0 times, with a maximum of 39 tries before failing
    Jun 27 17:32:23.620: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.30.60.150:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-8195 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:32:23.620: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:32:23.621: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:32:23.621: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-8195/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.30.60.150%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 27 17:32:23.942: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 27 17:32:23.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8195" for this suite. 06/27/23 17:32:23.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:32:24.014
Jun 27 17:32:24.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename endpointslice 06/27/23 17:32:24.016
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:32:24.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:32:24.209
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Jun 27 17:32:24.424: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
Jun 27 17:32:24.424: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 27 17:32:24.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-1281" for this suite. 06/27/23 17:32:24.497
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":224,"skipped":4343,"failed":0}
------------------------------
â€¢ [0.502 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:32:24.014
    Jun 27 17:32:24.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename endpointslice 06/27/23 17:32:24.016
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:32:24.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:32:24.209
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Jun 27 17:32:24.424: INFO: Endpoints addresses: [172.20.0.1] , ports: [2040]
    Jun 27 17:32:24.424: INFO: EndpointSlices addresses: [172.20.0.1] , ports: [2040]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 27 17:32:24.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-1281" for this suite. 06/27/23 17:32:24.497
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:32:24.516
Jun 27 17:32:24.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename var-expansion 06/27/23 17:32:24.519
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:32:24.563
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:32:24.58
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 06/27/23 17:32:24.591
STEP: waiting for pod running 06/27/23 17:32:24.639
Jun 27 17:32:24.639: INFO: Waiting up to 2m0s for pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" in namespace "var-expansion-491" to be "running"
Jun 27 17:32:24.678: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600": Phase="Pending", Reason="", readiness=false. Elapsed: 39.09505ms
Jun 27 17:32:26.691: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051877037s
Jun 27 17:32:28.697: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600": Phase="Running", Reason="", readiness=true. Elapsed: 4.058027906s
Jun 27 17:32:28.697: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" satisfied condition "running"
STEP: creating a file in subpath 06/27/23 17:32:28.697
Jun 27 17:32:28.715: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-491 PodName:var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:32:28.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:32:28.715: INFO: ExecWithOptions: Clientset creation
Jun 27 17:32:28.715: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-491/pods/var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 06/27/23 17:32:29.009
Jun 27 17:32:29.023: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-491 PodName:var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:32:29.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:32:29.024: INFO: ExecWithOptions: Clientset creation
Jun 27 17:32:29.025: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-491/pods/var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 06/27/23 17:32:29.35
Jun 27 17:32:29.895: INFO: Successfully updated pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600"
STEP: waiting for annotated pod running 06/27/23 17:32:29.896
Jun 27 17:32:29.897: INFO: Waiting up to 2m0s for pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" in namespace "var-expansion-491" to be "running"
Jun 27 17:32:29.909: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600": Phase="Running", Reason="", readiness=true. Elapsed: 12.12875ms
Jun 27 17:32:29.910: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" satisfied condition "running"
STEP: deleting the pod gracefully 06/27/23 17:32:29.91
Jun 27 17:32:29.910: INFO: Deleting pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" in namespace "var-expansion-491"
Jun 27 17:32:29.927: INFO: Wait up to 5m0s for pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 27 17:33:03.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-491" for this suite. 06/27/23 17:33:03.996
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":225,"skipped":4345,"failed":0}
------------------------------
â€¢ [SLOW TEST] [39.503 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:32:24.516
    Jun 27 17:32:24.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename var-expansion 06/27/23 17:32:24.519
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:32:24.563
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:32:24.58
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 06/27/23 17:32:24.591
    STEP: waiting for pod running 06/27/23 17:32:24.639
    Jun 27 17:32:24.639: INFO: Waiting up to 2m0s for pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" in namespace "var-expansion-491" to be "running"
    Jun 27 17:32:24.678: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600": Phase="Pending", Reason="", readiness=false. Elapsed: 39.09505ms
    Jun 27 17:32:26.691: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600": Phase="Pending", Reason="", readiness=false. Elapsed: 2.051877037s
    Jun 27 17:32:28.697: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600": Phase="Running", Reason="", readiness=true. Elapsed: 4.058027906s
    Jun 27 17:32:28.697: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" satisfied condition "running"
    STEP: creating a file in subpath 06/27/23 17:32:28.697
    Jun 27 17:32:28.715: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-491 PodName:var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:32:28.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:32:28.715: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:32:28.715: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-491/pods/var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 06/27/23 17:32:29.009
    Jun 27 17:32:29.023: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-491 PodName:var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:32:29.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:32:29.024: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:32:29.025: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/var-expansion-491/pods/var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 06/27/23 17:32:29.35
    Jun 27 17:32:29.895: INFO: Successfully updated pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600"
    STEP: waiting for annotated pod running 06/27/23 17:32:29.896
    Jun 27 17:32:29.897: INFO: Waiting up to 2m0s for pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" in namespace "var-expansion-491" to be "running"
    Jun 27 17:32:29.909: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600": Phase="Running", Reason="", readiness=true. Elapsed: 12.12875ms
    Jun 27 17:32:29.910: INFO: Pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" satisfied condition "running"
    STEP: deleting the pod gracefully 06/27/23 17:32:29.91
    Jun 27 17:32:29.910: INFO: Deleting pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" in namespace "var-expansion-491"
    Jun 27 17:32:29.927: INFO: Wait up to 5m0s for pod "var-expansion-f2dff651-94b3-4ea5-9501-1a1a4dd8f600" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 27 17:33:03.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-491" for this suite. 06/27/23 17:33:03.996
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:33:04.024
Jun 27 17:33:04.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 17:33:04.026
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:04.063
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:04.077
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-9163/secret-test-5fbce630-ca4f-4647-8299-1463372d79c5 06/27/23 17:33:04.089
STEP: Creating a pod to test consume secrets 06/27/23 17:33:04.109
Jun 27 17:33:04.178: INFO: Waiting up to 5m0s for pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0" in namespace "secrets-9163" to be "Succeeded or Failed"
Jun 27 17:33:04.192: INFO: Pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.735617ms
Jun 27 17:33:06.203: INFO: Pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024838534s
Jun 27 17:33:08.204: INFO: Pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025172182s
STEP: Saw pod success 06/27/23 17:33:08.204
Jun 27 17:33:08.204: INFO: Pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0" satisfied condition "Succeeded or Failed"
Jun 27 17:33:08.213: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0 container env-test: <nil>
STEP: delete the pod 06/27/23 17:33:08.245
Jun 27 17:33:08.275: INFO: Waiting for pod pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0 to disappear
Jun 27 17:33:08.284: INFO: Pod pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 27 17:33:08.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9163" for this suite. 06/27/23 17:33:08.301
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":226,"skipped":4383,"failed":0}
------------------------------
â€¢ [4.294 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:33:04.024
    Jun 27 17:33:04.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 17:33:04.026
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:04.063
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:04.077
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-9163/secret-test-5fbce630-ca4f-4647-8299-1463372d79c5 06/27/23 17:33:04.089
    STEP: Creating a pod to test consume secrets 06/27/23 17:33:04.109
    Jun 27 17:33:04.178: INFO: Waiting up to 5m0s for pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0" in namespace "secrets-9163" to be "Succeeded or Failed"
    Jun 27 17:33:04.192: INFO: Pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0": Phase="Pending", Reason="", readiness=false. Elapsed: 13.735617ms
    Jun 27 17:33:06.203: INFO: Pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024838534s
    Jun 27 17:33:08.204: INFO: Pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.025172182s
    STEP: Saw pod success 06/27/23 17:33:08.204
    Jun 27 17:33:08.204: INFO: Pod "pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0" satisfied condition "Succeeded or Failed"
    Jun 27 17:33:08.213: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0 container env-test: <nil>
    STEP: delete the pod 06/27/23 17:33:08.245
    Jun 27 17:33:08.275: INFO: Waiting for pod pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0 to disappear
    Jun 27 17:33:08.284: INFO: Pod pod-configmaps-75d245a5-4380-4932-9c42-21b20d5f45d0 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 17:33:08.285: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9163" for this suite. 06/27/23 17:33:08.301
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:33:08.326
Jun 27 17:33:08.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:33:08.327
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:08.366
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:08.379
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:33:08.39
Jun 27 17:33:08.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3" in namespace "projected-7172" to be "Succeeded or Failed"
Jun 27 17:33:08.507: INFO: Pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.916134ms
Jun 27 17:33:10.520: INFO: Pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029621719s
Jun 27 17:33:12.518: INFO: Pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027326584s
STEP: Saw pod success 06/27/23 17:33:12.518
Jun 27 17:33:12.518: INFO: Pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3" satisfied condition "Succeeded or Failed"
Jun 27 17:33:12.528: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3 container client-container: <nil>
STEP: delete the pod 06/27/23 17:33:12.557
Jun 27 17:33:12.586: INFO: Waiting for pod downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3 to disappear
Jun 27 17:33:12.600: INFO: Pod downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 17:33:12.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7172" for this suite. 06/27/23 17:33:12.617
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":227,"skipped":4412,"failed":0}
------------------------------
â€¢ [4.309 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:33:08.326
    Jun 27 17:33:08.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:33:08.327
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:08.366
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:08.379
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:33:08.39
    Jun 27 17:33:08.490: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3" in namespace "projected-7172" to be "Succeeded or Failed"
    Jun 27 17:33:08.507: INFO: Pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3": Phase="Pending", Reason="", readiness=false. Elapsed: 16.916134ms
    Jun 27 17:33:10.520: INFO: Pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029621719s
    Jun 27 17:33:12.518: INFO: Pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.027326584s
    STEP: Saw pod success 06/27/23 17:33:12.518
    Jun 27 17:33:12.518: INFO: Pod "downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3" satisfied condition "Succeeded or Failed"
    Jun 27 17:33:12.528: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:33:12.557
    Jun 27 17:33:12.586: INFO: Waiting for pod downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3 to disappear
    Jun 27 17:33:12.600: INFO: Pod downwardapi-volume-d84c0e9f-624d-4b69-9997-2731a200f4b3 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 17:33:12.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7172" for this suite. 06/27/23 17:33:12.617
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:33:12.643
Jun 27 17:33:12.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename security-context 06/27/23 17:33:12.644
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:12.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:12.695
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/27/23 17:33:12.704
Jun 27 17:33:12.760: INFO: Waiting up to 5m0s for pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d" in namespace "security-context-2083" to be "Succeeded or Failed"
Jun 27 17:33:12.769: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351602ms
Jun 27 17:33:14.782: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022193224s
Jun 27 17:33:16.792: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032396404s
Jun 27 17:33:18.780: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019868245s
STEP: Saw pod success 06/27/23 17:33:18.78
Jun 27 17:33:18.780: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d" satisfied condition "Succeeded or Failed"
Jun 27 17:33:18.789: INFO: Trying to get logs from node 10.113.180.90 pod security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d container test-container: <nil>
STEP: delete the pod 06/27/23 17:33:18.821
Jun 27 17:33:18.856: INFO: Waiting for pod security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d to disappear
Jun 27 17:33:18.866: INFO: Pod security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Jun 27 17:33:18.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-2083" for this suite. 06/27/23 17:33:18.882
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":228,"skipped":4432,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.257 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:33:12.643
    Jun 27 17:33:12.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename security-context 06/27/23 17:33:12.644
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:12.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:12.695
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 06/27/23 17:33:12.704
    Jun 27 17:33:12.760: INFO: Waiting up to 5m0s for pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d" in namespace "security-context-2083" to be "Succeeded or Failed"
    Jun 27 17:33:12.769: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d": Phase="Pending", Reason="", readiness=false. Elapsed: 9.351602ms
    Jun 27 17:33:14.782: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022193224s
    Jun 27 17:33:16.792: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032396404s
    Jun 27 17:33:18.780: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.019868245s
    STEP: Saw pod success 06/27/23 17:33:18.78
    Jun 27 17:33:18.780: INFO: Pod "security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d" satisfied condition "Succeeded or Failed"
    Jun 27 17:33:18.789: INFO: Trying to get logs from node 10.113.180.90 pod security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d container test-container: <nil>
    STEP: delete the pod 06/27/23 17:33:18.821
    Jun 27 17:33:18.856: INFO: Waiting for pod security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d to disappear
    Jun 27 17:33:18.866: INFO: Pod security-context-a9cfad7f-bc54-4e27-a95a-de311eee984d no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Jun 27 17:33:18.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-2083" for this suite. 06/27/23 17:33:18.882
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:33:18.911
Jun 27 17:33:18.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 17:33:18.912
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:18.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:18.957
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 06/27/23 17:33:18.971
STEP: submitting the pod to kubernetes 06/27/23 17:33:18.971
Jun 27 17:33:19.056: INFO: Waiting up to 5m0s for pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a" in namespace "pods-17" to be "running and ready"
Jun 27 17:33:19.067: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.000572ms
Jun 27 17:33:19.067: INFO: The phase of Pod pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:33:21.077: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021884567s
Jun 27 17:33:21.078: INFO: The phase of Pod pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:33:23.078: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a": Phase="Running", Reason="", readiness=true. Elapsed: 4.022649065s
Jun 27 17:33:23.078: INFO: The phase of Pod pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a is Running (Ready = true)
Jun 27 17:33:23.078: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/27/23 17:33:23.089
STEP: updating the pod 06/27/23 17:33:23.099
Jun 27 17:33:23.641: INFO: Successfully updated pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a"
Jun 27 17:33:23.642: INFO: Waiting up to 5m0s for pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a" in namespace "pods-17" to be "running"
Jun 27 17:33:23.651: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a": Phase="Running", Reason="", readiness=true. Elapsed: 9.289139ms
Jun 27 17:33:23.651: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 06/27/23 17:33:23.651
Jun 27 17:33:23.661: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 17:33:23.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-17" for this suite. 06/27/23 17:33:23.676
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":229,"skipped":4477,"failed":0}
------------------------------
â€¢ [4.782 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:33:18.911
    Jun 27 17:33:18.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 17:33:18.912
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:18.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:18.957
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 06/27/23 17:33:18.971
    STEP: submitting the pod to kubernetes 06/27/23 17:33:18.971
    Jun 27 17:33:19.056: INFO: Waiting up to 5m0s for pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a" in namespace "pods-17" to be "running and ready"
    Jun 27 17:33:19.067: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a": Phase="Pending", Reason="", readiness=false. Elapsed: 11.000572ms
    Jun 27 17:33:19.067: INFO: The phase of Pod pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:33:21.077: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021884567s
    Jun 27 17:33:21.078: INFO: The phase of Pod pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:33:23.078: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a": Phase="Running", Reason="", readiness=true. Elapsed: 4.022649065s
    Jun 27 17:33:23.078: INFO: The phase of Pod pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a is Running (Ready = true)
    Jun 27 17:33:23.078: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/27/23 17:33:23.089
    STEP: updating the pod 06/27/23 17:33:23.099
    Jun 27 17:33:23.641: INFO: Successfully updated pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a"
    Jun 27 17:33:23.642: INFO: Waiting up to 5m0s for pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a" in namespace "pods-17" to be "running"
    Jun 27 17:33:23.651: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a": Phase="Running", Reason="", readiness=true. Elapsed: 9.289139ms
    Jun 27 17:33:23.651: INFO: Pod "pod-update-3308d2b3-3ef4-4aea-b87a-87245036649a" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 06/27/23 17:33:23.651
    Jun 27 17:33:23.661: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 17:33:23.661: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-17" for this suite. 06/27/23 17:33:23.676
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:33:23.698
Jun 27 17:33:23.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:33:23.7
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:23.737
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:23.747
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
Jun 27 17:33:23.774: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating secret with name s-test-opt-del-f29a31ca-efbe-4988-aa64-e0ad86160484 06/27/23 17:33:23.775
STEP: Creating secret with name s-test-opt-upd-ed3393de-8dd8-4c82-beac-391c278fa76b 06/27/23 17:33:23.791
STEP: Creating the pod 06/27/23 17:33:23.809
Jun 27 17:33:23.861: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132" in namespace "projected-1079" to be "running and ready"
Jun 27 17:33:23.871: INFO: Pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132": Phase="Pending", Reason="", readiness=false. Elapsed: 9.75703ms
Jun 27 17:33:23.871: INFO: The phase of Pod pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:33:25.883: INFO: Pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02179677s
Jun 27 17:33:25.883: INFO: The phase of Pod pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:33:27.883: INFO: Pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132": Phase="Running", Reason="", readiness=true. Elapsed: 4.022470252s
Jun 27 17:33:27.883: INFO: The phase of Pod pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132 is Running (Ready = true)
Jun 27 17:33:27.884: INFO: Pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-f29a31ca-efbe-4988-aa64-e0ad86160484 06/27/23 17:33:28.008
STEP: Updating secret s-test-opt-upd-ed3393de-8dd8-4c82-beac-391c278fa76b 06/27/23 17:33:28.025
STEP: Creating secret with name s-test-opt-create-4c846bba-949a-429c-9154-8e3cc94e1af6 06/27/23 17:33:28.038
STEP: waiting to observe update in volume 06/27/23 17:33:28.051
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 27 17:34:33.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1079" for this suite. 06/27/23 17:34:33.374
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":230,"skipped":4486,"failed":0}
------------------------------
â€¢ [SLOW TEST] [69.694 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:33:23.698
    Jun 27 17:33:23.698: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:33:23.7
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:33:23.737
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:33:23.747
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    Jun 27 17:33:23.774: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating secret with name s-test-opt-del-f29a31ca-efbe-4988-aa64-e0ad86160484 06/27/23 17:33:23.775
    STEP: Creating secret with name s-test-opt-upd-ed3393de-8dd8-4c82-beac-391c278fa76b 06/27/23 17:33:23.791
    STEP: Creating the pod 06/27/23 17:33:23.809
    Jun 27 17:33:23.861: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132" in namespace "projected-1079" to be "running and ready"
    Jun 27 17:33:23.871: INFO: Pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132": Phase="Pending", Reason="", readiness=false. Elapsed: 9.75703ms
    Jun 27 17:33:23.871: INFO: The phase of Pod pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:33:25.883: INFO: Pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132": Phase="Pending", Reason="", readiness=false. Elapsed: 2.02179677s
    Jun 27 17:33:25.883: INFO: The phase of Pod pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:33:27.883: INFO: Pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132": Phase="Running", Reason="", readiness=true. Elapsed: 4.022470252s
    Jun 27 17:33:27.883: INFO: The phase of Pod pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132 is Running (Ready = true)
    Jun 27 17:33:27.884: INFO: Pod "pod-projected-secrets-8598b95a-f67e-4360-867a-c399b2b7b132" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-f29a31ca-efbe-4988-aa64-e0ad86160484 06/27/23 17:33:28.008
    STEP: Updating secret s-test-opt-upd-ed3393de-8dd8-4c82-beac-391c278fa76b 06/27/23 17:33:28.025
    STEP: Creating secret with name s-test-opt-create-4c846bba-949a-429c-9154-8e3cc94e1af6 06/27/23 17:33:28.038
    STEP: waiting to observe update in volume 06/27/23 17:33:28.051
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 27 17:34:33.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1079" for this suite. 06/27/23 17:34:33.374
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:34:33.393
Jun 27 17:34:33.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-probe 06/27/23 17:34:33.394
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:34:33.439
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:34:33.448
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 27 17:35:33.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-3567" for this suite. 06/27/23 17:35:33.536
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":231,"skipped":4493,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.166 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:34:33.393
    Jun 27 17:34:33.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-probe 06/27/23 17:34:33.394
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:34:33.439
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:34:33.448
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 27 17:35:33.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-3567" for this suite. 06/27/23 17:35:33.536
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:35:33.56
Jun 27 17:35:33.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename deployment 06/27/23 17:35:33.563
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:33.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:33.612
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Jun 27 17:35:33.622: INFO: Creating simple deployment test-new-deployment
Jun 27 17:35:33.668: INFO: deployment "test-new-deployment" doesn't have the required revision set
Jun 27 17:35:35.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 35, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 35, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: getting scale subresource 06/27/23 17:35:37.727
STEP: updating a scale subresource 06/27/23 17:35:37.737
STEP: verifying the deployment Spec.Replicas was modified 06/27/23 17:35:37.75
STEP: Patch a scale subresource 06/27/23 17:35:37.76
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 27 17:35:37.871: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-2058  190df22a-c9ca-4e8b-976f-98f479f94d69 122388 3 2023-06-27 17:35:33 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-27 17:35:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009d3c5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-06-27 17:35:35 +0000 UTC,LastTransitionTime:2023-06-27 17:35:33 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-27 17:35:37 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Jun 27 17:35:37.884: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-2058  949cbeae-1dd5-4fce-9998-035e3e118430 122393 3 2023-06-27 17:35:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 190df22a-c9ca-4e8b-976f-98f479f94d69 0xc009d3ca37 0xc009d3ca38}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"190df22a-c9ca-4e8b-976f-98f479f94d69\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009d3cac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Jun 27 17:35:37.901: INFO: Pod "test-new-deployment-845c8977d9-9m9j2" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-9m9j2 test-new-deployment-845c8977d9- deployment-2058  9a6b6bec-a098-4ed2-a785-850bb3ed5924 122397 0 2023-06-27 17:35:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 949cbeae-1dd5-4fce-9998-035e3e118430 0xc009d3ced7 0xc009d3ced8}] [] [{kube-controller-manager Update v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"949cbeae-1dd5-4fce-9998-035e3e118430\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6w6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6w6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kj4bz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:35:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Jun 27 17:35:37.901: INFO: Pod "test-new-deployment-845c8977d9-xptrg" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-xptrg test-new-deployment-845c8977d9- deployment-2058  96907571-a114-4539-bd68-886f32dbb885 122372 0 2023-06-27 17:35:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:47fb02490020a6ac836bc2765f629e2e1cb56dfb50c985dd7848c38d0778f462 cni.projectcalico.org/podIP:172.30.250.224/32 cni.projectcalico.org/podIPs:172.30.250.224/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.224"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.224"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 949cbeae-1dd5-4fce-9998-035e3e118430 0xc009d3d0e7 0xc009d3d0e8}] [] [{kube-controller-manager Update v1 2023-06-27 17:35:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"949cbeae-1dd5-4fce-9998-035e3e118430\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:35:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hq8jk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hq8jk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.224,StartTime:2023-06-27 17:35:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:35:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d7d9c9de71bea14078d03229fb68c17e7bf6d0d302123c18f748c860085869c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 27 17:35:37.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-2058" for this suite. 06/27/23 17:35:37.92
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":232,"skipped":4497,"failed":0}
------------------------------
â€¢ [4.376 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:35:33.56
    Jun 27 17:35:33.561: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename deployment 06/27/23 17:35:33.563
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:33.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:33.612
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Jun 27 17:35:33.622: INFO: Creating simple deployment test-new-deployment
    Jun 27 17:35:33.668: INFO: deployment "test-new-deployment" doesn't have the required revision set
    Jun 27 17:35:35.707: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 35, 33, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 35, 33, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 35, 33, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-new-deployment-845c8977d9\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: getting scale subresource 06/27/23 17:35:37.727
    STEP: updating a scale subresource 06/27/23 17:35:37.737
    STEP: verifying the deployment Spec.Replicas was modified 06/27/23 17:35:37.75
    STEP: Patch a scale subresource 06/27/23 17:35:37.76
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 27 17:35:37.871: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-2058  190df22a-c9ca-4e8b-976f-98f479f94d69 122388 3 2023-06-27 17:35:33 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-06-27 17:35:33 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009d3c5f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-06-27 17:35:35 +0000 UTC,LastTransitionTime:2023-06-27 17:35:33 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-27 17:35:37 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Jun 27 17:35:37.884: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-2058  949cbeae-1dd5-4fce-9998-035e3e118430 122393 3 2023-06-27 17:35:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 190df22a-c9ca-4e8b-976f-98f479f94d69 0xc009d3ca37 0xc009d3ca38}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"190df22a-c9ca-4e8b-976f-98f479f94d69\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc009d3cac8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 17:35:37.901: INFO: Pod "test-new-deployment-845c8977d9-9m9j2" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-9m9j2 test-new-deployment-845c8977d9- deployment-2058  9a6b6bec-a098-4ed2-a785-850bb3ed5924 122397 0 2023-06-27 17:35:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 949cbeae-1dd5-4fce-9998-035e3e118430 0xc009d3ced7 0xc009d3ced8}] [] [{kube-controller-manager Update v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"949cbeae-1dd5-4fce-9998-035e3e118430\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:35:37 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h6w6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h6w6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-kj4bz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:,StartTime:2023-06-27 17:35:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Jun 27 17:35:37.901: INFO: Pod "test-new-deployment-845c8977d9-xptrg" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-xptrg test-new-deployment-845c8977d9- deployment-2058  96907571-a114-4539-bd68-886f32dbb885 122372 0 2023-06-27 17:35:33 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[cni.projectcalico.org/containerID:47fb02490020a6ac836bc2765f629e2e1cb56dfb50c985dd7848c38d0778f462 cni.projectcalico.org/podIP:172.30.250.224/32 cni.projectcalico.org/podIPs:172.30.250.224/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.224"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.224"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 949cbeae-1dd5-4fce-9998-035e3e118430 0xc009d3d0e7 0xc009d3d0e8}] [] [{kube-controller-manager Update v1 2023-06-27 17:35:33 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"949cbeae-1dd5-4fce-9998-035e3e118430\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:35:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:35:35 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hq8jk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hq8jk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c58,c22,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:35 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:35:33 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.224,StartTime:2023-06-27 17:35:33 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:35:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://d7d9c9de71bea14078d03229fb68c17e7bf6d0d302123c18f748c860085869c6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 27 17:35:37.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-2058" for this suite. 06/27/23 17:35:37.92
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:35:37.94
Jun 27 17:35:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 17:35:37.943
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:37.975
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:37.984
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 06/27/23 17:35:37.994
STEP: Getting a ResourceQuota 06/27/23 17:35:38.007
STEP: Listing all ResourceQuotas with LabelSelector 06/27/23 17:35:38.018
STEP: Patching the ResourceQuota 06/27/23 17:35:38.029
STEP: Deleting a Collection of ResourceQuotas 06/27/23 17:35:38.055
STEP: Verifying the deleted ResourceQuota 06/27/23 17:35:38.08
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 17:35:38.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3396" for this suite. 06/27/23 17:35:38.104
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":233,"skipped":4498,"failed":0}
------------------------------
â€¢ [0.182 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:35:37.94
    Jun 27 17:35:37.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 17:35:37.943
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:37.975
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:37.984
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 06/27/23 17:35:37.994
    STEP: Getting a ResourceQuota 06/27/23 17:35:38.007
    STEP: Listing all ResourceQuotas with LabelSelector 06/27/23 17:35:38.018
    STEP: Patching the ResourceQuota 06/27/23 17:35:38.029
    STEP: Deleting a Collection of ResourceQuotas 06/27/23 17:35:38.055
    STEP: Verifying the deleted ResourceQuota 06/27/23 17:35:38.08
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 17:35:38.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3396" for this suite. 06/27/23 17:35:38.104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:35:38.142
Jun 27 17:35:38.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename proxy 06/27/23 17:35:38.143
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:38.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:38.187
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Jun 27 17:35:38.197: INFO: Creating pod...
Jun 27 17:35:38.244: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1295" to be "running"
Jun 27 17:35:38.258: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.501931ms
Jun 27 17:35:40.273: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.029491537s
Jun 27 17:35:40.273: INFO: Pod "agnhost" satisfied condition "running"
Jun 27 17:35:40.273: INFO: Creating service...
Jun 27 17:35:40.306: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=DELETE
Jun 27 17:35:40.332: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 27 17:35:40.332: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=OPTIONS
Jun 27 17:35:40.352: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 27 17:35:40.352: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=PATCH
Jun 27 17:35:40.369: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 27 17:35:40.369: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=POST
Jun 27 17:35:40.387: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 27 17:35:40.388: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=PUT
Jun 27 17:35:40.411: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 27 17:35:40.411: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=DELETE
Jun 27 17:35:40.435: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Jun 27 17:35:40.435: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=OPTIONS
Jun 27 17:35:40.463: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Jun 27 17:35:40.463: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=PATCH
Jun 27 17:35:40.496: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Jun 27 17:35:40.496: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=POST
Jun 27 17:35:40.523: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Jun 27 17:35:40.523: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=PUT
Jun 27 17:35:40.547: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Jun 27 17:35:40.547: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=GET
Jun 27 17:35:40.555: INFO: http.Client request:GET StatusCode:301
Jun 27 17:35:40.555: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=GET
Jun 27 17:35:40.570: INFO: http.Client request:GET StatusCode:301
Jun 27 17:35:40.570: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=HEAD
Jun 27 17:35:40.585: INFO: http.Client request:HEAD StatusCode:301
Jun 27 17:35:40.586: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=HEAD
Jun 27 17:35:40.603: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Jun 27 17:35:40.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1295" for this suite. 06/27/23 17:35:40.619
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":234,"skipped":4560,"failed":0}
------------------------------
â€¢ [2.495 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:35:38.142
    Jun 27 17:35:38.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename proxy 06/27/23 17:35:38.143
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:38.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:38.187
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Jun 27 17:35:38.197: INFO: Creating pod...
    Jun 27 17:35:38.244: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-1295" to be "running"
    Jun 27 17:35:38.258: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 14.501931ms
    Jun 27 17:35:40.273: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.029491537s
    Jun 27 17:35:40.273: INFO: Pod "agnhost" satisfied condition "running"
    Jun 27 17:35:40.273: INFO: Creating service...
    Jun 27 17:35:40.306: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=DELETE
    Jun 27 17:35:40.332: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 27 17:35:40.332: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=OPTIONS
    Jun 27 17:35:40.352: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 27 17:35:40.352: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=PATCH
    Jun 27 17:35:40.369: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 27 17:35:40.369: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=POST
    Jun 27 17:35:40.387: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 27 17:35:40.388: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=PUT
    Jun 27 17:35:40.411: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 27 17:35:40.411: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=DELETE
    Jun 27 17:35:40.435: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Jun 27 17:35:40.435: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Jun 27 17:35:40.463: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Jun 27 17:35:40.463: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=PATCH
    Jun 27 17:35:40.496: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Jun 27 17:35:40.496: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=POST
    Jun 27 17:35:40.523: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Jun 27 17:35:40.523: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=PUT
    Jun 27 17:35:40.547: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Jun 27 17:35:40.547: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=GET
    Jun 27 17:35:40.555: INFO: http.Client request:GET StatusCode:301
    Jun 27 17:35:40.555: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=GET
    Jun 27 17:35:40.570: INFO: http.Client request:GET StatusCode:301
    Jun 27 17:35:40.570: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/pods/agnhost/proxy?method=HEAD
    Jun 27 17:35:40.585: INFO: http.Client request:HEAD StatusCode:301
    Jun 27 17:35:40.586: INFO: Starting http.Client for https://172.21.0.1:443/api/v1/namespaces/proxy-1295/services/e2e-proxy-test-service/proxy?method=HEAD
    Jun 27 17:35:40.603: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Jun 27 17:35:40.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1295" for this suite. 06/27/23 17:35:40.619
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:35:40.652
Jun 27 17:35:40.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 17:35:40.654
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:40.686
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:40.699
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 06/27/23 17:35:40.713
STEP: Ensuring ResourceQuota status is calculated 06/27/23 17:35:40.728
STEP: Creating a ResourceQuota with not best effort scope 06/27/23 17:35:42.74
STEP: Ensuring ResourceQuota status is calculated 06/27/23 17:35:42.766
STEP: Creating a best-effort pod 06/27/23 17:35:44.783
STEP: Ensuring resource quota with best effort scope captures the pod usage 06/27/23 17:35:44.836
STEP: Ensuring resource quota with not best effort ignored the pod usage 06/27/23 17:35:46.848
STEP: Deleting the pod 06/27/23 17:35:48.858
STEP: Ensuring resource quota status released the pod usage 06/27/23 17:35:48.892
STEP: Creating a not best-effort pod 06/27/23 17:35:50.905
STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/27/23 17:35:50.949
STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/27/23 17:35:52.962
STEP: Deleting the pod 06/27/23 17:35:54.975
STEP: Ensuring resource quota status released the pod usage 06/27/23 17:35:54.998
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 17:35:57.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7444" for this suite. 06/27/23 17:35:57.027
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":235,"skipped":4587,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.395 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:35:40.652
    Jun 27 17:35:40.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 17:35:40.654
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:40.686
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:40.699
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 06/27/23 17:35:40.713
    STEP: Ensuring ResourceQuota status is calculated 06/27/23 17:35:40.728
    STEP: Creating a ResourceQuota with not best effort scope 06/27/23 17:35:42.74
    STEP: Ensuring ResourceQuota status is calculated 06/27/23 17:35:42.766
    STEP: Creating a best-effort pod 06/27/23 17:35:44.783
    STEP: Ensuring resource quota with best effort scope captures the pod usage 06/27/23 17:35:44.836
    STEP: Ensuring resource quota with not best effort ignored the pod usage 06/27/23 17:35:46.848
    STEP: Deleting the pod 06/27/23 17:35:48.858
    STEP: Ensuring resource quota status released the pod usage 06/27/23 17:35:48.892
    STEP: Creating a not best-effort pod 06/27/23 17:35:50.905
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 06/27/23 17:35:50.949
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 06/27/23 17:35:52.962
    STEP: Deleting the pod 06/27/23 17:35:54.975
    STEP: Ensuring resource quota status released the pod usage 06/27/23 17:35:54.998
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 17:35:57.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7444" for this suite. 06/27/23 17:35:57.027
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:35:57.05
Jun 27 17:35:57.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-preemption 06/27/23 17:35:57.052
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:57.101
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:57.115
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 27 17:35:57.174: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 27 17:36:57.327: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:36:57.37
Jun 27 17:36:57.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-preemption-path 06/27/23 17:36:57.372
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:36:57.404
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:36:57.414
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Jun 27 17:36:57.536: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Jun 27 17:36:57.555: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Jun 27 17:36:57.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-524" for this suite. 06/27/23 17:36:57.633
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:36:57.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4977" for this suite. 06/27/23 17:36:57.697
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":236,"skipped":4602,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.799 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:35:57.05
    Jun 27 17:35:57.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-preemption 06/27/23 17:35:57.052
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:35:57.101
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:35:57.115
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 27 17:35:57.174: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 27 17:36:57.327: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:36:57.37
    Jun 27 17:36:57.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-preemption-path 06/27/23 17:36:57.372
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:36:57.404
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:36:57.414
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Jun 27 17:36:57.536: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Jun 27 17:36:57.555: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Jun 27 17:36:57.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-524" for this suite. 06/27/23 17:36:57.633
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:36:57.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4977" for this suite. 06/27/23 17:36:57.697
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:36:57.859
Jun 27 17:36:57.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename gc 06/27/23 17:36:57.861
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:36:57.929
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:36:57.94
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 06/27/23 17:36:57.981
STEP: create the rc2 06/27/23 17:36:58.015
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/27/23 17:37:03.057
STEP: delete the rc simpletest-rc-to-be-deleted 06/27/23 17:37:04.27
STEP: wait for the rc to be deleted 06/27/23 17:37:04.301
STEP: Gathering metrics 06/27/23 17:37:09.459
W0627 17:37:09.502296      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
Jun 27 17:37:09.502: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Jun 27 17:37:09.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-2945q" in namespace "gc-7130"
Jun 27 17:37:09.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-29g6p" in namespace "gc-7130"
Jun 27 17:37:09.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bc2b" in namespace "gc-7130"
Jun 27 17:37:09.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nfhg" in namespace "gc-7130"
Jun 27 17:37:09.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vk56" in namespace "gc-7130"
Jun 27 17:37:09.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-54hr8" in namespace "gc-7130"
Jun 27 17:37:09.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dbsd" in namespace "gc-7130"
Jun 27 17:37:09.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l5sr" in namespace "gc-7130"
Jun 27 17:37:09.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bwkm" in namespace "gc-7130"
Jun 27 17:37:09.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hvj7" in namespace "gc-7130"
Jun 27 17:37:10.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pkfd" in namespace "gc-7130"
Jun 27 17:37:10.064: INFO: Deleting pod "simpletest-rc-to-be-deleted-79smr" in namespace "gc-7130"
Jun 27 17:37:10.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dp79" in namespace "gc-7130"
Jun 27 17:37:10.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-7f2hv" in namespace "gc-7130"
Jun 27 17:37:10.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hn6v" in namespace "gc-7130"
Jun 27 17:37:10.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-7x2k5" in namespace "gc-7130"
Jun 27 17:37:10.285: INFO: Deleting pod "simpletest-rc-to-be-deleted-8stsm" in namespace "gc-7130"
Jun 27 17:37:10.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-95rvf" in namespace "gc-7130"
Jun 27 17:37:10.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sjhv" in namespace "gc-7130"
Jun 27 17:37:10.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh268" in namespace "gc-7130"
Jun 27 17:37:10.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-bknkd" in namespace "gc-7130"
Jun 27 17:37:10.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnmdv" in namespace "gc-7130"
Jun 27 17:37:10.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-brv7h" in namespace "gc-7130"
Jun 27 17:37:10.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmpz4" in namespace "gc-7130"
Jun 27 17:37:10.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqvqm" in namespace "gc-7130"
Jun 27 17:37:10.663: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw6xq" in namespace "gc-7130"
Jun 27 17:37:10.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-czgk9" in namespace "gc-7130"
Jun 27 17:37:10.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-d58kn" in namespace "gc-7130"
Jun 27 17:37:10.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-df2rk" in namespace "gc-7130"
Jun 27 17:37:10.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-djd6q" in namespace "gc-7130"
Jun 27 17:37:10.813: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlm7s" in namespace "gc-7130"
Jun 27 17:37:10.838: INFO: Deleting pod "simpletest-rc-to-be-deleted-dn5fd" in namespace "gc-7130"
Jun 27 17:37:10.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkmxq" in namespace "gc-7130"
Jun 27 17:37:10.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-fktvn" in namespace "gc-7130"
Jun 27 17:37:10.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnrlk" in namespace "gc-7130"
Jun 27 17:37:10.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwvlv" in namespace "gc-7130"
Jun 27 17:37:11.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-g94kq" in namespace "gc-7130"
Jun 27 17:37:11.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbxq8" in namespace "gc-7130"
Jun 27 17:37:11.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdt9p" in namespace "gc-7130"
Jun 27 17:37:11.102: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnbgp" in namespace "gc-7130"
Jun 27 17:37:11.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnstz" in namespace "gc-7130"
Jun 27 17:37:11.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsprb" in namespace "gc-7130"
Jun 27 17:37:11.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5qjb" in namespace "gc-7130"
Jun 27 17:37:11.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-h86x8" in namespace "gc-7130"
Jun 27 17:37:11.261: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhkx2" in namespace "gc-7130"
Jun 27 17:37:11.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqdhv" in namespace "gc-7130"
Jun 27 17:37:11.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxp6h" in namespace "gc-7130"
Jun 27 17:37:11.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4xbx" in namespace "gc-7130"
Jun 27 17:37:11.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcd85" in namespace "gc-7130"
Jun 27 17:37:11.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtlxb" in namespace "gc-7130"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Jun 27 17:37:11.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7130" for this suite. 06/27/23 17:37:11.471
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":237,"skipped":4630,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.633 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:36:57.859
    Jun 27 17:36:57.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename gc 06/27/23 17:36:57.861
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:36:57.929
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:36:57.94
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 06/27/23 17:36:57.981
    STEP: create the rc2 06/27/23 17:36:58.015
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 06/27/23 17:37:03.057
    STEP: delete the rc simpletest-rc-to-be-deleted 06/27/23 17:37:04.27
    STEP: wait for the rc to be deleted 06/27/23 17:37:04.301
    STEP: Gathering metrics 06/27/23 17:37:09.459
    W0627 17:37:09.502296      23 metrics_grabber.go:151] Can't find kube-controller-manager pod. Grabbing metrics from kube-controller-manager is disabled.
    Jun 27 17:37:09.502: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Jun 27 17:37:09.502: INFO: Deleting pod "simpletest-rc-to-be-deleted-2945q" in namespace "gc-7130"
    Jun 27 17:37:09.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-29g6p" in namespace "gc-7130"
    Jun 27 17:37:09.598: INFO: Deleting pod "simpletest-rc-to-be-deleted-4bc2b" in namespace "gc-7130"
    Jun 27 17:37:09.685: INFO: Deleting pod "simpletest-rc-to-be-deleted-4nfhg" in namespace "gc-7130"
    Jun 27 17:37:09.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-4vk56" in namespace "gc-7130"
    Jun 27 17:37:09.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-54hr8" in namespace "gc-7130"
    Jun 27 17:37:09.818: INFO: Deleting pod "simpletest-rc-to-be-deleted-5dbsd" in namespace "gc-7130"
    Jun 27 17:37:09.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-5l5sr" in namespace "gc-7130"
    Jun 27 17:37:09.880: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bwkm" in namespace "gc-7130"
    Jun 27 17:37:09.941: INFO: Deleting pod "simpletest-rc-to-be-deleted-6hvj7" in namespace "gc-7130"
    Jun 27 17:37:10.010: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pkfd" in namespace "gc-7130"
    Jun 27 17:37:10.064: INFO: Deleting pod "simpletest-rc-to-be-deleted-79smr" in namespace "gc-7130"
    Jun 27 17:37:10.100: INFO: Deleting pod "simpletest-rc-to-be-deleted-7dp79" in namespace "gc-7130"
    Jun 27 17:37:10.152: INFO: Deleting pod "simpletest-rc-to-be-deleted-7f2hv" in namespace "gc-7130"
    Jun 27 17:37:10.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-7hn6v" in namespace "gc-7130"
    Jun 27 17:37:10.260: INFO: Deleting pod "simpletest-rc-to-be-deleted-7x2k5" in namespace "gc-7130"
    Jun 27 17:37:10.285: INFO: Deleting pod "simpletest-rc-to-be-deleted-8stsm" in namespace "gc-7130"
    Jun 27 17:37:10.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-95rvf" in namespace "gc-7130"
    Jun 27 17:37:10.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-9sjhv" in namespace "gc-7130"
    Jun 27 17:37:10.373: INFO: Deleting pod "simpletest-rc-to-be-deleted-bh268" in namespace "gc-7130"
    Jun 27 17:37:10.422: INFO: Deleting pod "simpletest-rc-to-be-deleted-bknkd" in namespace "gc-7130"
    Jun 27 17:37:10.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-bnmdv" in namespace "gc-7130"
    Jun 27 17:37:10.500: INFO: Deleting pod "simpletest-rc-to-be-deleted-brv7h" in namespace "gc-7130"
    Jun 27 17:37:10.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmpz4" in namespace "gc-7130"
    Jun 27 17:37:10.625: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqvqm" in namespace "gc-7130"
    Jun 27 17:37:10.663: INFO: Deleting pod "simpletest-rc-to-be-deleted-cw6xq" in namespace "gc-7130"
    Jun 27 17:37:10.687: INFO: Deleting pod "simpletest-rc-to-be-deleted-czgk9" in namespace "gc-7130"
    Jun 27 17:37:10.722: INFO: Deleting pod "simpletest-rc-to-be-deleted-d58kn" in namespace "gc-7130"
    Jun 27 17:37:10.754: INFO: Deleting pod "simpletest-rc-to-be-deleted-df2rk" in namespace "gc-7130"
    Jun 27 17:37:10.790: INFO: Deleting pod "simpletest-rc-to-be-deleted-djd6q" in namespace "gc-7130"
    Jun 27 17:37:10.813: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlm7s" in namespace "gc-7130"
    Jun 27 17:37:10.838: INFO: Deleting pod "simpletest-rc-to-be-deleted-dn5fd" in namespace "gc-7130"
    Jun 27 17:37:10.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-fkmxq" in namespace "gc-7130"
    Jun 27 17:37:10.925: INFO: Deleting pod "simpletest-rc-to-be-deleted-fktvn" in namespace "gc-7130"
    Jun 27 17:37:10.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnrlk" in namespace "gc-7130"
    Jun 27 17:37:10.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwvlv" in namespace "gc-7130"
    Jun 27 17:37:11.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-g94kq" in namespace "gc-7130"
    Jun 27 17:37:11.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbxq8" in namespace "gc-7130"
    Jun 27 17:37:11.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-gdt9p" in namespace "gc-7130"
    Jun 27 17:37:11.102: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnbgp" in namespace "gc-7130"
    Jun 27 17:37:11.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-gnstz" in namespace "gc-7130"
    Jun 27 17:37:11.163: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsprb" in namespace "gc-7130"
    Jun 27 17:37:11.198: INFO: Deleting pod "simpletest-rc-to-be-deleted-h5qjb" in namespace "gc-7130"
    Jun 27 17:37:11.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-h86x8" in namespace "gc-7130"
    Jun 27 17:37:11.261: INFO: Deleting pod "simpletest-rc-to-be-deleted-hhkx2" in namespace "gc-7130"
    Jun 27 17:37:11.301: INFO: Deleting pod "simpletest-rc-to-be-deleted-hqdhv" in namespace "gc-7130"
    Jun 27 17:37:11.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-hxp6h" in namespace "gc-7130"
    Jun 27 17:37:11.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4xbx" in namespace "gc-7130"
    Jun 27 17:37:11.401: INFO: Deleting pod "simpletest-rc-to-be-deleted-jcd85" in namespace "gc-7130"
    Jun 27 17:37:11.430: INFO: Deleting pod "simpletest-rc-to-be-deleted-jtlxb" in namespace "gc-7130"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Jun 27 17:37:11.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7130" for this suite. 06/27/23 17:37:11.471
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:37:11.493
Jun 27 17:37:11.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 17:37:11.495
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:37:11.533
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:37:11.539
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-3fe0449a-fa20-4c4d-a303-cf691359af53 06/27/23 17:37:11.55
STEP: Creating a pod to test consume configMaps 06/27/23 17:37:11.564
Jun 27 17:37:11.634: INFO: Waiting up to 5m0s for pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964" in namespace "configmap-9193" to be "Succeeded or Failed"
Jun 27 17:37:11.645: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Pending", Reason="", readiness=false. Elapsed: 10.972858ms
Jun 27 17:37:13.659: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024964997s
Jun 27 17:37:15.662: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028468768s
Jun 27 17:37:17.670: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035805043s
Jun 27 17:37:19.663: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029524661s
STEP: Saw pod success 06/27/23 17:37:19.663
Jun 27 17:37:19.664: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964" satisfied condition "Succeeded or Failed"
Jun 27 17:37:19.702: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964 container agnhost-container: <nil>
STEP: delete the pod 06/27/23 17:37:19.774
Jun 27 17:37:19.804: INFO: Waiting for pod pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964 to disappear
Jun 27 17:37:19.814: INFO: Pod pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 17:37:19.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9193" for this suite. 06/27/23 17:37:19.832
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":238,"skipped":4631,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.366 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:37:11.493
    Jun 27 17:37:11.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 17:37:11.495
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:37:11.533
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:37:11.539
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-3fe0449a-fa20-4c4d-a303-cf691359af53 06/27/23 17:37:11.55
    STEP: Creating a pod to test consume configMaps 06/27/23 17:37:11.564
    Jun 27 17:37:11.634: INFO: Waiting up to 5m0s for pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964" in namespace "configmap-9193" to be "Succeeded or Failed"
    Jun 27 17:37:11.645: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Pending", Reason="", readiness=false. Elapsed: 10.972858ms
    Jun 27 17:37:13.659: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024964997s
    Jun 27 17:37:15.662: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028468768s
    Jun 27 17:37:17.670: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Pending", Reason="", readiness=false. Elapsed: 6.035805043s
    Jun 27 17:37:19.663: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964": Phase="Succeeded", Reason="", readiness=false. Elapsed: 8.029524661s
    STEP: Saw pod success 06/27/23 17:37:19.663
    Jun 27 17:37:19.664: INFO: Pod "pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964" satisfied condition "Succeeded or Failed"
    Jun 27 17:37:19.702: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964 container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 17:37:19.774
    Jun 27 17:37:19.804: INFO: Waiting for pod pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964 to disappear
    Jun 27 17:37:19.814: INFO: Pod pod-configmaps-c9a67615-acf6-449b-a5db-674b0ff62964 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 17:37:19.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9193" for this suite. 06/27/23 17:37:19.832
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:37:19.86
Jun 27 17:37:19.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename subpath 06/27/23 17:37:19.864
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:37:19.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:37:19.905
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/27/23 17:37:19.916
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-lvzw 06/27/23 17:37:19.953
STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:37:19.953
Jun 27 17:37:20.000: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lvzw" in namespace "subpath-8650" to be "Succeeded or Failed"
Jun 27 17:37:20.011: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.795368ms
Jun 27 17:37:22.037: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037206187s
Jun 27 17:37:24.024: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 4.024382805s
Jun 27 17:37:26.021: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 6.021205432s
Jun 27 17:37:28.027: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 8.027345206s
Jun 27 17:37:30.024: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 10.023740335s
Jun 27 17:37:32.021: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 12.021391575s
Jun 27 17:37:34.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 14.022305659s
Jun 27 17:37:36.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 16.021561266s
Jun 27 17:37:38.034: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 18.03402818s
Jun 27 17:37:40.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 20.022167412s
Jun 27 17:37:42.028: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 22.028470882s
Jun 27 17:37:44.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=false. Elapsed: 24.022306225s
Jun 27 17:37:46.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.022104986s
STEP: Saw pod success 06/27/23 17:37:46.022
Jun 27 17:37:46.023: INFO: Pod "pod-subpath-test-configmap-lvzw" satisfied condition "Succeeded or Failed"
Jun 27 17:37:46.037: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-configmap-lvzw container test-container-subpath-configmap-lvzw: <nil>
STEP: delete the pod 06/27/23 17:37:46.075
Jun 27 17:37:46.107: INFO: Waiting for pod pod-subpath-test-configmap-lvzw to disappear
Jun 27 17:37:46.117: INFO: Pod pod-subpath-test-configmap-lvzw no longer exists
STEP: Deleting pod pod-subpath-test-configmap-lvzw 06/27/23 17:37:46.117
Jun 27 17:37:46.117: INFO: Deleting pod "pod-subpath-test-configmap-lvzw" in namespace "subpath-8650"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 27 17:37:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8650" for this suite. 06/27/23 17:37:46.142
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":239,"skipped":4637,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.300 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:37:19.86
    Jun 27 17:37:19.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename subpath 06/27/23 17:37:19.864
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:37:19.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:37:19.905
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/27/23 17:37:19.916
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-lvzw 06/27/23 17:37:19.953
    STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:37:19.953
    Jun 27 17:37:20.000: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-lvzw" in namespace "subpath-8650" to be "Succeeded or Failed"
    Jun 27 17:37:20.011: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Pending", Reason="", readiness=false. Elapsed: 10.795368ms
    Jun 27 17:37:22.037: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Pending", Reason="", readiness=false. Elapsed: 2.037206187s
    Jun 27 17:37:24.024: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 4.024382805s
    Jun 27 17:37:26.021: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 6.021205432s
    Jun 27 17:37:28.027: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 8.027345206s
    Jun 27 17:37:30.024: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 10.023740335s
    Jun 27 17:37:32.021: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 12.021391575s
    Jun 27 17:37:34.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 14.022305659s
    Jun 27 17:37:36.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 16.021561266s
    Jun 27 17:37:38.034: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 18.03402818s
    Jun 27 17:37:40.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 20.022167412s
    Jun 27 17:37:42.028: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=true. Elapsed: 22.028470882s
    Jun 27 17:37:44.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Running", Reason="", readiness=false. Elapsed: 24.022306225s
    Jun 27 17:37:46.022: INFO: Pod "pod-subpath-test-configmap-lvzw": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.022104986s
    STEP: Saw pod success 06/27/23 17:37:46.022
    Jun 27 17:37:46.023: INFO: Pod "pod-subpath-test-configmap-lvzw" satisfied condition "Succeeded or Failed"
    Jun 27 17:37:46.037: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-configmap-lvzw container test-container-subpath-configmap-lvzw: <nil>
    STEP: delete the pod 06/27/23 17:37:46.075
    Jun 27 17:37:46.107: INFO: Waiting for pod pod-subpath-test-configmap-lvzw to disappear
    Jun 27 17:37:46.117: INFO: Pod pod-subpath-test-configmap-lvzw no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-lvzw 06/27/23 17:37:46.117
    Jun 27 17:37:46.117: INFO: Deleting pod "pod-subpath-test-configmap-lvzw" in namespace "subpath-8650"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 27 17:37:46.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8650" for this suite. 06/27/23 17:37:46.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:37:46.17
Jun 27 17:37:46.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename ephemeral-containers-test 06/27/23 17:37:46.172
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:37:46.204
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:37:46.216
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 06/27/23 17:37:46.227
Jun 27 17:37:46.276: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3485" to be "running and ready"
Jun 27 17:37:46.286: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.806626ms
Jun 27 17:37:46.287: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:37:48.299: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022460872s
Jun 27 17:37:48.299: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:37:50.297: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.020902693s
Jun 27 17:37:50.298: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Jun 27 17:37:50.298: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 06/27/23 17:37:50.311
Jun 27 17:37:50.335: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3485" to be "container debugger running"
Jun 27 17:37:50.345: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.347249ms
Jun 27 17:37:52.359: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024350232s
Jun 27 17:37:54.364: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.02901166s
Jun 27 17:37:54.364: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 06/27/23 17:37:54.364
Jun 27 17:37:54.365: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3485 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:37:54.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:37:54.365: INFO: ExecWithOptions: Clientset creation
Jun 27 17:37:54.366: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-3485/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Jun 27 17:37:54.881: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 27 17:37:54.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-3485" for this suite. 06/27/23 17:37:54.933
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":240,"skipped":4676,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.796 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:37:46.17
    Jun 27 17:37:46.170: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename ephemeral-containers-test 06/27/23 17:37:46.172
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:37:46.204
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:37:46.216
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 06/27/23 17:37:46.227
    Jun 27 17:37:46.276: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3485" to be "running and ready"
    Jun 27 17:37:46.286: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 9.806626ms
    Jun 27 17:37:46.287: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:37:48.299: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.022460872s
    Jun 27 17:37:48.299: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:37:50.297: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.020902693s
    Jun 27 17:37:50.298: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Jun 27 17:37:50.298: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 06/27/23 17:37:50.311
    Jun 27 17:37:50.335: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-3485" to be "container debugger running"
    Jun 27 17:37:50.345: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 10.347249ms
    Jun 27 17:37:52.359: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.024350232s
    Jun 27 17:37:54.364: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.02901166s
    Jun 27 17:37:54.364: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 06/27/23 17:37:54.364
    Jun 27 17:37:54.365: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3485 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:37:54.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:37:54.365: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:37:54.366: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/ephemeral-containers-test-3485/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Jun 27 17:37:54.881: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 27 17:37:54.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-3485" for this suite. 06/27/23 17:37:54.933
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:37:54.984
Jun 27 17:37:54.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename aggregator 06/27/23 17:37:54.998
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:37:55.058
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:37:55.086
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Jun 27 17:37:55.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 06/27/23 17:37:55.121
Jun 27 17:37:56.361: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Jun 27 17:37:58.507: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:00.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:02.526: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:04.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:06.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:08.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:10.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:12.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:14.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:16.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:18.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:20.553: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:38:22.754: INFO: Waited 180.769006ms for the sample-apiserver to be ready to handle requests.
I0627 17:38:23.889849      23 request.go:690] Waited for 1.004008967s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/whereabouts.cni.cncf.io/v1alpha1
STEP: Read Status for v1alpha1.wardle.example.com 06/27/23 17:38:24.254
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/27/23 17:38:24.268
STEP: List APIServices 06/27/23 17:38:24.329
Jun 27 17:38:24.403: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Jun 27 17:38:25.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-6925" for this suite. 06/27/23 17:38:25.203
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":241,"skipped":4687,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.305 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:37:54.984
    Jun 27 17:37:54.984: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename aggregator 06/27/23 17:37:54.998
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:37:55.058
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:37:55.086
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Jun 27 17:37:55.120: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 06/27/23 17:37:55.121
    Jun 27 17:37:56.361: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Jun 27 17:37:58.507: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:00.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:02.526: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:04.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:06.517: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:08.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:10.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:12.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:14.539: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:16.518: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:18.519: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:20.553: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 37, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:38:22.754: INFO: Waited 180.769006ms for the sample-apiserver to be ready to handle requests.
    I0627 17:38:23.889849      23 request.go:690] Waited for 1.004008967s due to client-side throttling, not priority and fairness, request: GET:https://172.21.0.1:443/apis/whereabouts.cni.cncf.io/v1alpha1
    STEP: Read Status for v1alpha1.wardle.example.com 06/27/23 17:38:24.254
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 06/27/23 17:38:24.268
    STEP: List APIServices 06/27/23 17:38:24.329
    Jun 27 17:38:24.403: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Jun 27 17:38:25.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-6925" for this suite. 06/27/23 17:38:25.203
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:38:25.289
Jun 27 17:38:25.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir-wrapper 06/27/23 17:38:25.29
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:25.367
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:25.378
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Jun 27 17:38:25.497: INFO: Waiting up to 5m0s for pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8" in namespace "emptydir-wrapper-7704" to be "running and ready"
Jun 27 17:38:25.505: INFO: Pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604607ms
Jun 27 17:38:25.505: INFO: The phase of Pod pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:38:27.516: INFO: Pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019085992s
Jun 27 17:38:27.516: INFO: The phase of Pod pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:38:29.521: INFO: Pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8": Phase="Running", Reason="", readiness=true. Elapsed: 4.023145353s
Jun 27 17:38:29.521: INFO: The phase of Pod pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8 is Running (Ready = true)
Jun 27 17:38:29.521: INFO: Pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8" satisfied condition "running and ready"
STEP: Cleaning up the secret 06/27/23 17:38:29.533
STEP: Cleaning up the configmap 06/27/23 17:38:29.555
STEP: Cleaning up the pod 06/27/23 17:38:29.572
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jun 27 17:38:29.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7704" for this suite. 06/27/23 17:38:29.643
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":242,"skipped":4688,"failed":0}
------------------------------
â€¢ [4.381 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:38:25.289
    Jun 27 17:38:25.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir-wrapper 06/27/23 17:38:25.29
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:25.367
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:25.378
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Jun 27 17:38:25.497: INFO: Waiting up to 5m0s for pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8" in namespace "emptydir-wrapper-7704" to be "running and ready"
    Jun 27 17:38:25.505: INFO: Pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604607ms
    Jun 27 17:38:25.505: INFO: The phase of Pod pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:38:27.516: INFO: Pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019085992s
    Jun 27 17:38:27.516: INFO: The phase of Pod pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:38:29.521: INFO: Pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8": Phase="Running", Reason="", readiness=true. Elapsed: 4.023145353s
    Jun 27 17:38:29.521: INFO: The phase of Pod pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8 is Running (Ready = true)
    Jun 27 17:38:29.521: INFO: Pod "pod-secrets-4b5b2923-00fe-4720-988a-c429ee9be6b8" satisfied condition "running and ready"
    STEP: Cleaning up the secret 06/27/23 17:38:29.533
    STEP: Cleaning up the configmap 06/27/23 17:38:29.555
    STEP: Cleaning up the pod 06/27/23 17:38:29.572
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:38:29.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-7704" for this suite. 06/27/23 17:38:29.643
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:38:29.672
Jun 27 17:38:29.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename init-container 06/27/23 17:38:29.677
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:29.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:29.757
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 06/27/23 17:38:29.774
Jun 27 17:38:29.775: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 27 17:38:35.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-3563" for this suite. 06/27/23 17:38:35.863
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":243,"skipped":4700,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.241 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:38:29.672
    Jun 27 17:38:29.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename init-container 06/27/23 17:38:29.677
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:29.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:29.757
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 06/27/23 17:38:29.774
    Jun 27 17:38:29.775: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 27 17:38:35.840: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-3563" for this suite. 06/27/23 17:38:35.863
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:38:35.923
Jun 27 17:38:35.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename runtimeclass 06/27/23 17:38:35.924
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:35.979
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:35.99
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-3151-delete-me 06/27/23 17:38:36.029
STEP: Waiting for the RuntimeClass to disappear 06/27/23 17:38:36.079
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 27 17:38:36.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-3151" for this suite. 06/27/23 17:38:36.143
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":244,"skipped":4720,"failed":0}
------------------------------
â€¢ [0.270 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:38:35.923
    Jun 27 17:38:35.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename runtimeclass 06/27/23 17:38:35.924
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:35.979
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:35.99
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-3151-delete-me 06/27/23 17:38:36.029
    STEP: Waiting for the RuntimeClass to disappear 06/27/23 17:38:36.079
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 27 17:38:36.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-3151" for this suite. 06/27/23 17:38:36.143
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:38:36.198
Jun 27 17:38:36.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename svcaccounts 06/27/23 17:38:36.201
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:36.236
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:36.274
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  06/27/23 17:38:36.284
Jun 27 17:38:36.341: INFO: Waiting up to 5m0s for pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe" in namespace "svcaccounts-5654" to be "Succeeded or Failed"
Jun 27 17:38:36.361: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 19.854815ms
Jun 27 17:38:38.371: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030159544s
Jun 27 17:38:40.374: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032982925s
Jun 27 17:38:42.376: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035142209s
STEP: Saw pod success 06/27/23 17:38:42.376
Jun 27 17:38:42.376: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe" satisfied condition "Succeeded or Failed"
Jun 27 17:38:42.387: INFO: Trying to get logs from node 10.113.180.90 pod test-pod-bde03922-2db1-4774-9302-21e2447e4cbe container agnhost-container: <nil>
STEP: delete the pod 06/27/23 17:38:42.437
Jun 27 17:38:42.483: INFO: Waiting for pod test-pod-bde03922-2db1-4774-9302-21e2447e4cbe to disappear
Jun 27 17:38:42.521: INFO: Pod test-pod-bde03922-2db1-4774-9302-21e2447e4cbe no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 27 17:38:42.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-5654" for this suite. 06/27/23 17:38:42.569
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":245,"skipped":4726,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.412 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:38:36.198
    Jun 27 17:38:36.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename svcaccounts 06/27/23 17:38:36.201
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:36.236
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:36.274
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  06/27/23 17:38:36.284
    Jun 27 17:38:36.341: INFO: Waiting up to 5m0s for pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe" in namespace "svcaccounts-5654" to be "Succeeded or Failed"
    Jun 27 17:38:36.361: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 19.854815ms
    Jun 27 17:38:38.371: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030159544s
    Jun 27 17:38:40.374: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032982925s
    Jun 27 17:38:42.376: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.035142209s
    STEP: Saw pod success 06/27/23 17:38:42.376
    Jun 27 17:38:42.376: INFO: Pod "test-pod-bde03922-2db1-4774-9302-21e2447e4cbe" satisfied condition "Succeeded or Failed"
    Jun 27 17:38:42.387: INFO: Trying to get logs from node 10.113.180.90 pod test-pod-bde03922-2db1-4774-9302-21e2447e4cbe container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 17:38:42.437
    Jun 27 17:38:42.483: INFO: Waiting for pod test-pod-bde03922-2db1-4774-9302-21e2447e4cbe to disappear
    Jun 27 17:38:42.521: INFO: Pod test-pod-bde03922-2db1-4774-9302-21e2447e4cbe no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 27 17:38:42.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-5654" for this suite. 06/27/23 17:38:42.569
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:38:42.615
Jun 27 17:38:42.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 17:38:42.622
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:42.693
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:42.706
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-9d71e017-d68a-4a2a-8bb5-3addc97b4a05 06/27/23 17:38:42.724
STEP: Creating a pod to test consume configMaps 06/27/23 17:38:42.759
Jun 27 17:38:42.851: INFO: Waiting up to 5m0s for pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77" in namespace "configmap-5543" to be "Succeeded or Failed"
Jun 27 17:38:42.869: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77": Phase="Pending", Reason="", readiness=false. Elapsed: 18.120103ms
Jun 27 17:38:44.880: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028363116s
Jun 27 17:38:46.880: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028552197s
Jun 27 17:38:48.883: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031322184s
STEP: Saw pod success 06/27/23 17:38:48.883
Jun 27 17:38:48.883: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77" satisfied condition "Succeeded or Failed"
Jun 27 17:38:48.903: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77 container agnhost-container: <nil>
STEP: delete the pod 06/27/23 17:38:48.94
Jun 27 17:38:48.986: INFO: Waiting for pod pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77 to disappear
Jun 27 17:38:48.997: INFO: Pod pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 17:38:48.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-5543" for this suite. 06/27/23 17:38:49.018
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":246,"skipped":4737,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.425 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:38:42.615
    Jun 27 17:38:42.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 17:38:42.622
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:42.693
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:42.706
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-9d71e017-d68a-4a2a-8bb5-3addc97b4a05 06/27/23 17:38:42.724
    STEP: Creating a pod to test consume configMaps 06/27/23 17:38:42.759
    Jun 27 17:38:42.851: INFO: Waiting up to 5m0s for pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77" in namespace "configmap-5543" to be "Succeeded or Failed"
    Jun 27 17:38:42.869: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77": Phase="Pending", Reason="", readiness=false. Elapsed: 18.120103ms
    Jun 27 17:38:44.880: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028363116s
    Jun 27 17:38:46.880: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028552197s
    Jun 27 17:38:48.883: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031322184s
    STEP: Saw pod success 06/27/23 17:38:48.883
    Jun 27 17:38:48.883: INFO: Pod "pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77" satisfied condition "Succeeded or Failed"
    Jun 27 17:38:48.903: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77 container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 17:38:48.94
    Jun 27 17:38:48.986: INFO: Waiting for pod pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77 to disappear
    Jun 27 17:38:48.997: INFO: Pod pod-configmaps-78df1d45-0936-4d9e-9c27-9245a80b0e77 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 17:38:48.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-5543" for this suite. 06/27/23 17:38:49.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:38:49.044
Jun 27 17:38:49.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename daemonsets 06/27/23 17:38:49.046
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:49.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:49.095
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Jun 27 17:38:49.221: INFO: Create a RollingUpdate DaemonSet
Jun 27 17:38:49.233: INFO: Check that daemon pods launch on every node of the cluster
Jun 27 17:38:49.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:38:49.258: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:38:50.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:38:50.299: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:38:51.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Jun 27 17:38:51.287: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:38:52.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 27 17:38:52.287: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
Jun 27 17:38:52.287: INFO: Update the DaemonSet to trigger a rollout
Jun 27 17:38:52.384: INFO: Updating DaemonSet daemon-set
Jun 27 17:38:56.442: INFO: Roll back the DaemonSet before rollout is complete
Jun 27 17:38:56.480: INFO: Updating DaemonSet daemon-set
Jun 27 17:38:56.481: INFO: Make sure DaemonSet rollback is complete
Jun 27 17:38:56.515: INFO: Wrong image for pod: daemon-set-p74vd. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Jun 27 17:38:56.516: INFO: Pod daemon-set-p74vd is not available
Jun 27 17:39:01.568: INFO: Pod daemon-set-rzrsj is not available
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/27/23 17:39:01.607
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2276, will wait for the garbage collector to delete the pods 06/27/23 17:39:01.608
Jun 27 17:39:01.691: INFO: Deleting DaemonSet.extensions daemon-set took: 18.034369ms
Jun 27 17:39:01.794: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.207341ms
Jun 27 17:39:06.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:39:06.005: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 27 17:39:06.015: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"126774"},"items":null}

Jun 27 17:39:06.026: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"126774"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:39:06.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2276" for this suite. 06/27/23 17:39:06.089
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":247,"skipped":4744,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.063 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:38:49.044
    Jun 27 17:38:49.044: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename daemonsets 06/27/23 17:38:49.046
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:38:49.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:38:49.095
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Jun 27 17:38:49.221: INFO: Create a RollingUpdate DaemonSet
    Jun 27 17:38:49.233: INFO: Check that daemon pods launch on every node of the cluster
    Jun 27 17:38:49.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:38:49.258: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:38:50.299: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:38:50.299: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:38:51.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Jun 27 17:38:51.287: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:38:52.287: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 27 17:38:52.287: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    Jun 27 17:38:52.287: INFO: Update the DaemonSet to trigger a rollout
    Jun 27 17:38:52.384: INFO: Updating DaemonSet daemon-set
    Jun 27 17:38:56.442: INFO: Roll back the DaemonSet before rollout is complete
    Jun 27 17:38:56.480: INFO: Updating DaemonSet daemon-set
    Jun 27 17:38:56.481: INFO: Make sure DaemonSet rollback is complete
    Jun 27 17:38:56.515: INFO: Wrong image for pod: daemon-set-p74vd. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Jun 27 17:38:56.516: INFO: Pod daemon-set-p74vd is not available
    Jun 27 17:39:01.568: INFO: Pod daemon-set-rzrsj is not available
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/27/23 17:39:01.607
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2276, will wait for the garbage collector to delete the pods 06/27/23 17:39:01.608
    Jun 27 17:39:01.691: INFO: Deleting DaemonSet.extensions daemon-set took: 18.034369ms
    Jun 27 17:39:01.794: INFO: Terminating DaemonSet.extensions daemon-set pods took: 103.207341ms
    Jun 27 17:39:06.005: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:39:06.005: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 27 17:39:06.015: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"126774"},"items":null}

    Jun 27 17:39:06.026: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"126774"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:39:06.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2276" for this suite. 06/27/23 17:39:06.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:39:06.111
Jun 27 17:39:06.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:39:06.113
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:06.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:06.163
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-3f693ffe-b88a-4a25-a7b5-3309f3b35e6e 06/27/23 17:39:06.175
STEP: Creating a pod to test consume secrets 06/27/23 17:39:06.19
Jun 27 17:39:06.265: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485" in namespace "projected-5255" to be "Succeeded or Failed"
Jun 27 17:39:06.278: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485": Phase="Pending", Reason="", readiness=false. Elapsed: 12.442147ms
Jun 27 17:39:08.292: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026066165s
Jun 27 17:39:10.290: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024535465s
Jun 27 17:39:12.290: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02410123s
STEP: Saw pod success 06/27/23 17:39:12.29
Jun 27 17:39:12.290: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485" satisfied condition "Succeeded or Failed"
Jun 27 17:39:12.301: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485 container projected-secret-volume-test: <nil>
STEP: delete the pod 06/27/23 17:39:12.363
Jun 27 17:39:12.409: INFO: Waiting for pod pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485 to disappear
Jun 27 17:39:12.418: INFO: Pod pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Jun 27 17:39:12.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5255" for this suite. 06/27/23 17:39:12.462
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":248,"skipped":4760,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.416 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:39:06.111
    Jun 27 17:39:06.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:39:06.113
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:06.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:06.163
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-3f693ffe-b88a-4a25-a7b5-3309f3b35e6e 06/27/23 17:39:06.175
    STEP: Creating a pod to test consume secrets 06/27/23 17:39:06.19
    Jun 27 17:39:06.265: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485" in namespace "projected-5255" to be "Succeeded or Failed"
    Jun 27 17:39:06.278: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485": Phase="Pending", Reason="", readiness=false. Elapsed: 12.442147ms
    Jun 27 17:39:08.292: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485": Phase="Pending", Reason="", readiness=false. Elapsed: 2.026066165s
    Jun 27 17:39:10.290: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024535465s
    Jun 27 17:39:12.290: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.02410123s
    STEP: Saw pod success 06/27/23 17:39:12.29
    Jun 27 17:39:12.290: INFO: Pod "pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485" satisfied condition "Succeeded or Failed"
    Jun 27 17:39:12.301: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485 container projected-secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:39:12.363
    Jun 27 17:39:12.409: INFO: Waiting for pod pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485 to disappear
    Jun 27 17:39:12.418: INFO: Pod pod-projected-secrets-dd36c3de-3e29-4344-84ed-0799f670c485 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Jun 27 17:39:12.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5255" for this suite. 06/27/23 17:39:12.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:39:12.528
Jun 27 17:39:12.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename ingress 06/27/23 17:39:12.529
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:12.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:12.669
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 06/27/23 17:39:12.685
STEP: getting /apis/networking.k8s.io 06/27/23 17:39:12.693
STEP: getting /apis/networking.k8s.iov1 06/27/23 17:39:12.7
STEP: creating 06/27/23 17:39:12.706
STEP: getting 06/27/23 17:39:12.75
STEP: listing 06/27/23 17:39:12.766
STEP: watching 06/27/23 17:39:12.781
Jun 27 17:39:12.781: INFO: starting watch
STEP: cluster-wide listing 06/27/23 17:39:12.786
STEP: cluster-wide watching 06/27/23 17:39:12.798
Jun 27 17:39:12.799: INFO: starting watch
STEP: patching 06/27/23 17:39:12.803
STEP: updating 06/27/23 17:39:12.82
Jun 27 17:39:12.855: INFO: waiting for watch events with expected annotations
Jun 27 17:39:12.855: INFO: saw patched and updated annotations
STEP: patching /status 06/27/23 17:39:12.855
STEP: updating /status 06/27/23 17:39:12.874
STEP: get /status 06/27/23 17:39:12.905
STEP: deleting 06/27/23 17:39:12.917
STEP: deleting a collection 06/27/23 17:39:12.967
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Jun 27 17:39:13.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-2482" for this suite. 06/27/23 17:39:13.04
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":249,"skipped":4765,"failed":0}
------------------------------
â€¢ [0.534 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:39:12.528
    Jun 27 17:39:12.528: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename ingress 06/27/23 17:39:12.529
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:12.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:12.669
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 06/27/23 17:39:12.685
    STEP: getting /apis/networking.k8s.io 06/27/23 17:39:12.693
    STEP: getting /apis/networking.k8s.iov1 06/27/23 17:39:12.7
    STEP: creating 06/27/23 17:39:12.706
    STEP: getting 06/27/23 17:39:12.75
    STEP: listing 06/27/23 17:39:12.766
    STEP: watching 06/27/23 17:39:12.781
    Jun 27 17:39:12.781: INFO: starting watch
    STEP: cluster-wide listing 06/27/23 17:39:12.786
    STEP: cluster-wide watching 06/27/23 17:39:12.798
    Jun 27 17:39:12.799: INFO: starting watch
    STEP: patching 06/27/23 17:39:12.803
    STEP: updating 06/27/23 17:39:12.82
    Jun 27 17:39:12.855: INFO: waiting for watch events with expected annotations
    Jun 27 17:39:12.855: INFO: saw patched and updated annotations
    STEP: patching /status 06/27/23 17:39:12.855
    STEP: updating /status 06/27/23 17:39:12.874
    STEP: get /status 06/27/23 17:39:12.905
    STEP: deleting 06/27/23 17:39:12.917
    STEP: deleting a collection 06/27/23 17:39:12.967
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Jun 27 17:39:13.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-2482" for this suite. 06/27/23 17:39:13.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:39:13.066
Jun 27 17:39:13.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename subpath 06/27/23 17:39:13.068
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:13.1
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:13.121
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/27/23 17:39:13.133
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-76xk 06/27/23 17:39:13.167
STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:39:13.168
Jun 27 17:39:13.219: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-76xk" in namespace "subpath-7939" to be "Succeeded or Failed"
Jun 27 17:39:13.229: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Pending", Reason="", readiness=false. Elapsed: 9.744751ms
Jun 27 17:39:15.244: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 2.025057436s
Jun 27 17:39:17.242: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 4.023438644s
Jun 27 17:39:19.276: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 6.057334088s
Jun 27 17:39:21.241: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 8.021659118s
Jun 27 17:39:23.240: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 10.020830254s
Jun 27 17:39:25.240: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 12.020891449s
Jun 27 17:39:27.242: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 14.022616906s
Jun 27 17:39:29.242: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 16.023269769s
Jun 27 17:39:31.242: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 18.022537839s
Jun 27 17:39:33.241: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 20.022360264s
Jun 27 17:39:35.240: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=false. Elapsed: 22.021443192s
Jun 27 17:39:37.240: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020756023s
STEP: Saw pod success 06/27/23 17:39:37.24
Jun 27 17:39:37.240: INFO: Pod "pod-subpath-test-projected-76xk" satisfied condition "Succeeded or Failed"
Jun 27 17:39:37.250: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-projected-76xk container test-container-subpath-projected-76xk: <nil>
STEP: delete the pod 06/27/23 17:39:37.279
Jun 27 17:39:37.315: INFO: Waiting for pod pod-subpath-test-projected-76xk to disappear
Jun 27 17:39:37.324: INFO: Pod pod-subpath-test-projected-76xk no longer exists
STEP: Deleting pod pod-subpath-test-projected-76xk 06/27/23 17:39:37.324
Jun 27 17:39:37.325: INFO: Deleting pod "pod-subpath-test-projected-76xk" in namespace "subpath-7939"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 27 17:39:37.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-7939" for this suite. 06/27/23 17:39:37.354
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":250,"skipped":4790,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.313 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:39:13.066
    Jun 27 17:39:13.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename subpath 06/27/23 17:39:13.068
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:13.1
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:13.121
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/27/23 17:39:13.133
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-76xk 06/27/23 17:39:13.167
    STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:39:13.168
    Jun 27 17:39:13.219: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-76xk" in namespace "subpath-7939" to be "Succeeded or Failed"
    Jun 27 17:39:13.229: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Pending", Reason="", readiness=false. Elapsed: 9.744751ms
    Jun 27 17:39:15.244: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 2.025057436s
    Jun 27 17:39:17.242: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 4.023438644s
    Jun 27 17:39:19.276: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 6.057334088s
    Jun 27 17:39:21.241: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 8.021659118s
    Jun 27 17:39:23.240: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 10.020830254s
    Jun 27 17:39:25.240: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 12.020891449s
    Jun 27 17:39:27.242: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 14.022616906s
    Jun 27 17:39:29.242: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 16.023269769s
    Jun 27 17:39:31.242: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 18.022537839s
    Jun 27 17:39:33.241: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=true. Elapsed: 20.022360264s
    Jun 27 17:39:35.240: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Running", Reason="", readiness=false. Elapsed: 22.021443192s
    Jun 27 17:39:37.240: INFO: Pod "pod-subpath-test-projected-76xk": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.020756023s
    STEP: Saw pod success 06/27/23 17:39:37.24
    Jun 27 17:39:37.240: INFO: Pod "pod-subpath-test-projected-76xk" satisfied condition "Succeeded or Failed"
    Jun 27 17:39:37.250: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-projected-76xk container test-container-subpath-projected-76xk: <nil>
    STEP: delete the pod 06/27/23 17:39:37.279
    Jun 27 17:39:37.315: INFO: Waiting for pod pod-subpath-test-projected-76xk to disappear
    Jun 27 17:39:37.324: INFO: Pod pod-subpath-test-projected-76xk no longer exists
    STEP: Deleting pod pod-subpath-test-projected-76xk 06/27/23 17:39:37.324
    Jun 27 17:39:37.325: INFO: Deleting pod "pod-subpath-test-projected-76xk" in namespace "subpath-7939"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 27 17:39:37.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-7939" for this suite. 06/27/23 17:39:37.354
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:39:37.381
Jun 27 17:39:37.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-webhook 06/27/23 17:39:37.383
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:37.426
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:37.437
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 06/27/23 17:39:37.45
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/27/23 17:39:37.626
STEP: Deploying the custom resource conversion webhook pod 06/27/23 17:39:37.658
STEP: Wait for the deployment to be ready 06/27/23 17:39:37.695
Jun 27 17:39:37.717: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
Jun 27 17:39:39.748: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 39, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 17:39:41.76
STEP: Verifying the service has paired with the endpoint 06/27/23 17:39:41.807
Jun 27 17:39:42.813: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Jun 27 17:39:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Creating a v1 custom resource 06/27/23 17:39:45.592
STEP: v2 custom resource should be converted 06/27/23 17:39:45.608
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:39:46.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-3144" for this suite. 06/27/23 17:39:46.192
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":251,"skipped":4799,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.045 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:39:37.381
    Jun 27 17:39:37.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-webhook 06/27/23 17:39:37.383
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:37.426
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:37.437
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 06/27/23 17:39:37.45
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 06/27/23 17:39:37.626
    STEP: Deploying the custom resource conversion webhook pod 06/27/23 17:39:37.658
    STEP: Wait for the deployment to be ready 06/27/23 17:39:37.695
    Jun 27 17:39:37.717: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    Jun 27 17:39:39.748: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 39, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 39, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 39, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-59dfc5db8d\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 17:39:41.76
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:39:41.807
    Jun 27 17:39:42.813: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Jun 27 17:39:42.837: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Creating a v1 custom resource 06/27/23 17:39:45.592
    STEP: v2 custom resource should be converted 06/27/23 17:39:45.608
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:39:46.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-3144" for this suite. 06/27/23 17:39:46.192
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:39:46.456
Jun 27 17:39:46.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:39:46.457
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:46.494
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:46.515
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 06/27/23 17:39:46.529
Jun 27 17:39:46.572: INFO: Waiting up to 5m0s for pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63" in namespace "downward-api-8879" to be "Succeeded or Failed"
Jun 27 17:39:46.588: INFO: Pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63": Phase="Pending", Reason="", readiness=false. Elapsed: 15.989094ms
Jun 27 17:39:48.613: INFO: Pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041018779s
Jun 27 17:39:50.602: INFO: Pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029630207s
STEP: Saw pod success 06/27/23 17:39:50.602
Jun 27 17:39:50.604: INFO: Pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63" satisfied condition "Succeeded or Failed"
Jun 27 17:39:50.615: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63 container dapi-container: <nil>
STEP: delete the pod 06/27/23 17:39:50.647
Jun 27 17:39:50.675: INFO: Waiting for pod downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63 to disappear
Jun 27 17:39:50.684: INFO: Pod downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 27 17:39:50.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8879" for this suite. 06/27/23 17:39:50.701
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":252,"skipped":4826,"failed":0}
------------------------------
â€¢ [4.262 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:39:46.456
    Jun 27 17:39:46.456: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:39:46.457
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:46.494
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:46.515
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 06/27/23 17:39:46.529
    Jun 27 17:39:46.572: INFO: Waiting up to 5m0s for pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63" in namespace "downward-api-8879" to be "Succeeded or Failed"
    Jun 27 17:39:46.588: INFO: Pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63": Phase="Pending", Reason="", readiness=false. Elapsed: 15.989094ms
    Jun 27 17:39:48.613: INFO: Pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041018779s
    Jun 27 17:39:50.602: INFO: Pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.029630207s
    STEP: Saw pod success 06/27/23 17:39:50.602
    Jun 27 17:39:50.604: INFO: Pod "downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63" satisfied condition "Succeeded or Failed"
    Jun 27 17:39:50.615: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63 container dapi-container: <nil>
    STEP: delete the pod 06/27/23 17:39:50.647
    Jun 27 17:39:50.675: INFO: Waiting for pod downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63 to disappear
    Jun 27 17:39:50.684: INFO: Pod downward-api-a0fd72c8-9076-469a-bbf4-3d40f888cf63 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 27 17:39:50.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8879" for this suite. 06/27/23 17:39:50.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:39:50.726
Jun 27 17:39:50.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename deployment 06/27/23 17:39:50.728
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:50.768
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:50.781
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Jun 27 17:39:50.790: INFO: Creating deployment "test-recreate-deployment"
Jun 27 17:39:50.807: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Jun 27 17:39:50.841: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Jun 27 17:39:52.904: INFO: Waiting deployment "test-recreate-deployment" to complete
Jun 27 17:39:52.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 39, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 39, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 39, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 39, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:39:54.927: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Jun 27 17:39:54.951: INFO: Updating deployment test-recreate-deployment
Jun 27 17:39:54.951: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 27 17:39:55.114: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-8022  bc69f0ae-028b-4a13-be21-e2da350e689b 127567 2 2023-06-27 17:39:50 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-27 17:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b69bfd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-27 17:39:55 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-06-27 17:39:55 +0000 UTC,LastTransitionTime:2023-06-27 17:39:50 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Jun 27 17:39:55.125: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-8022  4dd2adb3-92af-4dd5-974c-4966779d80ca 127566 1 2023-06-27 17:39:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bc69f0ae-028b-4a13-be21-e2da350e689b 0xc00cb5e480 0xc00cb5e481}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc69f0ae-028b-4a13-be21-e2da350e689b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cb5e518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 27 17:39:55.125: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Jun 27 17:39:55.125: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-8022  e90d3787-525a-4e18-ad9d-45fba470cc89 127557 2 2023-06-27 17:39:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bc69f0ae-028b-4a13-be21-e2da350e689b 0xc00cb5e367 0xc00cb5e368}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc69f0ae-028b-4a13-be21-e2da350e689b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cb5e418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Jun 27 17:39:55.150: INFO: Pod "test-recreate-deployment-9d58999df-w5kqk" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-w5kqk test-recreate-deployment-9d58999df- deployment-8022  d92ad468-0cae-490e-9569-56e1d2d86a47 127568 0 2023-06-27 17:39:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 4dd2adb3-92af-4dd5-974c-4966779d80ca 0xc00383a157 0xc00383a158}] [] [{kube-controller-manager Update v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4dd2adb3-92af-4dd5-974c-4966779d80ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ml7pn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ml7pn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-447xg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:39:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 27 17:39:55.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8022" for this suite. 06/27/23 17:39:55.178
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":253,"skipped":4854,"failed":0}
------------------------------
â€¢ [4.492 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:39:50.726
    Jun 27 17:39:50.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename deployment 06/27/23 17:39:50.728
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:50.768
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:50.781
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Jun 27 17:39:50.790: INFO: Creating deployment "test-recreate-deployment"
    Jun 27 17:39:50.807: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Jun 27 17:39:50.841: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Jun 27 17:39:52.904: INFO: Waiting deployment "test-recreate-deployment" to complete
    Jun 27 17:39:52.915: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 39, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 39, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 39, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 39, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-recreate-deployment-7d8b6f647f\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:39:54.927: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Jun 27 17:39:54.951: INFO: Updating deployment test-recreate-deployment
    Jun 27 17:39:54.951: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 27 17:39:55.114: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-8022  bc69f0ae-028b-4a13-be21-e2da350e689b 127567 2 2023-06-27 17:39:50 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-06-27 17:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b69bfd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-06-27 17:39:55 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-06-27 17:39:55 +0000 UTC,LastTransitionTime:2023-06-27 17:39:50 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Jun 27 17:39:55.125: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-8022  4dd2adb3-92af-4dd5-974c-4966779d80ca 127566 1 2023-06-27 17:39:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment bc69f0ae-028b-4a13-be21-e2da350e689b 0xc00cb5e480 0xc00cb5e481}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc69f0ae-028b-4a13-be21-e2da350e689b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cb5e518 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 17:39:55.125: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Jun 27 17:39:55.125: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-8022  e90d3787-525a-4e18-ad9d-45fba470cc89 127557 2 2023-06-27 17:39:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment bc69f0ae-028b-4a13-be21-e2da350e689b 0xc00cb5e367 0xc00cb5e368}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:39:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bc69f0ae-028b-4a13-be21-e2da350e689b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00cb5e418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Jun 27 17:39:55.150: INFO: Pod "test-recreate-deployment-9d58999df-w5kqk" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-w5kqk test-recreate-deployment-9d58999df- deployment-8022  d92ad468-0cae-490e-9569-56e1d2d86a47 127568 0 2023-06-27 17:39:55 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 4dd2adb3-92af-4dd5-974c-4966779d80ca 0xc00383a157 0xc00383a158}] [] [{kube-controller-manager Update v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4dd2adb3-92af-4dd5-974c-4966779d80ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-06-27 17:39:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ml7pn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ml7pn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c15,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-447xg,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:39:55 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:,StartTime:2023-06-27 17:39:55 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 27 17:39:55.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8022" for this suite. 06/27/23 17:39:55.178
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:39:55.221
Jun 27 17:39:55.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 17:39:55.222
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:55.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:55.272
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 06/27/23 17:39:55.288
STEP: Counting existing ResourceQuota 06/27/23 17:40:01.319
STEP: Creating a ResourceQuota 06/27/23 17:40:06.337
STEP: Ensuring resource quota status is calculated 06/27/23 17:40:06.351
STEP: Creating a Secret 06/27/23 17:40:08.364
STEP: Ensuring resource quota status captures secret creation 06/27/23 17:40:08.395
STEP: Deleting a secret 06/27/23 17:40:10.408
STEP: Ensuring resource quota status released usage 06/27/23 17:40:10.425
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 17:40:12.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3902" for this suite. 06/27/23 17:40:12.481
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":254,"skipped":4872,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.281 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:39:55.221
    Jun 27 17:39:55.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 17:39:55.222
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:39:55.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:39:55.272
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 06/27/23 17:39:55.288
    STEP: Counting existing ResourceQuota 06/27/23 17:40:01.319
    STEP: Creating a ResourceQuota 06/27/23 17:40:06.337
    STEP: Ensuring resource quota status is calculated 06/27/23 17:40:06.351
    STEP: Creating a Secret 06/27/23 17:40:08.364
    STEP: Ensuring resource quota status captures secret creation 06/27/23 17:40:08.395
    STEP: Deleting a secret 06/27/23 17:40:10.408
    STEP: Ensuring resource quota status released usage 06/27/23 17:40:10.425
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 17:40:12.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3902" for this suite. 06/27/23 17:40:12.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:40:12.59
Jun 27 17:40:12.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replicaset 06/27/23 17:40:12.592
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:40:12.664
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:40:12.681
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 06/27/23 17:40:12.76
STEP: Verify that the required pods have come up. 06/27/23 17:40:12.792
Jun 27 17:40:12.809: INFO: Pod name sample-pod: Found 0 pods out of 1
Jun 27 17:40:17.832: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 06/27/23 17:40:17.832
STEP: Getting /status 06/27/23 17:40:17.832
Jun 27 17:40:17.844: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 06/27/23 17:40:17.844
Jun 27 17:40:17.867: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 06/27/23 17:40:17.867
Jun 27 17:40:17.874: INFO: Observed &ReplicaSet event: ADDED
Jun 27 17:40:17.874: INFO: Observed &ReplicaSet event: MODIFIED
Jun 27 17:40:17.874: INFO: Observed &ReplicaSet event: MODIFIED
Jun 27 17:40:17.875: INFO: Observed &ReplicaSet event: MODIFIED
Jun 27 17:40:17.875: INFO: Found replicaset test-rs in namespace replicaset-709 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Jun 27 17:40:17.875: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 06/27/23 17:40:17.875
Jun 27 17:40:17.875: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Jun 27 17:40:17.890: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 06/27/23 17:40:17.89
Jun 27 17:40:17.897: INFO: Observed &ReplicaSet event: ADDED
Jun 27 17:40:17.898: INFO: Observed &ReplicaSet event: MODIFIED
Jun 27 17:40:17.898: INFO: Observed &ReplicaSet event: MODIFIED
Jun 27 17:40:17.899: INFO: Observed &ReplicaSet event: MODIFIED
Jun 27 17:40:17.899: INFO: Observed replicaset test-rs in namespace replicaset-709 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Jun 27 17:40:17.900: INFO: Observed &ReplicaSet event: MODIFIED
Jun 27 17:40:17.900: INFO: Found replicaset test-rs in namespace replicaset-709 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Jun 27 17:40:17.900: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 27 17:40:17.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-709" for this suite. 06/27/23 17:40:17.936
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":255,"skipped":4883,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.363 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:40:12.59
    Jun 27 17:40:12.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replicaset 06/27/23 17:40:12.592
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:40:12.664
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:40:12.681
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 06/27/23 17:40:12.76
    STEP: Verify that the required pods have come up. 06/27/23 17:40:12.792
    Jun 27 17:40:12.809: INFO: Pod name sample-pod: Found 0 pods out of 1
    Jun 27 17:40:17.832: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 06/27/23 17:40:17.832
    STEP: Getting /status 06/27/23 17:40:17.832
    Jun 27 17:40:17.844: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 06/27/23 17:40:17.844
    Jun 27 17:40:17.867: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 06/27/23 17:40:17.867
    Jun 27 17:40:17.874: INFO: Observed &ReplicaSet event: ADDED
    Jun 27 17:40:17.874: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 27 17:40:17.874: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 27 17:40:17.875: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 27 17:40:17.875: INFO: Found replicaset test-rs in namespace replicaset-709 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Jun 27 17:40:17.875: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 06/27/23 17:40:17.875
    Jun 27 17:40:17.875: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Jun 27 17:40:17.890: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 06/27/23 17:40:17.89
    Jun 27 17:40:17.897: INFO: Observed &ReplicaSet event: ADDED
    Jun 27 17:40:17.898: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 27 17:40:17.898: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 27 17:40:17.899: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 27 17:40:17.899: INFO: Observed replicaset test-rs in namespace replicaset-709 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Jun 27 17:40:17.900: INFO: Observed &ReplicaSet event: MODIFIED
    Jun 27 17:40:17.900: INFO: Found replicaset test-rs in namespace replicaset-709 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Jun 27 17:40:17.900: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 27 17:40:17.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-709" for this suite. 06/27/23 17:40:17.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:40:17.961
Jun 27 17:40:17.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:40:17.963
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:40:18.02
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:40:18.037
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 06/27/23 17:40:18.06
Jun 27 17:40:18.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: mark a version not serverd 06/27/23 17:40:39.616
STEP: check the unserved version gets removed 06/27/23 17:40:39.695
STEP: check the other version is not changed 06/27/23 17:40:49.336
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:41:06.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7199" for this suite. 06/27/23 17:41:06.142
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":256,"skipped":4896,"failed":0}
------------------------------
â€¢ [SLOW TEST] [48.208 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:40:17.961
    Jun 27 17:40:17.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:40:17.963
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:40:18.02
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:40:18.037
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 06/27/23 17:40:18.06
    Jun 27 17:40:18.061: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: mark a version not serverd 06/27/23 17:40:39.616
    STEP: check the unserved version gets removed 06/27/23 17:40:39.695
    STEP: check the other version is not changed 06/27/23 17:40:49.336
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:41:06.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7199" for this suite. 06/27/23 17:41:06.142
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:41:06.174
Jun 27 17:41:06.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-lifecycle-hook 06/27/23 17:41:06.177
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:41:06.227
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:41:06.249
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/27/23 17:41:06.325
Jun 27 17:41:06.395: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2389" to be "running and ready"
Jun 27 17:41:06.416: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 20.458466ms
Jun 27 17:41:06.416: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:41:08.448: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052046917s
Jun 27 17:41:08.448: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:41:10.437: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.041130413s
Jun 27 17:41:10.437: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 27 17:41:10.437: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 06/27/23 17:41:10.455
Jun 27 17:41:10.535: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2389" to be "running and ready"
Jun 27 17:41:10.553: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 18.133483ms
Jun 27 17:41:10.553: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:41:12.571: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035794435s
Jun 27 17:41:12.571: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:41:14.576: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.041181262s
Jun 27 17:41:14.576: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Jun 27 17:41:14.576: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 06/27/23 17:41:14.597
Jun 27 17:41:14.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:41:14.655: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:41:16.656: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:41:16.701: INFO: Pod pod-with-prestop-exec-hook still exists
Jun 27 17:41:18.657: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Jun 27 17:41:18.682: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 06/27/23 17:41:18.682
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 27 17:41:18.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-2389" for this suite. 06/27/23 17:41:18.825
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":257,"skipped":4911,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.686 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:41:06.174
    Jun 27 17:41:06.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/27/23 17:41:06.177
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:41:06.227
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:41:06.249
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/27/23 17:41:06.325
    Jun 27 17:41:06.395: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-2389" to be "running and ready"
    Jun 27 17:41:06.416: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 20.458466ms
    Jun 27 17:41:06.416: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:41:08.448: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052046917s
    Jun 27 17:41:08.448: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:41:10.437: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.041130413s
    Jun 27 17:41:10.437: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 27 17:41:10.437: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 06/27/23 17:41:10.455
    Jun 27 17:41:10.535: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-2389" to be "running and ready"
    Jun 27 17:41:10.553: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 18.133483ms
    Jun 27 17:41:10.553: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:41:12.571: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035794435s
    Jun 27 17:41:12.571: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:41:14.576: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 4.041181262s
    Jun 27 17:41:14.576: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Jun 27 17:41:14.576: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 06/27/23 17:41:14.597
    Jun 27 17:41:14.634: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 27 17:41:14.655: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun 27 17:41:16.656: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 27 17:41:16.701: INFO: Pod pod-with-prestop-exec-hook still exists
    Jun 27 17:41:18.657: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Jun 27 17:41:18.682: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 06/27/23 17:41:18.682
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 27 17:41:18.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-2389" for this suite. 06/27/23 17:41:18.825
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:41:18.862
Jun 27 17:41:18.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename deployment 06/27/23 17:41:18.864
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:41:18.957
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:41:18.982
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 06/27/23 17:41:19.06
STEP: waiting for Deployment to be created 06/27/23 17:41:19.085
STEP: waiting for all Replicas to be Ready 06/27/23 17:41:19.092
Jun 27 17:41:19.100: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 27 17:41:19.100: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 27 17:41:19.107: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 27 17:41:19.108: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 27 17:41:19.128: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 27 17:41:19.128: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 27 17:41:19.226: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 27 17:41:19.226: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Jun 27 17:41:20.761: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 27 17:41:20.761: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Jun 27 17:41:22.211: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 06/27/23 17:41:22.211
W0627 17:41:22.238619      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Jun 27 17:41:22.246: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 06/27/23 17:41:22.246
Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:22.258: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:22.258: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:22.283: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:22.283: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:22.310: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:22.310: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:22.328: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:22.328: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:24.798: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:24.798: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:24.835: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
STEP: listing Deployments 06/27/23 17:41:24.835
Jun 27 17:41:24.872: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 06/27/23 17:41:24.872
Jun 27 17:41:24.944: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 06/27/23 17:41:24.944
Jun 27 17:41:24.983: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:24.983: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:24.994: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:25.015: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:25.028: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:25.051: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:26.882: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:26.909: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:26.931: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:26.963: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Jun 27 17:41:29.035: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 06/27/23 17:41:29.065
STEP: fetching the DeploymentStatus 06/27/23 17:41:29.107
Jun 27 17:41:29.133: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:29.134: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:29.134: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:29.134: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:29.135: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:29.135: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
Jun 27 17:41:29.136: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:29.136: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:29.136: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:29.137: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
Jun 27 17:41:29.137: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 3
STEP: deleting the Deployment 06/27/23 17:41:29.138
Jun 27 17:41:29.194: INFO: observed event type MODIFIED
Jun 27 17:41:29.194: INFO: observed event type MODIFIED
Jun 27 17:41:29.194: INFO: observed event type MODIFIED
Jun 27 17:41:29.196: INFO: observed event type MODIFIED
Jun 27 17:41:29.196: INFO: observed event type MODIFIED
Jun 27 17:41:29.197: INFO: observed event type MODIFIED
Jun 27 17:41:29.197: INFO: observed event type MODIFIED
Jun 27 17:41:29.197: INFO: observed event type MODIFIED
Jun 27 17:41:29.197: INFO: observed event type MODIFIED
Jun 27 17:41:29.198: INFO: observed event type MODIFIED
Jun 27 17:41:29.198: INFO: observed event type MODIFIED
Jun 27 17:41:29.198: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Jun 27 17:41:29.242: INFO: Log out all the ReplicaSets if there is no deployment created
Jun 27 17:41:29.260: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-4908  305b3949-9efc-43aa-9dab-c0b4e704cbee 128634 4 2023-06-27 17:41:22 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 0ca6df94-9c37-4d26-8e8b-4b052d6bc089 0xc0051c1587 0xc0051c1588}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:41:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ca6df94-9c37-4d26-8e8b-4b052d6bc089\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:41:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051c1610 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Jun 27 17:41:29.280: INFO: pod: "test-deployment-54cc775c4b-j676b":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-j676b test-deployment-54cc775c4b- deployment-4908  3d6a2785-82c1-494f-a425-4dd8882182d1 128621 0 2023-06-27 17:41:24 +0000 UTC 2023-06-27 17:41:27 +0000 UTC 0xc0055499a8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:9355afbdad976c63b0297db7a25cb71ebd5ffb4454de027515552bd4452ed5e7 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs: k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.115"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.60.115"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 305b3949-9efc-43aa-9dab-c0b4e704cbee 0xc005549a07 0xc005549a08}] [] [{kube-controller-manager Update v1 2023-06-27 17:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"305b3949-9efc-43aa-9dab-c0b4e704cbee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-06-27 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:41:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vv967,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vv967,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fh4cz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.115,StartTime:2023-06-27 17:41:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:41:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://c76d6c01ee9231d8655fbb8a69478505fe7fed01f74532abc22e04e8c2ab433a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 27 17:41:29.281: INFO: pod: "test-deployment-54cc775c4b-lk259":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-lk259 test-deployment-54cc775c4b- deployment-4908  0e6f785b-656f-4e64-9a2e-8a94da115a84 128629 0 2023-06-27 17:41:22 +0000 UTC 2023-06-27 17:41:30 +0000 UTC 0xc005549d40 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:86d338da64d002e81fa078a2a5e003d0a28376845fe394f6c493fd5818a36f0c cni.projectcalico.org/podIP:172.30.250.224/32 cni.projectcalico.org/podIPs:172.30.250.224/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.224"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.224"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 305b3949-9efc-43aa-9dab-c0b4e704cbee 0xc005549dd7 0xc005549dd8}] [] [{kube-controller-manager Update v1 2023-06-27 17:41:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"305b3949-9efc-43aa-9dab-c0b4e704cbee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:41:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gs97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gs97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fh4cz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.224,StartTime:2023-06-27 17:41:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:41:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://5d56375e23b886f15a186130e3a2aadf0b7f031c7055caf5c546658c8de9f624,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 27 17:41:29.282: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
&ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-4908  69b42965-a7c8-4733-b24b-3f612df3d2a2 128624 2 2023-06-27 17:41:24 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 0ca6df94-9c37-4d26-8e8b-4b052d6bc089 0xc0051c1677 0xc0051c1678}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ca6df94-9c37-4d26-8e8b-4b052d6bc089\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:41:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051c1700 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

Jun 27 17:41:29.301: INFO: pod: "test-deployment-7c7d8d58c8-7scj6":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-7scj6 test-deployment-7c7d8d58c8- deployment-4908  f5e3e6ba-d600-4c22-ae1e-f4da0246a3b8 128644 0 2023-06-27 17:41:26 +0000 UTC 2023-06-27 17:41:30 +0000 UTC 0xc00557a028 map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:d13b71bfcd17325757c6540746fb411903a3a4ceb45a93831af023f7d2f2f106 cni.projectcalico.org/podIP:172.30.106.177/32 cni.projectcalico.org/podIPs:172.30.106.177/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.177"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.106.177"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 69b42965-a7c8-4733-b24b-3f612df3d2a2 0xc00557a097 0xc00557a098}] [] [{kube-controller-manager Update v1 2023-06-27 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69b42965-a7c8-4733-b24b-3f612df3d2a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:41:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:41:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:41:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nrnkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nrnkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fh4cz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:172.30.106.177,StartTime:2023-06-27 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:41:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://db00d02b8b02ff52f83a65f4e80402c4e32807f26091d8c1eb5d72110463a530,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

Jun 27 17:41:29.301: INFO: pod: "test-deployment-7c7d8d58c8-rxcmg":
&Pod{ObjectMeta:{test-deployment-7c7d8d58c8-rxcmg test-deployment-7c7d8d58c8- deployment-4908  c6f4dcb8-e324-4ef9-9a0f-71b35fdd370b 128643 0 2023-06-27 17:41:24 +0000 UTC 2023-06-27 17:41:30 +0000 UTC 0xc00557a610 map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:bef10e3dd63ef629548ead47bfb18c463a25f0269b169c184f0766d241b86f0c cni.projectcalico.org/podIP:172.30.250.252/32 cni.projectcalico.org/podIPs:172.30.250.252/32 k8s.v1.cni.cncf.io/network-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.252"
    ],
    "default": true,
    "dns": {}
}] k8s.v1.cni.cncf.io/networks-status:[{
    "name": "k8s-pod-network",
    "ips": [
        "172.30.250.252"
    ],
    "default": true,
    "dns": {}
}] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 69b42965-a7c8-4733-b24b-3f612df3d2a2 0xc00557a697 0xc00557a698}] [] [{kube-controller-manager Update v1 2023-06-27 17:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69b42965-a7c8-4733-b24b-3f612df3d2a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:41:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:41:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:41:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tt5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tt5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fh4cz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.252,StartTime:2023-06-27 17:41:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:41:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://9ac0fe62c8e08388760d02a8f20db2856b282ae3fa3d56b2294fb7baf946840c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Jun 27 17:41:29.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-4908" for this suite. 06/27/23 17:41:29.328
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":258,"skipped":4915,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.491 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:41:18.862
    Jun 27 17:41:18.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename deployment 06/27/23 17:41:18.864
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:41:18.957
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:41:18.982
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 06/27/23 17:41:19.06
    STEP: waiting for Deployment to be created 06/27/23 17:41:19.085
    STEP: waiting for all Replicas to be Ready 06/27/23 17:41:19.092
    Jun 27 17:41:19.100: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 27 17:41:19.100: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 27 17:41:19.107: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 27 17:41:19.108: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 27 17:41:19.128: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 27 17:41:19.128: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 27 17:41:19.226: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 27 17:41:19.226: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Jun 27 17:41:20.761: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun 27 17:41:20.761: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Jun 27 17:41:22.211: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 06/27/23 17:41:22.211
    W0627 17:41:22.238619      23 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Jun 27 17:41:22.246: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 06/27/23 17:41:22.246
    Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
    Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
    Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
    Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
    Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
    Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
    Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
    Jun 27 17:41:22.253: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 0
    Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:22.254: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:22.258: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:22.258: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:22.283: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:22.283: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:22.310: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:22.310: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:22.328: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:22.328: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:24.798: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:24.798: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:24.835: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    STEP: listing Deployments 06/27/23 17:41:24.835
    Jun 27 17:41:24.872: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 06/27/23 17:41:24.872
    Jun 27 17:41:24.944: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 06/27/23 17:41:24.944
    Jun 27 17:41:24.983: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:24.983: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:24.994: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:25.015: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:25.028: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:25.051: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:26.882: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:26.909: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:26.931: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:26.963: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Jun 27 17:41:29.035: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 06/27/23 17:41:29.065
    STEP: fetching the DeploymentStatus 06/27/23 17:41:29.107
    Jun 27 17:41:29.133: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:29.134: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:29.134: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:29.134: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:29.135: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:29.135: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 1
    Jun 27 17:41:29.136: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:29.136: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:29.136: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:29.137: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 2
    Jun 27 17:41:29.137: INFO: observed Deployment test-deployment in namespace deployment-4908 with ReadyReplicas 3
    STEP: deleting the Deployment 06/27/23 17:41:29.138
    Jun 27 17:41:29.194: INFO: observed event type MODIFIED
    Jun 27 17:41:29.194: INFO: observed event type MODIFIED
    Jun 27 17:41:29.194: INFO: observed event type MODIFIED
    Jun 27 17:41:29.196: INFO: observed event type MODIFIED
    Jun 27 17:41:29.196: INFO: observed event type MODIFIED
    Jun 27 17:41:29.197: INFO: observed event type MODIFIED
    Jun 27 17:41:29.197: INFO: observed event type MODIFIED
    Jun 27 17:41:29.197: INFO: observed event type MODIFIED
    Jun 27 17:41:29.197: INFO: observed event type MODIFIED
    Jun 27 17:41:29.198: INFO: observed event type MODIFIED
    Jun 27 17:41:29.198: INFO: observed event type MODIFIED
    Jun 27 17:41:29.198: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Jun 27 17:41:29.242: INFO: Log out all the ReplicaSets if there is no deployment created
    Jun 27 17:41:29.260: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-4908  305b3949-9efc-43aa-9dab-c0b4e704cbee 128634 4 2023-06-27 17:41:22 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 0ca6df94-9c37-4d26-8e8b-4b052d6bc089 0xc0051c1587 0xc0051c1588}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:41:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ca6df94-9c37-4d26-8e8b-4b052d6bc089\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:41:29 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051c1610 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Jun 27 17:41:29.280: INFO: pod: "test-deployment-54cc775c4b-j676b":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-j676b test-deployment-54cc775c4b- deployment-4908  3d6a2785-82c1-494f-a425-4dd8882182d1 128621 0 2023-06-27 17:41:24 +0000 UTC 2023-06-27 17:41:27 +0000 UTC 0xc0055499a8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:9355afbdad976c63b0297db7a25cb71ebd5ffb4454de027515552bd4452ed5e7 cni.projectcalico.org/podIP: cni.projectcalico.org/podIPs: k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.115"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.60.115"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 305b3949-9efc-43aa-9dab-c0b4e704cbee 0xc005549a07 0xc005549a08}] [] [{kube-controller-manager Update v1 2023-06-27 17:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"305b3949-9efc-43aa-9dab-c0b4e704cbee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {multus Update v1 2023-06-27 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:41:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.60.115\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status} {calico Update v1 2023-06-27 17:41:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vv967,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vv967,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.96,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fh4cz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.96,PodIP:172.30.60.115,StartTime:2023-06-27 17:41:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:41:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://c76d6c01ee9231d8655fbb8a69478505fe7fed01f74532abc22e04e8c2ab433a,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.60.115,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 27 17:41:29.281: INFO: pod: "test-deployment-54cc775c4b-lk259":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-lk259 test-deployment-54cc775c4b- deployment-4908  0e6f785b-656f-4e64-9a2e-8a94da115a84 128629 0 2023-06-27 17:41:22 +0000 UTC 2023-06-27 17:41:30 +0000 UTC 0xc005549d40 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[cni.projectcalico.org/containerID:86d338da64d002e81fa078a2a5e003d0a28376845fe394f6c493fd5818a36f0c cni.projectcalico.org/podIP:172.30.250.224/32 cni.projectcalico.org/podIPs:172.30.250.224/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.224"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.224"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 305b3949-9efc-43aa-9dab-c0b4e704cbee 0xc005549dd7 0xc005549dd8}] [] [{kube-controller-manager Update v1 2023-06-27 17:41:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"305b3949-9efc-43aa-9dab-c0b4e704cbee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:41:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:41:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.224\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6gs97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6gs97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fh4cz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.224,StartTime:2023-06-27 17:41:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:41:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:cri-o://5d56375e23b886f15a186130e3a2aadf0b7f031c7055caf5c546658c8de9f624,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.224,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 27 17:41:29.282: INFO: ReplicaSet "test-deployment-7c7d8d58c8":
    &ReplicaSet{ObjectMeta:{test-deployment-7c7d8d58c8  deployment-4908  69b42965-a7c8-4733-b24b-3f612df3d2a2 128624 2 2023-06-27 17:41:24 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 0ca6df94-9c37-4d26-8e8b-4b052d6bc089 0xc0051c1677 0xc0051c1678}] [] [{kube-controller-manager Update apps/v1 2023-06-27 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0ca6df94-9c37-4d26-8e8b-4b052d6bc089\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-06-27 17:41:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 7c7d8d58c8,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0051c1700 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

    Jun 27 17:41:29.301: INFO: pod: "test-deployment-7c7d8d58c8-7scj6":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-7scj6 test-deployment-7c7d8d58c8- deployment-4908  f5e3e6ba-d600-4c22-ae1e-f4da0246a3b8 128644 0 2023-06-27 17:41:26 +0000 UTC 2023-06-27 17:41:30 +0000 UTC 0xc00557a028 map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:d13b71bfcd17325757c6540746fb411903a3a4ceb45a93831af023f7d2f2f106 cni.projectcalico.org/podIP:172.30.106.177/32 cni.projectcalico.org/podIPs:172.30.106.177/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.177"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.106.177"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 69b42965-a7c8-4733-b24b-3f612df3d2a2 0xc00557a097 0xc00557a098}] [] [{kube-controller-manager Update v1 2023-06-27 17:41:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69b42965-a7c8-4733-b24b-3f612df3d2a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:41:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:41:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:41:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.106.177\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nrnkp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nrnkp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.89,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fh4cz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.89,PodIP:172.30.106.177,StartTime:2023-06-27 17:41:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:41:28 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://db00d02b8b02ff52f83a65f4e80402c4e32807f26091d8c1eb5d72110463a530,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.106.177,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    Jun 27 17:41:29.301: INFO: pod: "test-deployment-7c7d8d58c8-rxcmg":
    &Pod{ObjectMeta:{test-deployment-7c7d8d58c8-rxcmg test-deployment-7c7d8d58c8- deployment-4908  c6f4dcb8-e324-4ef9-9a0f-71b35fdd370b 128643 0 2023-06-27 17:41:24 +0000 UTC 2023-06-27 17:41:30 +0000 UTC 0xc00557a610 map[pod-template-hash:7c7d8d58c8 test-deployment-static:true] map[cni.projectcalico.org/containerID:bef10e3dd63ef629548ead47bfb18c463a25f0269b169c184f0766d241b86f0c cni.projectcalico.org/podIP:172.30.250.252/32 cni.projectcalico.org/podIPs:172.30.250.252/32 k8s.v1.cni.cncf.io/network-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.252"
        ],
        "default": true,
        "dns": {}
    }] k8s.v1.cni.cncf.io/networks-status:[{
        "name": "k8s-pod-network",
        "ips": [
            "172.30.250.252"
        ],
        "default": true,
        "dns": {}
    }] openshift.io/scc:anyuid] [{apps/v1 ReplicaSet test-deployment-7c7d8d58c8 69b42965-a7c8-4733-b24b-3f612df3d2a2 0xc00557a697 0xc00557a698}] [] [{kube-controller-manager Update v1 2023-06-27 17:41:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"69b42965-a7c8-4733-b24b-3f612df3d2a2\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-06-27 17:41:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {multus Update v1 2023-06-27 17:41:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:k8s.v1.cni.cncf.io/network-status":{},"f:k8s.v1.cni.cncf.io/networks-status":{}}}} status} {kubelet Update v1 2023-06-27 17:41:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.30.250.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2tt5w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:openshift-service-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:service-ca.crt,Path:service-ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2tt5w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:&Capabilities{Add:[],Drop:[MKNOD],},Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:10.113.180.90,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:&SELinuxOptions{User:,Role:,Type:,Level:s0:c60,c40,},RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{LocalObjectReference{Name:default-dockercfg-fh4cz,},},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-06-27 17:41:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.113.180.90,PodIP:172.30.250.252,StartTime:2023-06-27 17:41:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-06-27 17:41:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:cri-o://9ac0fe62c8e08388760d02a8f20db2856b282ae3fa3d56b2294fb7baf946840c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.30.250.252,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Jun 27 17:41:29.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-4908" for this suite. 06/27/23 17:41:29.328
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:41:29.359
Jun 27 17:41:29.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-preemption 06/27/23 17:41:29.361
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:41:29.416
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:41:29.436
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 27 17:41:29.497: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 27 17:42:29.818: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 06/27/23 17:42:29.852
Jun 27 17:42:29.965: INFO: Created pod: pod0-0-sched-preemption-low-priority
Jun 27 17:42:30.014: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Jun 27 17:42:30.101: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Jun 27 17:42:30.155: INFO: Created pod: pod1-1-sched-preemption-medium-priority
Jun 27 17:42:30.293: INFO: Created pod: pod2-0-sched-preemption-medium-priority
Jun 27 17:42:30.339: INFO: Created pod: pod2-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 06/27/23 17:42:30.339
Jun 27 17:42:30.339: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4288" to be "running"
Jun 27 17:42:30.365: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 25.865613ms
Jun 27 17:42:32.386: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046443444s
Jun 27 17:42:34.383: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043245083s
Jun 27 17:42:36.384: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044413406s
Jun 27 17:42:38.388: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048155428s
Jun 27 17:42:40.385: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045161892s
Jun 27 17:42:42.389: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.049503563s
Jun 27 17:42:42.389: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Jun 27 17:42:42.389: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
Jun 27 17:42:42.409: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.071165ms
Jun 27 17:42:42.409: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 27 17:42:42.410: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
Jun 27 17:42:42.430: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.370997ms
Jun 27 17:42:42.430: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 27 17:42:42.430: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
Jun 27 17:42:42.455: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 24.503212ms
Jun 27 17:42:42.455: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
Jun 27 17:42:42.455: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
Jun 27 17:42:42.473: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.692ms
Jun 27 17:42:42.473: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
Jun 27 17:42:42.473: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
Jun 27 17:42:42.507: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 33.775858ms
Jun 27 17:42:42.507: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/27/23 17:42:42.507
Jun 27 17:42:42.562: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4288" to be "running"
Jun 27 17:42:42.607: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 45.499196ms
Jun 27 17:42:44.634: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072572427s
Jun 27 17:42:46.634: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072432911s
Jun 27 17:42:48.633: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071334867s
Jun 27 17:42:50.647: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.085391529s
Jun 27 17:42:50.647: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:42:50.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-4288" for this suite. 06/27/23 17:42:50.872
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":259,"skipped":4958,"failed":0}
------------------------------
â€¢ [SLOW TEST] [81.723 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:41:29.359
    Jun 27 17:41:29.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-preemption 06/27/23 17:41:29.361
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:41:29.416
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:41:29.436
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 27 17:41:29.497: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 27 17:42:29.818: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 06/27/23 17:42:29.852
    Jun 27 17:42:29.965: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Jun 27 17:42:30.014: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Jun 27 17:42:30.101: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Jun 27 17:42:30.155: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    Jun 27 17:42:30.293: INFO: Created pod: pod2-0-sched-preemption-medium-priority
    Jun 27 17:42:30.339: INFO: Created pod: pod2-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 06/27/23 17:42:30.339
    Jun 27 17:42:30.339: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-4288" to be "running"
    Jun 27 17:42:30.365: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 25.865613ms
    Jun 27 17:42:32.386: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.046443444s
    Jun 27 17:42:34.383: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043245083s
    Jun 27 17:42:36.384: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.044413406s
    Jun 27 17:42:38.388: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 8.048155428s
    Jun 27 17:42:40.385: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 10.045161892s
    Jun 27 17:42:42.389: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 12.049503563s
    Jun 27 17:42:42.389: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Jun 27 17:42:42.389: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
    Jun 27 17:42:42.409: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.071165ms
    Jun 27 17:42:42.409: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 27 17:42:42.410: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
    Jun 27 17:42:42.430: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 20.370997ms
    Jun 27 17:42:42.430: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 27 17:42:42.430: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
    Jun 27 17:42:42.455: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 24.503212ms
    Jun 27 17:42:42.455: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    Jun 27 17:42:42.455: INFO: Waiting up to 5m0s for pod "pod2-0-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
    Jun 27 17:42:42.473: INFO: Pod "pod2-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 17.692ms
    Jun 27 17:42:42.473: INFO: Pod "pod2-0-sched-preemption-medium-priority" satisfied condition "running"
    Jun 27 17:42:42.473: INFO: Waiting up to 5m0s for pod "pod2-1-sched-preemption-medium-priority" in namespace "sched-preemption-4288" to be "running"
    Jun 27 17:42:42.507: INFO: Pod "pod2-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 33.775858ms
    Jun 27 17:42:42.507: INFO: Pod "pod2-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 06/27/23 17:42:42.507
    Jun 27 17:42:42.562: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-4288" to be "running"
    Jun 27 17:42:42.607: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 45.499196ms
    Jun 27 17:42:44.634: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.072572427s
    Jun 27 17:42:46.634: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.072432911s
    Jun 27 17:42:48.633: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.071334867s
    Jun 27 17:42:50.647: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 8.085391529s
    Jun 27 17:42:50.647: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:42:50.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-4288" for this suite. 06/27/23 17:42:50.872
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:42:51.087
Jun 27 17:42:51.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 17:42:51.09
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:42:51.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:42:51.221
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 06/27/23 17:42:51.251
STEP: Creating a ResourceQuota 06/27/23 17:42:56.269
STEP: Ensuring resource quota status is calculated 06/27/23 17:42:56.289
STEP: Creating a ReplicaSet 06/27/23 17:42:58.303
STEP: Ensuring resource quota status captures replicaset creation 06/27/23 17:42:58.434
STEP: Deleting a ReplicaSet 06/27/23 17:43:00.449
STEP: Ensuring resource quota status released usage 06/27/23 17:43:00.476
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 17:43:02.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4047" for this suite. 06/27/23 17:43:02.521
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":260,"skipped":4962,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.460 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:42:51.087
    Jun 27 17:42:51.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 17:42:51.09
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:42:51.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:42:51.221
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 06/27/23 17:42:51.251
    STEP: Creating a ResourceQuota 06/27/23 17:42:56.269
    STEP: Ensuring resource quota status is calculated 06/27/23 17:42:56.289
    STEP: Creating a ReplicaSet 06/27/23 17:42:58.303
    STEP: Ensuring resource quota status captures replicaset creation 06/27/23 17:42:58.434
    STEP: Deleting a ReplicaSet 06/27/23 17:43:00.449
    STEP: Ensuring resource quota status released usage 06/27/23 17:43:00.476
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 17:43:02.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4047" for this suite. 06/27/23 17:43:02.521
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:43:02.554
Jun 27 17:43:02.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubelet-test 06/27/23 17:43:02.556
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:02.609
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:02.661
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Jun 27 17:43:02.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-5714" for this suite. 06/27/23 17:43:02.818
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":261,"skipped":4981,"failed":0}
------------------------------
â€¢ [0.299 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:43:02.554
    Jun 27 17:43:02.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubelet-test 06/27/23 17:43:02.556
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:02.609
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:02.661
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Jun 27 17:43:02.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-5714" for this suite. 06/27/23 17:43:02.818
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:43:02.87
Jun 27 17:43:02.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename subpath 06/27/23 17:43:02.871
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:02.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:02.938
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 06/27/23 17:43:02.954
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-qkn4 06/27/23 17:43:03.008
STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:43:03.009
Jun 27 17:43:03.097: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qkn4" in namespace "subpath-5385" to be "Succeeded or Failed"
Jun 27 17:43:03.116: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.385324ms
Jun 27 17:43:05.135: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038298215s
Jun 27 17:43:07.137: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 4.039886289s
Jun 27 17:43:09.135: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 6.038062859s
Jun 27 17:43:11.139: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 8.041973191s
Jun 27 17:43:13.137: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 10.040249793s
Jun 27 17:43:15.139: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 12.042450804s
Jun 27 17:43:17.136: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 14.039202445s
Jun 27 17:43:19.136: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 16.038910045s
Jun 27 17:43:21.134: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 18.037409503s
Jun 27 17:43:23.140: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 20.042903926s
Jun 27 17:43:25.137: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 22.040253695s
Jun 27 17:43:27.135: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=false. Elapsed: 24.038259776s
Jun 27 17:43:29.139: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.041959379s
STEP: Saw pod success 06/27/23 17:43:29.139
Jun 27 17:43:29.139: INFO: Pod "pod-subpath-test-secret-qkn4" satisfied condition "Succeeded or Failed"
Jun 27 17:43:29.155: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-secret-qkn4 container test-container-subpath-secret-qkn4: <nil>
STEP: delete the pod 06/27/23 17:43:29.251
Jun 27 17:43:29.300: INFO: Waiting for pod pod-subpath-test-secret-qkn4 to disappear
Jun 27 17:43:29.318: INFO: Pod pod-subpath-test-secret-qkn4 no longer exists
STEP: Deleting pod pod-subpath-test-secret-qkn4 06/27/23 17:43:29.318
Jun 27 17:43:29.319: INFO: Deleting pod "pod-subpath-test-secret-qkn4" in namespace "subpath-5385"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Jun 27 17:43:29.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5385" for this suite. 06/27/23 17:43:29.359
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":262,"skipped":5033,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.519 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:43:02.87
    Jun 27 17:43:02.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename subpath 06/27/23 17:43:02.871
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:02.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:02.938
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 06/27/23 17:43:02.954
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-qkn4 06/27/23 17:43:03.008
    STEP: Creating a pod to test atomic-volume-subpath 06/27/23 17:43:03.009
    Jun 27 17:43:03.097: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-qkn4" in namespace "subpath-5385" to be "Succeeded or Failed"
    Jun 27 17:43:03.116: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Pending", Reason="", readiness=false. Elapsed: 19.385324ms
    Jun 27 17:43:05.135: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038298215s
    Jun 27 17:43:07.137: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 4.039886289s
    Jun 27 17:43:09.135: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 6.038062859s
    Jun 27 17:43:11.139: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 8.041973191s
    Jun 27 17:43:13.137: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 10.040249793s
    Jun 27 17:43:15.139: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 12.042450804s
    Jun 27 17:43:17.136: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 14.039202445s
    Jun 27 17:43:19.136: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 16.038910045s
    Jun 27 17:43:21.134: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 18.037409503s
    Jun 27 17:43:23.140: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 20.042903926s
    Jun 27 17:43:25.137: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=true. Elapsed: 22.040253695s
    Jun 27 17:43:27.135: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Running", Reason="", readiness=false. Elapsed: 24.038259776s
    Jun 27 17:43:29.139: INFO: Pod "pod-subpath-test-secret-qkn4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 26.041959379s
    STEP: Saw pod success 06/27/23 17:43:29.139
    Jun 27 17:43:29.139: INFO: Pod "pod-subpath-test-secret-qkn4" satisfied condition "Succeeded or Failed"
    Jun 27 17:43:29.155: INFO: Trying to get logs from node 10.113.180.90 pod pod-subpath-test-secret-qkn4 container test-container-subpath-secret-qkn4: <nil>
    STEP: delete the pod 06/27/23 17:43:29.251
    Jun 27 17:43:29.300: INFO: Waiting for pod pod-subpath-test-secret-qkn4 to disappear
    Jun 27 17:43:29.318: INFO: Pod pod-subpath-test-secret-qkn4 no longer exists
    STEP: Deleting pod pod-subpath-test-secret-qkn4 06/27/23 17:43:29.318
    Jun 27 17:43:29.319: INFO: Deleting pod "pod-subpath-test-secret-qkn4" in namespace "subpath-5385"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Jun 27 17:43:29.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5385" for this suite. 06/27/23 17:43:29.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:43:29.391
Jun 27 17:43:29.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 17:43:29.392
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:29.448
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:29.472
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 17:43:29.567
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:43:30.032
STEP: Deploying the webhook pod 06/27/23 17:43:30.076
STEP: Wait for the deployment to be ready 06/27/23 17:43:30.138
Jun 27 17:43:30.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Jun 27 17:43:32.306: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 17:43:34.317
STEP: Verifying the service has paired with the endpoint 06/27/23 17:43:34.386
Jun 27 17:43:35.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 06/27/23 17:43:35.402
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/27/23 17:43:35.408
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/27/23 17:43:35.408
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/27/23 17:43:35.408
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/27/23 17:43:35.413
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/27/23 17:43:35.414
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/27/23 17:43:35.419
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:43:35.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4844" for this suite. 06/27/23 17:43:35.442
STEP: Destroying namespace "webhook-4844-markers" for this suite. 06/27/23 17:43:35.486
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":263,"skipped":5060,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.295 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:43:29.391
    Jun 27 17:43:29.391: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 17:43:29.392
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:29.448
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:29.472
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 17:43:29.567
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:43:30.032
    STEP: Deploying the webhook pod 06/27/23 17:43:30.076
    STEP: Wait for the deployment to be ready 06/27/23 17:43:30.138
    Jun 27 17:43:30.280: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Jun 27 17:43:32.306: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 43, 30, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 17:43:34.317
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:43:34.386
    Jun 27 17:43:35.388: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 06/27/23 17:43:35.402
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 06/27/23 17:43:35.408
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 06/27/23 17:43:35.408
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 06/27/23 17:43:35.408
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 06/27/23 17:43:35.413
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 06/27/23 17:43:35.414
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 06/27/23 17:43:35.419
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:43:35.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4844" for this suite. 06/27/23 17:43:35.442
    STEP: Destroying namespace "webhook-4844-markers" for this suite. 06/27/23 17:43:35.486
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:43:35.687
Jun 27 17:43:35.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename var-expansion 06/27/23 17:43:35.688
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:35.73
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:35.748
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Jun 27 17:43:35.912: INFO: Waiting up to 2m0s for pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1" in namespace "var-expansion-8260" to be "container 0 failed with reason CreateContainerConfigError"
Jun 27 17:43:35.928: INFO: Pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.265569ms
Jun 27 17:43:37.947: INFO: Pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034618679s
Jun 27 17:43:37.947: INFO: Pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun 27 17:43:37.947: INFO: Deleting pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1" in namespace "var-expansion-8260"
Jun 27 17:43:37.979: INFO: Wait up to 5m0s for pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 27 17:43:42.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-8260" for this suite. 06/27/23 17:43:42.046
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":264,"skipped":5072,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.387 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:43:35.687
    Jun 27 17:43:35.687: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename var-expansion 06/27/23 17:43:35.688
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:35.73
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:35.748
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Jun 27 17:43:35.912: INFO: Waiting up to 2m0s for pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1" in namespace "var-expansion-8260" to be "container 0 failed with reason CreateContainerConfigError"
    Jun 27 17:43:35.928: INFO: Pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1": Phase="Pending", Reason="", readiness=false. Elapsed: 15.265569ms
    Jun 27 17:43:37.947: INFO: Pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034618679s
    Jun 27 17:43:37.947: INFO: Pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun 27 17:43:37.947: INFO: Deleting pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1" in namespace "var-expansion-8260"
    Jun 27 17:43:37.979: INFO: Wait up to 5m0s for pod "var-expansion-dfa015db-afdc-45d2-8a1f-68a4542e38d1" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 27 17:43:42.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-8260" for this suite. 06/27/23 17:43:42.046
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:43:42.078
Jun 27 17:43:42.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename var-expansion 06/27/23 17:43:42.081
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:42.122
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:42.154
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 06/27/23 17:43:42.184
Jun 27 17:43:42.286: INFO: Waiting up to 5m0s for pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e" in namespace "var-expansion-649" to be "Succeeded or Failed"
Jun 27 17:43:42.306: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.793115ms
Jun 27 17:43:44.342: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056256851s
Jun 27 17:43:46.329: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042997338s
Jun 27 17:43:48.331: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044607873s
STEP: Saw pod success 06/27/23 17:43:48.331
Jun 27 17:43:48.331: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e" satisfied condition "Succeeded or Failed"
Jun 27 17:43:48.348: INFO: Trying to get logs from node 10.113.180.90 pod var-expansion-7c58afed-d7c7-4647-9838-d546f745594e container dapi-container: <nil>
STEP: delete the pod 06/27/23 17:43:48.387
Jun 27 17:43:48.442: INFO: Waiting for pod var-expansion-7c58afed-d7c7-4647-9838-d546f745594e to disappear
Jun 27 17:43:48.459: INFO: Pod var-expansion-7c58afed-d7c7-4647-9838-d546f745594e no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 27 17:43:48.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-649" for this suite. 06/27/23 17:43:48.49
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":265,"skipped":5095,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.440 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:43:42.078
    Jun 27 17:43:42.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename var-expansion 06/27/23 17:43:42.081
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:42.122
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:42.154
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 06/27/23 17:43:42.184
    Jun 27 17:43:42.286: INFO: Waiting up to 5m0s for pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e" in namespace "var-expansion-649" to be "Succeeded or Failed"
    Jun 27 17:43:42.306: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e": Phase="Pending", Reason="", readiness=false. Elapsed: 19.793115ms
    Jun 27 17:43:44.342: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056256851s
    Jun 27 17:43:46.329: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.042997338s
    Jun 27 17:43:48.331: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.044607873s
    STEP: Saw pod success 06/27/23 17:43:48.331
    Jun 27 17:43:48.331: INFO: Pod "var-expansion-7c58afed-d7c7-4647-9838-d546f745594e" satisfied condition "Succeeded or Failed"
    Jun 27 17:43:48.348: INFO: Trying to get logs from node 10.113.180.90 pod var-expansion-7c58afed-d7c7-4647-9838-d546f745594e container dapi-container: <nil>
    STEP: delete the pod 06/27/23 17:43:48.387
    Jun 27 17:43:48.442: INFO: Waiting for pod var-expansion-7c58afed-d7c7-4647-9838-d546f745594e to disappear
    Jun 27 17:43:48.459: INFO: Pod var-expansion-7c58afed-d7c7-4647-9838-d546f745594e no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 27 17:43:48.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-649" for this suite. 06/27/23 17:43:48.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:43:48.521
Jun 27 17:43:48.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename var-expansion 06/27/23 17:43:48.524
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:48.585
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:48.604
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Jun 27 17:43:48.705: INFO: Waiting up to 2m0s for pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3" in namespace "var-expansion-2609" to be "container 0 failed with reason CreateContainerConfigError"
Jun 27 17:43:48.725: INFO: Pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3": Phase="Pending", Reason="", readiness=false. Elapsed: 19.530939ms
Jun 27 17:43:50.765: INFO: Pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060116098s
Jun 27 17:43:50.765: INFO: Pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Jun 27 17:43:50.765: INFO: Deleting pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3" in namespace "var-expansion-2609"
Jun 27 17:43:50.814: INFO: Wait up to 5m0s for pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 27 17:43:54.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2609" for this suite. 06/27/23 17:43:54.896
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":266,"skipped":5105,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.402 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:43:48.521
    Jun 27 17:43:48.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename var-expansion 06/27/23 17:43:48.524
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:48.585
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:48.604
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Jun 27 17:43:48.705: INFO: Waiting up to 2m0s for pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3" in namespace "var-expansion-2609" to be "container 0 failed with reason CreateContainerConfigError"
    Jun 27 17:43:48.725: INFO: Pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3": Phase="Pending", Reason="", readiness=false. Elapsed: 19.530939ms
    Jun 27 17:43:50.765: INFO: Pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.060116098s
    Jun 27 17:43:50.765: INFO: Pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Jun 27 17:43:50.765: INFO: Deleting pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3" in namespace "var-expansion-2609"
    Jun 27 17:43:50.814: INFO: Wait up to 5m0s for pod "var-expansion-d2bc1e0d-ceca-49a8-875b-fba8ff6760e3" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 27 17:43:54.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2609" for this suite. 06/27/23 17:43:54.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:43:54.929
Jun 27 17:43:54.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:43:54.932
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:55.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:55.025
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Jun 27 17:43:55.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/27/23 17:44:04.836
Jun 27 17:44:04.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 --namespace=crd-publish-openapi-1085 create -f -'
Jun 27 17:44:07.187: INFO: stderr: ""
Jun 27 17:44:07.187: INFO: stdout: "e2e-test-crd-publish-openapi-2153-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 27 17:44:07.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 --namespace=crd-publish-openapi-1085 delete e2e-test-crd-publish-openapi-2153-crds test-cr'
Jun 27 17:44:07.383: INFO: stderr: ""
Jun 27 17:44:07.383: INFO: stdout: "e2e-test-crd-publish-openapi-2153-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Jun 27 17:44:07.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 --namespace=crd-publish-openapi-1085 apply -f -'
Jun 27 17:44:07.972: INFO: stderr: ""
Jun 27 17:44:07.972: INFO: stdout: "e2e-test-crd-publish-openapi-2153-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Jun 27 17:44:07.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 --namespace=crd-publish-openapi-1085 delete e2e-test-crd-publish-openapi-2153-crds test-cr'
Jun 27 17:44:08.147: INFO: stderr: ""
Jun 27 17:44:08.147: INFO: stdout: "e2e-test-crd-publish-openapi-2153-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/27/23 17:44:08.147
Jun 27 17:44:08.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 explain e2e-test-crd-publish-openapi-2153-crds'
Jun 27 17:44:09.903: INFO: stderr: ""
Jun 27 17:44:09.903: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2153-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:44:19.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1085" for this suite. 06/27/23 17:44:19.994
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":267,"skipped":5118,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.096 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:43:54.929
    Jun 27 17:43:54.930: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:43:54.932
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:43:55.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:43:55.025
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Jun 27 17:43:55.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/27/23 17:44:04.836
    Jun 27 17:44:04.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 --namespace=crd-publish-openapi-1085 create -f -'
    Jun 27 17:44:07.187: INFO: stderr: ""
    Jun 27 17:44:07.187: INFO: stdout: "e2e-test-crd-publish-openapi-2153-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun 27 17:44:07.187: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 --namespace=crd-publish-openapi-1085 delete e2e-test-crd-publish-openapi-2153-crds test-cr'
    Jun 27 17:44:07.383: INFO: stderr: ""
    Jun 27 17:44:07.383: INFO: stdout: "e2e-test-crd-publish-openapi-2153-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Jun 27 17:44:07.383: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 --namespace=crd-publish-openapi-1085 apply -f -'
    Jun 27 17:44:07.972: INFO: stderr: ""
    Jun 27 17:44:07.972: INFO: stdout: "e2e-test-crd-publish-openapi-2153-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Jun 27 17:44:07.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 --namespace=crd-publish-openapi-1085 delete e2e-test-crd-publish-openapi-2153-crds test-cr'
    Jun 27 17:44:08.147: INFO: stderr: ""
    Jun 27 17:44:08.147: INFO: stdout: "e2e-test-crd-publish-openapi-2153-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/27/23 17:44:08.147
    Jun 27 17:44:08.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-1085 explain e2e-test-crd-publish-openapi-2153-crds'
    Jun 27 17:44:09.903: INFO: stderr: ""
    Jun 27 17:44:09.903: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-2153-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:44:19.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1085" for this suite. 06/27/23 17:44:19.994
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:44:20.027
Jun 27 17:44:20.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:44:20.033
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:44:20.095
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:44:20.121
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 06/27/23 17:44:20.145
Jun 27 17:44:20.226: INFO: Waiting up to 5m0s for pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783" in namespace "downward-api-5651" to be "Succeeded or Failed"
Jun 27 17:44:20.253: INFO: Pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783": Phase="Pending", Reason="", readiness=false. Elapsed: 18.671856ms
Jun 27 17:44:22.276: INFO: Pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042314224s
Jun 27 17:44:24.271: INFO: Pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037546138s
STEP: Saw pod success 06/27/23 17:44:24.272
Jun 27 17:44:24.272: INFO: Pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783" satisfied condition "Succeeded or Failed"
Jun 27 17:44:24.288: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783 container dapi-container: <nil>
STEP: delete the pod 06/27/23 17:44:24.373
Jun 27 17:44:24.426: INFO: Waiting for pod downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783 to disappear
Jun 27 17:44:24.447: INFO: Pod downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Jun 27 17:44:24.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5651" for this suite. 06/27/23 17:44:24.481
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":268,"skipped":5127,"failed":0}
------------------------------
â€¢ [4.484 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:44:20.027
    Jun 27 17:44:20.028: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:44:20.033
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:44:20.095
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:44:20.121
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 06/27/23 17:44:20.145
    Jun 27 17:44:20.226: INFO: Waiting up to 5m0s for pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783" in namespace "downward-api-5651" to be "Succeeded or Failed"
    Jun 27 17:44:20.253: INFO: Pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783": Phase="Pending", Reason="", readiness=false. Elapsed: 18.671856ms
    Jun 27 17:44:22.276: INFO: Pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783": Phase="Pending", Reason="", readiness=false. Elapsed: 2.042314224s
    Jun 27 17:44:24.271: INFO: Pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.037546138s
    STEP: Saw pod success 06/27/23 17:44:24.272
    Jun 27 17:44:24.272: INFO: Pod "downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783" satisfied condition "Succeeded or Failed"
    Jun 27 17:44:24.288: INFO: Trying to get logs from node 10.113.180.90 pod downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783 container dapi-container: <nil>
    STEP: delete the pod 06/27/23 17:44:24.373
    Jun 27 17:44:24.426: INFO: Waiting for pod downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783 to disappear
    Jun 27 17:44:24.447: INFO: Pod downward-api-8e6e2620-fe82-4e5f-82d3-ad8656cad783 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Jun 27 17:44:24.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5651" for this suite. 06/27/23 17:44:24.481
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:44:24.514
Jun 27 17:44:24.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 17:44:24.518
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:44:24.576
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:44:24.606
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4977 06/27/23 17:44:24.632
STEP: changing the ExternalName service to type=ClusterIP 06/27/23 17:44:24.654
STEP: creating replication controller externalname-service in namespace services-4977 06/27/23 17:44:24.722
I0627 17:44:24.763274      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4977, replica count: 2
I0627 17:44:27.814744      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 17:44:27.814: INFO: Creating new exec pod
Jun 27 17:44:27.868: INFO: Waiting up to 5m0s for pod "execpodm85p8" in namespace "services-4977" to be "running"
Jun 27 17:44:27.886: INFO: Pod "execpodm85p8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.588328ms
Jun 27 17:44:29.904: INFO: Pod "execpodm85p8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035361742s
Jun 27 17:44:31.907: INFO: Pod "execpodm85p8": Phase="Running", Reason="", readiness=true. Elapsed: 4.038494434s
Jun 27 17:44:31.907: INFO: Pod "execpodm85p8" satisfied condition "running"
Jun 27 17:44:32.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 27 17:44:33.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 27 17:44:33.303: INFO: stdout: ""
Jun 27 17:44:34.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 27 17:44:34.721: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 27 17:44:34.721: INFO: stdout: ""
Jun 27 17:44:35.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 27 17:44:35.747: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 27 17:44:35.747: INFO: stdout: ""
Jun 27 17:44:36.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 27 17:44:36.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 27 17:44:36.682: INFO: stdout: ""
Jun 27 17:44:37.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 27 17:44:37.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 27 17:44:37.706: INFO: stdout: ""
Jun 27 17:44:38.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 27 17:44:38.712: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 27 17:44:38.712: INFO: stdout: ""
Jun 27 17:44:39.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Jun 27 17:44:39.766: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Jun 27 17:44:39.766: INFO: stdout: "externalname-service-2jdf8"
Jun 27 17:44:39.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.109.99 80'
Jun 27 17:44:40.112: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.109.99 80\nConnection to 172.21.109.99 80 port [tcp/http] succeeded!\n"
Jun 27 17:44:40.112: INFO: stdout: "externalname-service-rkdmz"
Jun 27 17:44:40.112: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 17:44:40.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4977" for this suite. 06/27/23 17:44:40.212
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":269,"skipped":5136,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.726 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:44:24.514
    Jun 27 17:44:24.514: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 17:44:24.518
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:44:24.576
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:44:24.606
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4977 06/27/23 17:44:24.632
    STEP: changing the ExternalName service to type=ClusterIP 06/27/23 17:44:24.654
    STEP: creating replication controller externalname-service in namespace services-4977 06/27/23 17:44:24.722
    I0627 17:44:24.763274      23 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4977, replica count: 2
    I0627 17:44:27.814744      23 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 27 17:44:27.814: INFO: Creating new exec pod
    Jun 27 17:44:27.868: INFO: Waiting up to 5m0s for pod "execpodm85p8" in namespace "services-4977" to be "running"
    Jun 27 17:44:27.886: INFO: Pod "execpodm85p8": Phase="Pending", Reason="", readiness=false. Elapsed: 17.588328ms
    Jun 27 17:44:29.904: INFO: Pod "execpodm85p8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035361742s
    Jun 27 17:44:31.907: INFO: Pod "execpodm85p8": Phase="Running", Reason="", readiness=true. Elapsed: 4.038494434s
    Jun 27 17:44:31.907: INFO: Pod "execpodm85p8" satisfied condition "running"
    Jun 27 17:44:32.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 27 17:44:33.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 27 17:44:33.303: INFO: stdout: ""
    Jun 27 17:44:34.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 27 17:44:34.721: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 27 17:44:34.721: INFO: stdout: ""
    Jun 27 17:44:35.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 27 17:44:35.747: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 27 17:44:35.747: INFO: stdout: ""
    Jun 27 17:44:36.303: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 27 17:44:36.682: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 27 17:44:36.682: INFO: stdout: ""
    Jun 27 17:44:37.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 27 17:44:37.706: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 27 17:44:37.706: INFO: stdout: ""
    Jun 27 17:44:38.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 27 17:44:38.712: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 27 17:44:38.712: INFO: stdout: ""
    Jun 27 17:44:39.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Jun 27 17:44:39.766: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Jun 27 17:44:39.766: INFO: stdout: "externalname-service-2jdf8"
    Jun 27 17:44:39.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-4977 exec execpodm85p8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.109.99 80'
    Jun 27 17:44:40.112: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.109.99 80\nConnection to 172.21.109.99 80 port [tcp/http] succeeded!\n"
    Jun 27 17:44:40.112: INFO: stdout: "externalname-service-rkdmz"
    Jun 27 17:44:40.112: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 17:44:40.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4977" for this suite. 06/27/23 17:44:40.212
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:44:40.245
Jun 27 17:44:40.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 17:44:40.247
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:44:40.302
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:44:40.323
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Jun 27 17:44:40.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:44:43.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7380" for this suite. 06/27/23 17:44:43.799
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":270,"skipped":5176,"failed":0}
------------------------------
â€¢ [3.594 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:44:40.245
    Jun 27 17:44:40.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 17:44:40.247
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:44:40.302
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:44:40.323
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Jun 27 17:44:40.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:44:43.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7380" for this suite. 06/27/23 17:44:43.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:44:43.843
Jun 27 17:44:43.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-preemption 06/27/23 17:44:43.845
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:44:43.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:44:43.934
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Jun 27 17:44:44.019: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 27 17:45:44.307: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:45:44.346
Jun 27 17:45:44.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-preemption-path 06/27/23 17:45:44.349
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:45:44.424
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:45:44.444
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 06/27/23 17:45:44.468
STEP: Trying to launch a pod without a label to get a node which can launch it. 06/27/23 17:45:44.468
Jun 27 17:45:44.536: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1276" to be "running"
Jun 27 17:45:44.552: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 15.862517ms
Jun 27 17:45:46.579: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.041999227s
Jun 27 17:45:46.579: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 06/27/23 17:45:46.598
Jun 27 17:45:46.651: INFO: found a healthy node: 10.113.180.90
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Jun 27 17:45:59.048: INFO: pods created so far: [1 1 1]
Jun 27 17:45:59.048: INFO: length of pods created so far: 3
Jun 27 17:46:03.121: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Jun 27 17:46:10.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1276" for this suite. 06/27/23 17:46:10.166
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:46:10.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-2120" for this suite. 06/27/23 17:46:10.362
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":271,"skipped":5237,"failed":0}
------------------------------
â€¢ [SLOW TEST] [86.781 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:44:43.843
    Jun 27 17:44:43.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-preemption 06/27/23 17:44:43.845
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:44:43.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:44:43.934
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Jun 27 17:44:44.019: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 27 17:45:44.307: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:45:44.346
    Jun 27 17:45:44.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-preemption-path 06/27/23 17:45:44.349
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:45:44.424
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:45:44.444
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 06/27/23 17:45:44.468
    STEP: Trying to launch a pod without a label to get a node which can launch it. 06/27/23 17:45:44.468
    Jun 27 17:45:44.536: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-1276" to be "running"
    Jun 27 17:45:44.552: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 15.862517ms
    Jun 27 17:45:46.579: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.041999227s
    Jun 27 17:45:46.579: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 06/27/23 17:45:46.598
    Jun 27 17:45:46.651: INFO: found a healthy node: 10.113.180.90
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Jun 27 17:45:59.048: INFO: pods created so far: [1 1 1]
    Jun 27 17:45:59.048: INFO: length of pods created so far: 3
    Jun 27 17:46:03.121: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Jun 27 17:46:10.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-1276" for this suite. 06/27/23 17:46:10.166
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:46:10.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-2120" for this suite. 06/27/23 17:46:10.362
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:46:10.626
Jun 27 17:46:10.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename discovery 06/27/23 17:46:10.628
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:10.684
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:10.708
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 06/27/23 17:46:10.739
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Jun 27 17:46:11.262: INFO: Checking APIGroup: apiregistration.k8s.io
Jun 27 17:46:11.269: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Jun 27 17:46:11.269: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Jun 27 17:46:11.270: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Jun 27 17:46:11.270: INFO: Checking APIGroup: apps
Jun 27 17:46:11.278: INFO: PreferredVersion.GroupVersion: apps/v1
Jun 27 17:46:11.278: INFO: Versions found [{apps/v1 v1}]
Jun 27 17:46:11.278: INFO: apps/v1 matches apps/v1
Jun 27 17:46:11.278: INFO: Checking APIGroup: events.k8s.io
Jun 27 17:46:11.287: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Jun 27 17:46:11.287: INFO: Versions found [{events.k8s.io/v1 v1}]
Jun 27 17:46:11.287: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Jun 27 17:46:11.287: INFO: Checking APIGroup: authentication.k8s.io
Jun 27 17:46:11.296: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Jun 27 17:46:11.296: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Jun 27 17:46:11.296: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Jun 27 17:46:11.296: INFO: Checking APIGroup: authorization.k8s.io
Jun 27 17:46:11.307: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Jun 27 17:46:11.307: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Jun 27 17:46:11.307: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Jun 27 17:46:11.307: INFO: Checking APIGroup: autoscaling
Jun 27 17:46:11.319: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Jun 27 17:46:11.319: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Jun 27 17:46:11.319: INFO: autoscaling/v2 matches autoscaling/v2
Jun 27 17:46:11.319: INFO: Checking APIGroup: batch
Jun 27 17:46:11.336: INFO: PreferredVersion.GroupVersion: batch/v1
Jun 27 17:46:11.336: INFO: Versions found [{batch/v1 v1}]
Jun 27 17:46:11.336: INFO: batch/v1 matches batch/v1
Jun 27 17:46:11.336: INFO: Checking APIGroup: certificates.k8s.io
Jun 27 17:46:11.345: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Jun 27 17:46:11.345: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Jun 27 17:46:11.345: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Jun 27 17:46:11.345: INFO: Checking APIGroup: networking.k8s.io
Jun 27 17:46:11.353: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Jun 27 17:46:11.353: INFO: Versions found [{networking.k8s.io/v1 v1}]
Jun 27 17:46:11.353: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Jun 27 17:46:11.353: INFO: Checking APIGroup: policy
Jun 27 17:46:11.360: INFO: PreferredVersion.GroupVersion: policy/v1
Jun 27 17:46:11.360: INFO: Versions found [{policy/v1 v1}]
Jun 27 17:46:11.360: INFO: policy/v1 matches policy/v1
Jun 27 17:46:11.360: INFO: Checking APIGroup: rbac.authorization.k8s.io
Jun 27 17:46:11.368: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Jun 27 17:46:11.368: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Jun 27 17:46:11.368: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Jun 27 17:46:11.368: INFO: Checking APIGroup: storage.k8s.io
Jun 27 17:46:11.376: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Jun 27 17:46:11.376: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Jun 27 17:46:11.376: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Jun 27 17:46:11.376: INFO: Checking APIGroup: admissionregistration.k8s.io
Jun 27 17:46:11.383: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Jun 27 17:46:11.383: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Jun 27 17:46:11.383: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Jun 27 17:46:11.384: INFO: Checking APIGroup: apiextensions.k8s.io
Jun 27 17:46:11.391: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Jun 27 17:46:11.391: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Jun 27 17:46:11.391: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Jun 27 17:46:11.391: INFO: Checking APIGroup: scheduling.k8s.io
Jun 27 17:46:11.398: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Jun 27 17:46:11.398: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Jun 27 17:46:11.398: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Jun 27 17:46:11.399: INFO: Checking APIGroup: coordination.k8s.io
Jun 27 17:46:11.408: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Jun 27 17:46:11.408: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Jun 27 17:46:11.408: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Jun 27 17:46:11.408: INFO: Checking APIGroup: node.k8s.io
Jun 27 17:46:11.416: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Jun 27 17:46:11.416: INFO: Versions found [{node.k8s.io/v1 v1}]
Jun 27 17:46:11.416: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Jun 27 17:46:11.417: INFO: Checking APIGroup: discovery.k8s.io
Jun 27 17:46:11.424: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Jun 27 17:46:11.425: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Jun 27 17:46:11.425: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Jun 27 17:46:11.425: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Jun 27 17:46:11.432: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Jun 27 17:46:11.432: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Jun 27 17:46:11.432: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Jun 27 17:46:11.432: INFO: Checking APIGroup: apps.openshift.io
Jun 27 17:46:11.441: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
Jun 27 17:46:11.441: INFO: Versions found [{apps.openshift.io/v1 v1}]
Jun 27 17:46:11.441: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
Jun 27 17:46:11.441: INFO: Checking APIGroup: authorization.openshift.io
Jun 27 17:46:11.451: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
Jun 27 17:46:11.451: INFO: Versions found [{authorization.openshift.io/v1 v1}]
Jun 27 17:46:11.451: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
Jun 27 17:46:11.451: INFO: Checking APIGroup: build.openshift.io
Jun 27 17:46:11.461: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
Jun 27 17:46:11.461: INFO: Versions found [{build.openshift.io/v1 v1}]
Jun 27 17:46:11.461: INFO: build.openshift.io/v1 matches build.openshift.io/v1
Jun 27 17:46:11.461: INFO: Checking APIGroup: image.openshift.io
Jun 27 17:46:11.471: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
Jun 27 17:46:11.471: INFO: Versions found [{image.openshift.io/v1 v1}]
Jun 27 17:46:11.471: INFO: image.openshift.io/v1 matches image.openshift.io/v1
Jun 27 17:46:11.472: INFO: Checking APIGroup: oauth.openshift.io
Jun 27 17:46:11.479: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
Jun 27 17:46:11.479: INFO: Versions found [{oauth.openshift.io/v1 v1}]
Jun 27 17:46:11.479: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
Jun 27 17:46:11.479: INFO: Checking APIGroup: project.openshift.io
Jun 27 17:46:11.495: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
Jun 27 17:46:11.496: INFO: Versions found [{project.openshift.io/v1 v1}]
Jun 27 17:46:11.496: INFO: project.openshift.io/v1 matches project.openshift.io/v1
Jun 27 17:46:11.496: INFO: Checking APIGroup: quota.openshift.io
Jun 27 17:46:11.506: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
Jun 27 17:46:11.506: INFO: Versions found [{quota.openshift.io/v1 v1}]
Jun 27 17:46:11.506: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
Jun 27 17:46:11.506: INFO: Checking APIGroup: route.openshift.io
Jun 27 17:46:11.534: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
Jun 27 17:46:11.534: INFO: Versions found [{route.openshift.io/v1 v1}]
Jun 27 17:46:11.534: INFO: route.openshift.io/v1 matches route.openshift.io/v1
Jun 27 17:46:11.534: INFO: Checking APIGroup: security.openshift.io
Jun 27 17:46:11.542: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
Jun 27 17:46:11.542: INFO: Versions found [{security.openshift.io/v1 v1}]
Jun 27 17:46:11.542: INFO: security.openshift.io/v1 matches security.openshift.io/v1
Jun 27 17:46:11.542: INFO: Checking APIGroup: template.openshift.io
Jun 27 17:46:11.553: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
Jun 27 17:46:11.553: INFO: Versions found [{template.openshift.io/v1 v1}]
Jun 27 17:46:11.553: INFO: template.openshift.io/v1 matches template.openshift.io/v1
Jun 27 17:46:11.553: INFO: Checking APIGroup: user.openshift.io
Jun 27 17:46:11.563: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
Jun 27 17:46:11.563: INFO: Versions found [{user.openshift.io/v1 v1}]
Jun 27 17:46:11.563: INFO: user.openshift.io/v1 matches user.openshift.io/v1
Jun 27 17:46:11.563: INFO: Checking APIGroup: packages.operators.coreos.com
Jun 27 17:46:11.571: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
Jun 27 17:46:11.571: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
Jun 27 17:46:11.571: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
Jun 27 17:46:11.571: INFO: Checking APIGroup: config.openshift.io
Jun 27 17:46:11.585: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
Jun 27 17:46:11.585: INFO: Versions found [{config.openshift.io/v1 v1}]
Jun 27 17:46:11.585: INFO: config.openshift.io/v1 matches config.openshift.io/v1
Jun 27 17:46:11.585: INFO: Checking APIGroup: operator.openshift.io
Jun 27 17:46:11.593: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
Jun 27 17:46:11.593: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
Jun 27 17:46:11.593: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
Jun 27 17:46:11.593: INFO: Checking APIGroup: apiserver.openshift.io
Jun 27 17:46:11.601: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
Jun 27 17:46:11.601: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
Jun 27 17:46:11.601: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
Jun 27 17:46:11.601: INFO: Checking APIGroup: cloudcredential.openshift.io
Jun 27 17:46:11.608: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
Jun 27 17:46:11.608: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
Jun 27 17:46:11.608: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
Jun 27 17:46:11.608: INFO: Checking APIGroup: console.openshift.io
Jun 27 17:46:11.625: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
Jun 27 17:46:11.625: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
Jun 27 17:46:11.625: INFO: console.openshift.io/v1 matches console.openshift.io/v1
Jun 27 17:46:11.625: INFO: Checking APIGroup: crd.projectcalico.org
Jun 27 17:46:11.635: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
Jun 27 17:46:11.635: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
Jun 27 17:46:11.635: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
Jun 27 17:46:11.635: INFO: Checking APIGroup: imageregistry.operator.openshift.io
Jun 27 17:46:11.654: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
Jun 27 17:46:11.654: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
Jun 27 17:46:11.654: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
Jun 27 17:46:11.654: INFO: Checking APIGroup: ingress.operator.openshift.io
Jun 27 17:46:11.663: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
Jun 27 17:46:11.663: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
Jun 27 17:46:11.663: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
Jun 27 17:46:11.663: INFO: Checking APIGroup: k8s.cni.cncf.io
Jun 27 17:46:11.672: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
Jun 27 17:46:11.672: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
Jun 27 17:46:11.672: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
Jun 27 17:46:11.672: INFO: Checking APIGroup: machineconfiguration.openshift.io
Jun 27 17:46:11.686: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
Jun 27 17:46:11.686: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
Jun 27 17:46:11.686: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
Jun 27 17:46:11.686: INFO: Checking APIGroup: monitoring.coreos.com
Jun 27 17:46:11.696: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
Jun 27 17:46:11.696: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
Jun 27 17:46:11.696: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
Jun 27 17:46:11.696: INFO: Checking APIGroup: network.operator.openshift.io
Jun 27 17:46:11.704: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
Jun 27 17:46:11.704: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
Jun 27 17:46:11.704: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
Jun 27 17:46:11.704: INFO: Checking APIGroup: operator.tigera.io
Jun 27 17:46:11.731: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
Jun 27 17:46:11.732: INFO: Versions found [{operator.tigera.io/v1 v1}]
Jun 27 17:46:11.732: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
Jun 27 17:46:11.732: INFO: Checking APIGroup: operators.coreos.com
Jun 27 17:46:11.740: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
Jun 27 17:46:11.740: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
Jun 27 17:46:11.740: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
Jun 27 17:46:11.740: INFO: Checking APIGroup: performance.openshift.io
Jun 27 17:46:11.748: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
Jun 27 17:46:11.748: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
Jun 27 17:46:11.748: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
Jun 27 17:46:11.748: INFO: Checking APIGroup: samples.operator.openshift.io
Jun 27 17:46:11.756: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
Jun 27 17:46:11.756: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
Jun 27 17:46:11.756: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
Jun 27 17:46:11.756: INFO: Checking APIGroup: security.internal.openshift.io
Jun 27 17:46:11.764: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
Jun 27 17:46:11.764: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
Jun 27 17:46:11.764: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
Jun 27 17:46:11.764: INFO: Checking APIGroup: snapshot.storage.k8s.io
Jun 27 17:46:11.772: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
Jun 27 17:46:11.772: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
Jun 27 17:46:11.773: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
Jun 27 17:46:11.773: INFO: Checking APIGroup: tuned.openshift.io
Jun 27 17:46:11.788: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
Jun 27 17:46:11.788: INFO: Versions found [{tuned.openshift.io/v1 v1}]
Jun 27 17:46:11.788: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
Jun 27 17:46:11.788: INFO: Checking APIGroup: controlplane.operator.openshift.io
Jun 27 17:46:11.802: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
Jun 27 17:46:11.802: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
Jun 27 17:46:11.802: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
Jun 27 17:46:11.802: INFO: Checking APIGroup: ibm.com
Jun 27 17:46:11.811: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
Jun 27 17:46:11.811: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
Jun 27 17:46:11.811: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
Jun 27 17:46:11.811: INFO: Checking APIGroup: migration.k8s.io
Jun 27 17:46:11.841: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
Jun 27 17:46:11.841: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
Jun 27 17:46:11.841: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
Jun 27 17:46:11.841: INFO: Checking APIGroup: whereabouts.cni.cncf.io
Jun 27 17:46:11.849: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
Jun 27 17:46:11.849: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
Jun 27 17:46:11.849: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
Jun 27 17:46:11.849: INFO: Checking APIGroup: helm.openshift.io
Jun 27 17:46:11.858: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
Jun 27 17:46:11.858: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
Jun 27 17:46:11.858: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
Jun 27 17:46:11.858: INFO: Checking APIGroup: metrics.k8s.io
Jun 27 17:46:11.868: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
Jun 27 17:46:11.868: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
Jun 27 17:46:11.868: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Jun 27 17:46:11.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1494" for this suite. 06/27/23 17:46:11.899
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":272,"skipped":5239,"failed":0}
------------------------------
â€¢ [1.302 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:46:10.626
    Jun 27 17:46:10.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename discovery 06/27/23 17:46:10.628
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:10.684
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:10.708
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 06/27/23 17:46:10.739
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Jun 27 17:46:11.262: INFO: Checking APIGroup: apiregistration.k8s.io
    Jun 27 17:46:11.269: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Jun 27 17:46:11.269: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Jun 27 17:46:11.270: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Jun 27 17:46:11.270: INFO: Checking APIGroup: apps
    Jun 27 17:46:11.278: INFO: PreferredVersion.GroupVersion: apps/v1
    Jun 27 17:46:11.278: INFO: Versions found [{apps/v1 v1}]
    Jun 27 17:46:11.278: INFO: apps/v1 matches apps/v1
    Jun 27 17:46:11.278: INFO: Checking APIGroup: events.k8s.io
    Jun 27 17:46:11.287: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Jun 27 17:46:11.287: INFO: Versions found [{events.k8s.io/v1 v1}]
    Jun 27 17:46:11.287: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Jun 27 17:46:11.287: INFO: Checking APIGroup: authentication.k8s.io
    Jun 27 17:46:11.296: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Jun 27 17:46:11.296: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Jun 27 17:46:11.296: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Jun 27 17:46:11.296: INFO: Checking APIGroup: authorization.k8s.io
    Jun 27 17:46:11.307: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Jun 27 17:46:11.307: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Jun 27 17:46:11.307: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Jun 27 17:46:11.307: INFO: Checking APIGroup: autoscaling
    Jun 27 17:46:11.319: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Jun 27 17:46:11.319: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Jun 27 17:46:11.319: INFO: autoscaling/v2 matches autoscaling/v2
    Jun 27 17:46:11.319: INFO: Checking APIGroup: batch
    Jun 27 17:46:11.336: INFO: PreferredVersion.GroupVersion: batch/v1
    Jun 27 17:46:11.336: INFO: Versions found [{batch/v1 v1}]
    Jun 27 17:46:11.336: INFO: batch/v1 matches batch/v1
    Jun 27 17:46:11.336: INFO: Checking APIGroup: certificates.k8s.io
    Jun 27 17:46:11.345: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Jun 27 17:46:11.345: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Jun 27 17:46:11.345: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Jun 27 17:46:11.345: INFO: Checking APIGroup: networking.k8s.io
    Jun 27 17:46:11.353: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Jun 27 17:46:11.353: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Jun 27 17:46:11.353: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Jun 27 17:46:11.353: INFO: Checking APIGroup: policy
    Jun 27 17:46:11.360: INFO: PreferredVersion.GroupVersion: policy/v1
    Jun 27 17:46:11.360: INFO: Versions found [{policy/v1 v1}]
    Jun 27 17:46:11.360: INFO: policy/v1 matches policy/v1
    Jun 27 17:46:11.360: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Jun 27 17:46:11.368: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Jun 27 17:46:11.368: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Jun 27 17:46:11.368: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Jun 27 17:46:11.368: INFO: Checking APIGroup: storage.k8s.io
    Jun 27 17:46:11.376: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Jun 27 17:46:11.376: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Jun 27 17:46:11.376: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Jun 27 17:46:11.376: INFO: Checking APIGroup: admissionregistration.k8s.io
    Jun 27 17:46:11.383: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Jun 27 17:46:11.383: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Jun 27 17:46:11.383: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Jun 27 17:46:11.384: INFO: Checking APIGroup: apiextensions.k8s.io
    Jun 27 17:46:11.391: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Jun 27 17:46:11.391: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Jun 27 17:46:11.391: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Jun 27 17:46:11.391: INFO: Checking APIGroup: scheduling.k8s.io
    Jun 27 17:46:11.398: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Jun 27 17:46:11.398: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Jun 27 17:46:11.398: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Jun 27 17:46:11.399: INFO: Checking APIGroup: coordination.k8s.io
    Jun 27 17:46:11.408: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Jun 27 17:46:11.408: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Jun 27 17:46:11.408: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Jun 27 17:46:11.408: INFO: Checking APIGroup: node.k8s.io
    Jun 27 17:46:11.416: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Jun 27 17:46:11.416: INFO: Versions found [{node.k8s.io/v1 v1}]
    Jun 27 17:46:11.416: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Jun 27 17:46:11.417: INFO: Checking APIGroup: discovery.k8s.io
    Jun 27 17:46:11.424: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Jun 27 17:46:11.425: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Jun 27 17:46:11.425: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Jun 27 17:46:11.425: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Jun 27 17:46:11.432: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Jun 27 17:46:11.432: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Jun 27 17:46:11.432: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Jun 27 17:46:11.432: INFO: Checking APIGroup: apps.openshift.io
    Jun 27 17:46:11.441: INFO: PreferredVersion.GroupVersion: apps.openshift.io/v1
    Jun 27 17:46:11.441: INFO: Versions found [{apps.openshift.io/v1 v1}]
    Jun 27 17:46:11.441: INFO: apps.openshift.io/v1 matches apps.openshift.io/v1
    Jun 27 17:46:11.441: INFO: Checking APIGroup: authorization.openshift.io
    Jun 27 17:46:11.451: INFO: PreferredVersion.GroupVersion: authorization.openshift.io/v1
    Jun 27 17:46:11.451: INFO: Versions found [{authorization.openshift.io/v1 v1}]
    Jun 27 17:46:11.451: INFO: authorization.openshift.io/v1 matches authorization.openshift.io/v1
    Jun 27 17:46:11.451: INFO: Checking APIGroup: build.openshift.io
    Jun 27 17:46:11.461: INFO: PreferredVersion.GroupVersion: build.openshift.io/v1
    Jun 27 17:46:11.461: INFO: Versions found [{build.openshift.io/v1 v1}]
    Jun 27 17:46:11.461: INFO: build.openshift.io/v1 matches build.openshift.io/v1
    Jun 27 17:46:11.461: INFO: Checking APIGroup: image.openshift.io
    Jun 27 17:46:11.471: INFO: PreferredVersion.GroupVersion: image.openshift.io/v1
    Jun 27 17:46:11.471: INFO: Versions found [{image.openshift.io/v1 v1}]
    Jun 27 17:46:11.471: INFO: image.openshift.io/v1 matches image.openshift.io/v1
    Jun 27 17:46:11.472: INFO: Checking APIGroup: oauth.openshift.io
    Jun 27 17:46:11.479: INFO: PreferredVersion.GroupVersion: oauth.openshift.io/v1
    Jun 27 17:46:11.479: INFO: Versions found [{oauth.openshift.io/v1 v1}]
    Jun 27 17:46:11.479: INFO: oauth.openshift.io/v1 matches oauth.openshift.io/v1
    Jun 27 17:46:11.479: INFO: Checking APIGroup: project.openshift.io
    Jun 27 17:46:11.495: INFO: PreferredVersion.GroupVersion: project.openshift.io/v1
    Jun 27 17:46:11.496: INFO: Versions found [{project.openshift.io/v1 v1}]
    Jun 27 17:46:11.496: INFO: project.openshift.io/v1 matches project.openshift.io/v1
    Jun 27 17:46:11.496: INFO: Checking APIGroup: quota.openshift.io
    Jun 27 17:46:11.506: INFO: PreferredVersion.GroupVersion: quota.openshift.io/v1
    Jun 27 17:46:11.506: INFO: Versions found [{quota.openshift.io/v1 v1}]
    Jun 27 17:46:11.506: INFO: quota.openshift.io/v1 matches quota.openshift.io/v1
    Jun 27 17:46:11.506: INFO: Checking APIGroup: route.openshift.io
    Jun 27 17:46:11.534: INFO: PreferredVersion.GroupVersion: route.openshift.io/v1
    Jun 27 17:46:11.534: INFO: Versions found [{route.openshift.io/v1 v1}]
    Jun 27 17:46:11.534: INFO: route.openshift.io/v1 matches route.openshift.io/v1
    Jun 27 17:46:11.534: INFO: Checking APIGroup: security.openshift.io
    Jun 27 17:46:11.542: INFO: PreferredVersion.GroupVersion: security.openshift.io/v1
    Jun 27 17:46:11.542: INFO: Versions found [{security.openshift.io/v1 v1}]
    Jun 27 17:46:11.542: INFO: security.openshift.io/v1 matches security.openshift.io/v1
    Jun 27 17:46:11.542: INFO: Checking APIGroup: template.openshift.io
    Jun 27 17:46:11.553: INFO: PreferredVersion.GroupVersion: template.openshift.io/v1
    Jun 27 17:46:11.553: INFO: Versions found [{template.openshift.io/v1 v1}]
    Jun 27 17:46:11.553: INFO: template.openshift.io/v1 matches template.openshift.io/v1
    Jun 27 17:46:11.553: INFO: Checking APIGroup: user.openshift.io
    Jun 27 17:46:11.563: INFO: PreferredVersion.GroupVersion: user.openshift.io/v1
    Jun 27 17:46:11.563: INFO: Versions found [{user.openshift.io/v1 v1}]
    Jun 27 17:46:11.563: INFO: user.openshift.io/v1 matches user.openshift.io/v1
    Jun 27 17:46:11.563: INFO: Checking APIGroup: packages.operators.coreos.com
    Jun 27 17:46:11.571: INFO: PreferredVersion.GroupVersion: packages.operators.coreos.com/v1
    Jun 27 17:46:11.571: INFO: Versions found [{packages.operators.coreos.com/v1 v1}]
    Jun 27 17:46:11.571: INFO: packages.operators.coreos.com/v1 matches packages.operators.coreos.com/v1
    Jun 27 17:46:11.571: INFO: Checking APIGroup: config.openshift.io
    Jun 27 17:46:11.585: INFO: PreferredVersion.GroupVersion: config.openshift.io/v1
    Jun 27 17:46:11.585: INFO: Versions found [{config.openshift.io/v1 v1}]
    Jun 27 17:46:11.585: INFO: config.openshift.io/v1 matches config.openshift.io/v1
    Jun 27 17:46:11.585: INFO: Checking APIGroup: operator.openshift.io
    Jun 27 17:46:11.593: INFO: PreferredVersion.GroupVersion: operator.openshift.io/v1
    Jun 27 17:46:11.593: INFO: Versions found [{operator.openshift.io/v1 v1} {operator.openshift.io/v1alpha1 v1alpha1}]
    Jun 27 17:46:11.593: INFO: operator.openshift.io/v1 matches operator.openshift.io/v1
    Jun 27 17:46:11.593: INFO: Checking APIGroup: apiserver.openshift.io
    Jun 27 17:46:11.601: INFO: PreferredVersion.GroupVersion: apiserver.openshift.io/v1
    Jun 27 17:46:11.601: INFO: Versions found [{apiserver.openshift.io/v1 v1}]
    Jun 27 17:46:11.601: INFO: apiserver.openshift.io/v1 matches apiserver.openshift.io/v1
    Jun 27 17:46:11.601: INFO: Checking APIGroup: cloudcredential.openshift.io
    Jun 27 17:46:11.608: INFO: PreferredVersion.GroupVersion: cloudcredential.openshift.io/v1
    Jun 27 17:46:11.608: INFO: Versions found [{cloudcredential.openshift.io/v1 v1}]
    Jun 27 17:46:11.608: INFO: cloudcredential.openshift.io/v1 matches cloudcredential.openshift.io/v1
    Jun 27 17:46:11.608: INFO: Checking APIGroup: console.openshift.io
    Jun 27 17:46:11.625: INFO: PreferredVersion.GroupVersion: console.openshift.io/v1
    Jun 27 17:46:11.625: INFO: Versions found [{console.openshift.io/v1 v1} {console.openshift.io/v1alpha1 v1alpha1}]
    Jun 27 17:46:11.625: INFO: console.openshift.io/v1 matches console.openshift.io/v1
    Jun 27 17:46:11.625: INFO: Checking APIGroup: crd.projectcalico.org
    Jun 27 17:46:11.635: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
    Jun 27 17:46:11.635: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
    Jun 27 17:46:11.635: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
    Jun 27 17:46:11.635: INFO: Checking APIGroup: imageregistry.operator.openshift.io
    Jun 27 17:46:11.654: INFO: PreferredVersion.GroupVersion: imageregistry.operator.openshift.io/v1
    Jun 27 17:46:11.654: INFO: Versions found [{imageregistry.operator.openshift.io/v1 v1}]
    Jun 27 17:46:11.654: INFO: imageregistry.operator.openshift.io/v1 matches imageregistry.operator.openshift.io/v1
    Jun 27 17:46:11.654: INFO: Checking APIGroup: ingress.operator.openshift.io
    Jun 27 17:46:11.663: INFO: PreferredVersion.GroupVersion: ingress.operator.openshift.io/v1
    Jun 27 17:46:11.663: INFO: Versions found [{ingress.operator.openshift.io/v1 v1}]
    Jun 27 17:46:11.663: INFO: ingress.operator.openshift.io/v1 matches ingress.operator.openshift.io/v1
    Jun 27 17:46:11.663: INFO: Checking APIGroup: k8s.cni.cncf.io
    Jun 27 17:46:11.672: INFO: PreferredVersion.GroupVersion: k8s.cni.cncf.io/v1
    Jun 27 17:46:11.672: INFO: Versions found [{k8s.cni.cncf.io/v1 v1}]
    Jun 27 17:46:11.672: INFO: k8s.cni.cncf.io/v1 matches k8s.cni.cncf.io/v1
    Jun 27 17:46:11.672: INFO: Checking APIGroup: machineconfiguration.openshift.io
    Jun 27 17:46:11.686: INFO: PreferredVersion.GroupVersion: machineconfiguration.openshift.io/v1
    Jun 27 17:46:11.686: INFO: Versions found [{machineconfiguration.openshift.io/v1 v1}]
    Jun 27 17:46:11.686: INFO: machineconfiguration.openshift.io/v1 matches machineconfiguration.openshift.io/v1
    Jun 27 17:46:11.686: INFO: Checking APIGroup: monitoring.coreos.com
    Jun 27 17:46:11.696: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
    Jun 27 17:46:11.696: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1beta1 v1beta1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
    Jun 27 17:46:11.696: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
    Jun 27 17:46:11.696: INFO: Checking APIGroup: network.operator.openshift.io
    Jun 27 17:46:11.704: INFO: PreferredVersion.GroupVersion: network.operator.openshift.io/v1
    Jun 27 17:46:11.704: INFO: Versions found [{network.operator.openshift.io/v1 v1}]
    Jun 27 17:46:11.704: INFO: network.operator.openshift.io/v1 matches network.operator.openshift.io/v1
    Jun 27 17:46:11.704: INFO: Checking APIGroup: operator.tigera.io
    Jun 27 17:46:11.731: INFO: PreferredVersion.GroupVersion: operator.tigera.io/v1
    Jun 27 17:46:11.732: INFO: Versions found [{operator.tigera.io/v1 v1}]
    Jun 27 17:46:11.732: INFO: operator.tigera.io/v1 matches operator.tigera.io/v1
    Jun 27 17:46:11.732: INFO: Checking APIGroup: operators.coreos.com
    Jun 27 17:46:11.740: INFO: PreferredVersion.GroupVersion: operators.coreos.com/v2
    Jun 27 17:46:11.740: INFO: Versions found [{operators.coreos.com/v2 v2} {operators.coreos.com/v1 v1} {operators.coreos.com/v1alpha2 v1alpha2} {operators.coreos.com/v1alpha1 v1alpha1}]
    Jun 27 17:46:11.740: INFO: operators.coreos.com/v2 matches operators.coreos.com/v2
    Jun 27 17:46:11.740: INFO: Checking APIGroup: performance.openshift.io
    Jun 27 17:46:11.748: INFO: PreferredVersion.GroupVersion: performance.openshift.io/v2
    Jun 27 17:46:11.748: INFO: Versions found [{performance.openshift.io/v2 v2} {performance.openshift.io/v1 v1} {performance.openshift.io/v1alpha1 v1alpha1}]
    Jun 27 17:46:11.748: INFO: performance.openshift.io/v2 matches performance.openshift.io/v2
    Jun 27 17:46:11.748: INFO: Checking APIGroup: samples.operator.openshift.io
    Jun 27 17:46:11.756: INFO: PreferredVersion.GroupVersion: samples.operator.openshift.io/v1
    Jun 27 17:46:11.756: INFO: Versions found [{samples.operator.openshift.io/v1 v1}]
    Jun 27 17:46:11.756: INFO: samples.operator.openshift.io/v1 matches samples.operator.openshift.io/v1
    Jun 27 17:46:11.756: INFO: Checking APIGroup: security.internal.openshift.io
    Jun 27 17:46:11.764: INFO: PreferredVersion.GroupVersion: security.internal.openshift.io/v1
    Jun 27 17:46:11.764: INFO: Versions found [{security.internal.openshift.io/v1 v1}]
    Jun 27 17:46:11.764: INFO: security.internal.openshift.io/v1 matches security.internal.openshift.io/v1
    Jun 27 17:46:11.764: INFO: Checking APIGroup: snapshot.storage.k8s.io
    Jun 27 17:46:11.772: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
    Jun 27 17:46:11.772: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1}]
    Jun 27 17:46:11.773: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
    Jun 27 17:46:11.773: INFO: Checking APIGroup: tuned.openshift.io
    Jun 27 17:46:11.788: INFO: PreferredVersion.GroupVersion: tuned.openshift.io/v1
    Jun 27 17:46:11.788: INFO: Versions found [{tuned.openshift.io/v1 v1}]
    Jun 27 17:46:11.788: INFO: tuned.openshift.io/v1 matches tuned.openshift.io/v1
    Jun 27 17:46:11.788: INFO: Checking APIGroup: controlplane.operator.openshift.io
    Jun 27 17:46:11.802: INFO: PreferredVersion.GroupVersion: controlplane.operator.openshift.io/v1alpha1
    Jun 27 17:46:11.802: INFO: Versions found [{controlplane.operator.openshift.io/v1alpha1 v1alpha1}]
    Jun 27 17:46:11.802: INFO: controlplane.operator.openshift.io/v1alpha1 matches controlplane.operator.openshift.io/v1alpha1
    Jun 27 17:46:11.802: INFO: Checking APIGroup: ibm.com
    Jun 27 17:46:11.811: INFO: PreferredVersion.GroupVersion: ibm.com/v1alpha1
    Jun 27 17:46:11.811: INFO: Versions found [{ibm.com/v1alpha1 v1alpha1}]
    Jun 27 17:46:11.811: INFO: ibm.com/v1alpha1 matches ibm.com/v1alpha1
    Jun 27 17:46:11.811: INFO: Checking APIGroup: migration.k8s.io
    Jun 27 17:46:11.841: INFO: PreferredVersion.GroupVersion: migration.k8s.io/v1alpha1
    Jun 27 17:46:11.841: INFO: Versions found [{migration.k8s.io/v1alpha1 v1alpha1}]
    Jun 27 17:46:11.841: INFO: migration.k8s.io/v1alpha1 matches migration.k8s.io/v1alpha1
    Jun 27 17:46:11.841: INFO: Checking APIGroup: whereabouts.cni.cncf.io
    Jun 27 17:46:11.849: INFO: PreferredVersion.GroupVersion: whereabouts.cni.cncf.io/v1alpha1
    Jun 27 17:46:11.849: INFO: Versions found [{whereabouts.cni.cncf.io/v1alpha1 v1alpha1}]
    Jun 27 17:46:11.849: INFO: whereabouts.cni.cncf.io/v1alpha1 matches whereabouts.cni.cncf.io/v1alpha1
    Jun 27 17:46:11.849: INFO: Checking APIGroup: helm.openshift.io
    Jun 27 17:46:11.858: INFO: PreferredVersion.GroupVersion: helm.openshift.io/v1beta1
    Jun 27 17:46:11.858: INFO: Versions found [{helm.openshift.io/v1beta1 v1beta1}]
    Jun 27 17:46:11.858: INFO: helm.openshift.io/v1beta1 matches helm.openshift.io/v1beta1
    Jun 27 17:46:11.858: INFO: Checking APIGroup: metrics.k8s.io
    Jun 27 17:46:11.868: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
    Jun 27 17:46:11.868: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
    Jun 27 17:46:11.868: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Jun 27 17:46:11.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-1494" for this suite. 06/27/23 17:46:11.899
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:46:11.932
Jun 27 17:46:11.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 17:46:11.934
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:11.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:12.017
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 17:46:12.143
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:46:12.777
STEP: Deploying the webhook pod 06/27/23 17:46:12.913
STEP: Wait for the deployment to be ready 06/27/23 17:46:12.968
Jun 27 17:46:13.009: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 06/27/23 17:46:15.083
STEP: Verifying the service has paired with the endpoint 06/27/23 17:46:15.124
Jun 27 17:46:16.125: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Jun 27 17:46:16.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/27/23 17:46:16.693
STEP: Creating a custom resource that should be denied by the webhook 06/27/23 17:46:16.805
STEP: Creating a custom resource whose deletion would be denied by the webhook 06/27/23 17:46:18.916
STEP: Updating the custom resource with disallowed data should be denied 06/27/23 17:46:18.981
STEP: Deleting the custom resource should be denied 06/27/23 17:46:19.091
STEP: Remove the offending key and value from the custom resource data 06/27/23 17:46:19.129
STEP: Deleting the updated custom resource should be successful 06/27/23 17:46:19.215
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:46:19.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9157" for this suite. 06/27/23 17:46:19.889
STEP: Destroying namespace "webhook-9157-markers" for this suite. 06/27/23 17:46:19.931
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":273,"skipped":5246,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.247 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:46:11.932
    Jun 27 17:46:11.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 17:46:11.934
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:11.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:12.017
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 17:46:12.143
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:46:12.777
    STEP: Deploying the webhook pod 06/27/23 17:46:12.913
    STEP: Wait for the deployment to be ready 06/27/23 17:46:12.968
    Jun 27 17:46:13.009: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 06/27/23 17:46:15.083
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:46:15.124
    Jun 27 17:46:16.125: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Jun 27 17:46:16.150: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 06/27/23 17:46:16.693
    STEP: Creating a custom resource that should be denied by the webhook 06/27/23 17:46:16.805
    STEP: Creating a custom resource whose deletion would be denied by the webhook 06/27/23 17:46:18.916
    STEP: Updating the custom resource with disallowed data should be denied 06/27/23 17:46:18.981
    STEP: Deleting the custom resource should be denied 06/27/23 17:46:19.091
    STEP: Remove the offending key and value from the custom resource data 06/27/23 17:46:19.129
    STEP: Deleting the updated custom resource should be successful 06/27/23 17:46:19.215
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:46:19.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9157" for this suite. 06/27/23 17:46:19.889
    STEP: Destroying namespace "webhook-9157-markers" for this suite. 06/27/23 17:46:19.931
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:46:20.183
Jun 27 17:46:20.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-probe 06/27/23 17:46:20.185
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:20.275
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:20.306
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Jun 27 17:46:20.420: INFO: Waiting up to 5m0s for pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a" in namespace "container-probe-7484" to be "running and ready"
Jun 27 17:46:20.437: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.858152ms
Jun 27 17:46:20.437: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:46:22.455: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034642857s
Jun 27 17:46:22.455: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:46:24.456: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 4.036071641s
Jun 27 17:46:24.456: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
Jun 27 17:46:26.454: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 6.033993s
Jun 27 17:46:26.454: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
Jun 27 17:46:28.455: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 8.035244507s
Jun 27 17:46:28.455: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
Jun 27 17:46:30.455: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 10.035021128s
Jun 27 17:46:30.455: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
Jun 27 17:46:32.459: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 12.038574725s
Jun 27 17:46:32.459: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
Jun 27 17:46:34.462: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 14.042207276s
Jun 27 17:46:34.462: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
Jun 27 17:46:36.457: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 16.037109414s
Jun 27 17:46:36.457: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
Jun 27 17:46:38.461: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 18.040956995s
Jun 27 17:46:38.461: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
Jun 27 17:46:40.456: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 20.036066183s
Jun 27 17:46:40.456: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
Jun 27 17:46:42.456: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=true. Elapsed: 22.035775233s
Jun 27 17:46:42.456: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = true)
Jun 27 17:46:42.456: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a" satisfied condition "running and ready"
Jun 27 17:46:42.474: INFO: Container started at 2023-06-27 17:46:21 +0000 UTC, pod became ready at 2023-06-27 17:46:40 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 27 17:46:42.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-7484" for this suite. 06/27/23 17:46:42.506
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":274,"skipped":5265,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.358 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:46:20.183
    Jun 27 17:46:20.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-probe 06/27/23 17:46:20.185
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:20.275
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:20.306
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Jun 27 17:46:20.420: INFO: Waiting up to 5m0s for pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a" in namespace "container-probe-7484" to be "running and ready"
    Jun 27 17:46:20.437: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 16.858152ms
    Jun 27 17:46:20.437: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:46:22.455: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034642857s
    Jun 27 17:46:22.455: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:46:24.456: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 4.036071641s
    Jun 27 17:46:24.456: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
    Jun 27 17:46:26.454: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 6.033993s
    Jun 27 17:46:26.454: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
    Jun 27 17:46:28.455: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 8.035244507s
    Jun 27 17:46:28.455: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
    Jun 27 17:46:30.455: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 10.035021128s
    Jun 27 17:46:30.455: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
    Jun 27 17:46:32.459: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 12.038574725s
    Jun 27 17:46:32.459: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
    Jun 27 17:46:34.462: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 14.042207276s
    Jun 27 17:46:34.462: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
    Jun 27 17:46:36.457: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 16.037109414s
    Jun 27 17:46:36.457: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
    Jun 27 17:46:38.461: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 18.040956995s
    Jun 27 17:46:38.461: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
    Jun 27 17:46:40.456: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=false. Elapsed: 20.036066183s
    Jun 27 17:46:40.456: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = false)
    Jun 27 17:46:42.456: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a": Phase="Running", Reason="", readiness=true. Elapsed: 22.035775233s
    Jun 27 17:46:42.456: INFO: The phase of Pod test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a is Running (Ready = true)
    Jun 27 17:46:42.456: INFO: Pod "test-webserver-a0750943-0765-44a1-9e47-bdb10f543f6a" satisfied condition "running and ready"
    Jun 27 17:46:42.474: INFO: Container started at 2023-06-27 17:46:21 +0000 UTC, pod became ready at 2023-06-27 17:46:40 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 27 17:46:42.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-7484" for this suite. 06/27/23 17:46:42.506
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:46:42.547
Jun 27 17:46:42.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 17:46:42.55
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:42.601
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:42.629
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Jun 27 17:46:42.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:46:49.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-7921" for this suite. 06/27/23 17:46:49.919
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":275,"skipped":5272,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.408 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:46:42.547
    Jun 27 17:46:42.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename custom-resource-definition 06/27/23 17:46:42.55
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:42.601
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:42.629
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Jun 27 17:46:42.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:46:49.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-7921" for this suite. 06/27/23 17:46:49.919
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:46:49.958
Jun 27 17:46:49.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename certificates 06/27/23 17:46:49.961
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:50.027
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:50.053
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 06/27/23 17:46:50.631
STEP: getting /apis/certificates.k8s.io 06/27/23 17:46:50.659
STEP: getting /apis/certificates.k8s.io/v1 06/27/23 17:46:50.671
STEP: creating 06/27/23 17:46:50.691
STEP: getting 06/27/23 17:46:50.765
STEP: listing 06/27/23 17:46:50.793
STEP: watching 06/27/23 17:46:50.84
Jun 27 17:46:50.840: INFO: starting watch
STEP: patching 06/27/23 17:46:50.857
STEP: updating 06/27/23 17:46:50.889
Jun 27 17:46:50.913: INFO: waiting for watch events with expected annotations
Jun 27 17:46:50.913: INFO: saw patched and updated annotations
STEP: getting /approval 06/27/23 17:46:50.913
STEP: patching /approval 06/27/23 17:46:50.936
STEP: updating /approval 06/27/23 17:46:50.978
STEP: getting /status 06/27/23 17:46:51.043
STEP: patching /status 06/27/23 17:46:51.063
STEP: updating /status 06/27/23 17:46:51.099
STEP: deleting 06/27/23 17:46:51.135
STEP: deleting a collection 06/27/23 17:46:51.206
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:46:51.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-4046" for this suite. 06/27/23 17:46:51.331
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":276,"skipped":5288,"failed":0}
------------------------------
â€¢ [1.405 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:46:49.958
    Jun 27 17:46:49.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename certificates 06/27/23 17:46:49.961
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:50.027
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:50.053
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 06/27/23 17:46:50.631
    STEP: getting /apis/certificates.k8s.io 06/27/23 17:46:50.659
    STEP: getting /apis/certificates.k8s.io/v1 06/27/23 17:46:50.671
    STEP: creating 06/27/23 17:46:50.691
    STEP: getting 06/27/23 17:46:50.765
    STEP: listing 06/27/23 17:46:50.793
    STEP: watching 06/27/23 17:46:50.84
    Jun 27 17:46:50.840: INFO: starting watch
    STEP: patching 06/27/23 17:46:50.857
    STEP: updating 06/27/23 17:46:50.889
    Jun 27 17:46:50.913: INFO: waiting for watch events with expected annotations
    Jun 27 17:46:50.913: INFO: saw patched and updated annotations
    STEP: getting /approval 06/27/23 17:46:50.913
    STEP: patching /approval 06/27/23 17:46:50.936
    STEP: updating /approval 06/27/23 17:46:50.978
    STEP: getting /status 06/27/23 17:46:51.043
    STEP: patching /status 06/27/23 17:46:51.063
    STEP: updating /status 06/27/23 17:46:51.099
    STEP: deleting 06/27/23 17:46:51.135
    STEP: deleting a collection 06/27/23 17:46:51.206
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:46:51.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-4046" for this suite. 06/27/23 17:46:51.331
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:46:51.365
Jun 27 17:46:51.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename csistoragecapacity 06/27/23 17:46:51.367
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:51.452
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:51.477
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 06/27/23 17:46:51.503
STEP: getting /apis/storage.k8s.io 06/27/23 17:46:51.523
STEP: getting /apis/storage.k8s.io/v1 06/27/23 17:46:51.536
STEP: creating 06/27/23 17:46:51.548
STEP: watching 06/27/23 17:46:51.656
Jun 27 17:46:51.657: INFO: starting watch
STEP: getting 06/27/23 17:46:51.702
STEP: listing in namespace 06/27/23 17:46:51.718
STEP: listing across namespaces 06/27/23 17:46:51.741
STEP: patching 06/27/23 17:46:51.758
STEP: updating 06/27/23 17:46:51.781
Jun 27 17:46:51.806: INFO: waiting for watch events with expected annotations in namespace
Jun 27 17:46:51.807: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 06/27/23 17:46:51.807
STEP: deleting a collection 06/27/23 17:46:51.864
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Jun 27 17:46:51.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-2425" for this suite. 06/27/23 17:46:51.96
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":277,"skipped":5288,"failed":0}
------------------------------
â€¢ [0.645 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:46:51.365
    Jun 27 17:46:51.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename csistoragecapacity 06/27/23 17:46:51.367
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:51.452
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:51.477
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 06/27/23 17:46:51.503
    STEP: getting /apis/storage.k8s.io 06/27/23 17:46:51.523
    STEP: getting /apis/storage.k8s.io/v1 06/27/23 17:46:51.536
    STEP: creating 06/27/23 17:46:51.548
    STEP: watching 06/27/23 17:46:51.656
    Jun 27 17:46:51.657: INFO: starting watch
    STEP: getting 06/27/23 17:46:51.702
    STEP: listing in namespace 06/27/23 17:46:51.718
    STEP: listing across namespaces 06/27/23 17:46:51.741
    STEP: patching 06/27/23 17:46:51.758
    STEP: updating 06/27/23 17:46:51.781
    Jun 27 17:46:51.806: INFO: waiting for watch events with expected annotations in namespace
    Jun 27 17:46:51.807: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 06/27/23 17:46:51.807
    STEP: deleting a collection 06/27/23 17:46:51.864
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Jun 27 17:46:51.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-2425" for this suite. 06/27/23 17:46:51.96
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:46:52.015
Jun 27 17:46:52.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:46:52.017
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:52.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:52.098
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 06/27/23 17:46:52.123
Jun 27 17:46:52.195: INFO: Waiting up to 5m0s for pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc" in namespace "projected-1062" to be "running and ready"
Jun 27 17:46:52.212: INFO: Pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.429201ms
Jun 27 17:46:52.212: INFO: The phase of Pod labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:46:54.233: INFO: Pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038130757s
Jun 27 17:46:54.234: INFO: The phase of Pod labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:46:56.230: INFO: Pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc": Phase="Running", Reason="", readiness=true. Elapsed: 4.035154793s
Jun 27 17:46:56.231: INFO: The phase of Pod labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc is Running (Ready = true)
Jun 27 17:46:56.231: INFO: Pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc" satisfied condition "running and ready"
Jun 27 17:46:56.927: INFO: Successfully updated pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 17:46:59.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1062" for this suite. 06/27/23 17:46:59.043
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":278,"skipped":5293,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.055 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:46:52.015
    Jun 27 17:46:52.015: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:46:52.017
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:52.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:52.098
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 06/27/23 17:46:52.123
    Jun 27 17:46:52.195: INFO: Waiting up to 5m0s for pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc" in namespace "projected-1062" to be "running and ready"
    Jun 27 17:46:52.212: INFO: Pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc": Phase="Pending", Reason="", readiness=false. Elapsed: 16.429201ms
    Jun 27 17:46:52.212: INFO: The phase of Pod labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:46:54.233: INFO: Pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038130757s
    Jun 27 17:46:54.234: INFO: The phase of Pod labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:46:56.230: INFO: Pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc": Phase="Running", Reason="", readiness=true. Elapsed: 4.035154793s
    Jun 27 17:46:56.231: INFO: The phase of Pod labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc is Running (Ready = true)
    Jun 27 17:46:56.231: INFO: Pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc" satisfied condition "running and ready"
    Jun 27 17:46:56.927: INFO: Successfully updated pod "labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 17:46:59.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1062" for this suite. 06/27/23 17:46:59.043
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:46:59.072
Jun 27 17:46:59.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sched-pred 06/27/23 17:46:59.075
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:59.131
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:59.218
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Jun 27 17:46:59.255: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Jun 27 17:46:59.319: INFO: Waiting for terminating namespaces to be deleted...
Jun 27 17:46:59.365: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.89 before test
Jun 27 17:46:59.442: INFO: calico-node-vplt6 from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 17:46:59.442: INFO: calico-typha-657df678d7-65kdh from calico-system started at 2023-06-27 13:54:48 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container calico-typha ready: true, restart count 0
Jun 27 17:46:59.442: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
Jun 27 17:46:59.442: INFO: ibm-keepalived-watcher-9rf8d from kube-system started at 2023-06-27 13:53:32 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 17:46:59.442: INFO: ibm-master-proxy-static-10.113.180.89 from kube-system started at 2023-06-27 13:53:30 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 17:46:59.442: INFO: 	Container pause ready: true, restart count 0
Jun 27 17:46:59.442: INFO: ibmcloud-block-storage-driver-5ml2s from kube-system started at 2023-06-27 13:53:43 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 17:46:59.442: INFO: vpn-546f6bf578-7ks4f from kube-system started at 2023-06-27 14:06:39 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container vpn ready: true, restart count 0
Jun 27 17:46:59.442: INFO: tuned-mwm8w from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container tuned ready: true, restart count 0
Jun 27 17:46:59.442: INFO: csi-snapshot-controller-5694c47cbb-9pvhd from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 27 17:46:59.442: INFO: csi-snapshot-webhook-5d7cc7f6cb-lwgl9 from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container webhook ready: true, restart count 0
Jun 27 17:46:59.442: INFO: console-6c8658586b-jlcfh from openshift-console started at 2023-06-27 14:03:39 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container console ready: true, restart count 0
Jun 27 17:46:59.442: INFO: downloads-57bd479866-2fpqm from openshift-console started at 2023-06-27 13:56:16 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container download-server ready: true, restart count 0
Jun 27 17:46:59.442: INFO: dns-default-z76hf from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container dns ready: true, restart count 0
Jun 27 17:46:59.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.442: INFO: node-resolver-cwltw from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 17:46:59.442: INFO: image-registry-7f546fc5bb-n7w67 from openshift-image-registry started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container registry ready: true, restart count 0
Jun 27 17:46:59.442: INFO: node-ca-d4q6s from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 17:46:59.442: INFO: registry-pvc-permissions-4bk2b from openshift-image-registry started at 2023-06-27 14:02:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container pvc-permissions ready: false, restart count 0
Jun 27 17:46:59.442: INFO: ingress-canary-bmwtq from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 17:46:59.442: INFO: router-default-7f97cd5c5f-m5pcx from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container router ready: true, restart count 0
Jun 27 17:46:59.442: INFO: openshift-kube-proxy-b4224 from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 17:46:59.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.442: INFO: certified-operators-799hx from openshift-marketplace started at 2023-06-27 16:44:15 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.442: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 17:46:59.442: INFO: community-operators-9jptm from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 17:46:59.443: INFO: redhat-marketplace-qzk6b from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 17:46:59.443: INFO: redhat-operators-jqjb9 from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container registry-server ready: true, restart count 0
Jun 27 17:46:59.443: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-06-27 14:02:16 +0000 UTC (6 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container alertmanager ready: true, restart count 1
Jun 27 17:46:59.443: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: kube-state-metrics-685444f59b-5dlq4 from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container kube-state-metrics ready: true, restart count 0
Jun 27 17:46:59.443: INFO: node-exporter-lqlws from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 17:46:59.443: INFO: openshift-state-metrics-6d79d8c586-9mgzr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container openshift-state-metrics ready: true, restart count 0
Jun 27 17:46:59.443: INFO: prometheus-adapter-648f68fcc-h27rt from openshift-monitoring started at 2023-06-27 14:01:39 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 27 17:46:59.443: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-06-27 14:01:59 +0000 UTC (6 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container prometheus ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 27 17:46:59.443: INFO: prometheus-operator-68dfcc5c8-kn6tn from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container prometheus-operator ready: true, restart count 0
Jun 27 17:46:59.443: INFO: prometheus-operator-admission-webhook-6c667b594b-bds22 from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jun 27 17:46:59.443: INFO: telemeter-client-7dc8fdddc8-kpccw from openshift-monitoring started at 2023-06-27 14:00:33 +0000 UTC (3 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container reload ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container telemeter-client ready: true, restart count 0
Jun 27 17:46:59.443: INFO: thanos-querier-754f675f77-6nf9l from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container thanos-query ready: true, restart count 0
Jun 27 17:46:59.443: INFO: multus-7zznf from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 17:46:59.443: INFO: multus-additional-cni-plugins-pn2hb from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 17:46:59.443: INFO: multus-admission-controller-754d449d79-strmv from openshift-multus started at 2023-06-27 16:44:18 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 27 17:46:59.443: INFO: network-metrics-daemon-p8dh4 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 17:46:59.443: INFO: network-check-target-7qghz from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 17:46:59.443: INFO: packageserver-8d78bf5dd-q8ct5 from openshift-operator-lifecycle-manager started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container packageserver ready: true, restart count 0
Jun 27 17:46:59.443: INFO: sonobuoy from sonobuoy started at 2023-06-27 16:21:42 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Jun 27 17:46:59.443: INFO: sonobuoy-e2e-job-0e516cd149fa4871 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container e2e ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 17:46:59.443: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 27 17:46:59.443: INFO: tigera-operator-687c49f5c8-qcfmm from tigera-operator started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.443: INFO: 	Container tigera-operator ready: true, restart count 0
Jun 27 17:46:59.443: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.90 before test
Jun 27 17:46:59.502: INFO: calico-node-6j4xp from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 17:46:59.502: INFO: ibm-keepalived-watcher-bpqvm from kube-system started at 2023-06-27 13:53:26 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 17:46:59.502: INFO: ibm-master-proxy-static-10.113.180.90 from kube-system started at 2023-06-27 13:53:22 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 17:46:59.502: INFO: 	Container pause ready: true, restart count 0
Jun 27 17:46:59.502: INFO: ibmcloud-block-storage-driver-jb9wr from kube-system started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 17:46:59.502: INFO: tuned-md8mr from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container tuned ready: true, restart count 0
Jun 27 17:46:59.502: INFO: dns-default-q6q9l from openshift-dns started at 2023-06-27 16:44:33 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container dns ready: true, restart count 0
Jun 27 17:46:59.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.502: INFO: node-resolver-dtvx7 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 17:46:59.502: INFO: node-ca-8pgp8 from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 17:46:59.502: INFO: ingress-canary-hfzkr from openshift-ingress-canary started at 2023-06-27 16:44:33 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 17:46:59.502: INFO: openshift-kube-proxy-bf9cj from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 17:46:59.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.502: INFO: node-exporter-5vgpm from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.502: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 17:46:59.502: INFO: multus-additional-cni-plugins-qszdk from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 17:46:59.502: INFO: multus-skrpb from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 17:46:59.502: INFO: network-metrics-daemon-4jmw9 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.502: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 17:46:59.502: INFO: network-check-target-zmtm2 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 17:46:59.502: INFO: collect-profiles-28131435-prxkn from openshift-operator-lifecycle-manager started at 2023-06-27 17:15:00 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 27 17:46:59.502: INFO: collect-profiles-28131450-glnjt from openshift-operator-lifecycle-manager started at 2023-06-27 17:30:00 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 27 17:46:59.502: INFO: collect-profiles-28131465-4lwc8 from openshift-operator-lifecycle-manager started at 2023-06-27 17:45:00 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container collect-profiles ready: false, restart count 0
Jun 27 17:46:59.502: INFO: labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc from projected-1062 started at 2023-06-27 17:46:52 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container client-container ready: true, restart count 0
Jun 27 17:46:59.502: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.502: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 17:46:59.502: INFO: 	Container systemd-logs ready: true, restart count 0
Jun 27 17:46:59.502: INFO: 
Logging pods the apiserver thinks is on node 10.113.180.96 before test
Jun 27 17:46:59.595: INFO: calico-kube-controllers-79f474fb8-m45qw from calico-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container calico-kube-controllers ready: true, restart count 0
Jun 27 17:46:59.595: INFO: calico-node-z28vk from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container calico-node ready: true, restart count 0
Jun 27 17:46:59.595: INFO: calico-typha-657df678d7-4ch2t from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container calico-typha ready: true, restart count 0
Jun 27 17:46:59.595: INFO: managed-storage-validation-webhooks-665764b6b4-r77pv from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 17:46:59.595: INFO: managed-storage-validation-webhooks-665764b6b4-v4g8s from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 17:46:59.595: INFO: managed-storage-validation-webhooks-665764b6b4-vhqr7 from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
Jun 27 17:46:59.595: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-tnr88 from ibm-system started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
Jun 27 17:46:59.595: INFO: ibm-file-plugin-84cc66cdb-97nzr from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
Jun 27 17:46:59.595: INFO: ibm-keepalived-watcher-h8p9q from kube-system started at 2023-06-27 13:53:48 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container keepalived-watcher ready: true, restart count 0
Jun 27 17:46:59.595: INFO: ibm-master-proxy-static-10.113.180.96 from kube-system started at 2023-06-27 13:53:45 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
Jun 27 17:46:59.595: INFO: 	Container pause ready: true, restart count 0
Jun 27 17:46:59.595: INFO: ibm-storage-metrics-agent-746d76fb56-nvvw9 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
Jun 27 17:46:59.595: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
Jun 27 17:46:59.595: INFO: ibm-storage-watcher-676b4ddbc6-dz54r from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
Jun 27 17:46:59.595: INFO: ibmcloud-block-storage-driver-pkhxd from kube-system started at 2023-06-27 13:53:53 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
Jun 27 17:46:59.595: INFO: ibmcloud-block-storage-plugin-5f677b8577-8f7s7 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
Jun 27 17:46:59.595: INFO: cluster-node-tuning-operator-848d57c5bb-7ln5t from openshift-cluster-node-tuning-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
Jun 27 17:46:59.595: INFO: tuned-kk4gn from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.595: INFO: 	Container tuned ready: true, restart count 0
Jun 27 17:46:59.595: INFO: cluster-samples-operator-769ddfc8cc-88z62 from openshift-cluster-samples-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container cluster-samples-operator ready: true, restart count 0
Jun 27 17:46:59.596: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
Jun 27 17:46:59.596: INFO: cluster-storage-operator-5fc995c76b-jhmzr from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container cluster-storage-operator ready: true, restart count 1
Jun 27 17:46:59.596: INFO: csi-snapshot-controller-5694c47cbb-qfvbs from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container snapshot-controller ready: true, restart count 0
Jun 27 17:46:59.596: INFO: csi-snapshot-controller-operator-64ff6856c8-l29px from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
Jun 27 17:46:59.596: INFO: csi-snapshot-webhook-5d7cc7f6cb-gzt6x from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container webhook ready: true, restart count 0
Jun 27 17:46:59.596: INFO: console-operator-5694fff656-5wp7b from openshift-console-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container console-operator ready: true, restart count 1
Jun 27 17:46:59.596: INFO: 	Container conversion-webhook-server ready: true, restart count 2
Jun 27 17:46:59.596: INFO: console-6c8658586b-rc55z from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container console ready: true, restart count 0
Jun 27 17:46:59.596: INFO: downloads-57bd479866-8bj2l from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container download-server ready: true, restart count 0
Jun 27 17:46:59.596: INFO: dns-operator-b64d4b4c7-5p5hs from openshift-dns-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container dns-operator ready: true, restart count 0
Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.596: INFO: dns-default-q8pl2 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container dns ready: true, restart count 0
Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.596: INFO: node-resolver-nbx64 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container dns-node-resolver ready: true, restart count 0
Jun 27 17:46:59.596: INFO: cluster-image-registry-operator-55bdbc85cd-vpldl from openshift-image-registry started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
Jun 27 17:46:59.596: INFO: node-ca-tbfkf from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container node-ca ready: true, restart count 0
Jun 27 17:46:59.596: INFO: ingress-canary-mtqfl from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
Jun 27 17:46:59.596: INFO: ingress-operator-5c457b57b4-67rvh from openshift-ingress-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container ingress-operator ready: true, restart count 0
Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.596: INFO: router-default-7f97cd5c5f-n56jj from openshift-ingress started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container router ready: true, restart count 0
Jun 27 17:46:59.596: INFO: insights-operator-8f4d6c949-n2nlt from openshift-insights started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container insights-operator ready: true, restart count 1
Jun 27 17:46:59.596: INFO: openshift-kube-proxy-mpv6g from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container kube-proxy ready: true, restart count 0
Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.596: INFO: kube-storage-version-migrator-operator-59f4c7c696-jq8gm from openshift-kube-storage-version-migrator-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
Jun 27 17:46:59.596: INFO: migrator-6795cdbdb7-qsfqv from openshift-kube-storage-version-migrator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container migrator ready: true, restart count 0
Jun 27 17:46:59.596: INFO: marketplace-operator-57bd87cfb8-5hhm8 from openshift-marketplace started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container marketplace-operator ready: true, restart count 0
Jun 27 17:46:59.596: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-06-27 16:44:14 +0000 UTC (6 container statuses recorded)
Jun 27 17:46:59.596: INFO: 	Container alertmanager ready: true, restart count 1
Jun 27 17:46:59.596: INFO: 	Container alertmanager-proxy ready: true, restart count 0
Jun 27 17:46:59.596: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: cluster-monitoring-operator-644d445879-l5jj8 from openshift-monitoring started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: node-exporter-l5dxr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container node-exporter ready: true, restart count 0
Jun 27 17:46:59.597: INFO: prometheus-adapter-648f68fcc-mpvkt from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container prometheus-adapter ready: true, restart count 0
Jun 27 17:46:59.597: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-06-27 16:44:13 +0000 UTC (6 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container config-reloader ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container prometheus ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container prometheus-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container thanos-sidecar ready: true, restart count 0
Jun 27 17:46:59.597: INFO: prometheus-operator-admission-webhook-6c667b594b-c9dw4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
Jun 27 17:46:59.597: INFO: thanos-querier-754f675f77-gs6k4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (6 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container oauth-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container prom-label-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container thanos-query ready: true, restart count 0
Jun 27 17:46:59.597: INFO: multus-additional-cni-plugins-xqsgs from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
Jun 27 17:46:59.597: INFO: multus-admission-controller-754d449d79-6cbgx from openshift-multus started at 2023-06-27 16:44:27 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container multus-admission-controller ready: true, restart count 0
Jun 27 17:46:59.597: INFO: multus-kx7qm from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container kube-multus ready: true, restart count 0
Jun 27 17:46:59.597: INFO: network-metrics-daemon-cbrq2 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
Jun 27 17:46:59.597: INFO: 	Container network-metrics-daemon ready: true, restart count 0
Jun 27 17:46:59.597: INFO: network-check-source-6b8766f86f-s8s7j from openshift-network-diagnostics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container check-endpoints ready: true, restart count 0
Jun 27 17:46:59.597: INFO: network-check-target-6lv98 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container network-check-target-container ready: true, restart count 0
Jun 27 17:46:59.597: INFO: network-operator-ffb9884c5-25r42 from openshift-network-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container network-operator ready: true, restart count 0
Jun 27 17:46:59.597: INFO: catalog-operator-58898485cd-qjmms from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.597: INFO: 	Container catalog-operator ready: true, restart count 0
Jun 27 17:46:59.598: INFO: olm-operator-7959cfcd9f-dt77g from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.598: INFO: 	Container olm-operator ready: true, restart count 0
Jun 27 17:46:59.598: INFO: package-server-manager-f7b6c466f-9bbzw from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.598: INFO: 	Container package-server-manager ready: true, restart count 0
Jun 27 17:46:59.598: INFO: packageserver-8d78bf5dd-r9g9t from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.598: INFO: 	Container packageserver ready: true, restart count 0
Jun 27 17:46:59.598: INFO: metrics-6d46d44d8f-jbnwq from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.598: INFO: 	Container metrics ready: true, restart count 3
Jun 27 17:46:59.598: INFO: push-gateway-5465b544cc-f8v8q from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.598: INFO: 	Container push-gateway ready: true, restart count 0
Jun 27 17:46:59.598: INFO: service-ca-operator-74dd8bfc8-cl4bp from openshift-service-ca-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.598: INFO: 	Container service-ca-operator ready: true, restart count 1
Jun 27 17:46:59.598: INFO: service-ca-6f86485857-drj9g from openshift-service-ca started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
Jun 27 17:46:59.598: INFO: 	Container service-ca-controller ready: false, restart count 0
Jun 27 17:46:59.598: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
Jun 27 17:46:59.598: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Jun 27 17:46:59.598: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node 10.113.180.89 06/27/23 17:46:59.8
STEP: verifying the node has the label node 10.113.180.90 06/27/23 17:46:59.86
STEP: verifying the node has the label node 10.113.180.96 06/27/23 17:46:59.929
Jun 27 17:47:00.063: INFO: Pod calico-kube-controllers-79f474fb8-m45qw requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.063: INFO: Pod calico-node-6j4xp requesting resource cpu=250m on Node 10.113.180.90
Jun 27 17:47:00.063: INFO: Pod calico-node-vplt6 requesting resource cpu=250m on Node 10.113.180.89
Jun 27 17:47:00.063: INFO: Pod calico-node-z28vk requesting resource cpu=250m on Node 10.113.180.96
Jun 27 17:47:00.063: INFO: Pod calico-typha-657df678d7-4ch2t requesting resource cpu=250m on Node 10.113.180.96
Jun 27 17:47:00.063: INFO: Pod calico-typha-657df678d7-65kdh requesting resource cpu=250m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod managed-storage-validation-webhooks-665764b6b4-r77pv requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod managed-storage-validation-webhooks-665764b6b4-v4g8s requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod managed-storage-validation-webhooks-665764b6b4-vhqr7 requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp requesting resource cpu=5m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod ibm-cloud-provider-ip-5-10-126-211-5c8746498b-tnr88 requesting resource cpu=5m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ibm-file-plugin-84cc66cdb-97nzr requesting resource cpu=50m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ibm-keepalived-watcher-9rf8d requesting resource cpu=5m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod ibm-keepalived-watcher-bpqvm requesting resource cpu=5m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod ibm-keepalived-watcher-h8p9q requesting resource cpu=5m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ibm-master-proxy-static-10.113.180.89 requesting resource cpu=26m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod ibm-master-proxy-static-10.113.180.90 requesting resource cpu=26m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod ibm-master-proxy-static-10.113.180.96 requesting resource cpu=26m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ibm-storage-metrics-agent-746d76fb56-nvvw9 requesting resource cpu=60m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ibm-storage-watcher-676b4ddbc6-dz54r requesting resource cpu=50m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ibmcloud-block-storage-driver-5ml2s requesting resource cpu=50m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod ibmcloud-block-storage-driver-jb9wr requesting resource cpu=50m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod ibmcloud-block-storage-driver-pkhxd requesting resource cpu=50m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ibmcloud-block-storage-plugin-5f677b8577-8f7s7 requesting resource cpu=50m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod vpn-546f6bf578-7ks4f requesting resource cpu=5m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod cluster-node-tuning-operator-848d57c5bb-7ln5t requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod tuned-kk4gn requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod tuned-md8mr requesting resource cpu=10m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod tuned-mwm8w requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod cluster-samples-operator-769ddfc8cc-88z62 requesting resource cpu=20m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod cluster-storage-operator-5fc995c76b-jhmzr requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod csi-snapshot-controller-5694c47cbb-9pvhd requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod csi-snapshot-controller-5694c47cbb-qfvbs requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod csi-snapshot-controller-operator-64ff6856c8-l29px requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod csi-snapshot-webhook-5d7cc7f6cb-gzt6x requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod csi-snapshot-webhook-5d7cc7f6cb-lwgl9 requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod console-operator-5694fff656-5wp7b requesting resource cpu=20m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod console-6c8658586b-jlcfh requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod console-6c8658586b-rc55z requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod downloads-57bd479866-2fpqm requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod downloads-57bd479866-8bj2l requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod dns-operator-b64d4b4c7-5p5hs requesting resource cpu=20m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod dns-default-q6q9l requesting resource cpu=60m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod dns-default-q8pl2 requesting resource cpu=60m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod dns-default-z76hf requesting resource cpu=60m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod node-resolver-cwltw requesting resource cpu=5m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod node-resolver-dtvx7 requesting resource cpu=5m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod node-resolver-nbx64 requesting resource cpu=5m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod cluster-image-registry-operator-55bdbc85cd-vpldl requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod image-registry-7f546fc5bb-n7w67 requesting resource cpu=100m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod node-ca-8pgp8 requesting resource cpu=10m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod node-ca-d4q6s requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod node-ca-tbfkf requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ingress-canary-bmwtq requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod ingress-canary-hfzkr requesting resource cpu=10m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod ingress-canary-mtqfl requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod ingress-operator-5c457b57b4-67rvh requesting resource cpu=20m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod router-default-7f97cd5c5f-m5pcx requesting resource cpu=100m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod router-default-7f97cd5c5f-n56jj requesting resource cpu=100m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod insights-operator-8f4d6c949-n2nlt requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod openshift-kube-proxy-b4224 requesting resource cpu=110m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod openshift-kube-proxy-bf9cj requesting resource cpu=110m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod openshift-kube-proxy-mpv6g requesting resource cpu=110m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod kube-storage-version-migrator-operator-59f4c7c696-jq8gm requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod migrator-6795cdbdb7-qsfqv requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod certified-operators-799hx requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod community-operators-9jptm requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod marketplace-operator-57bd87cfb8-5hhm8 requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod redhat-marketplace-qzk6b requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod redhat-operators-jqjb9 requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod cluster-monitoring-operator-644d445879-l5jj8 requesting resource cpu=11m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod kube-state-metrics-685444f59b-5dlq4 requesting resource cpu=4m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod node-exporter-5vgpm requesting resource cpu=9m on Node 10.113.180.90
Jun 27 17:47:00.064: INFO: Pod node-exporter-l5dxr requesting resource cpu=9m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod node-exporter-lqlws requesting resource cpu=9m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod openshift-state-metrics-6d79d8c586-9mgzr requesting resource cpu=3m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod prometheus-adapter-648f68fcc-h27rt requesting resource cpu=1m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod prometheus-adapter-648f68fcc-mpvkt requesting resource cpu=1m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.113.180.96
Jun 27 17:47:00.064: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod prometheus-operator-68dfcc5c8-kn6tn requesting resource cpu=6m on Node 10.113.180.89
Jun 27 17:47:00.064: INFO: Pod prometheus-operator-admission-webhook-6c667b594b-bds22 requesting resource cpu=5m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod prometheus-operator-admission-webhook-6c667b594b-c9dw4 requesting resource cpu=5m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod telemeter-client-7dc8fdddc8-kpccw requesting resource cpu=3m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod thanos-querier-754f675f77-6nf9l requesting resource cpu=15m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod thanos-querier-754f675f77-gs6k4 requesting resource cpu=15m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod multus-7zznf requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod multus-additional-cni-plugins-pn2hb requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod multus-additional-cni-plugins-qszdk requesting resource cpu=10m on Node 10.113.180.90
Jun 27 17:47:00.065: INFO: Pod multus-additional-cni-plugins-xqsgs requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod multus-admission-controller-754d449d79-6cbgx requesting resource cpu=20m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod multus-admission-controller-754d449d79-strmv requesting resource cpu=20m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod multus-kx7qm requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod multus-skrpb requesting resource cpu=10m on Node 10.113.180.90
Jun 27 17:47:00.065: INFO: Pod network-metrics-daemon-4jmw9 requesting resource cpu=20m on Node 10.113.180.90
Jun 27 17:47:00.065: INFO: Pod network-metrics-daemon-cbrq2 requesting resource cpu=20m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod network-metrics-daemon-p8dh4 requesting resource cpu=20m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod network-check-source-6b8766f86f-s8s7j requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod network-check-target-6lv98 requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod network-check-target-7qghz requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod network-check-target-zmtm2 requesting resource cpu=10m on Node 10.113.180.90
Jun 27 17:47:00.065: INFO: Pod network-operator-ffb9884c5-25r42 requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod catalog-operator-58898485cd-qjmms requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod olm-operator-7959cfcd9f-dt77g requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod package-server-manager-f7b6c466f-9bbzw requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod packageserver-8d78bf5dd-q8ct5 requesting resource cpu=10m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod packageserver-8d78bf5dd-r9g9t requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod metrics-6d46d44d8f-jbnwq requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod push-gateway-5465b544cc-f8v8q requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod service-ca-operator-74dd8bfc8-cl4bp requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod service-ca-6f86485857-drj9g requesting resource cpu=10m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc requesting resource cpu=0m on Node 10.113.180.90
Jun 27 17:47:00.065: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod sonobuoy-e2e-job-0e516cd149fa4871 requesting resource cpu=0m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj requesting resource cpu=0m on Node 10.113.180.90
Jun 27 17:47:00.065: INFO: Pod sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf requesting resource cpu=0m on Node 10.113.180.89
Jun 27 17:47:00.065: INFO: Pod sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 requesting resource cpu=0m on Node 10.113.180.96
Jun 27 17:47:00.065: INFO: Pod tigera-operator-687c49f5c8-qcfmm requesting resource cpu=100m on Node 10.113.180.89
STEP: Starting Pods to consume most of the cluster CPU. 06/27/23 17:47:00.065
Jun 27 17:47:00.065: INFO: Creating a pod which consumes cpu=1766m on Node 10.113.180.89
Jun 27 17:47:00.195: INFO: Creating a pod which consumes cpu=2320m on Node 10.113.180.90
Jun 27 17:47:00.254: INFO: Creating a pod which consumes cpu=1591m on Node 10.113.180.96
Jun 27 17:47:00.353: INFO: Waiting up to 5m0s for pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229" in namespace "sched-pred-5812" to be "running"
Jun 27 17:47:00.403: INFO: Pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229": Phase="Pending", Reason="", readiness=false. Elapsed: 49.283043ms
Jun 27 17:47:02.455: INFO: Pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229": Phase="Pending", Reason="", readiness=false. Elapsed: 2.101852331s
Jun 27 17:47:04.422: INFO: Pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229": Phase="Running", Reason="", readiness=true. Elapsed: 4.068421653s
Jun 27 17:47:04.422: INFO: Pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229" satisfied condition "running"
Jun 27 17:47:04.422: INFO: Waiting up to 5m0s for pod "filler-pod-60277d43-6509-4112-bb49-56c0259951dc" in namespace "sched-pred-5812" to be "running"
Jun 27 17:47:04.438: INFO: Pod "filler-pod-60277d43-6509-4112-bb49-56c0259951dc": Phase="Running", Reason="", readiness=true. Elapsed: 15.345847ms
Jun 27 17:47:04.438: INFO: Pod "filler-pod-60277d43-6509-4112-bb49-56c0259951dc" satisfied condition "running"
Jun 27 17:47:04.438: INFO: Waiting up to 5m0s for pod "filler-pod-f358b651-cd8e-4437-9599-3adc87293894" in namespace "sched-pred-5812" to be "running"
Jun 27 17:47:04.454: INFO: Pod "filler-pod-f358b651-cd8e-4437-9599-3adc87293894": Phase="Running", Reason="", readiness=true. Elapsed: 16.086936ms
Jun 27 17:47:04.454: INFO: Pod "filler-pod-f358b651-cd8e-4437-9599-3adc87293894" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 06/27/23 17:47:04.454
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c9536277d8763], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5812/filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229 to 10.113.180.89] 06/27/23 17:47:04.474
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c953668f12d88], Reason = [AddedInterface], Message = [Add eth0 [172.30.106.187/32] from k8s-pod-network] 06/27/23 17:47:04.474
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c95367b83dbc0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/27/23 17:47:04.474
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c95368c57258e], Reason = [Created], Message = [Created container filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229] 06/27/23 17:47:04.475
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c95368ee16779], Reason = [Started], Message = [Started container filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229] 06/27/23 17:47:04.475
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c95362de9c840], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5812/filler-pod-60277d43-6509-4112-bb49-56c0259951dc to 10.113.180.90] 06/27/23 17:47:04.475
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c953676e162c6], Reason = [AddedInterface], Message = [Add eth0 [172.30.250.248/32] from k8s-pod-network] 06/27/23 17:47:04.475
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c95368df68700], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/27/23 17:47:04.475
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c953699b2694f], Reason = [Created], Message = [Created container filler-pod-60277d43-6509-4112-bb49-56c0259951dc] 06/27/23 17:47:04.476
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c95369d56b533], Reason = [Started], Message = [Started container filler-pod-60277d43-6509-4112-bb49-56c0259951dc] 06/27/23 17:47:04.476
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c953632eac1a9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5812/filler-pod-f358b651-cd8e-4437-9599-3adc87293894 to 10.113.180.96] 06/27/23 17:47:04.476
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c95366d027da0], Reason = [AddedInterface], Message = [Add eth0 [172.30.60.108/32] from k8s-pod-network] 06/27/23 17:47:04.476
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c953682fb4c68], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/27/23 17:47:04.476
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c953694eaacf4], Reason = [Created], Message = [Created container filler-pod-f358b651-cd8e-4437-9599-3adc87293894] 06/27/23 17:47:04.477
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c95369778fcd0], Reason = [Started], Message = [Started container filler-pod-f358b651-cd8e-4437-9599-3adc87293894] 06/27/23 17:47:04.477
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.176c95372a992ef5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 06/27/23 17:47:04.551
STEP: removing the label node off the node 10.113.180.89 06/27/23 17:47:05.555
STEP: verifying the node doesn't have the label node 06/27/23 17:47:05.644
STEP: removing the label node off the node 10.113.180.90 06/27/23 17:47:05.674
STEP: verifying the node doesn't have the label node 06/27/23 17:47:05.746
STEP: removing the label node off the node 10.113.180.96 06/27/23 17:47:05.787
STEP: verifying the node doesn't have the label node 06/27/23 17:47:05.848
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:47:05.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5812" for this suite. 06/27/23 17:47:05.892
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":279,"skipped":5311,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.850 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:46:59.072
    Jun 27 17:46:59.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sched-pred 06/27/23 17:46:59.075
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:46:59.131
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:46:59.218
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Jun 27 17:46:59.255: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Jun 27 17:46:59.319: INFO: Waiting for terminating namespaces to be deleted...
    Jun 27 17:46:59.365: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.89 before test
    Jun 27 17:46:59.442: INFO: calico-node-vplt6 from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: calico-typha-657df678d7-65kdh from calico-system started at 2023-06-27 13:54:48 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp from ibm-system started at 2023-06-27 14:03:25 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: ibm-keepalived-watcher-9rf8d from kube-system started at 2023-06-27 13:53:32 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: ibm-master-proxy-static-10.113.180.89 from kube-system started at 2023-06-27 13:53:30 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: 	Container pause ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: ibmcloud-block-storage-driver-5ml2s from kube-system started at 2023-06-27 13:53:43 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: vpn-546f6bf578-7ks4f from kube-system started at 2023-06-27 14:06:39 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container vpn ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: tuned-mwm8w from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: csi-snapshot-controller-5694c47cbb-9pvhd from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: csi-snapshot-webhook-5d7cc7f6cb-lwgl9 from openshift-cluster-storage-operator started at 2023-06-27 13:56:06 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container webhook ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: console-6c8658586b-jlcfh from openshift-console started at 2023-06-27 14:03:39 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container console ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: downloads-57bd479866-2fpqm from openshift-console started at 2023-06-27 13:56:16 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container download-server ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: dns-default-z76hf from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container dns ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: node-resolver-cwltw from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: image-registry-7f546fc5bb-n7w67 from openshift-image-registry started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container registry ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: node-ca-d4q6s from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: registry-pvc-permissions-4bk2b from openshift-image-registry started at 2023-06-27 14:02:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container pvc-permissions ready: false, restart count 0
    Jun 27 17:46:59.442: INFO: ingress-canary-bmwtq from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: router-default-7f97cd5c5f-m5pcx from openshift-ingress started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container router ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: openshift-kube-proxy-b4224 from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: certified-operators-799hx from openshift-marketplace started at 2023-06-27 16:44:15 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.442: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 17:46:59.442: INFO: community-operators-9jptm from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: redhat-marketplace-qzk6b from openshift-marketplace started at 2023-06-27 13:58:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: redhat-operators-jqjb9 from openshift-marketplace started at 2023-06-27 16:44:14 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container registry-server ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: alertmanager-main-0 from openshift-monitoring started at 2023-06-27 14:02:16 +0000 UTC (6 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container alertmanager ready: true, restart count 1
    Jun 27 17:46:59.443: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: kube-state-metrics-685444f59b-5dlq4 from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container kube-state-metrics ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: node-exporter-lqlws from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: openshift-state-metrics-6d79d8c586-9mgzr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (3 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container openshift-state-metrics ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: prometheus-adapter-648f68fcc-h27rt from openshift-monitoring started at 2023-06-27 14:01:39 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: prometheus-k8s-1 from openshift-monitoring started at 2023-06-27 14:01:59 +0000 UTC (6 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container prometheus ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: prometheus-operator-68dfcc5c8-kn6tn from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container prometheus-operator ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: prometheus-operator-admission-webhook-6c667b594b-bds22 from openshift-monitoring started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: telemeter-client-7dc8fdddc8-kpccw from openshift-monitoring started at 2023-06-27 14:00:33 +0000 UTC (3 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container reload ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container telemeter-client ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: thanos-querier-754f675f77-6nf9l from openshift-monitoring started at 2023-06-27 14:00:40 +0000 UTC (6 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container thanos-query ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: multus-7zznf from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: multus-additional-cni-plugins-pn2hb from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: multus-admission-controller-754d449d79-strmv from openshift-multus started at 2023-06-27 16:44:18 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: network-metrics-daemon-p8dh4 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: network-check-target-7qghz from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: packageserver-8d78bf5dd-q8ct5 from openshift-operator-lifecycle-manager started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container packageserver ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: sonobuoy from sonobuoy started at 2023-06-27 16:21:42 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: sonobuoy-e2e-job-0e516cd149fa4871 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container e2e ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: tigera-operator-687c49f5c8-qcfmm from tigera-operator started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.443: INFO: 	Container tigera-operator ready: true, restart count 0
    Jun 27 17:46:59.443: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.90 before test
    Jun 27 17:46:59.502: INFO: calico-node-6j4xp from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: ibm-keepalived-watcher-bpqvm from kube-system started at 2023-06-27 13:53:26 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: ibm-master-proxy-static-10.113.180.90 from kube-system started at 2023-06-27 13:53:22 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: 	Container pause ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: ibmcloud-block-storage-driver-jb9wr from kube-system started at 2023-06-27 13:53:36 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: tuned-md8mr from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: dns-default-q6q9l from openshift-dns started at 2023-06-27 16:44:33 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container dns ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: node-resolver-dtvx7 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: node-ca-8pgp8 from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: ingress-canary-hfzkr from openshift-ingress-canary started at 2023-06-27 16:44:33 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: openshift-kube-proxy-bf9cj from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: node-exporter-5vgpm from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: multus-additional-cni-plugins-qszdk from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: multus-skrpb from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: network-metrics-daemon-4jmw9 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: network-check-target-zmtm2 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: collect-profiles-28131435-prxkn from openshift-operator-lifecycle-manager started at 2023-06-27 17:15:00 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container collect-profiles ready: false, restart count 0
    Jun 27 17:46:59.502: INFO: collect-profiles-28131450-glnjt from openshift-operator-lifecycle-manager started at 2023-06-27 17:30:00 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container collect-profiles ready: false, restart count 0
    Jun 27 17:46:59.502: INFO: collect-profiles-28131465-4lwc8 from openshift-operator-lifecycle-manager started at 2023-06-27 17:45:00 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container collect-profiles ready: false, restart count 0
    Jun 27 17:46:59.502: INFO: labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc from projected-1062 started at 2023-06-27 17:46:52 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container client-container ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.502: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: 	Container systemd-logs ready: true, restart count 0
    Jun 27 17:46:59.502: INFO: 
    Logging pods the apiserver thinks is on node 10.113.180.96 before test
    Jun 27 17:46:59.595: INFO: calico-kube-controllers-79f474fb8-m45qw from calico-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container calico-kube-controllers ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: calico-node-z28vk from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container calico-node ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: calico-typha-657df678d7-4ch2t from calico-system started at 2023-06-27 13:54:40 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container calico-typha ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: managed-storage-validation-webhooks-665764b6b4-r77pv from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: managed-storage-validation-webhooks-665764b6b4-v4g8s from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: managed-storage-validation-webhooks-665764b6b4-vhqr7 from ibm-odf-validation-webhook started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container managed-storage-validation-webhooks ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: ibm-cloud-provider-ip-5-10-126-211-5c8746498b-tnr88 from ibm-system started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container ibm-cloud-provider-ip-5-10-126-211 ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: ibm-file-plugin-84cc66cdb-97nzr from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container ibm-file-plugin-container ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: ibm-keepalived-watcher-h8p9q from kube-system started at 2023-06-27 13:53:48 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container keepalived-watcher ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: ibm-master-proxy-static-10.113.180.96 from kube-system started at 2023-06-27 13:53:45 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container ibm-master-proxy-static ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: 	Container pause ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: ibm-storage-metrics-agent-746d76fb56-nvvw9 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container ibm-storage-metrics-agent ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: 	Container storage-secret-sidecar ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: ibm-storage-watcher-676b4ddbc6-dz54r from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container ibm-storage-watcher-container ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: ibmcloud-block-storage-driver-pkhxd from kube-system started at 2023-06-27 13:53:53 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container ibmcloud-block-storage-driver-container ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: ibmcloud-block-storage-plugin-5f677b8577-8f7s7 from kube-system started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container ibmcloud-block-storage-plugin-container ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: cluster-node-tuning-operator-848d57c5bb-7ln5t from openshift-cluster-node-tuning-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container cluster-node-tuning-operator ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: tuned-kk4gn from openshift-cluster-node-tuning-operator started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.595: INFO: 	Container tuned ready: true, restart count 0
    Jun 27 17:46:59.595: INFO: cluster-samples-operator-769ddfc8cc-88z62 from openshift-cluster-samples-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container cluster-samples-operator ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: 	Container cluster-samples-operator-watch ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: cluster-storage-operator-5fc995c76b-jhmzr from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container cluster-storage-operator ready: true, restart count 1
    Jun 27 17:46:59.596: INFO: csi-snapshot-controller-5694c47cbb-qfvbs from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container snapshot-controller ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: csi-snapshot-controller-operator-64ff6856c8-l29px from openshift-cluster-storage-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container csi-snapshot-controller-operator ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: csi-snapshot-webhook-5d7cc7f6cb-gzt6x from openshift-cluster-storage-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container webhook ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: console-operator-5694fff656-5wp7b from openshift-console-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container console-operator ready: true, restart count 1
    Jun 27 17:46:59.596: INFO: 	Container conversion-webhook-server ready: true, restart count 2
    Jun 27 17:46:59.596: INFO: console-6c8658586b-rc55z from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container console ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: downloads-57bd479866-8bj2l from openshift-console started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container download-server ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: dns-operator-b64d4b4c7-5p5hs from openshift-dns-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container dns-operator ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: dns-default-q8pl2 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container dns ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: node-resolver-nbx64 from openshift-dns started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container dns-node-resolver ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: cluster-image-registry-operator-55bdbc85cd-vpldl from openshift-image-registry started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container cluster-image-registry-operator ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: node-ca-tbfkf from openshift-image-registry started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container node-ca ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: ingress-canary-mtqfl from openshift-ingress-canary started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container serve-healthcheck-canary ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: ingress-operator-5c457b57b4-67rvh from openshift-ingress-operator started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container ingress-operator ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: router-default-7f97cd5c5f-n56jj from openshift-ingress started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container router ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: insights-operator-8f4d6c949-n2nlt from openshift-insights started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container insights-operator ready: true, restart count 1
    Jun 27 17:46:59.596: INFO: openshift-kube-proxy-mpv6g from openshift-kube-proxy started at 2023-06-27 13:54:13 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container kube-proxy ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: kube-storage-version-migrator-operator-59f4c7c696-jq8gm from openshift-kube-storage-version-migrator-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container kube-storage-version-migrator-operator ready: true, restart count 1
    Jun 27 17:46:59.596: INFO: migrator-6795cdbdb7-qsfqv from openshift-kube-storage-version-migrator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container migrator ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: marketplace-operator-57bd87cfb8-5hhm8 from openshift-marketplace started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container marketplace-operator ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: alertmanager-main-1 from openshift-monitoring started at 2023-06-27 16:44:14 +0000 UTC (6 container statuses recorded)
    Jun 27 17:46:59.596: INFO: 	Container alertmanager ready: true, restart count 1
    Jun 27 17:46:59.596: INFO: 	Container alertmanager-proxy ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.596: INFO: 	Container kube-rbac-proxy-metric ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: cluster-monitoring-operator-644d445879-l5jj8 from openshift-monitoring started at 2023-06-27 13:55:20 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container cluster-monitoring-operator ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: node-exporter-l5dxr from openshift-monitoring started at 2023-06-27 14:00:27 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container node-exporter ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: prometheus-adapter-648f68fcc-mpvkt from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container prometheus-adapter ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: prometheus-k8s-0 from openshift-monitoring started at 2023-06-27 16:44:13 +0000 UTC (6 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container config-reloader ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy-thanos ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container prometheus ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container prometheus-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container thanos-sidecar ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: prometheus-operator-admission-webhook-6c667b594b-c9dw4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container prometheus-operator-admission-webhook ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: thanos-querier-754f675f77-gs6k4 from openshift-monitoring started at 2023-06-27 16:44:07 +0000 UTC (6 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy-metrics ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy-rules ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container oauth-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container prom-label-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container thanos-query ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: multus-additional-cni-plugins-xqsgs from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container kube-multus-additional-cni-plugins ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: multus-admission-controller-754d449d79-6cbgx from openshift-multus started at 2023-06-27 16:44:27 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container multus-admission-controller ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: multus-kx7qm from openshift-multus started at 2023-06-27 13:54:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container kube-multus ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: network-metrics-daemon-cbrq2 from openshift-multus started at 2023-06-27 13:54:09 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: 	Container network-metrics-daemon ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: network-check-source-6b8766f86f-s8s7j from openshift-network-diagnostics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container check-endpoints ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: network-check-target-6lv98 from openshift-network-diagnostics started at 2023-06-27 13:54:17 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container network-check-target-container ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: network-operator-ffb9884c5-25r42 from openshift-network-operator started at 2023-06-27 16:44:07 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container network-operator ready: true, restart count 0
    Jun 27 17:46:59.597: INFO: catalog-operator-58898485cd-qjmms from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.597: INFO: 	Container catalog-operator ready: true, restart count 0
    Jun 27 17:46:59.598: INFO: olm-operator-7959cfcd9f-dt77g from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.598: INFO: 	Container olm-operator ready: true, restart count 0
    Jun 27 17:46:59.598: INFO: package-server-manager-f7b6c466f-9bbzw from openshift-operator-lifecycle-manager started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.598: INFO: 	Container package-server-manager ready: true, restart count 0
    Jun 27 17:46:59.598: INFO: packageserver-8d78bf5dd-r9g9t from openshift-operator-lifecycle-manager started at 2023-06-27 14:00:03 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.598: INFO: 	Container packageserver ready: true, restart count 0
    Jun 27 17:46:59.598: INFO: metrics-6d46d44d8f-jbnwq from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.598: INFO: 	Container metrics ready: true, restart count 3
    Jun 27 17:46:59.598: INFO: push-gateway-5465b544cc-f8v8q from openshift-roks-metrics started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.598: INFO: 	Container push-gateway ready: true, restart count 0
    Jun 27 17:46:59.598: INFO: service-ca-operator-74dd8bfc8-cl4bp from openshift-service-ca-operator started at 2023-06-27 13:55:20 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.598: INFO: 	Container service-ca-operator ready: true, restart count 1
    Jun 27 17:46:59.598: INFO: service-ca-6f86485857-drj9g from openshift-service-ca started at 2023-06-27 16:44:08 +0000 UTC (1 container statuses recorded)
    Jun 27 17:46:59.598: INFO: 	Container service-ca-controller ready: false, restart count 0
    Jun 27 17:46:59.598: INFO: sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 from sonobuoy started at 2023-06-27 16:21:50 +0000 UTC (2 container statuses recorded)
    Jun 27 17:46:59.598: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Jun 27 17:46:59.598: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node 10.113.180.89 06/27/23 17:46:59.8
    STEP: verifying the node has the label node 10.113.180.90 06/27/23 17:46:59.86
    STEP: verifying the node has the label node 10.113.180.96 06/27/23 17:46:59.929
    Jun 27 17:47:00.063: INFO: Pod calico-kube-controllers-79f474fb8-m45qw requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.063: INFO: Pod calico-node-6j4xp requesting resource cpu=250m on Node 10.113.180.90
    Jun 27 17:47:00.063: INFO: Pod calico-node-vplt6 requesting resource cpu=250m on Node 10.113.180.89
    Jun 27 17:47:00.063: INFO: Pod calico-node-z28vk requesting resource cpu=250m on Node 10.113.180.96
    Jun 27 17:47:00.063: INFO: Pod calico-typha-657df678d7-4ch2t requesting resource cpu=250m on Node 10.113.180.96
    Jun 27 17:47:00.063: INFO: Pod calico-typha-657df678d7-65kdh requesting resource cpu=250m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod managed-storage-validation-webhooks-665764b6b4-r77pv requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod managed-storage-validation-webhooks-665764b6b4-v4g8s requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod managed-storage-validation-webhooks-665764b6b4-vhqr7 requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp requesting resource cpu=5m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod ibm-cloud-provider-ip-5-10-126-211-5c8746498b-tnr88 requesting resource cpu=5m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ibm-file-plugin-84cc66cdb-97nzr requesting resource cpu=50m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ibm-keepalived-watcher-9rf8d requesting resource cpu=5m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod ibm-keepalived-watcher-bpqvm requesting resource cpu=5m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod ibm-keepalived-watcher-h8p9q requesting resource cpu=5m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ibm-master-proxy-static-10.113.180.89 requesting resource cpu=26m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod ibm-master-proxy-static-10.113.180.90 requesting resource cpu=26m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod ibm-master-proxy-static-10.113.180.96 requesting resource cpu=26m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ibm-storage-metrics-agent-746d76fb56-nvvw9 requesting resource cpu=60m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ibm-storage-watcher-676b4ddbc6-dz54r requesting resource cpu=50m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ibmcloud-block-storage-driver-5ml2s requesting resource cpu=50m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod ibmcloud-block-storage-driver-jb9wr requesting resource cpu=50m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod ibmcloud-block-storage-driver-pkhxd requesting resource cpu=50m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ibmcloud-block-storage-plugin-5f677b8577-8f7s7 requesting resource cpu=50m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod vpn-546f6bf578-7ks4f requesting resource cpu=5m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod cluster-node-tuning-operator-848d57c5bb-7ln5t requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod tuned-kk4gn requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod tuned-md8mr requesting resource cpu=10m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod tuned-mwm8w requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod cluster-samples-operator-769ddfc8cc-88z62 requesting resource cpu=20m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod cluster-storage-operator-5fc995c76b-jhmzr requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod csi-snapshot-controller-5694c47cbb-9pvhd requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod csi-snapshot-controller-5694c47cbb-qfvbs requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod csi-snapshot-controller-operator-64ff6856c8-l29px requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod csi-snapshot-webhook-5d7cc7f6cb-gzt6x requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod csi-snapshot-webhook-5d7cc7f6cb-lwgl9 requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod console-operator-5694fff656-5wp7b requesting resource cpu=20m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod console-6c8658586b-jlcfh requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod console-6c8658586b-rc55z requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod downloads-57bd479866-2fpqm requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod downloads-57bd479866-8bj2l requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod dns-operator-b64d4b4c7-5p5hs requesting resource cpu=20m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod dns-default-q6q9l requesting resource cpu=60m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod dns-default-q8pl2 requesting resource cpu=60m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod dns-default-z76hf requesting resource cpu=60m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod node-resolver-cwltw requesting resource cpu=5m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod node-resolver-dtvx7 requesting resource cpu=5m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod node-resolver-nbx64 requesting resource cpu=5m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod cluster-image-registry-operator-55bdbc85cd-vpldl requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod image-registry-7f546fc5bb-n7w67 requesting resource cpu=100m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod node-ca-8pgp8 requesting resource cpu=10m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod node-ca-d4q6s requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod node-ca-tbfkf requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ingress-canary-bmwtq requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod ingress-canary-hfzkr requesting resource cpu=10m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod ingress-canary-mtqfl requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod ingress-operator-5c457b57b4-67rvh requesting resource cpu=20m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod router-default-7f97cd5c5f-m5pcx requesting resource cpu=100m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod router-default-7f97cd5c5f-n56jj requesting resource cpu=100m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod insights-operator-8f4d6c949-n2nlt requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod openshift-kube-proxy-b4224 requesting resource cpu=110m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod openshift-kube-proxy-bf9cj requesting resource cpu=110m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod openshift-kube-proxy-mpv6g requesting resource cpu=110m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod kube-storage-version-migrator-operator-59f4c7c696-jq8gm requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod migrator-6795cdbdb7-qsfqv requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod certified-operators-799hx requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod community-operators-9jptm requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod marketplace-operator-57bd87cfb8-5hhm8 requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod redhat-marketplace-qzk6b requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod redhat-operators-jqjb9 requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod alertmanager-main-0 requesting resource cpu=9m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod alertmanager-main-1 requesting resource cpu=9m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod cluster-monitoring-operator-644d445879-l5jj8 requesting resource cpu=11m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod kube-state-metrics-685444f59b-5dlq4 requesting resource cpu=4m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod node-exporter-5vgpm requesting resource cpu=9m on Node 10.113.180.90
    Jun 27 17:47:00.064: INFO: Pod node-exporter-l5dxr requesting resource cpu=9m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod node-exporter-lqlws requesting resource cpu=9m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod openshift-state-metrics-6d79d8c586-9mgzr requesting resource cpu=3m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod prometheus-adapter-648f68fcc-h27rt requesting resource cpu=1m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod prometheus-adapter-648f68fcc-mpvkt requesting resource cpu=1m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod prometheus-k8s-0 requesting resource cpu=75m on Node 10.113.180.96
    Jun 27 17:47:00.064: INFO: Pod prometheus-k8s-1 requesting resource cpu=75m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod prometheus-operator-68dfcc5c8-kn6tn requesting resource cpu=6m on Node 10.113.180.89
    Jun 27 17:47:00.064: INFO: Pod prometheus-operator-admission-webhook-6c667b594b-bds22 requesting resource cpu=5m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod prometheus-operator-admission-webhook-6c667b594b-c9dw4 requesting resource cpu=5m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod telemeter-client-7dc8fdddc8-kpccw requesting resource cpu=3m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod thanos-querier-754f675f77-6nf9l requesting resource cpu=15m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod thanos-querier-754f675f77-gs6k4 requesting resource cpu=15m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod multus-7zznf requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod multus-additional-cni-plugins-pn2hb requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod multus-additional-cni-plugins-qszdk requesting resource cpu=10m on Node 10.113.180.90
    Jun 27 17:47:00.065: INFO: Pod multus-additional-cni-plugins-xqsgs requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod multus-admission-controller-754d449d79-6cbgx requesting resource cpu=20m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod multus-admission-controller-754d449d79-strmv requesting resource cpu=20m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod multus-kx7qm requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod multus-skrpb requesting resource cpu=10m on Node 10.113.180.90
    Jun 27 17:47:00.065: INFO: Pod network-metrics-daemon-4jmw9 requesting resource cpu=20m on Node 10.113.180.90
    Jun 27 17:47:00.065: INFO: Pod network-metrics-daemon-cbrq2 requesting resource cpu=20m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod network-metrics-daemon-p8dh4 requesting resource cpu=20m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod network-check-source-6b8766f86f-s8s7j requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod network-check-target-6lv98 requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod network-check-target-7qghz requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod network-check-target-zmtm2 requesting resource cpu=10m on Node 10.113.180.90
    Jun 27 17:47:00.065: INFO: Pod network-operator-ffb9884c5-25r42 requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod catalog-operator-58898485cd-qjmms requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod olm-operator-7959cfcd9f-dt77g requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod package-server-manager-f7b6c466f-9bbzw requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod packageserver-8d78bf5dd-q8ct5 requesting resource cpu=10m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod packageserver-8d78bf5dd-r9g9t requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod metrics-6d46d44d8f-jbnwq requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod push-gateway-5465b544cc-f8v8q requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod service-ca-operator-74dd8bfc8-cl4bp requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod service-ca-6f86485857-drj9g requesting resource cpu=10m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod labelsupdate67cd7366-32e8-42cb-8a70-0faaa30703dc requesting resource cpu=0m on Node 10.113.180.90
    Jun 27 17:47:00.065: INFO: Pod sonobuoy requesting resource cpu=0m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod sonobuoy-e2e-job-0e516cd149fa4871 requesting resource cpu=0m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod sonobuoy-systemd-logs-daemon-set-21facd0441c84618-7f2xj requesting resource cpu=0m on Node 10.113.180.90
    Jun 27 17:47:00.065: INFO: Pod sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf requesting resource cpu=0m on Node 10.113.180.89
    Jun 27 17:47:00.065: INFO: Pod sonobuoy-systemd-logs-daemon-set-21facd0441c84618-qjl25 requesting resource cpu=0m on Node 10.113.180.96
    Jun 27 17:47:00.065: INFO: Pod tigera-operator-687c49f5c8-qcfmm requesting resource cpu=100m on Node 10.113.180.89
    STEP: Starting Pods to consume most of the cluster CPU. 06/27/23 17:47:00.065
    Jun 27 17:47:00.065: INFO: Creating a pod which consumes cpu=1766m on Node 10.113.180.89
    Jun 27 17:47:00.195: INFO: Creating a pod which consumes cpu=2320m on Node 10.113.180.90
    Jun 27 17:47:00.254: INFO: Creating a pod which consumes cpu=1591m on Node 10.113.180.96
    Jun 27 17:47:00.353: INFO: Waiting up to 5m0s for pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229" in namespace "sched-pred-5812" to be "running"
    Jun 27 17:47:00.403: INFO: Pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229": Phase="Pending", Reason="", readiness=false. Elapsed: 49.283043ms
    Jun 27 17:47:02.455: INFO: Pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229": Phase="Pending", Reason="", readiness=false. Elapsed: 2.101852331s
    Jun 27 17:47:04.422: INFO: Pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229": Phase="Running", Reason="", readiness=true. Elapsed: 4.068421653s
    Jun 27 17:47:04.422: INFO: Pod "filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229" satisfied condition "running"
    Jun 27 17:47:04.422: INFO: Waiting up to 5m0s for pod "filler-pod-60277d43-6509-4112-bb49-56c0259951dc" in namespace "sched-pred-5812" to be "running"
    Jun 27 17:47:04.438: INFO: Pod "filler-pod-60277d43-6509-4112-bb49-56c0259951dc": Phase="Running", Reason="", readiness=true. Elapsed: 15.345847ms
    Jun 27 17:47:04.438: INFO: Pod "filler-pod-60277d43-6509-4112-bb49-56c0259951dc" satisfied condition "running"
    Jun 27 17:47:04.438: INFO: Waiting up to 5m0s for pod "filler-pod-f358b651-cd8e-4437-9599-3adc87293894" in namespace "sched-pred-5812" to be "running"
    Jun 27 17:47:04.454: INFO: Pod "filler-pod-f358b651-cd8e-4437-9599-3adc87293894": Phase="Running", Reason="", readiness=true. Elapsed: 16.086936ms
    Jun 27 17:47:04.454: INFO: Pod "filler-pod-f358b651-cd8e-4437-9599-3adc87293894" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 06/27/23 17:47:04.454
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c9536277d8763], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5812/filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229 to 10.113.180.89] 06/27/23 17:47:04.474
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c953668f12d88], Reason = [AddedInterface], Message = [Add eth0 [172.30.106.187/32] from k8s-pod-network] 06/27/23 17:47:04.474
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c95367b83dbc0], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/27/23 17:47:04.474
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c95368c57258e], Reason = [Created], Message = [Created container filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229] 06/27/23 17:47:04.475
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229.176c95368ee16779], Reason = [Started], Message = [Started container filler-pod-46104e05-ca73-43a4-a13b-3fbde9895229] 06/27/23 17:47:04.475
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c95362de9c840], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5812/filler-pod-60277d43-6509-4112-bb49-56c0259951dc to 10.113.180.90] 06/27/23 17:47:04.475
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c953676e162c6], Reason = [AddedInterface], Message = [Add eth0 [172.30.250.248/32] from k8s-pod-network] 06/27/23 17:47:04.475
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c95368df68700], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/27/23 17:47:04.475
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c953699b2694f], Reason = [Created], Message = [Created container filler-pod-60277d43-6509-4112-bb49-56c0259951dc] 06/27/23 17:47:04.476
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-60277d43-6509-4112-bb49-56c0259951dc.176c95369d56b533], Reason = [Started], Message = [Started container filler-pod-60277d43-6509-4112-bb49-56c0259951dc] 06/27/23 17:47:04.476
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c953632eac1a9], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5812/filler-pod-f358b651-cd8e-4437-9599-3adc87293894 to 10.113.180.96] 06/27/23 17:47:04.476
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c95366d027da0], Reason = [AddedInterface], Message = [Add eth0 [172.30.60.108/32] from k8s-pod-network] 06/27/23 17:47:04.476
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c953682fb4c68], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 06/27/23 17:47:04.476
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c953694eaacf4], Reason = [Created], Message = [Created container filler-pod-f358b651-cd8e-4437-9599-3adc87293894] 06/27/23 17:47:04.477
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-f358b651-cd8e-4437-9599-3adc87293894.176c95369778fcd0], Reason = [Started], Message = [Started container filler-pod-f358b651-cd8e-4437-9599-3adc87293894] 06/27/23 17:47:04.477
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.176c95372a992ef5], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod.] 06/27/23 17:47:04.551
    STEP: removing the label node off the node 10.113.180.89 06/27/23 17:47:05.555
    STEP: verifying the node doesn't have the label node 06/27/23 17:47:05.644
    STEP: removing the label node off the node 10.113.180.90 06/27/23 17:47:05.674
    STEP: verifying the node doesn't have the label node 06/27/23 17:47:05.746
    STEP: removing the label node off the node 10.113.180.96 06/27/23 17:47:05.787
    STEP: verifying the node doesn't have the label node 06/27/23 17:47:05.848
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:47:05.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5812" for this suite. 06/27/23 17:47:05.892
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:47:05.937
Jun 27 17:47:05.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename dns 06/27/23 17:47:05.938
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:06.038
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/27/23 17:47:06.067
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 06/27/23 17:47:06.067
STEP: creating a pod to probe DNS 06/27/23 17:47:06.068
STEP: submitting the pod to kubernetes 06/27/23 17:47:06.068
Jun 27 17:47:06.132: INFO: Waiting up to 15m0s for pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038" in namespace "dns-9409" to be "running"
Jun 27 17:47:06.161: INFO: Pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038": Phase="Pending", Reason="", readiness=false. Elapsed: 28.265766ms
Jun 27 17:47:08.180: INFO: Pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047819513s
Jun 27 17:47:10.182: INFO: Pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038": Phase="Running", Reason="", readiness=true. Elapsed: 4.049964044s
Jun 27 17:47:10.183: INFO: Pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038" satisfied condition "running"
STEP: retrieving the pod 06/27/23 17:47:10.183
STEP: looking for the results for each expected name from probers 06/27/23 17:47:10.2
Jun 27 17:47:10.324: INFO: DNS probes using dns-9409/dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038 succeeded

STEP: deleting the pod 06/27/23 17:47:10.324
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 27 17:47:10.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9409" for this suite. 06/27/23 17:47:10.427
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":280,"skipped":5341,"failed":0}
------------------------------
â€¢ [4.529 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:47:05.937
    Jun 27 17:47:05.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename dns 06/27/23 17:47:05.938
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:06.038
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/27/23 17:47:06.067
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     06/27/23 17:47:06.067
    STEP: creating a pod to probe DNS 06/27/23 17:47:06.068
    STEP: submitting the pod to kubernetes 06/27/23 17:47:06.068
    Jun 27 17:47:06.132: INFO: Waiting up to 15m0s for pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038" in namespace "dns-9409" to be "running"
    Jun 27 17:47:06.161: INFO: Pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038": Phase="Pending", Reason="", readiness=false. Elapsed: 28.265766ms
    Jun 27 17:47:08.180: INFO: Pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038": Phase="Pending", Reason="", readiness=false. Elapsed: 2.047819513s
    Jun 27 17:47:10.182: INFO: Pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038": Phase="Running", Reason="", readiness=true. Elapsed: 4.049964044s
    Jun 27 17:47:10.183: INFO: Pod "dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038" satisfied condition "running"
    STEP: retrieving the pod 06/27/23 17:47:10.183
    STEP: looking for the results for each expected name from probers 06/27/23 17:47:10.2
    Jun 27 17:47:10.324: INFO: DNS probes using dns-9409/dns-test-52d8e724-74be-44b6-aeb5-ea27d4814038 succeeded

    STEP: deleting the pod 06/27/23 17:47:10.324
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 27 17:47:10.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9409" for this suite. 06/27/23 17:47:10.427
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:47:10.476
Jun 27 17:47:10.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename endpointslice 06/27/23 17:47:10.478
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:10.542
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:10.569
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 06/27/23 17:47:10.587
STEP: getting /apis/discovery.k8s.io 06/27/23 17:47:10.608
STEP: getting /apis/discovery.k8s.iov1 06/27/23 17:47:10.616
STEP: creating 06/27/23 17:47:10.628
STEP: getting 06/27/23 17:47:10.726
STEP: listing 06/27/23 17:47:10.757
STEP: watching 06/27/23 17:47:10.776
Jun 27 17:47:10.776: INFO: starting watch
STEP: cluster-wide listing 06/27/23 17:47:10.785
STEP: cluster-wide watching 06/27/23 17:47:10.825
Jun 27 17:47:10.825: INFO: starting watch
STEP: patching 06/27/23 17:47:10.845
STEP: updating 06/27/23 17:47:10.875
Jun 27 17:47:10.918: INFO: waiting for watch events with expected annotations
Jun 27 17:47:10.918: INFO: saw patched and updated annotations
STEP: deleting 06/27/23 17:47:10.918
STEP: deleting a collection 06/27/23 17:47:10.985
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Jun 27 17:47:11.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-2515" for this suite. 06/27/23 17:47:11.071
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":281,"skipped":5355,"failed":0}
------------------------------
â€¢ [0.634 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:47:10.476
    Jun 27 17:47:10.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename endpointslice 06/27/23 17:47:10.478
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:10.542
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:10.569
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 06/27/23 17:47:10.587
    STEP: getting /apis/discovery.k8s.io 06/27/23 17:47:10.608
    STEP: getting /apis/discovery.k8s.iov1 06/27/23 17:47:10.616
    STEP: creating 06/27/23 17:47:10.628
    STEP: getting 06/27/23 17:47:10.726
    STEP: listing 06/27/23 17:47:10.757
    STEP: watching 06/27/23 17:47:10.776
    Jun 27 17:47:10.776: INFO: starting watch
    STEP: cluster-wide listing 06/27/23 17:47:10.785
    STEP: cluster-wide watching 06/27/23 17:47:10.825
    Jun 27 17:47:10.825: INFO: starting watch
    STEP: patching 06/27/23 17:47:10.845
    STEP: updating 06/27/23 17:47:10.875
    Jun 27 17:47:10.918: INFO: waiting for watch events with expected annotations
    Jun 27 17:47:10.918: INFO: saw patched and updated annotations
    STEP: deleting 06/27/23 17:47:10.918
    STEP: deleting a collection 06/27/23 17:47:10.985
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Jun 27 17:47:11.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-2515" for this suite. 06/27/23 17:47:11.071
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:47:11.111
Jun 27 17:47:11.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-runtime 06/27/23 17:47:11.114
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:11.194
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:11.222
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 06/27/23 17:47:11.259
STEP: wait for the container to reach Succeeded 06/27/23 17:47:11.347
STEP: get the container status 06/27/23 17:47:16.604
STEP: the container should be terminated 06/27/23 17:47:16.622
STEP: the termination message should be set 06/27/23 17:47:16.622
Jun 27 17:47:16.623: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 06/27/23 17:47:16.623
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 27 17:47:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5790" for this suite. 06/27/23 17:47:16.722
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":282,"skipped":5356,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.643 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:47:11.111
    Jun 27 17:47:11.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-runtime 06/27/23 17:47:11.114
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:11.194
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:11.222
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 06/27/23 17:47:11.259
    STEP: wait for the container to reach Succeeded 06/27/23 17:47:11.347
    STEP: get the container status 06/27/23 17:47:16.604
    STEP: the container should be terminated 06/27/23 17:47:16.622
    STEP: the termination message should be set 06/27/23 17:47:16.622
    Jun 27 17:47:16.623: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 06/27/23 17:47:16.623
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 27 17:47:16.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5790" for this suite. 06/27/23 17:47:16.722
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:47:16.754
Jun 27 17:47:16.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:47:16.757
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:16.824
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:16.844
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:47:16.862
Jun 27 17:47:16.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10" in namespace "downward-api-8924" to be "Succeeded or Failed"
Jun 27 17:47:16.952: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10": Phase="Pending", Reason="", readiness=false. Elapsed: 15.69693ms
Jun 27 17:47:18.973: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036757955s
Jun 27 17:47:20.969: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033297397s
Jun 27 17:47:22.985: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049373449s
STEP: Saw pod success 06/27/23 17:47:22.986
Jun 27 17:47:22.986: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10" satisfied condition "Succeeded or Failed"
Jun 27 17:47:23.007: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10 container client-container: <nil>
STEP: delete the pod 06/27/23 17:47:23.053
Jun 27 17:47:23.102: INFO: Waiting for pod downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10 to disappear
Jun 27 17:47:23.122: INFO: Pod downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 17:47:23.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8924" for this suite. 06/27/23 17:47:23.151
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":283,"skipped":5358,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.444 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:47:16.754
    Jun 27 17:47:16.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:47:16.757
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:16.824
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:16.844
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:47:16.862
    Jun 27 17:47:16.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10" in namespace "downward-api-8924" to be "Succeeded or Failed"
    Jun 27 17:47:16.952: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10": Phase="Pending", Reason="", readiness=false. Elapsed: 15.69693ms
    Jun 27 17:47:18.973: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10": Phase="Pending", Reason="", readiness=false. Elapsed: 2.036757955s
    Jun 27 17:47:20.969: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10": Phase="Pending", Reason="", readiness=false. Elapsed: 4.033297397s
    Jun 27 17:47:22.985: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.049373449s
    STEP: Saw pod success 06/27/23 17:47:22.986
    Jun 27 17:47:22.986: INFO: Pod "downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10" satisfied condition "Succeeded or Failed"
    Jun 27 17:47:23.007: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:47:23.053
    Jun 27 17:47:23.102: INFO: Waiting for pod downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10 to disappear
    Jun 27 17:47:23.122: INFO: Pod downwardapi-volume-206886b8-1cd9-445c-a83a-474f5994ec10 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 17:47:23.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8924" for this suite. 06/27/23 17:47:23.151
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:47:23.204
Jun 27 17:47:23.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename daemonsets 06/27/23 17:47:23.206
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:23.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:23.296
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 06/27/23 17:47:23.439
STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 17:47:23.466
Jun 27 17:47:23.515: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:47:23.515: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:47:24.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:47:24.562: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:47:25.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:47:25.565: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:47:26.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 27 17:47:26.572: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/27/23 17:47:26.587
Jun 27 17:47:26.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:47:26.715: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:47:27.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:47:27.766: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:47:28.769: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:47:28.769: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:47:29.764: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 27 17:47:29.764: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 06/27/23 17:47:29.764
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/27/23 17:47:29.799
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4014, will wait for the garbage collector to delete the pods 06/27/23 17:47:29.8
Jun 27 17:47:29.896: INFO: Deleting DaemonSet.extensions daemon-set took: 25.349034ms
Jun 27 17:47:29.997: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.9426ms
Jun 27 17:47:32.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:47:32.917: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 27 17:47:32.937: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"133238"},"items":null}

Jun 27 17:47:32.965: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"133238"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:47:33.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-4014" for this suite. 06/27/23 17:47:33.126
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":284,"skipped":5365,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.955 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:47:23.204
    Jun 27 17:47:23.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename daemonsets 06/27/23 17:47:23.206
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:23.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:23.296
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 06/27/23 17:47:23.439
    STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 17:47:23.466
    Jun 27 17:47:23.515: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:47:23.515: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:47:24.562: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:47:24.562: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:47:25.565: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:47:25.565: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:47:26.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 27 17:47:26.572: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 06/27/23 17:47:26.587
    Jun 27 17:47:26.715: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:47:26.715: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:47:27.766: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:47:27.766: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:47:28.769: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:47:28.769: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:47:29.764: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 27 17:47:29.764: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 06/27/23 17:47:29.764
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/27/23 17:47:29.799
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4014, will wait for the garbage collector to delete the pods 06/27/23 17:47:29.8
    Jun 27 17:47:29.896: INFO: Deleting DaemonSet.extensions daemon-set took: 25.349034ms
    Jun 27 17:47:29.997: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.9426ms
    Jun 27 17:47:32.917: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:47:32.917: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 27 17:47:32.937: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"133238"},"items":null}

    Jun 27 17:47:32.965: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"133238"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:47:33.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-4014" for this suite. 06/27/23 17:47:33.126
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:47:33.16
Jun 27 17:47:33.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 17:47:33.162
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:33.226
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:33.266
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 06/27/23 17:47:33.292
Jun 27 17:47:33.293: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4511 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 06/27/23 17:47:33.349
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 17:47:33.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4511" for this suite. 06/27/23 17:47:33.4
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":285,"skipped":5370,"failed":0}
------------------------------
â€¢ [0.291 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:47:33.16
    Jun 27 17:47:33.160: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 17:47:33.162
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:33.226
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:33.266
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 06/27/23 17:47:33.292
    Jun 27 17:47:33.293: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-4511 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 06/27/23 17:47:33.349
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 17:47:33.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4511" for this suite. 06/27/23 17:47:33.4
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:47:33.452
Jun 27 17:47:33.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 17:47:33.454
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:33.531
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:33.558
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 27 17:47:33.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3638" for this suite. 06/27/23 17:47:33.935
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":286,"skipped":5376,"failed":0}
------------------------------
â€¢ [0.511 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:47:33.452
    Jun 27 17:47:33.452: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 17:47:33.454
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:33.531
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:33.558
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 17:47:33.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3638" for this suite. 06/27/23 17:47:33.935
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:47:33.966
Jun 27 17:47:33.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename taint-single-pod 06/27/23 17:47:33.967
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:34.083
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:34.113
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Jun 27 17:47:34.143: INFO: Waiting up to 1m0s for all nodes to be ready
Jun 27 17:48:34.513: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Jun 27 17:48:34.576: INFO: Starting informer...
STEP: Starting pod... 06/27/23 17:48:34.576
Jun 27 17:48:34.709: INFO: Pod is running on 10.113.180.90. Tainting Node
STEP: Trying to apply a taint on the Node 06/27/23 17:48:34.709
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/27/23 17:48:34.769
STEP: Waiting short time to make sure Pod is queued for deletion 06/27/23 17:48:34.804
Jun 27 17:48:34.804: INFO: Pod wasn't evicted. Proceeding
Jun 27 17:48:34.804: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/27/23 17:48:34.866
STEP: Waiting some time to make sure that toleration time passed. 06/27/23 17:48:34.898
Jun 27 17:49:49.903: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:49:49.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-5843" for this suite. 06/27/23 17:49:49.953
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":287,"skipped":5391,"failed":0}
------------------------------
â€¢ [SLOW TEST] [136.016 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:47:33.966
    Jun 27 17:47:33.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename taint-single-pod 06/27/23 17:47:33.967
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:47:34.083
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:47:34.113
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Jun 27 17:47:34.143: INFO: Waiting up to 1m0s for all nodes to be ready
    Jun 27 17:48:34.513: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Jun 27 17:48:34.576: INFO: Starting informer...
    STEP: Starting pod... 06/27/23 17:48:34.576
    Jun 27 17:48:34.709: INFO: Pod is running on 10.113.180.90. Tainting Node
    STEP: Trying to apply a taint on the Node 06/27/23 17:48:34.709
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/27/23 17:48:34.769
    STEP: Waiting short time to make sure Pod is queued for deletion 06/27/23 17:48:34.804
    Jun 27 17:48:34.804: INFO: Pod wasn't evicted. Proceeding
    Jun 27 17:48:34.804: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 06/27/23 17:48:34.866
    STEP: Waiting some time to make sure that toleration time passed. 06/27/23 17:48:34.898
    Jun 27 17:49:49.903: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:49:49.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-5843" for this suite. 06/27/23 17:49:49.953
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:49:49.986
Jun 27 17:49:49.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-runtime 06/27/23 17:49:49.99
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:49:50.062
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:49:50.09
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 06/27/23 17:49:50.128
STEP: wait for the container to reach Succeeded 06/27/23 17:49:50.199
STEP: get the container status 06/27/23 17:49:55.346
STEP: the container should be terminated 06/27/23 17:49:55.393
STEP: the termination message should be set 06/27/23 17:49:55.394
Jun 27 17:49:55.394: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 06/27/23 17:49:55.394
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 27 17:49:55.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8796" for this suite. 06/27/23 17:49:55.526
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":288,"skipped":5397,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.566 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:49:49.986
    Jun 27 17:49:49.987: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-runtime 06/27/23 17:49:49.99
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:49:50.062
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:49:50.09
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 06/27/23 17:49:50.128
    STEP: wait for the container to reach Succeeded 06/27/23 17:49:50.199
    STEP: get the container status 06/27/23 17:49:55.346
    STEP: the container should be terminated 06/27/23 17:49:55.393
    STEP: the termination message should be set 06/27/23 17:49:55.394
    Jun 27 17:49:55.394: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 06/27/23 17:49:55.394
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 27 17:49:55.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8796" for this suite. 06/27/23 17:49:55.526
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:49:55.556
Jun 27 17:49:55.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-probe 06/27/23 17:49:55.557
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:49:55.611
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:49:55.629
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b in namespace container-probe-5322 06/27/23 17:49:55.656
Jun 27 17:49:55.717: INFO: Waiting up to 5m0s for pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b" in namespace "container-probe-5322" to be "not pending"
Jun 27 17:49:55.733: INFO: Pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.913834ms
Jun 27 17:49:57.751: INFO: Pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033696736s
Jun 27 17:49:59.763: INFO: Pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b": Phase="Running", Reason="", readiness=true. Elapsed: 4.046635522s
Jun 27 17:49:59.764: INFO: Pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b" satisfied condition "not pending"
Jun 27 17:49:59.764: INFO: Started pod liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b in namespace container-probe-5322
STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 17:49:59.764
Jun 27 17:49:59.779: INFO: Initial restart count of pod liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is 0
Jun 27 17:50:18.001: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 1 (18.221913425s elapsed)
Jun 27 17:50:38.193: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 2 (38.413878617s elapsed)
Jun 27 17:50:58.724: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 3 (58.944644159s elapsed)
Jun 27 17:51:16.906: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 4 (1m17.126125297s elapsed)
Jun 27 17:52:29.726: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 5 (2m29.946269388s elapsed)
STEP: deleting the pod 06/27/23 17:52:29.726
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 27 17:52:29.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5322" for this suite. 06/27/23 17:52:29.812
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":289,"skipped":5409,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.312 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:49:55.556
    Jun 27 17:49:55.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-probe 06/27/23 17:49:55.557
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:49:55.611
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:49:55.629
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b in namespace container-probe-5322 06/27/23 17:49:55.656
    Jun 27 17:49:55.717: INFO: Waiting up to 5m0s for pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b" in namespace "container-probe-5322" to be "not pending"
    Jun 27 17:49:55.733: INFO: Pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.913834ms
    Jun 27 17:49:57.751: INFO: Pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.033696736s
    Jun 27 17:49:59.763: INFO: Pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b": Phase="Running", Reason="", readiness=true. Elapsed: 4.046635522s
    Jun 27 17:49:59.764: INFO: Pod "liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b" satisfied condition "not pending"
    Jun 27 17:49:59.764: INFO: Started pod liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b in namespace container-probe-5322
    STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 17:49:59.764
    Jun 27 17:49:59.779: INFO: Initial restart count of pod liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is 0
    Jun 27 17:50:18.001: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 1 (18.221913425s elapsed)
    Jun 27 17:50:38.193: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 2 (38.413878617s elapsed)
    Jun 27 17:50:58.724: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 3 (58.944644159s elapsed)
    Jun 27 17:51:16.906: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 4 (1m17.126125297s elapsed)
    Jun 27 17:52:29.726: INFO: Restart count of pod container-probe-5322/liveness-c8390dea-06e2-459d-a2f7-5051e7d7c21b is now 5 (2m29.946269388s elapsed)
    STEP: deleting the pod 06/27/23 17:52:29.726
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 27 17:52:29.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5322" for this suite. 06/27/23 17:52:29.812
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:52:29.88
Jun 27 17:52:29.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename podtemplate 06/27/23 17:52:29.881
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:52:29.945
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:52:29.969
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 06/27/23 17:52:29.996
Jun 27 17:52:30.025: INFO: created test-podtemplate-1
Jun 27 17:52:30.050: INFO: created test-podtemplate-2
Jun 27 17:52:30.072: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 06/27/23 17:52:30.072
STEP: delete collection of pod templates 06/27/23 17:52:30.09
Jun 27 17:52:30.091: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 06/27/23 17:52:30.169
Jun 27 17:52:30.169: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Jun 27 17:52:30.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1255" for this suite. 06/27/23 17:52:30.218
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":290,"skipped":5508,"failed":0}
------------------------------
â€¢ [0.371 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:52:29.88
    Jun 27 17:52:29.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename podtemplate 06/27/23 17:52:29.881
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:52:29.945
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:52:29.969
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 06/27/23 17:52:29.996
    Jun 27 17:52:30.025: INFO: created test-podtemplate-1
    Jun 27 17:52:30.050: INFO: created test-podtemplate-2
    Jun 27 17:52:30.072: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 06/27/23 17:52:30.072
    STEP: delete collection of pod templates 06/27/23 17:52:30.09
    Jun 27 17:52:30.091: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 06/27/23 17:52:30.169
    Jun 27 17:52:30.169: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Jun 27 17:52:30.186: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1255" for this suite. 06/27/23 17:52:30.218
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:52:30.256
Jun 27 17:52:30.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-runtime 06/27/23 17:52:30.258
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:52:30.322
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:52:30.342
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 06/27/23 17:52:30.408
STEP: wait for the container to reach Failed 06/27/23 17:52:30.488
STEP: get the container status 06/27/23 17:52:35.615
STEP: the container should be terminated 06/27/23 17:52:35.631
STEP: the termination message should be set 06/27/23 17:52:35.631
Jun 27 17:52:35.632: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/27/23 17:52:35.632
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 27 17:52:35.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-5950" for this suite. 06/27/23 17:52:35.75
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":291,"skipped":5540,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.526 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:52:30.256
    Jun 27 17:52:30.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-runtime 06/27/23 17:52:30.258
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:52:30.322
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:52:30.342
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 06/27/23 17:52:30.408
    STEP: wait for the container to reach Failed 06/27/23 17:52:30.488
    STEP: get the container status 06/27/23 17:52:35.615
    STEP: the container should be terminated 06/27/23 17:52:35.631
    STEP: the termination message should be set 06/27/23 17:52:35.631
    Jun 27 17:52:35.632: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/27/23 17:52:35.632
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 27 17:52:35.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-5950" for this suite. 06/27/23 17:52:35.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:52:35.795
Jun 27 17:52:35.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 17:52:35.796
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:52:35.884
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:52:35.909
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-fd59a2b5-ca7e-4758-9fd3-33419f122eed 06/27/23 17:52:35.927
STEP: Creating a pod to test consume configMaps 06/27/23 17:52:35.951
Jun 27 17:52:36.019: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb" in namespace "configmap-2742" to be "Succeeded or Failed"
Jun 27 17:52:36.038: INFO: Pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.437663ms
Jun 27 17:52:38.058: INFO: Pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039057021s
Jun 27 17:52:40.066: INFO: Pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047646131s
STEP: Saw pod success 06/27/23 17:52:40.066
Jun 27 17:52:40.067: INFO: Pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb" satisfied condition "Succeeded or Failed"
Jun 27 17:52:40.083: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb container configmap-volume-test: <nil>
STEP: delete the pod 06/27/23 17:52:40.164
Jun 27 17:52:40.208: INFO: Waiting for pod pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb to disappear
Jun 27 17:52:40.222: INFO: Pod pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 17:52:40.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2742" for this suite. 06/27/23 17:52:40.256
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":292,"skipped":5590,"failed":0}
------------------------------
â€¢ [4.502 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:52:35.795
    Jun 27 17:52:35.795: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 17:52:35.796
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:52:35.884
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:52:35.909
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-fd59a2b5-ca7e-4758-9fd3-33419f122eed 06/27/23 17:52:35.927
    STEP: Creating a pod to test consume configMaps 06/27/23 17:52:35.951
    Jun 27 17:52:36.019: INFO: Waiting up to 5m0s for pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb" in namespace "configmap-2742" to be "Succeeded or Failed"
    Jun 27 17:52:36.038: INFO: Pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb": Phase="Pending", Reason="", readiness=false. Elapsed: 19.437663ms
    Jun 27 17:52:38.058: INFO: Pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.039057021s
    Jun 27 17:52:40.066: INFO: Pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.047646131s
    STEP: Saw pod success 06/27/23 17:52:40.066
    Jun 27 17:52:40.067: INFO: Pod "pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb" satisfied condition "Succeeded or Failed"
    Jun 27 17:52:40.083: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb container configmap-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:52:40.164
    Jun 27 17:52:40.208: INFO: Waiting for pod pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb to disappear
    Jun 27 17:52:40.222: INFO: Pod pod-configmaps-bf64411b-6f32-4315-816c-f245ebb211eb no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 17:52:40.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2742" for this suite. 06/27/23 17:52:40.256
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:52:40.306
Jun 27 17:52:40.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename cronjob 06/27/23 17:52:40.308
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:52:40.387
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:52:40.415
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 06/27/23 17:52:40.437
STEP: Ensuring a job is scheduled 06/27/23 17:52:40.466
STEP: Ensuring exactly one is scheduled 06/27/23 17:53:00.488
STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/27/23 17:53:00.505
STEP: Ensuring the job is replaced with a new one 06/27/23 17:53:00.527
STEP: Removing cronjob 06/27/23 17:54:00.562
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 27 17:54:00.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-2770" for this suite. 06/27/23 17:54:00.648
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":293,"skipped":5623,"failed":0}
------------------------------
â€¢ [SLOW TEST] [80.373 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:52:40.306
    Jun 27 17:52:40.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename cronjob 06/27/23 17:52:40.308
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:52:40.387
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:52:40.415
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 06/27/23 17:52:40.437
    STEP: Ensuring a job is scheduled 06/27/23 17:52:40.466
    STEP: Ensuring exactly one is scheduled 06/27/23 17:53:00.488
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 06/27/23 17:53:00.505
    STEP: Ensuring the job is replaced with a new one 06/27/23 17:53:00.527
    STEP: Removing cronjob 06/27/23 17:54:00.562
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 27 17:54:00.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-2770" for this suite. 06/27/23 17:54:00.648
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:54:00.684
Jun 27 17:54:00.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 17:54:00.688
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:00.764
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:00.824
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-dd642608-272f-4ed2-a404-f96e792718db 06/27/23 17:54:00.851
STEP: Creating a pod to test consume secrets 06/27/23 17:54:00.874
Jun 27 17:54:00.930: INFO: Waiting up to 5m0s for pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2" in namespace "secrets-4621" to be "Succeeded or Failed"
Jun 27 17:54:00.951: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.112748ms
Jun 27 17:54:02.974: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043783296s
Jun 27 17:54:04.971: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040348191s
Jun 27 17:54:06.969: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038588493s
STEP: Saw pod success 06/27/23 17:54:06.969
Jun 27 17:54:06.969: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2" satisfied condition "Succeeded or Failed"
Jun 27 17:54:06.986: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2 container secret-volume-test: <nil>
STEP: delete the pod 06/27/23 17:54:07.025
Jun 27 17:54:07.063: INFO: Waiting for pod pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2 to disappear
Jun 27 17:54:07.079: INFO: Pod pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 27 17:54:07.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4621" for this suite. 06/27/23 17:54:07.113
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":294,"skipped":5650,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.456 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:54:00.684
    Jun 27 17:54:00.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 17:54:00.688
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:00.764
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:00.824
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-dd642608-272f-4ed2-a404-f96e792718db 06/27/23 17:54:00.851
    STEP: Creating a pod to test consume secrets 06/27/23 17:54:00.874
    Jun 27 17:54:00.930: INFO: Waiting up to 5m0s for pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2" in namespace "secrets-4621" to be "Succeeded or Failed"
    Jun 27 17:54:00.951: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 21.112748ms
    Jun 27 17:54:02.974: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.043783296s
    Jun 27 17:54:04.971: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.040348191s
    Jun 27 17:54:06.969: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.038588493s
    STEP: Saw pod success 06/27/23 17:54:06.969
    Jun 27 17:54:06.969: INFO: Pod "pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2" satisfied condition "Succeeded or Failed"
    Jun 27 17:54:06.986: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2 container secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:54:07.025
    Jun 27 17:54:07.063: INFO: Waiting for pod pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2 to disappear
    Jun 27 17:54:07.079: INFO: Pod pod-secrets-2578de6f-f339-40f0-87bd-b5e428e8c4f2 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 17:54:07.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4621" for this suite. 06/27/23 17:54:07.113
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:54:07.141
Jun 27 17:54:07.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename disruption 06/27/23 17:54:07.143
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:07.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:07.31
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 06/27/23 17:54:07.351
STEP: Waiting for all pods to be running 06/27/23 17:54:09.655
Jun 27 17:54:09.676: INFO: running pods: 0 < 3
Jun 27 17:54:11.695: INFO: running pods: 2 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 27 17:54:13.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1730" for this suite. 06/27/23 17:54:13.74
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":295,"skipped":5650,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.629 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:54:07.141
    Jun 27 17:54:07.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename disruption 06/27/23 17:54:07.143
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:07.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:07.31
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 06/27/23 17:54:07.351
    STEP: Waiting for all pods to be running 06/27/23 17:54:09.655
    Jun 27 17:54:09.676: INFO: running pods: 0 < 3
    Jun 27 17:54:11.695: INFO: running pods: 2 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 27 17:54:13.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1730" for this suite. 06/27/23 17:54:13.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:54:13.772
Jun 27 17:54:13.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 17:54:13.775
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:13.838
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:13.857
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 17:54:13.942
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:54:14.177
STEP: Deploying the webhook pod 06/27/23 17:54:14.218
STEP: Wait for the deployment to be ready 06/27/23 17:54:14.263
Jun 27 17:54:14.303: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 27 17:54:16.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 54, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 54, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 54, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 54, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 17:54:18.376
STEP: Verifying the service has paired with the endpoint 06/27/23 17:54:18.417
Jun 27 17:54:19.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Jun 27 17:54:19.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7673-crds.webhook.example.com via the AdmissionRegistration API 06/27/23 17:54:20.04
STEP: Creating a custom resource while v1 is storage version 06/27/23 17:54:20.169
STEP: Patching Custom Resource Definition to set v2 as storage 06/27/23 17:54:22.406
STEP: Patching the custom resource while v2 is storage version 06/27/23 17:54:22.457
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:54:23.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-395" for this suite. 06/27/23 17:54:23.282
STEP: Destroying namespace "webhook-395-markers" for this suite. 06/27/23 17:54:23.321
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":296,"skipped":5664,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.756 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:54:13.772
    Jun 27 17:54:13.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 17:54:13.775
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:13.838
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:13.857
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 17:54:13.942
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:54:14.177
    STEP: Deploying the webhook pod 06/27/23 17:54:14.218
    STEP: Wait for the deployment to be ready 06/27/23 17:54:14.263
    Jun 27 17:54:14.303: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 27 17:54:16.358: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 54, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 54, 14, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 54, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 54, 14, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 17:54:18.376
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:54:18.417
    Jun 27 17:54:19.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Jun 27 17:54:19.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7673-crds.webhook.example.com via the AdmissionRegistration API 06/27/23 17:54:20.04
    STEP: Creating a custom resource while v1 is storage version 06/27/23 17:54:20.169
    STEP: Patching Custom Resource Definition to set v2 as storage 06/27/23 17:54:22.406
    STEP: Patching the custom resource while v2 is storage version 06/27/23 17:54:22.457
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:54:23.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-395" for this suite. 06/27/23 17:54:23.282
    STEP: Destroying namespace "webhook-395-markers" for this suite. 06/27/23 17:54:23.321
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:54:23.53
Jun 27 17:54:23.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:54:23.532
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:23.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:23.613
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:54:23.637
Jun 27 17:54:23.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558" in namespace "downward-api-5039" to be "Succeeded or Failed"
Jun 27 17:54:23.722: INFO: Pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558": Phase="Pending", Reason="", readiness=false. Elapsed: 22.713284ms
Jun 27 17:54:25.738: INFO: Pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038740444s
Jun 27 17:54:27.738: INFO: Pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039599538s
STEP: Saw pod success 06/27/23 17:54:27.739
Jun 27 17:54:27.739: INFO: Pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558" satisfied condition "Succeeded or Failed"
Jun 27 17:54:27.758: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558 container client-container: <nil>
STEP: delete the pod 06/27/23 17:54:27.805
Jun 27 17:54:27.845: INFO: Waiting for pod downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558 to disappear
Jun 27 17:54:27.863: INFO: Pod downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 17:54:27.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5039" for this suite. 06/27/23 17:54:27.891
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":297,"skipped":5674,"failed":0}
------------------------------
â€¢ [4.391 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:54:23.53
    Jun 27 17:54:23.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:54:23.532
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:23.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:23.613
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:54:23.637
    Jun 27 17:54:23.699: INFO: Waiting up to 5m0s for pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558" in namespace "downward-api-5039" to be "Succeeded or Failed"
    Jun 27 17:54:23.722: INFO: Pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558": Phase="Pending", Reason="", readiness=false. Elapsed: 22.713284ms
    Jun 27 17:54:25.738: INFO: Pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038740444s
    Jun 27 17:54:27.738: INFO: Pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.039599538s
    STEP: Saw pod success 06/27/23 17:54:27.739
    Jun 27 17:54:27.739: INFO: Pod "downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558" satisfied condition "Succeeded or Failed"
    Jun 27 17:54:27.758: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:54:27.805
    Jun 27 17:54:27.845: INFO: Waiting for pod downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558 to disappear
    Jun 27 17:54:27.863: INFO: Pod downwardapi-volume-18e64d1d-b07c-4626-97a4-361c70add558 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 17:54:27.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5039" for this suite. 06/27/23 17:54:27.891
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:54:27.927
Jun 27 17:54:27.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 17:54:27.929
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:27.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:28.014
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 06/27/23 17:54:28.032
Jun 27 17:54:28.106: INFO: Waiting up to 5m0s for pod "pod-glnsz" in namespace "pods-6715" to be "running"
Jun 27 17:54:28.126: INFO: Pod "pod-glnsz": Phase="Pending", Reason="", readiness=false. Elapsed: 19.989561ms
Jun 27 17:54:30.161: INFO: Pod "pod-glnsz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054624241s
Jun 27 17:54:32.142: INFO: Pod "pod-glnsz": Phase="Running", Reason="", readiness=true. Elapsed: 4.036181754s
Jun 27 17:54:32.142: INFO: Pod "pod-glnsz" satisfied condition "running"
STEP: patching /status 06/27/23 17:54:32.142
Jun 27 17:54:32.170: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 17:54:32.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6715" for this suite. 06/27/23 17:54:32.202
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":298,"skipped":5693,"failed":0}
------------------------------
â€¢ [4.303 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:54:27.927
    Jun 27 17:54:27.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 17:54:27.929
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:27.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:28.014
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 06/27/23 17:54:28.032
    Jun 27 17:54:28.106: INFO: Waiting up to 5m0s for pod "pod-glnsz" in namespace "pods-6715" to be "running"
    Jun 27 17:54:28.126: INFO: Pod "pod-glnsz": Phase="Pending", Reason="", readiness=false. Elapsed: 19.989561ms
    Jun 27 17:54:30.161: INFO: Pod "pod-glnsz": Phase="Pending", Reason="", readiness=false. Elapsed: 2.054624241s
    Jun 27 17:54:32.142: INFO: Pod "pod-glnsz": Phase="Running", Reason="", readiness=true. Elapsed: 4.036181754s
    Jun 27 17:54:32.142: INFO: Pod "pod-glnsz" satisfied condition "running"
    STEP: patching /status 06/27/23 17:54:32.142
    Jun 27 17:54:32.170: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 17:54:32.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6715" for this suite. 06/27/23 17:54:32.202
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:54:32.233
Jun 27 17:54:32.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename lease-test 06/27/23 17:54:32.237
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:32.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:32.313
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Jun 27 17:54:32.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-2833" for this suite. 06/27/23 17:54:32.658
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":299,"skipped":5700,"failed":0}
------------------------------
â€¢ [0.463 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:54:32.233
    Jun 27 17:54:32.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename lease-test 06/27/23 17:54:32.237
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:32.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:32.313
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Jun 27 17:54:32.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-2833" for this suite. 06/27/23 17:54:32.658
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:54:32.697
Jun 27 17:54:32.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/27/23 17:54:32.698
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:32.752
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:32.781
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 06/27/23 17:54:32.804
STEP: Creating hostNetwork=false pod 06/27/23 17:54:32.804
Jun 27 17:54:32.862: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6984" to be "running and ready"
Jun 27 17:54:32.877: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.371716ms
Jun 27 17:54:32.877: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:54:34.918: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056807373s
Jun 27 17:54:34.919: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:54:36.903: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.041017374s
Jun 27 17:54:36.903: INFO: The phase of Pod test-pod is Running (Ready = true)
Jun 27 17:54:36.903: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 06/27/23 17:54:36.923
Jun 27 17:54:36.968: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6984" to be "running and ready"
Jun 27 17:54:36.984: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.81146ms
Jun 27 17:54:36.985: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:54:39.002: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03340502s
Jun 27 17:54:39.002: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Jun 27 17:54:39.002: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 06/27/23 17:54:39.018
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/27/23 17:54:39.018
Jun 27 17:54:39.018: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:39.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:39.020: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:39.020: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 27 17:54:39.392: INFO: Exec stderr: ""
Jun 27 17:54:39.392: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:39.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:39.394: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:39.394: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 27 17:54:39.787: INFO: Exec stderr: ""
Jun 27 17:54:39.787: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:39.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:39.789: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:39.789: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 27 17:54:40.035: INFO: Exec stderr: ""
Jun 27 17:54:40.035: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:40.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:40.037: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:40.038: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 27 17:54:40.271: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/27/23 17:54:40.271
Jun 27 17:54:40.272: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:40.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:40.273: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:40.273: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 27 17:54:40.517: INFO: Exec stderr: ""
Jun 27 17:54:40.517: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:40.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:40.519: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:40.519: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Jun 27 17:54:40.786: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/27/23 17:54:40.786
Jun 27 17:54:40.786: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:40.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:40.788: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:40.788: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 27 17:54:41.059: INFO: Exec stderr: ""
Jun 27 17:54:41.059: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:41.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:41.060: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:41.060: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Jun 27 17:54:41.312: INFO: Exec stderr: ""
Jun 27 17:54:41.312: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:41.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:41.314: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:41.314: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 27 17:54:41.548: INFO: Exec stderr: ""
Jun 27 17:54:41.548: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:54:41.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:54:41.550: INFO: ExecWithOptions: Clientset creation
Jun 27 17:54:41.550: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Jun 27 17:54:41.807: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Jun 27 17:54:41.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-6984" for this suite. 06/27/23 17:54:41.835
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":300,"skipped":5728,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.178 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:54:32.697
    Jun 27 17:54:32.697: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 06/27/23 17:54:32.698
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:32.752
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:32.781
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 06/27/23 17:54:32.804
    STEP: Creating hostNetwork=false pod 06/27/23 17:54:32.804
    Jun 27 17:54:32.862: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-6984" to be "running and ready"
    Jun 27 17:54:32.877: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.371716ms
    Jun 27 17:54:32.877: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:54:34.918: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.056807373s
    Jun 27 17:54:34.919: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:54:36.903: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.041017374s
    Jun 27 17:54:36.903: INFO: The phase of Pod test-pod is Running (Ready = true)
    Jun 27 17:54:36.903: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 06/27/23 17:54:36.923
    Jun 27 17:54:36.968: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-6984" to be "running and ready"
    Jun 27 17:54:36.984: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 15.81146ms
    Jun 27 17:54:36.985: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:54:39.002: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03340502s
    Jun 27 17:54:39.002: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Jun 27 17:54:39.002: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 06/27/23 17:54:39.018
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 06/27/23 17:54:39.018
    Jun 27 17:54:39.018: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:39.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:39.020: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:39.020: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 27 17:54:39.392: INFO: Exec stderr: ""
    Jun 27 17:54:39.392: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:39.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:39.394: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:39.394: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 27 17:54:39.787: INFO: Exec stderr: ""
    Jun 27 17:54:39.787: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:39.787: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:39.789: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:39.789: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 27 17:54:40.035: INFO: Exec stderr: ""
    Jun 27 17:54:40.035: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:40.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:40.037: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:40.038: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 27 17:54:40.271: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 06/27/23 17:54:40.271
    Jun 27 17:54:40.272: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:40.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:40.273: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:40.273: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun 27 17:54:40.517: INFO: Exec stderr: ""
    Jun 27 17:54:40.517: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:40.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:40.519: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:40.519: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Jun 27 17:54:40.786: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 06/27/23 17:54:40.786
    Jun 27 17:54:40.786: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:40.786: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:40.788: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:40.788: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 27 17:54:41.059: INFO: Exec stderr: ""
    Jun 27 17:54:41.059: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:41.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:41.060: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:41.060: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Jun 27 17:54:41.312: INFO: Exec stderr: ""
    Jun 27 17:54:41.312: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:41.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:41.314: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:41.314: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 27 17:54:41.548: INFO: Exec stderr: ""
    Jun 27 17:54:41.548: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-6984 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:54:41.548: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:54:41.550: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:54:41.550: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-6984/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Jun 27 17:54:41.807: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Jun 27 17:54:41.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-6984" for this suite. 06/27/23 17:54:41.835
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:54:41.877
Jun 27 17:54:41.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:54:41.879
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:41.941
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:41.978
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Jun 27 17:54:42.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/27/23 17:54:53.702
Jun 27 17:54:53.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 --namespace=crd-publish-openapi-4038 create -f -'
Jun 27 17:54:55.551: INFO: stderr: ""
Jun 27 17:54:55.551: INFO: stdout: "e2e-test-crd-publish-openapi-811-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 27 17:54:55.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 --namespace=crd-publish-openapi-4038 delete e2e-test-crd-publish-openapi-811-crds test-cr'
Jun 27 17:54:55.756: INFO: stderr: ""
Jun 27 17:54:55.756: INFO: stdout: "e2e-test-crd-publish-openapi-811-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Jun 27 17:54:55.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 --namespace=crd-publish-openapi-4038 apply -f -'
Jun 27 17:54:57.658: INFO: stderr: ""
Jun 27 17:54:57.658: INFO: stdout: "e2e-test-crd-publish-openapi-811-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Jun 27 17:54:57.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 --namespace=crd-publish-openapi-4038 delete e2e-test-crd-publish-openapi-811-crds test-cr'
Jun 27 17:54:57.912: INFO: stderr: ""
Jun 27 17:54:57.912: INFO: stdout: "e2e-test-crd-publish-openapi-811-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 06/27/23 17:54:57.912
Jun 27 17:54:57.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 explain e2e-test-crd-publish-openapi-811-crds'
Jun 27 17:54:58.413: INFO: stderr: ""
Jun 27 17:54:58.413: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-811-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:55:08.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4038" for this suite. 06/27/23 17:55:08.421
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":301,"skipped":5754,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.562 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:54:41.877
    Jun 27 17:54:41.878: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:54:41.879
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:54:41.941
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:54:41.978
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Jun 27 17:54:42.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 06/27/23 17:54:53.702
    Jun 27 17:54:53.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 --namespace=crd-publish-openapi-4038 create -f -'
    Jun 27 17:54:55.551: INFO: stderr: ""
    Jun 27 17:54:55.551: INFO: stdout: "e2e-test-crd-publish-openapi-811-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun 27 17:54:55.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 --namespace=crd-publish-openapi-4038 delete e2e-test-crd-publish-openapi-811-crds test-cr'
    Jun 27 17:54:55.756: INFO: stderr: ""
    Jun 27 17:54:55.756: INFO: stdout: "e2e-test-crd-publish-openapi-811-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Jun 27 17:54:55.756: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 --namespace=crd-publish-openapi-4038 apply -f -'
    Jun 27 17:54:57.658: INFO: stderr: ""
    Jun 27 17:54:57.658: INFO: stdout: "e2e-test-crd-publish-openapi-811-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Jun 27 17:54:57.658: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 --namespace=crd-publish-openapi-4038 delete e2e-test-crd-publish-openapi-811-crds test-cr'
    Jun 27 17:54:57.912: INFO: stderr: ""
    Jun 27 17:54:57.912: INFO: stdout: "e2e-test-crd-publish-openapi-811-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 06/27/23 17:54:57.912
    Jun 27 17:54:57.913: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=crd-publish-openapi-4038 explain e2e-test-crd-publish-openapi-811-crds'
    Jun 27 17:54:58.413: INFO: stderr: ""
    Jun 27 17:54:58.413: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-811-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:55:08.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4038" for this suite. 06/27/23 17:55:08.421
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:55:08.444
Jun 27 17:55:08.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir-wrapper 06/27/23 17:55:08.447
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:55:08.48
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:55:08.496
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 06/27/23 17:55:08.504
STEP: Creating RC which spawns configmap-volume pods 06/27/23 17:55:09.406
Jun 27 17:55:09.441: INFO: Pod name wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7: Found 0 pods out of 5
Jun 27 17:55:14.471: INFO: Pod name wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/27/23 17:55:14.471
Jun 27 17:55:14.472: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-4pztl" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:14.480: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-4pztl": Phase="Running", Reason="", readiness=true. Elapsed: 8.564867ms
Jun 27 17:55:14.480: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-4pztl" satisfied condition "running"
Jun 27 17:55:14.480: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-hk75n" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:14.489: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-hk75n": Phase="Running", Reason="", readiness=true. Elapsed: 8.366196ms
Jun 27 17:55:14.489: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-hk75n" satisfied condition "running"
Jun 27 17:55:14.489: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-jm85l" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:14.497: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-jm85l": Phase="Running", Reason="", readiness=true. Elapsed: 8.423886ms
Jun 27 17:55:14.497: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-jm85l" satisfied condition "running"
Jun 27 17:55:14.497: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-kf7nv" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:14.507: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-kf7nv": Phase="Running", Reason="", readiness=true. Elapsed: 9.307745ms
Jun 27 17:55:14.507: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-kf7nv" satisfied condition "running"
Jun 27 17:55:14.507: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-rcjhc" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:14.516: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-rcjhc": Phase="Running", Reason="", readiness=true. Elapsed: 9.03273ms
Jun 27 17:55:14.516: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-rcjhc" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7 in namespace emptydir-wrapper-2583, will wait for the garbage collector to delete the pods 06/27/23 17:55:14.516
Jun 27 17:55:14.605: INFO: Deleting ReplicationController wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7 took: 24.346286ms
Jun 27 17:55:14.705: INFO: Terminating ReplicationController wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7 pods took: 100.264315ms
STEP: Creating RC which spawns configmap-volume pods 06/27/23 17:55:18.625
Jun 27 17:55:18.659: INFO: Pod name wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388: Found 0 pods out of 5
Jun 27 17:55:23.684: INFO: Pod name wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/27/23 17:55:23.684
Jun 27 17:55:23.685: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-2w4mf" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:23.696: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-2w4mf": Phase="Running", Reason="", readiness=true. Elapsed: 11.3498ms
Jun 27 17:55:23.696: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-2w4mf" satisfied condition "running"
Jun 27 17:55:23.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-68xxv" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:23.706: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-68xxv": Phase="Running", Reason="", readiness=true. Elapsed: 9.110962ms
Jun 27 17:55:23.706: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-68xxv" satisfied condition "running"
Jun 27 17:55:23.706: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-77f22" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:23.723: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-77f22": Phase="Running", Reason="", readiness=true. Elapsed: 16.438038ms
Jun 27 17:55:23.723: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-77f22" satisfied condition "running"
Jun 27 17:55:23.723: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-9kw4l" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:23.733: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-9kw4l": Phase="Running", Reason="", readiness=true. Elapsed: 9.890869ms
Jun 27 17:55:23.733: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-9kw4l" satisfied condition "running"
Jun 27 17:55:23.733: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-jdb2r" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:23.767: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-jdb2r": Phase="Running", Reason="", readiness=true. Elapsed: 33.099187ms
Jun 27 17:55:23.767: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-jdb2r" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388 in namespace emptydir-wrapper-2583, will wait for the garbage collector to delete the pods 06/27/23 17:55:23.767
Jun 27 17:55:23.940: INFO: Deleting ReplicationController wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388 took: 78.553232ms
Jun 27 17:55:24.141: INFO: Terminating ReplicationController wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388 pods took: 201.009831ms
STEP: Creating RC which spawns configmap-volume pods 06/27/23 17:55:26.779
Jun 27 17:55:26.814: INFO: Pod name wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30: Found 0 pods out of 5
Jun 27 17:55:31.832: INFO: Pod name wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30: Found 5 pods out of 5
STEP: Ensuring each pod is running 06/27/23 17:55:31.832
Jun 27 17:55:31.833: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-5m6lt" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:31.842: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-5m6lt": Phase="Running", Reason="", readiness=true. Elapsed: 9.641142ms
Jun 27 17:55:31.842: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-5m6lt" satisfied condition "running"
Jun 27 17:55:31.842: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-6phjz" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:31.852: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-6phjz": Phase="Running", Reason="", readiness=true. Elapsed: 9.990409ms
Jun 27 17:55:31.852: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-6phjz" satisfied condition "running"
Jun 27 17:55:31.852: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-74l2m" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:31.861: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-74l2m": Phase="Running", Reason="", readiness=true. Elapsed: 8.743146ms
Jun 27 17:55:31.861: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-74l2m" satisfied condition "running"
Jun 27 17:55:31.861: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-n92jf" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:31.874: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-n92jf": Phase="Running", Reason="", readiness=true. Elapsed: 12.605154ms
Jun 27 17:55:31.874: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-n92jf" satisfied condition "running"
Jun 27 17:55:31.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-wd7ff" in namespace "emptydir-wrapper-2583" to be "running"
Jun 27 17:55:31.884: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-wd7ff": Phase="Running", Reason="", readiness=true. Elapsed: 9.21505ms
Jun 27 17:55:31.884: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-wd7ff" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30 in namespace emptydir-wrapper-2583, will wait for the garbage collector to delete the pods 06/27/23 17:55:31.884
Jun 27 17:55:31.977: INFO: Deleting ReplicationController wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30 took: 26.180613ms
Jun 27 17:55:32.078: INFO: Terminating ReplicationController wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30 pods took: 100.722293ms
STEP: Cleaning up the configMaps 06/27/23 17:55:36.179
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Jun 27 17:55:37.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-2583" for this suite. 06/27/23 17:55:37.039
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":302,"skipped":5758,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.623 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:55:08.444
    Jun 27 17:55:08.444: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir-wrapper 06/27/23 17:55:08.447
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:55:08.48
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:55:08.496
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 06/27/23 17:55:08.504
    STEP: Creating RC which spawns configmap-volume pods 06/27/23 17:55:09.406
    Jun 27 17:55:09.441: INFO: Pod name wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7: Found 0 pods out of 5
    Jun 27 17:55:14.471: INFO: Pod name wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/27/23 17:55:14.471
    Jun 27 17:55:14.472: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-4pztl" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:14.480: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-4pztl": Phase="Running", Reason="", readiness=true. Elapsed: 8.564867ms
    Jun 27 17:55:14.480: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-4pztl" satisfied condition "running"
    Jun 27 17:55:14.480: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-hk75n" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:14.489: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-hk75n": Phase="Running", Reason="", readiness=true. Elapsed: 8.366196ms
    Jun 27 17:55:14.489: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-hk75n" satisfied condition "running"
    Jun 27 17:55:14.489: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-jm85l" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:14.497: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-jm85l": Phase="Running", Reason="", readiness=true. Elapsed: 8.423886ms
    Jun 27 17:55:14.497: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-jm85l" satisfied condition "running"
    Jun 27 17:55:14.497: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-kf7nv" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:14.507: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-kf7nv": Phase="Running", Reason="", readiness=true. Elapsed: 9.307745ms
    Jun 27 17:55:14.507: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-kf7nv" satisfied condition "running"
    Jun 27 17:55:14.507: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-rcjhc" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:14.516: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-rcjhc": Phase="Running", Reason="", readiness=true. Elapsed: 9.03273ms
    Jun 27 17:55:14.516: INFO: Pod "wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7-rcjhc" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7 in namespace emptydir-wrapper-2583, will wait for the garbage collector to delete the pods 06/27/23 17:55:14.516
    Jun 27 17:55:14.605: INFO: Deleting ReplicationController wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7 took: 24.346286ms
    Jun 27 17:55:14.705: INFO: Terminating ReplicationController wrapped-volume-race-d5dc52bc-c2f0-47f3-af6d-16e7d8926ab7 pods took: 100.264315ms
    STEP: Creating RC which spawns configmap-volume pods 06/27/23 17:55:18.625
    Jun 27 17:55:18.659: INFO: Pod name wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388: Found 0 pods out of 5
    Jun 27 17:55:23.684: INFO: Pod name wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/27/23 17:55:23.684
    Jun 27 17:55:23.685: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-2w4mf" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:23.696: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-2w4mf": Phase="Running", Reason="", readiness=true. Elapsed: 11.3498ms
    Jun 27 17:55:23.696: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-2w4mf" satisfied condition "running"
    Jun 27 17:55:23.697: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-68xxv" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:23.706: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-68xxv": Phase="Running", Reason="", readiness=true. Elapsed: 9.110962ms
    Jun 27 17:55:23.706: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-68xxv" satisfied condition "running"
    Jun 27 17:55:23.706: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-77f22" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:23.723: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-77f22": Phase="Running", Reason="", readiness=true. Elapsed: 16.438038ms
    Jun 27 17:55:23.723: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-77f22" satisfied condition "running"
    Jun 27 17:55:23.723: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-9kw4l" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:23.733: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-9kw4l": Phase="Running", Reason="", readiness=true. Elapsed: 9.890869ms
    Jun 27 17:55:23.733: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-9kw4l" satisfied condition "running"
    Jun 27 17:55:23.733: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-jdb2r" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:23.767: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-jdb2r": Phase="Running", Reason="", readiness=true. Elapsed: 33.099187ms
    Jun 27 17:55:23.767: INFO: Pod "wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388-jdb2r" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388 in namespace emptydir-wrapper-2583, will wait for the garbage collector to delete the pods 06/27/23 17:55:23.767
    Jun 27 17:55:23.940: INFO: Deleting ReplicationController wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388 took: 78.553232ms
    Jun 27 17:55:24.141: INFO: Terminating ReplicationController wrapped-volume-race-506df646-f5ba-42dc-8cee-7f5b8ba7f388 pods took: 201.009831ms
    STEP: Creating RC which spawns configmap-volume pods 06/27/23 17:55:26.779
    Jun 27 17:55:26.814: INFO: Pod name wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30: Found 0 pods out of 5
    Jun 27 17:55:31.832: INFO: Pod name wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30: Found 5 pods out of 5
    STEP: Ensuring each pod is running 06/27/23 17:55:31.832
    Jun 27 17:55:31.833: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-5m6lt" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:31.842: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-5m6lt": Phase="Running", Reason="", readiness=true. Elapsed: 9.641142ms
    Jun 27 17:55:31.842: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-5m6lt" satisfied condition "running"
    Jun 27 17:55:31.842: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-6phjz" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:31.852: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-6phjz": Phase="Running", Reason="", readiness=true. Elapsed: 9.990409ms
    Jun 27 17:55:31.852: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-6phjz" satisfied condition "running"
    Jun 27 17:55:31.852: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-74l2m" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:31.861: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-74l2m": Phase="Running", Reason="", readiness=true. Elapsed: 8.743146ms
    Jun 27 17:55:31.861: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-74l2m" satisfied condition "running"
    Jun 27 17:55:31.861: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-n92jf" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:31.874: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-n92jf": Phase="Running", Reason="", readiness=true. Elapsed: 12.605154ms
    Jun 27 17:55:31.874: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-n92jf" satisfied condition "running"
    Jun 27 17:55:31.874: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-wd7ff" in namespace "emptydir-wrapper-2583" to be "running"
    Jun 27 17:55:31.884: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-wd7ff": Phase="Running", Reason="", readiness=true. Elapsed: 9.21505ms
    Jun 27 17:55:31.884: INFO: Pod "wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30-wd7ff" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30 in namespace emptydir-wrapper-2583, will wait for the garbage collector to delete the pods 06/27/23 17:55:31.884
    Jun 27 17:55:31.977: INFO: Deleting ReplicationController wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30 took: 26.180613ms
    Jun 27 17:55:32.078: INFO: Terminating ReplicationController wrapped-volume-race-7c71f0ee-b48e-4d8a-8df6-8708e7855a30 pods took: 100.722293ms
    STEP: Cleaning up the configMaps 06/27/23 17:55:36.179
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Jun 27 17:55:37.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-2583" for this suite. 06/27/23 17:55:37.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:55:37.077
Jun 27 17:55:37.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:55:37.079
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:55:37.114
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:55:37.128
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:55:37.137
Jun 27 17:55:37.238: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6" in namespace "projected-9489" to be "Succeeded or Failed"
Jun 27 17:55:37.257: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.03625ms
Jun 27 17:55:39.268: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028809175s
Jun 27 17:55:41.269: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029664693s
Jun 27 17:55:43.271: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031566999s
STEP: Saw pod success 06/27/23 17:55:43.271
Jun 27 17:55:43.271: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6" satisfied condition "Succeeded or Failed"
Jun 27 17:55:43.280: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6 container client-container: <nil>
STEP: delete the pod 06/27/23 17:55:43.34
Jun 27 17:55:43.374: INFO: Waiting for pod downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6 to disappear
Jun 27 17:55:43.385: INFO: Pod downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Jun 27 17:55:43.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9489" for this suite. 06/27/23 17:55:43.401
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":303,"skipped":5792,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.341 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:55:37.077
    Jun 27 17:55:37.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:55:37.079
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:55:37.114
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:55:37.128
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:55:37.137
    Jun 27 17:55:37.238: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6" in namespace "projected-9489" to be "Succeeded or Failed"
    Jun 27 17:55:37.257: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6": Phase="Pending", Reason="", readiness=false. Elapsed: 18.03625ms
    Jun 27 17:55:39.268: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.028809175s
    Jun 27 17:55:41.269: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.029664693s
    Jun 27 17:55:43.271: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.031566999s
    STEP: Saw pod success 06/27/23 17:55:43.271
    Jun 27 17:55:43.271: INFO: Pod "downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6" satisfied condition "Succeeded or Failed"
    Jun 27 17:55:43.280: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:55:43.34
    Jun 27 17:55:43.374: INFO: Waiting for pod downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6 to disappear
    Jun 27 17:55:43.385: INFO: Pod downwardapi-volume-e4b14e17-2cb7-4772-83f0-a584dcfa33d6 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Jun 27 17:55:43.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9489" for this suite. 06/27/23 17:55:43.401
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:55:43.426
Jun 27 17:55:43.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pod-network-test 06/27/23 17:55:43.429
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:55:43.458
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:55:43.468
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4505 06/27/23 17:55:43.484
STEP: creating a selector 06/27/23 17:55:43.496
STEP: Creating the service pods in kubernetes 06/27/23 17:55:43.496
Jun 27 17:55:43.496: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Jun 27 17:55:43.642: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4505" to be "running and ready"
Jun 27 17:55:43.654: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.682556ms
Jun 27 17:55:43.654: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:55:45.664: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021660892s
Jun 27 17:55:45.664: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:55:47.667: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.024405304s
Jun 27 17:55:47.667: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:55:49.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.021222958s
Jun 27 17:55:49.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:55:51.665: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.022772255s
Jun 27 17:55:51.665: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:55:53.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.021909541s
Jun 27 17:55:53.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:55:55.733: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.090465171s
Jun 27 17:55:55.733: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:55:57.676: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.033232408s
Jun 27 17:55:57.676: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:55:59.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021488817s
Jun 27 17:55:59.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:56:01.665: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.023056879s
Jun 27 17:56:01.665: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:56:03.666: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023279234s
Jun 27 17:56:03.666: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Jun 27 17:56:05.665: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.022333535s
Jun 27 17:56:05.665: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Jun 27 17:56:05.665: INFO: Pod "netserver-0" satisfied condition "running and ready"
Jun 27 17:56:05.675: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4505" to be "running and ready"
Jun 27 17:56:05.685: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.229766ms
Jun 27 17:56:05.685: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Jun 27 17:56:05.685: INFO: Pod "netserver-1" satisfied condition "running and ready"
Jun 27 17:56:05.694: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4505" to be "running and ready"
Jun 27 17:56:05.703: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.494569ms
Jun 27 17:56:05.703: INFO: The phase of Pod netserver-2 is Running (Ready = true)
Jun 27 17:56:05.703: INFO: Pod "netserver-2" satisfied condition "running and ready"
STEP: Creating test pods 06/27/23 17:56:05.714
Jun 27 17:56:05.782: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4505" to be "running"
Jun 27 17:56:05.795: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.605938ms
Jun 27 17:56:07.816: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03472594s
Jun 27 17:56:07.816: INFO: Pod "test-container-pod" satisfied condition "running"
Jun 27 17:56:07.825: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4505" to be "running"
Jun 27 17:56:07.836: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.268723ms
Jun 27 17:56:07.836: INFO: Pod "host-test-container-pod" satisfied condition "running"
Jun 27 17:56:07.850: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
Jun 27 17:56:07.851: INFO: Going to poll 172.30.106.160 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 27 17:56:07.859: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.106.160 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:56:07.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:56:07.862: INFO: ExecWithOptions: Clientset creation
Jun 27 17:56:07.862: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.106.160+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 27 17:56:09.162: INFO: Found all 1 expected endpoints: [netserver-0]
Jun 27 17:56:09.162: INFO: Going to poll 172.30.250.196 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 27 17:56:09.171: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.250.196 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:56:09.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:56:09.173: INFO: ExecWithOptions: Clientset creation
Jun 27 17:56:09.173: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.250.196+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 27 17:56:10.460: INFO: Found all 1 expected endpoints: [netserver-1]
Jun 27 17:56:10.460: INFO: Going to poll 172.30.60.116 on port 8081 at least 0 times, with a maximum of 39 tries before failing
Jun 27 17:56:10.475: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.60.116 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Jun 27 17:56:10.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:56:10.479: INFO: ExecWithOptions: Clientset creation
Jun 27 17:56:10.479: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.60.116+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Jun 27 17:56:11.756: INFO: Found all 1 expected endpoints: [netserver-2]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Jun 27 17:56:11.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4505" for this suite. 06/27/23 17:56:11.774
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":304,"skipped":5803,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.383 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:55:43.426
    Jun 27 17:55:43.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pod-network-test 06/27/23 17:55:43.429
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:55:43.458
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:55:43.468
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4505 06/27/23 17:55:43.484
    STEP: creating a selector 06/27/23 17:55:43.496
    STEP: Creating the service pods in kubernetes 06/27/23 17:55:43.496
    Jun 27 17:55:43.496: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Jun 27 17:55:43.642: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4505" to be "running and ready"
    Jun 27 17:55:43.654: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.682556ms
    Jun 27 17:55:43.654: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:55:45.664: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021660892s
    Jun 27 17:55:45.664: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:55:47.667: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.024405304s
    Jun 27 17:55:47.667: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:55:49.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.021222958s
    Jun 27 17:55:49.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:55:51.665: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.022772255s
    Jun 27 17:55:51.665: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:55:53.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.021909541s
    Jun 27 17:55:53.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:55:55.733: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.090465171s
    Jun 27 17:55:55.733: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:55:57.676: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.033232408s
    Jun 27 17:55:57.676: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:55:59.664: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.021488817s
    Jun 27 17:55:59.664: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:56:01.665: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.023056879s
    Jun 27 17:56:01.665: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:56:03.666: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.023279234s
    Jun 27 17:56:03.666: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Jun 27 17:56:05.665: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.022333535s
    Jun 27 17:56:05.665: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Jun 27 17:56:05.665: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Jun 27 17:56:05.675: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4505" to be "running and ready"
    Jun 27 17:56:05.685: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 10.229766ms
    Jun 27 17:56:05.685: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Jun 27 17:56:05.685: INFO: Pod "netserver-1" satisfied condition "running and ready"
    Jun 27 17:56:05.694: INFO: Waiting up to 5m0s for pod "netserver-2" in namespace "pod-network-test-4505" to be "running and ready"
    Jun 27 17:56:05.703: INFO: Pod "netserver-2": Phase="Running", Reason="", readiness=true. Elapsed: 8.494569ms
    Jun 27 17:56:05.703: INFO: The phase of Pod netserver-2 is Running (Ready = true)
    Jun 27 17:56:05.703: INFO: Pod "netserver-2" satisfied condition "running and ready"
    STEP: Creating test pods 06/27/23 17:56:05.714
    Jun 27 17:56:05.782: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4505" to be "running"
    Jun 27 17:56:05.795: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 13.605938ms
    Jun 27 17:56:07.816: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.03472594s
    Jun 27 17:56:07.816: INFO: Pod "test-container-pod" satisfied condition "running"
    Jun 27 17:56:07.825: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4505" to be "running"
    Jun 27 17:56:07.836: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 11.268723ms
    Jun 27 17:56:07.836: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Jun 27 17:56:07.850: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
    Jun 27 17:56:07.851: INFO: Going to poll 172.30.106.160 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 27 17:56:07.859: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.106.160 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:56:07.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:56:07.862: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:56:07.862: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.106.160+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 27 17:56:09.162: INFO: Found all 1 expected endpoints: [netserver-0]
    Jun 27 17:56:09.162: INFO: Going to poll 172.30.250.196 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 27 17:56:09.171: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.250.196 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:56:09.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:56:09.173: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:56:09.173: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.250.196+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 27 17:56:10.460: INFO: Found all 1 expected endpoints: [netserver-1]
    Jun 27 17:56:10.460: INFO: Going to poll 172.30.60.116 on port 8081 at least 0 times, with a maximum of 39 tries before failing
    Jun 27 17:56:10.475: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.30.60.116 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Jun 27 17:56:10.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:56:10.479: INFO: ExecWithOptions: Clientset creation
    Jun 27 17:56:10.479: INFO: ExecWithOptions: execute(POST https://172.21.0.1:443/api/v1/namespaces/pod-network-test-4505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.30.60.116+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Jun 27 17:56:11.756: INFO: Found all 1 expected endpoints: [netserver-2]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Jun 27 17:56:11.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4505" for this suite. 06/27/23 17:56:11.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:11.813
Jun 27 17:56:11.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:56:11.815
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:11.859
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:11.875
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
Jun 27 17:56:11.910: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating projection with configMap that has name projected-configmap-test-upd-8afd4571-0df1-4296-81fb-40b7f2aa9f75 06/27/23 17:56:11.91
STEP: Creating the pod 06/27/23 17:56:11.922
Jun 27 17:56:11.980: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb" in namespace "projected-721" to be "running and ready"
Jun 27 17:56:12.007: INFO: Pod "pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.51027ms
Jun 27 17:56:12.007: INFO: The phase of Pod pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:56:14.018: INFO: Pod "pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb": Phase="Running", Reason="", readiness=true. Elapsed: 2.038013384s
Jun 27 17:56:14.019: INFO: The phase of Pod pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb is Running (Ready = true)
Jun 27 17:56:14.019: INFO: Pod "pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-8afd4571-0df1-4296-81fb-40b7f2aa9f75 06/27/23 17:56:14.059
STEP: waiting to observe update in volume 06/27/23 17:56:14.072
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 27 17:56:16.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-721" for this suite. 06/27/23 17:56:16.147
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":305,"skipped":5810,"failed":0}
------------------------------
â€¢ [4.354 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:11.813
    Jun 27 17:56:11.813: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:56:11.815
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:11.859
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:11.875
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    Jun 27 17:56:11.910: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-8afd4571-0df1-4296-81fb-40b7f2aa9f75 06/27/23 17:56:11.91
    STEP: Creating the pod 06/27/23 17:56:11.922
    Jun 27 17:56:11.980: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb" in namespace "projected-721" to be "running and ready"
    Jun 27 17:56:12.007: INFO: Pod "pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb": Phase="Pending", Reason="", readiness=false. Elapsed: 26.51027ms
    Jun 27 17:56:12.007: INFO: The phase of Pod pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:56:14.018: INFO: Pod "pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb": Phase="Running", Reason="", readiness=true. Elapsed: 2.038013384s
    Jun 27 17:56:14.019: INFO: The phase of Pod pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb is Running (Ready = true)
    Jun 27 17:56:14.019: INFO: Pod "pod-projected-configmaps-7eef0b55-404a-4b6d-bd0c-b1101f64bccb" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-8afd4571-0df1-4296-81fb-40b7f2aa9f75 06/27/23 17:56:14.059
    STEP: waiting to observe update in volume 06/27/23 17:56:14.072
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 27 17:56:16.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-721" for this suite. 06/27/23 17:56:16.147
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:16.178
Jun 27 17:56:16.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:56:16.18
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:16.216
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:16.225
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-c6800b81-b892-47dc-b069-ce317d190c1c 06/27/23 17:56:16.238
STEP: Creating a pod to test consume configMaps 06/27/23 17:56:16.259
Jun 27 17:56:16.308: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa" in namespace "projected-3023" to be "Succeeded or Failed"
Jun 27 17:56:16.319: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa": Phase="Pending", Reason="", readiness=false. Elapsed: 11.012571ms
Jun 27 17:56:18.329: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020295023s
Jun 27 17:56:20.329: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020797114s
Jun 27 17:56:22.331: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022217674s
STEP: Saw pod success 06/27/23 17:56:22.331
Jun 27 17:56:22.331: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa" satisfied condition "Succeeded or Failed"
Jun 27 17:56:22.344: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa container agnhost-container: <nil>
STEP: delete the pod 06/27/23 17:56:22.371
Jun 27 17:56:22.402: INFO: Waiting for pod pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa to disappear
Jun 27 17:56:22.410: INFO: Pod pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 27 17:56:22.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3023" for this suite. 06/27/23 17:56:22.436
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":306,"skipped":5820,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.277 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:16.178
    Jun 27 17:56:16.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:56:16.18
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:16.216
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:16.225
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-c6800b81-b892-47dc-b069-ce317d190c1c 06/27/23 17:56:16.238
    STEP: Creating a pod to test consume configMaps 06/27/23 17:56:16.259
    Jun 27 17:56:16.308: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa" in namespace "projected-3023" to be "Succeeded or Failed"
    Jun 27 17:56:16.319: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa": Phase="Pending", Reason="", readiness=false. Elapsed: 11.012571ms
    Jun 27 17:56:18.329: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020295023s
    Jun 27 17:56:20.329: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa": Phase="Pending", Reason="", readiness=false. Elapsed: 4.020797114s
    Jun 27 17:56:22.331: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.022217674s
    STEP: Saw pod success 06/27/23 17:56:22.331
    Jun 27 17:56:22.331: INFO: Pod "pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa" satisfied condition "Succeeded or Failed"
    Jun 27 17:56:22.344: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 17:56:22.371
    Jun 27 17:56:22.402: INFO: Waiting for pod pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa to disappear
    Jun 27 17:56:22.410: INFO: Pod pod-projected-configmaps-c514e141-863f-4502-a705-c58a483642fa no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 27 17:56:22.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3023" for this suite. 06/27/23 17:56:22.436
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:22.461
Jun 27 17:56:22.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename namespaces 06/27/23 17:56:22.465
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:22.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:22.54
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 06/27/23 17:56:22.549
Jun 27 17:56:22.561: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 06/27/23 17:56:22.561
Jun 27 17:56:22.575: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 06/27/23 17:56:22.576
Jun 27 17:56:22.601: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:56:22.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-4347" for this suite. 06/27/23 17:56:22.616
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":307,"skipped":5821,"failed":0}
------------------------------
â€¢ [0.171 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:22.461
    Jun 27 17:56:22.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename namespaces 06/27/23 17:56:22.465
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:22.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:22.54
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 06/27/23 17:56:22.549
    Jun 27 17:56:22.561: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 06/27/23 17:56:22.561
    Jun 27 17:56:22.575: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 06/27/23 17:56:22.576
    Jun 27 17:56:22.601: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:56:22.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-4347" for this suite. 06/27/23 17:56:22.616
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:22.643
Jun 27 17:56:22.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename dns 06/27/23 17:56:22.646
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:22.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:22.69
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5690.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5690.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 06/27/23 17:56:22.7
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5690.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5690.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 06/27/23 17:56:22.7
STEP: creating a pod to probe /etc/hosts 06/27/23 17:56:22.7
STEP: submitting the pod to kubernetes 06/27/23 17:56:22.701
Jun 27 17:56:22.769: INFO: Waiting up to 15m0s for pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492" in namespace "dns-5690" to be "running"
Jun 27 17:56:22.783: INFO: Pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492": Phase="Pending", Reason="", readiness=false. Elapsed: 14.119422ms
Jun 27 17:56:24.794: INFO: Pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024838757s
Jun 27 17:56:26.793: INFO: Pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492": Phase="Running", Reason="", readiness=true. Elapsed: 4.024430186s
Jun 27 17:56:26.793: INFO: Pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492" satisfied condition "running"
STEP: retrieving the pod 06/27/23 17:56:26.793
STEP: looking for the results for each expected name from probers 06/27/23 17:56:26.805
Jun 27 17:56:26.898: INFO: DNS probes using dns-5690/dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492 succeeded

STEP: deleting the pod 06/27/23 17:56:26.898
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Jun 27 17:56:26.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5690" for this suite. 06/27/23 17:56:26.945
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":308,"skipped":5835,"failed":0}
------------------------------
â€¢ [4.318 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:22.643
    Jun 27 17:56:22.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename dns 06/27/23 17:56:22.646
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:22.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:22.69
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5690.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-5690.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     06/27/23 17:56:22.7
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-5690.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-5690.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     06/27/23 17:56:22.7
    STEP: creating a pod to probe /etc/hosts 06/27/23 17:56:22.7
    STEP: submitting the pod to kubernetes 06/27/23 17:56:22.701
    Jun 27 17:56:22.769: INFO: Waiting up to 15m0s for pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492" in namespace "dns-5690" to be "running"
    Jun 27 17:56:22.783: INFO: Pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492": Phase="Pending", Reason="", readiness=false. Elapsed: 14.119422ms
    Jun 27 17:56:24.794: INFO: Pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492": Phase="Pending", Reason="", readiness=false. Elapsed: 2.024838757s
    Jun 27 17:56:26.793: INFO: Pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492": Phase="Running", Reason="", readiness=true. Elapsed: 4.024430186s
    Jun 27 17:56:26.793: INFO: Pod "dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492" satisfied condition "running"
    STEP: retrieving the pod 06/27/23 17:56:26.793
    STEP: looking for the results for each expected name from probers 06/27/23 17:56:26.805
    Jun 27 17:56:26.898: INFO: DNS probes using dns-5690/dns-test-4c6b9f7c-9941-4bfe-8e25-874f004bd492 succeeded

    STEP: deleting the pod 06/27/23 17:56:26.898
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Jun 27 17:56:26.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5690" for this suite. 06/27/23 17:56:26.945
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:26.969
Jun 27 17:56:26.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename job 06/27/23 17:56:26.972
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:27.008
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 06/27/23 17:56:27.016
W0627 17:56:27.032504      23 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
STEP: Ensuring active pods == parallelism 06/27/23 17:56:27.032
STEP: Orphaning one of the Job's Pods 06/27/23 17:56:31.043
Jun 27 17:56:31.584: INFO: Successfully updated pod "adopt-release-4vnjm"
STEP: Checking that the Job readopts the Pod 06/27/23 17:56:31.584
Jun 27 17:56:31.584: INFO: Waiting up to 15m0s for pod "adopt-release-4vnjm" in namespace "job-271" to be "adopted"
Jun 27 17:56:31.597: INFO: Pod "adopt-release-4vnjm": Phase="Running", Reason="", readiness=true. Elapsed: 12.253935ms
Jun 27 17:56:33.607: INFO: Pod "adopt-release-4vnjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.022329451s
Jun 27 17:56:33.607: INFO: Pod "adopt-release-4vnjm" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 06/27/23 17:56:33.607
Jun 27 17:56:34.153: INFO: Successfully updated pod "adopt-release-4vnjm"
STEP: Checking that the Job releases the Pod 06/27/23 17:56:34.153
Jun 27 17:56:34.155: INFO: Waiting up to 15m0s for pod "adopt-release-4vnjm" in namespace "job-271" to be "released"
Jun 27 17:56:34.164: INFO: Pod "adopt-release-4vnjm": Phase="Running", Reason="", readiness=true. Elapsed: 8.368268ms
Jun 27 17:56:36.174: INFO: Pod "adopt-release-4vnjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.018397326s
Jun 27 17:56:36.174: INFO: Pod "adopt-release-4vnjm" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 27 17:56:36.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-271" for this suite. 06/27/23 17:56:36.19
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":309,"skipped":5836,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.240 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:26.969
    Jun 27 17:56:26.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename job 06/27/23 17:56:26.972
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:27.008
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 06/27/23 17:56:27.016
    W0627 17:56:27.032504      23 warnings.go:70] would violate PodSecurity "restricted:latest": allowPrivilegeEscalation != false (container "c" must set securityContext.allowPrivilegeEscalation=false), unrestricted capabilities (container "c" must set securityContext.capabilities.drop=["ALL"]), runAsNonRoot != true (pod or container "c" must set securityContext.runAsNonRoot=true), seccompProfile (pod or container "c" must set securityContext.seccompProfile.type to "RuntimeDefault" or "Localhost")
    STEP: Ensuring active pods == parallelism 06/27/23 17:56:27.032
    STEP: Orphaning one of the Job's Pods 06/27/23 17:56:31.043
    Jun 27 17:56:31.584: INFO: Successfully updated pod "adopt-release-4vnjm"
    STEP: Checking that the Job readopts the Pod 06/27/23 17:56:31.584
    Jun 27 17:56:31.584: INFO: Waiting up to 15m0s for pod "adopt-release-4vnjm" in namespace "job-271" to be "adopted"
    Jun 27 17:56:31.597: INFO: Pod "adopt-release-4vnjm": Phase="Running", Reason="", readiness=true. Elapsed: 12.253935ms
    Jun 27 17:56:33.607: INFO: Pod "adopt-release-4vnjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.022329451s
    Jun 27 17:56:33.607: INFO: Pod "adopt-release-4vnjm" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 06/27/23 17:56:33.607
    Jun 27 17:56:34.153: INFO: Successfully updated pod "adopt-release-4vnjm"
    STEP: Checking that the Job releases the Pod 06/27/23 17:56:34.153
    Jun 27 17:56:34.155: INFO: Waiting up to 15m0s for pod "adopt-release-4vnjm" in namespace "job-271" to be "released"
    Jun 27 17:56:34.164: INFO: Pod "adopt-release-4vnjm": Phase="Running", Reason="", readiness=true. Elapsed: 8.368268ms
    Jun 27 17:56:36.174: INFO: Pod "adopt-release-4vnjm": Phase="Running", Reason="", readiness=true. Elapsed: 2.018397326s
    Jun 27 17:56:36.174: INFO: Pod "adopt-release-4vnjm" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 27 17:56:36.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-271" for this suite. 06/27/23 17:56:36.19
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:36.21
Jun 27 17:56:36.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 17:56:36.213
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:36.249
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:36.26
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 17:56:36.33
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:56:36.666
STEP: Deploying the webhook pod 06/27/23 17:56:36.696
STEP: Wait for the deployment to be ready 06/27/23 17:56:36.721
Jun 27 17:56:36.740: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 27 17:56:38.787: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 56, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 56, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 56, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 56, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 17:56:40.813
STEP: Verifying the service has paired with the endpoint 06/27/23 17:56:40.857
Jun 27 17:56:41.858: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 06/27/23 17:56:42.104
STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 17:56:42.246
STEP: Deleting the collection of validation webhooks 06/27/23 17:56:42.316
STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 17:56:42.464
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:56:42.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9880" for this suite. 06/27/23 17:56:42.55
STEP: Destroying namespace "webhook-9880-markers" for this suite. 06/27/23 17:56:42.573
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":310,"skipped":5837,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.608 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:36.21
    Jun 27 17:56:36.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 17:56:36.213
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:36.249
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:36.26
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 17:56:36.33
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:56:36.666
    STEP: Deploying the webhook pod 06/27/23 17:56:36.696
    STEP: Wait for the deployment to be ready 06/27/23 17:56:36.721
    Jun 27 17:56:36.740: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 27 17:56:38.787: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 56, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 56, 36, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 56, 36, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 56, 36, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 17:56:40.813
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:56:40.857
    Jun 27 17:56:41.858: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 06/27/23 17:56:42.104
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 17:56:42.246
    STEP: Deleting the collection of validation webhooks 06/27/23 17:56:42.316
    STEP: Creating a configMap that does not comply to the validation webhook rules 06/27/23 17:56:42.464
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:56:42.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9880" for this suite. 06/27/23 17:56:42.55
    STEP: Destroying namespace "webhook-9880-markers" for this suite. 06/27/23 17:56:42.573
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:42.818
Jun 27 17:56:42.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename watch 06/27/23 17:56:42.821
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:42.895
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:42.901
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 06/27/23 17:56:42.908
STEP: creating a new configmap 06/27/23 17:56:42.916
STEP: modifying the configmap once 06/27/23 17:56:42.954
STEP: changing the label value of the configmap 06/27/23 17:56:43.013
STEP: Expecting to observe a delete notification for the watched object 06/27/23 17:56:43.093
Jun 27 17:56:43.093: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 138962 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:56:43.093: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 138968 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:56:43.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 138974 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 06/27/23 17:56:43.094
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/27/23 17:56:43.126
STEP: changing the label value of the configmap back 06/27/23 17:56:53.127
STEP: modifying the configmap a third time 06/27/23 17:56:53.15
STEP: deleting the configmap 06/27/23 17:56:53.18
STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/27/23 17:56:53.198
Jun 27 17:56:53.198: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 139081 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:56:53.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 139082 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 17:56:53.199: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 139083 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 27 17:56:53.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-6000" for this suite. 06/27/23 17:56:53.22
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":311,"skipped":5837,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.437 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:42.818
    Jun 27 17:56:42.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename watch 06/27/23 17:56:42.821
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:42.895
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:42.901
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 06/27/23 17:56:42.908
    STEP: creating a new configmap 06/27/23 17:56:42.916
    STEP: modifying the configmap once 06/27/23 17:56:42.954
    STEP: changing the label value of the configmap 06/27/23 17:56:43.013
    STEP: Expecting to observe a delete notification for the watched object 06/27/23 17:56:43.093
    Jun 27 17:56:43.093: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 138962 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:42 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:56:43.093: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 138968 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:56:43.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 138974 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:42 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 06/27/23 17:56:43.094
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 06/27/23 17:56:43.126
    STEP: changing the label value of the configmap back 06/27/23 17:56:53.127
    STEP: modifying the configmap a third time 06/27/23 17:56:53.15
    STEP: deleting the configmap 06/27/23 17:56:53.18
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 06/27/23 17:56:53.198
    Jun 27 17:56:53.198: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 139081 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:56:53.199: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 139082 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 17:56:53.199: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-6000  820974a5-6cf2-404e-9091-dea58f9d540d 139083 0 2023-06-27 17:56:42 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-06-27 17:56:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 27 17:56:53.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-6000" for this suite. 06/27/23 17:56:53.22
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:53.256
Jun 27 17:56:53.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 17:56:53.259
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:53.318
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:53.347
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 06/27/23 17:56:53.393
Jun 27 17:56:53.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9303 create -f -'
Jun 27 17:56:55.124: INFO: stderr: ""
Jun 27 17:56:55.124: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 06/27/23 17:56:55.124
Jun 27 17:56:55.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9303 diff -f -'
Jun 27 17:56:55.753: INFO: rc: 1
Jun 27 17:56:55.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9303 delete -f -'
Jun 27 17:56:55.910: INFO: stderr: ""
Jun 27 17:56:55.910: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 17:56:55.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9303" for this suite. 06/27/23 17:56:55.927
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":312,"skipped":5838,"failed":0}
------------------------------
â€¢ [2.689 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:53.256
    Jun 27 17:56:53.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 17:56:53.259
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:53.318
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:53.347
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 06/27/23 17:56:53.393
    Jun 27 17:56:53.394: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9303 create -f -'
    Jun 27 17:56:55.124: INFO: stderr: ""
    Jun 27 17:56:55.124: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 06/27/23 17:56:55.124
    Jun 27 17:56:55.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9303 diff -f -'
    Jun 27 17:56:55.753: INFO: rc: 1
    Jun 27 17:56:55.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9303 delete -f -'
    Jun 27 17:56:55.910: INFO: stderr: ""
    Jun 27 17:56:55.910: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 17:56:55.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9303" for this suite. 06/27/23 17:56:55.927
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:55.962
Jun 27 17:56:55.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename containers 06/27/23 17:56:55.964
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:56.034
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:56.042
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Jun 27 17:56:56.218: INFO: Waiting up to 5m0s for pod "client-containers-ce9d12db-c33b-4b3d-9bc5-bca17398efb2" in namespace "containers-3113" to be "running"
Jun 27 17:56:56.236: INFO: Pod "client-containers-ce9d12db-c33b-4b3d-9bc5-bca17398efb2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.083765ms
Jun 27 17:56:58.248: INFO: Pod "client-containers-ce9d12db-c33b-4b3d-9bc5-bca17398efb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029765635s
Jun 27 17:56:58.248: INFO: Pod "client-containers-ce9d12db-c33b-4b3d-9bc5-bca17398efb2" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Jun 27 17:56:58.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3113" for this suite. 06/27/23 17:56:58.305
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":313,"skipped":5906,"failed":0}
------------------------------
â€¢ [2.372 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:55.962
    Jun 27 17:56:55.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename containers 06/27/23 17:56:55.964
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:56.034
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:56.042
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Jun 27 17:56:56.218: INFO: Waiting up to 5m0s for pod "client-containers-ce9d12db-c33b-4b3d-9bc5-bca17398efb2" in namespace "containers-3113" to be "running"
    Jun 27 17:56:56.236: INFO: Pod "client-containers-ce9d12db-c33b-4b3d-9bc5-bca17398efb2": Phase="Pending", Reason="", readiness=false. Elapsed: 17.083765ms
    Jun 27 17:56:58.248: INFO: Pod "client-containers-ce9d12db-c33b-4b3d-9bc5-bca17398efb2": Phase="Running", Reason="", readiness=true. Elapsed: 2.029765635s
    Jun 27 17:56:58.248: INFO: Pod "client-containers-ce9d12db-c33b-4b3d-9bc5-bca17398efb2" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Jun 27 17:56:58.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3113" for this suite. 06/27/23 17:56:58.305
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:56:58.335
Jun 27 17:56:58.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:56:58.336
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:58.373
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:58.38
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:56:58.39
Jun 27 17:56:58.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74" in namespace "downward-api-4765" to be "Succeeded or Failed"
Jun 27 17:56:58.469: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74": Phase="Pending", Reason="", readiness=false. Elapsed: 24.598028ms
Jun 27 17:57:00.480: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034919259s
Jun 27 17:57:02.479: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034535776s
Jun 27 17:57:04.479: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03460887s
STEP: Saw pod success 06/27/23 17:57:04.48
Jun 27 17:57:04.480: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74" satisfied condition "Succeeded or Failed"
Jun 27 17:57:04.493: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74 container client-container: <nil>
STEP: delete the pod 06/27/23 17:57:04.531
Jun 27 17:57:04.556: INFO: Waiting for pod downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74 to disappear
Jun 27 17:57:04.565: INFO: Pod downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 17:57:04.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4765" for this suite. 06/27/23 17:57:04.582
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":314,"skipped":5930,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.279 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:56:58.335
    Jun 27 17:56:58.335: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:56:58.336
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:56:58.373
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:56:58.38
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:56:58.39
    Jun 27 17:56:58.445: INFO: Waiting up to 5m0s for pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74" in namespace "downward-api-4765" to be "Succeeded or Failed"
    Jun 27 17:56:58.469: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74": Phase="Pending", Reason="", readiness=false. Elapsed: 24.598028ms
    Jun 27 17:57:00.480: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74": Phase="Pending", Reason="", readiness=false. Elapsed: 2.034919259s
    Jun 27 17:57:02.479: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74": Phase="Pending", Reason="", readiness=false. Elapsed: 4.034535776s
    Jun 27 17:57:04.479: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.03460887s
    STEP: Saw pod success 06/27/23 17:57:04.48
    Jun 27 17:57:04.480: INFO: Pod "downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74" satisfied condition "Succeeded or Failed"
    Jun 27 17:57:04.493: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:57:04.531
    Jun 27 17:57:04.556: INFO: Waiting for pod downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74 to disappear
    Jun 27 17:57:04.565: INFO: Pod downwardapi-volume-f969f53c-bda2-44dd-bcaa-36cc6d4eed74 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 17:57:04.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4765" for this suite. 06/27/23 17:57:04.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:57:04.617
Jun 27 17:57:04.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename daemonsets 06/27/23 17:57:04.618
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:04.658
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:04.671
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 06/27/23 17:57:04.774
STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 17:57:04.786
Jun 27 17:57:04.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:57:04.808: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:57:05.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:57:05.831: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:57:06.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:57:06.835: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:57:07.839: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 27 17:57:07.839: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 06/27/23 17:57:07.85
Jun 27 17:57:07.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:57:07.905: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:57:08.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:57:08.933: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:57:09.968: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:57:09.968: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:57:10.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:57:10.938: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:57:11.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:57:11.937: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:57:12.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Jun 27 17:57:12.957: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
Jun 27 17:57:13.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
Jun 27 17:57:13.932: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 06/27/23 17:57:13.94
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2144, will wait for the garbage collector to delete the pods 06/27/23 17:57:13.941
Jun 27 17:57:14.052: INFO: Deleting DaemonSet.extensions daemon-set took: 20.95347ms
Jun 27 17:57:14.153: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.121437ms
Jun 27 17:57:17.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Jun 27 17:57:17.782: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Jun 27 17:57:17.811: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"139614"},"items":null}

Jun 27 17:57:17.830: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"139614"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Jun 27 17:57:17.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2144" for this suite. 06/27/23 17:57:17.924
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":315,"skipped":5955,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.326 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:57:04.617
    Jun 27 17:57:04.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename daemonsets 06/27/23 17:57:04.618
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:04.658
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:04.671
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 06/27/23 17:57:04.774
    STEP: Check that daemon pods launch on every node of the cluster. 06/27/23 17:57:04.786
    Jun 27 17:57:04.807: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:57:04.808: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:57:05.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:57:05.831: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:57:06.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:57:06.835: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:57:07.839: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 27 17:57:07.839: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 06/27/23 17:57:07.85
    Jun 27 17:57:07.905: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:57:07.905: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:57:08.933: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:57:08.933: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:57:09.968: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:57:09.968: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:57:10.938: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:57:10.938: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:57:11.937: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:57:11.937: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:57:12.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Jun 27 17:57:12.957: INFO: Node 10.113.180.89 is running 0 daemon pod, expected 1
    Jun 27 17:57:13.932: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
    Jun 27 17:57:13.932: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 06/27/23 17:57:13.94
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2144, will wait for the garbage collector to delete the pods 06/27/23 17:57:13.941
    Jun 27 17:57:14.052: INFO: Deleting DaemonSet.extensions daemon-set took: 20.95347ms
    Jun 27 17:57:14.153: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.121437ms
    Jun 27 17:57:17.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Jun 27 17:57:17.782: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Jun 27 17:57:17.811: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"139614"},"items":null}

    Jun 27 17:57:17.830: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"139614"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Jun 27 17:57:17.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2144" for this suite. 06/27/23 17:57:17.924
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:57:17.949
Jun 27 17:57:17.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 17:57:17.951
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:18.008
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:18.017
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-196a9667-3be6-4ce1-867a-6c3addb7517a 06/27/23 17:57:18.025
STEP: Creating a pod to test consume secrets 06/27/23 17:57:18.062
Jun 27 17:57:18.118: INFO: Waiting up to 5m0s for pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8" in namespace "secrets-6944" to be "Succeeded or Failed"
Jun 27 17:57:18.129: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.361926ms
Jun 27 17:57:20.140: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021937318s
Jun 27 17:57:22.140: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022189285s
Jun 27 17:57:24.140: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021741972s
STEP: Saw pod success 06/27/23 17:57:24.14
Jun 27 17:57:24.140: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8" satisfied condition "Succeeded or Failed"
Jun 27 17:57:24.148: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8 container secret-env-test: <nil>
STEP: delete the pod 06/27/23 17:57:24.179
Jun 27 17:57:24.210: INFO: Waiting for pod pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8 to disappear
Jun 27 17:57:24.219: INFO: Pod pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Jun 27 17:57:24.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6944" for this suite. 06/27/23 17:57:24.233
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":316,"skipped":5971,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.304 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:57:17.949
    Jun 27 17:57:17.949: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 17:57:17.951
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:18.008
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:18.017
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-196a9667-3be6-4ce1-867a-6c3addb7517a 06/27/23 17:57:18.025
    STEP: Creating a pod to test consume secrets 06/27/23 17:57:18.062
    Jun 27 17:57:18.118: INFO: Waiting up to 5m0s for pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8" in namespace "secrets-6944" to be "Succeeded or Failed"
    Jun 27 17:57:18.129: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8": Phase="Pending", Reason="", readiness=false. Elapsed: 11.361926ms
    Jun 27 17:57:20.140: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.021937318s
    Jun 27 17:57:22.140: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8": Phase="Pending", Reason="", readiness=false. Elapsed: 4.022189285s
    Jun 27 17:57:24.140: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.021741972s
    STEP: Saw pod success 06/27/23 17:57:24.14
    Jun 27 17:57:24.140: INFO: Pod "pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8" satisfied condition "Succeeded or Failed"
    Jun 27 17:57:24.148: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8 container secret-env-test: <nil>
    STEP: delete the pod 06/27/23 17:57:24.179
    Jun 27 17:57:24.210: INFO: Waiting for pod pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8 to disappear
    Jun 27 17:57:24.219: INFO: Pod pod-secrets-d21380e4-ea0a-43e1-8880-6f324b3e15a8 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 17:57:24.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6944" for this suite. 06/27/23 17:57:24.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:57:24.259
Jun 27 17:57:24.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename projected 06/27/23 17:57:24.26
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:24.321
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:24.331
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-cf5b1367-12a5-428f-928d-1f170551043d 06/27/23 17:57:24.347
STEP: Creating a pod to test consume configMaps 06/27/23 17:57:24.36
Jun 27 17:57:24.414: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11" in namespace "projected-9338" to be "Succeeded or Failed"
Jun 27 17:57:24.444: INFO: Pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11": Phase="Pending", Reason="", readiness=false. Elapsed: 30.789264ms
Jun 27 17:57:26.455: INFO: Pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041474801s
Jun 27 17:57:28.456: INFO: Pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042345963s
STEP: Saw pod success 06/27/23 17:57:28.456
Jun 27 17:57:28.457: INFO: Pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11" satisfied condition "Succeeded or Failed"
Jun 27 17:57:28.486: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11 container agnhost-container: <nil>
STEP: delete the pod 06/27/23 17:57:28.514
Jun 27 17:57:28.543: INFO: Waiting for pod pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11 to disappear
Jun 27 17:57:28.552: INFO: Pod pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Jun 27 17:57:28.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-9338" for this suite. 06/27/23 17:57:28.568
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":317,"skipped":6030,"failed":0}
------------------------------
â€¢ [4.327 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:57:24.259
    Jun 27 17:57:24.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename projected 06/27/23 17:57:24.26
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:24.321
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:24.331
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-cf5b1367-12a5-428f-928d-1f170551043d 06/27/23 17:57:24.347
    STEP: Creating a pod to test consume configMaps 06/27/23 17:57:24.36
    Jun 27 17:57:24.414: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11" in namespace "projected-9338" to be "Succeeded or Failed"
    Jun 27 17:57:24.444: INFO: Pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11": Phase="Pending", Reason="", readiness=false. Elapsed: 30.789264ms
    Jun 27 17:57:26.455: INFO: Pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11": Phase="Pending", Reason="", readiness=false. Elapsed: 2.041474801s
    Jun 27 17:57:28.456: INFO: Pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.042345963s
    STEP: Saw pod success 06/27/23 17:57:28.456
    Jun 27 17:57:28.457: INFO: Pod "pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11" satisfied condition "Succeeded or Failed"
    Jun 27 17:57:28.486: INFO: Trying to get logs from node 10.113.180.90 pod pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11 container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 17:57:28.514
    Jun 27 17:57:28.543: INFO: Waiting for pod pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11 to disappear
    Jun 27 17:57:28.552: INFO: Pod pod-projected-configmaps-96b1deb8-449d-472a-9ce6-f92ac3a7da11 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Jun 27 17:57:28.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-9338" for this suite. 06/27/23 17:57:28.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:57:28.591
Jun 27 17:57:28.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 17:57:28.593
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:28.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:28.636
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 06/27/23 17:57:28.647
Jun 27 17:57:28.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5" in namespace "downward-api-4434" to be "Succeeded or Failed"
Jun 27 17:57:28.722: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.513301ms
Jun 27 17:57:30.740: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035292586s
Jun 27 17:57:32.733: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027935252s
Jun 27 17:57:34.737: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032720107s
STEP: Saw pod success 06/27/23 17:57:34.741
Jun 27 17:57:34.741: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5" satisfied condition "Succeeded or Failed"
Jun 27 17:57:34.776: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5 container client-container: <nil>
STEP: delete the pod 06/27/23 17:57:34.818
Jun 27 17:57:34.884: INFO: Waiting for pod downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5 to disappear
Jun 27 17:57:34.895: INFO: Pod downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 17:57:34.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4434" for this suite. 06/27/23 17:57:34.914
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":318,"skipped":6042,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.340 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:57:28.591
    Jun 27 17:57:28.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 17:57:28.593
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:28.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:28.636
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 06/27/23 17:57:28.647
    Jun 27 17:57:28.702: INFO: Waiting up to 5m0s for pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5" in namespace "downward-api-4434" to be "Succeeded or Failed"
    Jun 27 17:57:28.722: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5": Phase="Pending", Reason="", readiness=false. Elapsed: 17.513301ms
    Jun 27 17:57:30.740: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.035292586s
    Jun 27 17:57:32.733: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.027935252s
    Jun 27 17:57:34.737: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.032720107s
    STEP: Saw pod success 06/27/23 17:57:34.741
    Jun 27 17:57:34.741: INFO: Pod "downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5" satisfied condition "Succeeded or Failed"
    Jun 27 17:57:34.776: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5 container client-container: <nil>
    STEP: delete the pod 06/27/23 17:57:34.818
    Jun 27 17:57:34.884: INFO: Waiting for pod downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5 to disappear
    Jun 27 17:57:34.895: INFO: Pod downwardapi-volume-89955bc5-8f1a-44b2-88c4-4324355454e5 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 17:57:34.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4434" for this suite. 06/27/23 17:57:34.914
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:57:34.933
Jun 27 17:57:34.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replication-controller 06/27/23 17:57:34.939
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:34.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:35.001
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 06/27/23 17:57:35.042
STEP: waiting for RC to be added 06/27/23 17:57:35.06
STEP: waiting for available Replicas 06/27/23 17:57:35.06
STEP: patching ReplicationController 06/27/23 17:57:37.268
STEP: waiting for RC to be modified 06/27/23 17:57:37.288
STEP: patching ReplicationController status 06/27/23 17:57:37.289
STEP: waiting for RC to be modified 06/27/23 17:57:37.308
STEP: waiting for available Replicas 06/27/23 17:57:37.308
STEP: fetching ReplicationController status 06/27/23 17:57:37.324
STEP: patching ReplicationController scale 06/27/23 17:57:37.349
STEP: waiting for RC to be modified 06/27/23 17:57:37.364
STEP: waiting for ReplicationController's scale to be the max amount 06/27/23 17:57:37.365
STEP: fetching ReplicationController; ensuring that it's patched 06/27/23 17:57:40.5
STEP: updating ReplicationController status 06/27/23 17:57:40.512
STEP: waiting for RC to be modified 06/27/23 17:57:40.535
STEP: listing all ReplicationControllers 06/27/23 17:57:40.535
STEP: checking that ReplicationController has expected values 06/27/23 17:57:40.55
STEP: deleting ReplicationControllers by collection 06/27/23 17:57:40.55
STEP: waiting for ReplicationController to have a DELETED watchEvent 06/27/23 17:57:40.587
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 27 17:57:40.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-2506" for this suite. 06/27/23 17:57:40.699
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":319,"skipped":6048,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.783 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:57:34.933
    Jun 27 17:57:34.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replication-controller 06/27/23 17:57:34.939
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:34.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:35.001
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 06/27/23 17:57:35.042
    STEP: waiting for RC to be added 06/27/23 17:57:35.06
    STEP: waiting for available Replicas 06/27/23 17:57:35.06
    STEP: patching ReplicationController 06/27/23 17:57:37.268
    STEP: waiting for RC to be modified 06/27/23 17:57:37.288
    STEP: patching ReplicationController status 06/27/23 17:57:37.289
    STEP: waiting for RC to be modified 06/27/23 17:57:37.308
    STEP: waiting for available Replicas 06/27/23 17:57:37.308
    STEP: fetching ReplicationController status 06/27/23 17:57:37.324
    STEP: patching ReplicationController scale 06/27/23 17:57:37.349
    STEP: waiting for RC to be modified 06/27/23 17:57:37.364
    STEP: waiting for ReplicationController's scale to be the max amount 06/27/23 17:57:37.365
    STEP: fetching ReplicationController; ensuring that it's patched 06/27/23 17:57:40.5
    STEP: updating ReplicationController status 06/27/23 17:57:40.512
    STEP: waiting for RC to be modified 06/27/23 17:57:40.535
    STEP: listing all ReplicationControllers 06/27/23 17:57:40.535
    STEP: checking that ReplicationController has expected values 06/27/23 17:57:40.55
    STEP: deleting ReplicationControllers by collection 06/27/23 17:57:40.55
    STEP: waiting for ReplicationController to have a DELETED watchEvent 06/27/23 17:57:40.587
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 27 17:57:40.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-2506" for this suite. 06/27/23 17:57:40.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:57:40.718
Jun 27 17:57:40.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 17:57:40.722
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:40.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:40.77
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-578 06/27/23 17:57:40.784
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[] 06/27/23 17:57:40.846
Jun 27 17:57:40.875: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-578 06/27/23 17:57:40.875
Jun 27 17:57:40.919: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-578" to be "running and ready"
Jun 27 17:57:40.926: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.545905ms
Jun 27 17:57:40.926: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:57:42.944: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025734448s
Jun 27 17:57:42.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:57:44.940: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.02107711s
Jun 27 17:57:44.940: INFO: The phase of Pod pod1 is Running (Ready = true)
Jun 27 17:57:44.940: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[pod1:[100]] 06/27/23 17:57:44.948
Jun 27 17:57:44.980: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-578 06/27/23 17:57:44.98
Jun 27 17:57:45.019: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-578" to be "running and ready"
Jun 27 17:57:45.027: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.271427ms
Jun 27 17:57:45.027: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:57:47.049: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.030261906s
Jun 27 17:57:47.049: INFO: The phase of Pod pod2 is Running (Ready = true)
Jun 27 17:57:47.049: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[pod1:[100] pod2:[101]] 06/27/23 17:57:47.06
Jun 27 17:57:47.151: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 06/27/23 17:57:47.151
Jun 27 17:57:47.151: INFO: Creating new exec pod
Jun 27 17:57:47.182: INFO: Waiting up to 5m0s for pod "execpod2bhsb" in namespace "services-578" to be "running"
Jun 27 17:57:47.192: INFO: Pod "execpod2bhsb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.515691ms
Jun 27 17:57:49.202: INFO: Pod "execpod2bhsb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019836945s
Jun 27 17:57:51.209: INFO: Pod "execpod2bhsb": Phase="Running", Reason="", readiness=true. Elapsed: 4.026333438s
Jun 27 17:57:51.209: INFO: Pod "execpod2bhsb" satisfied condition "running"
Jun 27 17:57:52.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-578 exec execpod2bhsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Jun 27 17:57:52.712: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Jun 27 17:57:52.712: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 17:57:52.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-578 exec execpod2bhsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.158.118 80'
Jun 27 17:57:53.028: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.158.118 80\nConnection to 172.21.158.118 80 port [tcp/http] succeeded!\n"
Jun 27 17:57:53.028: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 17:57:53.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-578 exec execpod2bhsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Jun 27 17:57:53.410: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Jun 27 17:57:53.410: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 17:57:53.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-578 exec execpod2bhsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.158.118 81'
Jun 27 17:57:53.793: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.158.118 81\nConnection to 172.21.158.118 81 port [tcp/*] succeeded!\n"
Jun 27 17:57:53.794: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-578 06/27/23 17:57:53.794
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[pod2:[101]] 06/27/23 17:57:53.819
Jun 27 17:57:53.878: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-578 06/27/23 17:57:53.878
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[] 06/27/23 17:57:53.937
Jun 27 17:57:54.043: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 17:57:54.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-578" for this suite. 06/27/23 17:57:54.116
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":320,"skipped":6056,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.415 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:57:40.718
    Jun 27 17:57:40.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 17:57:40.722
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:40.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:40.77
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-578 06/27/23 17:57:40.784
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[] 06/27/23 17:57:40.846
    Jun 27 17:57:40.875: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-578 06/27/23 17:57:40.875
    Jun 27 17:57:40.919: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-578" to be "running and ready"
    Jun 27 17:57:40.926: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 7.545905ms
    Jun 27 17:57:40.926: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:57:42.944: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025734448s
    Jun 27 17:57:42.945: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:57:44.940: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 4.02107711s
    Jun 27 17:57:44.940: INFO: The phase of Pod pod1 is Running (Ready = true)
    Jun 27 17:57:44.940: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[pod1:[100]] 06/27/23 17:57:44.948
    Jun 27 17:57:44.980: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-578 06/27/23 17:57:44.98
    Jun 27 17:57:45.019: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-578" to be "running and ready"
    Jun 27 17:57:45.027: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 8.271427ms
    Jun 27 17:57:45.027: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:57:47.049: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.030261906s
    Jun 27 17:57:47.049: INFO: The phase of Pod pod2 is Running (Ready = true)
    Jun 27 17:57:47.049: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[pod1:[100] pod2:[101]] 06/27/23 17:57:47.06
    Jun 27 17:57:47.151: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 06/27/23 17:57:47.151
    Jun 27 17:57:47.151: INFO: Creating new exec pod
    Jun 27 17:57:47.182: INFO: Waiting up to 5m0s for pod "execpod2bhsb" in namespace "services-578" to be "running"
    Jun 27 17:57:47.192: INFO: Pod "execpod2bhsb": Phase="Pending", Reason="", readiness=false. Elapsed: 9.515691ms
    Jun 27 17:57:49.202: INFO: Pod "execpod2bhsb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.019836945s
    Jun 27 17:57:51.209: INFO: Pod "execpod2bhsb": Phase="Running", Reason="", readiness=true. Elapsed: 4.026333438s
    Jun 27 17:57:51.209: INFO: Pod "execpod2bhsb" satisfied condition "running"
    Jun 27 17:57:52.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-578 exec execpod2bhsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Jun 27 17:57:52.712: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Jun 27 17:57:52.712: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 17:57:52.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-578 exec execpod2bhsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.158.118 80'
    Jun 27 17:57:53.028: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.158.118 80\nConnection to 172.21.158.118 80 port [tcp/http] succeeded!\n"
    Jun 27 17:57:53.028: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 17:57:53.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-578 exec execpod2bhsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Jun 27 17:57:53.410: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Jun 27 17:57:53.410: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 17:57:53.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-578 exec execpod2bhsb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.158.118 81'
    Jun 27 17:57:53.793: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.158.118 81\nConnection to 172.21.158.118 81 port [tcp/*] succeeded!\n"
    Jun 27 17:57:53.794: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-578 06/27/23 17:57:53.794
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[pod2:[101]] 06/27/23 17:57:53.819
    Jun 27 17:57:53.878: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-578 06/27/23 17:57:53.878
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-578 to expose endpoints map[] 06/27/23 17:57:53.937
    Jun 27 17:57:54.043: INFO: successfully validated that service multi-endpoint-test in namespace services-578 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 17:57:54.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-578" for this suite. 06/27/23 17:57:54.116
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:57:54.135
Jun 27 17:57:54.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename svcaccounts 06/27/23 17:57:54.138
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:54.193
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:54.263
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Jun 27 17:57:54.290: INFO: Got root ca configmap in namespace "svcaccounts-660"
Jun 27 17:57:54.307: INFO: Deleted root ca configmap in namespace "svcaccounts-660"
STEP: waiting for a new root ca configmap created 06/27/23 17:57:54.808
Jun 27 17:57:54.818: INFO: Recreated root ca configmap in namespace "svcaccounts-660"
Jun 27 17:57:54.832: INFO: Updated root ca configmap in namespace "svcaccounts-660"
STEP: waiting for the root ca configmap reconciled 06/27/23 17:57:55.333
Jun 27 17:57:55.343: INFO: Reconciled root ca configmap in namespace "svcaccounts-660"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 27 17:57:55.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-660" for this suite. 06/27/23 17:57:55.364
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":321,"skipped":6069,"failed":0}
------------------------------
â€¢ [1.246 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:57:54.135
    Jun 27 17:57:54.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename svcaccounts 06/27/23 17:57:54.138
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:54.193
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:54.263
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Jun 27 17:57:54.290: INFO: Got root ca configmap in namespace "svcaccounts-660"
    Jun 27 17:57:54.307: INFO: Deleted root ca configmap in namespace "svcaccounts-660"
    STEP: waiting for a new root ca configmap created 06/27/23 17:57:54.808
    Jun 27 17:57:54.818: INFO: Recreated root ca configmap in namespace "svcaccounts-660"
    Jun 27 17:57:54.832: INFO: Updated root ca configmap in namespace "svcaccounts-660"
    STEP: waiting for the root ca configmap reconciled 06/27/23 17:57:55.333
    Jun 27 17:57:55.343: INFO: Reconciled root ca configmap in namespace "svcaccounts-660"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 27 17:57:55.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-660" for this suite. 06/27/23 17:57:55.364
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:57:55.385
Jun 27 17:57:55.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replication-controller 06/27/23 17:57:55.392
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:55.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:55.436
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 06/27/23 17:57:55.45
Jun 27 17:57:55.499: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8243" to be "running and ready"
Jun 27 17:57:55.515: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 15.781502ms
Jun 27 17:57:55.515: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:57:57.527: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.027952898s
Jun 27 17:57:57.527: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Jun 27 17:57:57.527: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 06/27/23 17:57:57.537
STEP: Then the orphan pod is adopted 06/27/23 17:57:57.553
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Jun 27 17:57:58.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-8243" for this suite. 06/27/23 17:57:58.599
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":322,"skipped":6082,"failed":0}
------------------------------
â€¢ [3.231 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:57:55.385
    Jun 27 17:57:55.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replication-controller 06/27/23 17:57:55.392
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:55.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:55.436
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 06/27/23 17:57:55.45
    Jun 27 17:57:55.499: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-8243" to be "running and ready"
    Jun 27 17:57:55.515: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 15.781502ms
    Jun 27 17:57:55.515: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:57:57.527: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.027952898s
    Jun 27 17:57:57.527: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Jun 27 17:57:57.527: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 06/27/23 17:57:57.537
    STEP: Then the orphan pod is adopted 06/27/23 17:57:57.553
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Jun 27 17:57:58.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-8243" for this suite. 06/27/23 17:57:58.599
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:57:58.622
Jun 27 17:57:58.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 17:57:58.626
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:58.673
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:58.682
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9269 06/27/23 17:57:58.69
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/27/23 17:57:58.733
STEP: creating service externalsvc in namespace services-9269 06/27/23 17:57:58.733
STEP: creating replication controller externalsvc in namespace services-9269 06/27/23 17:57:58.765
I0627 17:57:58.794978      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9269, replica count: 2
I0627 17:58:01.856291      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 06/27/23 17:58:01.87
Jun 27 17:58:01.935: INFO: Creating new exec pod
Jun 27 17:58:01.979: INFO: Waiting up to 5m0s for pod "execpod8h7wc" in namespace "services-9269" to be "running"
Jun 27 17:58:01.999: INFO: Pod "execpod8h7wc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.44955ms
Jun 27 17:58:04.011: INFO: Pod "execpod8h7wc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031253733s
Jun 27 17:58:06.009: INFO: Pod "execpod8h7wc": Phase="Running", Reason="", readiness=true. Elapsed: 4.02997965s
Jun 27 17:58:06.010: INFO: Pod "execpod8h7wc" satisfied condition "running"
Jun 27 17:58:06.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-9269 exec execpod8h7wc -- /bin/sh -x -c nslookup clusterip-service.services-9269.svc.cluster.local'
Jun 27 17:58:06.547: INFO: stderr: "+ nslookup clusterip-service.services-9269.svc.cluster.local\n"
Jun 27 17:58:06.547: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-9269.svc.cluster.local\tcanonical name = externalsvc.services-9269.svc.cluster.local.\nName:\texternalsvc.services-9269.svc.cluster.local\nAddress: 172.21.185.19\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9269, will wait for the garbage collector to delete the pods 06/27/23 17:58:06.547
Jun 27 17:58:06.636: INFO: Deleting ReplicationController externalsvc took: 21.717028ms
Jun 27 17:58:06.736: INFO: Terminating ReplicationController externalsvc pods took: 100.316761ms
Jun 27 17:58:09.891: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 17:58:09.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9269" for this suite. 06/27/23 17:58:09.943
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":323,"skipped":6084,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.340 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:57:58.622
    Jun 27 17:57:58.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 17:57:58.626
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:57:58.673
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:57:58.682
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9269 06/27/23 17:57:58.69
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 06/27/23 17:57:58.733
    STEP: creating service externalsvc in namespace services-9269 06/27/23 17:57:58.733
    STEP: creating replication controller externalsvc in namespace services-9269 06/27/23 17:57:58.765
    I0627 17:57:58.794978      23 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9269, replica count: 2
    I0627 17:58:01.856291      23 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 06/27/23 17:58:01.87
    Jun 27 17:58:01.935: INFO: Creating new exec pod
    Jun 27 17:58:01.979: INFO: Waiting up to 5m0s for pod "execpod8h7wc" in namespace "services-9269" to be "running"
    Jun 27 17:58:01.999: INFO: Pod "execpod8h7wc": Phase="Pending", Reason="", readiness=false. Elapsed: 19.44955ms
    Jun 27 17:58:04.011: INFO: Pod "execpod8h7wc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.031253733s
    Jun 27 17:58:06.009: INFO: Pod "execpod8h7wc": Phase="Running", Reason="", readiness=true. Elapsed: 4.02997965s
    Jun 27 17:58:06.010: INFO: Pod "execpod8h7wc" satisfied condition "running"
    Jun 27 17:58:06.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-9269 exec execpod8h7wc -- /bin/sh -x -c nslookup clusterip-service.services-9269.svc.cluster.local'
    Jun 27 17:58:06.547: INFO: stderr: "+ nslookup clusterip-service.services-9269.svc.cluster.local\n"
    Jun 27 17:58:06.547: INFO: stdout: "Server:\t\t172.21.0.10\nAddress:\t172.21.0.10#53\n\nclusterip-service.services-9269.svc.cluster.local\tcanonical name = externalsvc.services-9269.svc.cluster.local.\nName:\texternalsvc.services-9269.svc.cluster.local\nAddress: 172.21.185.19\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9269, will wait for the garbage collector to delete the pods 06/27/23 17:58:06.547
    Jun 27 17:58:06.636: INFO: Deleting ReplicationController externalsvc took: 21.717028ms
    Jun 27 17:58:06.736: INFO: Terminating ReplicationController externalsvc pods took: 100.316761ms
    Jun 27 17:58:09.891: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 17:58:09.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9269" for this suite. 06/27/23 17:58:09.943
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:58:09.971
Jun 27 17:58:09.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 17:58:09.973
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:10.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:10.05
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 06/27/23 17:58:10.059
STEP: Creating a ResourceQuota 06/27/23 17:58:15.098
STEP: Ensuring resource quota status is calculated 06/27/23 17:58:15.111
STEP: Creating a ReplicationController 06/27/23 17:58:17.126
STEP: Ensuring resource quota status captures replication controller creation 06/27/23 17:58:17.242
STEP: Deleting a ReplicationController 06/27/23 17:58:19.253
STEP: Ensuring resource quota status released usage 06/27/23 17:58:19.276
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 17:58:21.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3427" for this suite. 06/27/23 17:58:21.304
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":324,"skipped":6089,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.351 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:58:09.971
    Jun 27 17:58:09.972: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 17:58:09.973
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:10.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:10.05
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 06/27/23 17:58:10.059
    STEP: Creating a ResourceQuota 06/27/23 17:58:15.098
    STEP: Ensuring resource quota status is calculated 06/27/23 17:58:15.111
    STEP: Creating a ReplicationController 06/27/23 17:58:17.126
    STEP: Ensuring resource quota status captures replication controller creation 06/27/23 17:58:17.242
    STEP: Deleting a ReplicationController 06/27/23 17:58:19.253
    STEP: Ensuring resource quota status released usage 06/27/23 17:58:19.276
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 17:58:21.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3427" for this suite. 06/27/23 17:58:21.304
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:58:21.33
Jun 27 17:58:21.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename var-expansion 06/27/23 17:58:21.334
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:21.382
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:21.389
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 06/27/23 17:58:21.402
Jun 27 17:58:21.453: INFO: Waiting up to 5m0s for pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a" in namespace "var-expansion-959" to be "Succeeded or Failed"
Jun 27 17:58:21.466: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.804983ms
Jun 27 17:58:23.478: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025425045s
Jun 27 17:58:25.477: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024514133s
Jun 27 17:58:27.510: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057626578s
STEP: Saw pod success 06/27/23 17:58:27.51
Jun 27 17:58:27.510: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a" satisfied condition "Succeeded or Failed"
Jun 27 17:58:27.519: INFO: Trying to get logs from node 10.113.180.90 pod var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a container dapi-container: <nil>
STEP: delete the pod 06/27/23 17:58:27.558
Jun 27 17:58:27.616: INFO: Waiting for pod var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a to disappear
Jun 27 17:58:27.647: INFO: Pod var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Jun 27 17:58:27.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-959" for this suite. 06/27/23 17:58:27.691
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":325,"skipped":6096,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.411 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:58:21.33
    Jun 27 17:58:21.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename var-expansion 06/27/23 17:58:21.334
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:21.382
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:21.389
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 06/27/23 17:58:21.402
    Jun 27 17:58:21.453: INFO: Waiting up to 5m0s for pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a" in namespace "var-expansion-959" to be "Succeeded or Failed"
    Jun 27 17:58:21.466: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a": Phase="Pending", Reason="", readiness=false. Elapsed: 13.804983ms
    Jun 27 17:58:23.478: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.025425045s
    Jun 27 17:58:25.477: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.024514133s
    Jun 27 17:58:27.510: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.057626578s
    STEP: Saw pod success 06/27/23 17:58:27.51
    Jun 27 17:58:27.510: INFO: Pod "var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a" satisfied condition "Succeeded or Failed"
    Jun 27 17:58:27.519: INFO: Trying to get logs from node 10.113.180.90 pod var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a container dapi-container: <nil>
    STEP: delete the pod 06/27/23 17:58:27.558
    Jun 27 17:58:27.616: INFO: Waiting for pod var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a to disappear
    Jun 27 17:58:27.647: INFO: Pod var-expansion-a2c1b218-ab7e-45d2-818f-6253592b383a no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Jun 27 17:58:27.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-959" for this suite. 06/27/23 17:58:27.691
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:58:27.744
Jun 27 17:58:27.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 17:58:27.746
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:27.814
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:27.821
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 17:58:27.92
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:58:28.36
STEP: Deploying the webhook pod 06/27/23 17:58:28.392
STEP: Wait for the deployment to be ready 06/27/23 17:58:28.42
Jun 27 17:58:28.438: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 27 17:58:30.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 58, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 58, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 58, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 58, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 17:58:32.475
STEP: Verifying the service has paired with the endpoint 06/27/23 17:58:32.505
Jun 27 17:58:33.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/27/23 17:58:33.523
STEP: create a configmap that should be updated by the webhook 06/27/23 17:58:33.642
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:58:33.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1105" for this suite. 06/27/23 17:58:33.826
STEP: Destroying namespace "webhook-1105-markers" for this suite. 06/27/23 17:58:33.842
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":326,"skipped":6109,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.252 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:58:27.744
    Jun 27 17:58:27.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 17:58:27.746
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:27.814
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:27.821
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 17:58:27.92
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 17:58:28.36
    STEP: Deploying the webhook pod 06/27/23 17:58:28.392
    STEP: Wait for the deployment to be ready 06/27/23 17:58:28.42
    Jun 27 17:58:28.438: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 27 17:58:30.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 17, 58, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 58, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 17, 58, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 17, 58, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 17:58:32.475
    STEP: Verifying the service has paired with the endpoint 06/27/23 17:58:32.505
    Jun 27 17:58:33.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 06/27/23 17:58:33.523
    STEP: create a configmap that should be updated by the webhook 06/27/23 17:58:33.642
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:58:33.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1105" for this suite. 06/27/23 17:58:33.826
    STEP: Destroying namespace "webhook-1105-markers" for this suite. 06/27/23 17:58:33.842
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:58:34.003
Jun 27 17:58:34.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 17:58:34.004
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:34.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:34.046
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-ffaa1c4f-d58a-4c9a-8683-b06a255a6007 06/27/23 17:58:34.056
STEP: Creating a pod to test consume configMaps 06/27/23 17:58:34.069
Jun 27 17:58:34.119: INFO: Waiting up to 5m0s for pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b" in namespace "configmap-323" to be "Succeeded or Failed"
Jun 27 17:58:34.131: INFO: Pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.68694ms
Jun 27 17:58:36.143: INFO: Pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023949458s
Jun 27 17:58:38.142: INFO: Pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023036169s
STEP: Saw pod success 06/27/23 17:58:38.142
Jun 27 17:58:38.142: INFO: Pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b" satisfied condition "Succeeded or Failed"
Jun 27 17:58:38.153: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b container agnhost-container: <nil>
STEP: delete the pod 06/27/23 17:58:38.209
Jun 27 17:58:38.243: INFO: Waiting for pod pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b to disappear
Jun 27 17:58:38.252: INFO: Pod pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 17:58:38.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-323" for this suite. 06/27/23 17:58:38.267
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":327,"skipped":6125,"failed":0}
------------------------------
â€¢ [4.286 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:58:34.003
    Jun 27 17:58:34.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 17:58:34.004
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:34.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:34.046
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-ffaa1c4f-d58a-4c9a-8683-b06a255a6007 06/27/23 17:58:34.056
    STEP: Creating a pod to test consume configMaps 06/27/23 17:58:34.069
    Jun 27 17:58:34.119: INFO: Waiting up to 5m0s for pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b" in namespace "configmap-323" to be "Succeeded or Failed"
    Jun 27 17:58:34.131: INFO: Pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 12.68694ms
    Jun 27 17:58:36.143: INFO: Pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.023949458s
    Jun 27 17:58:38.142: INFO: Pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.023036169s
    STEP: Saw pod success 06/27/23 17:58:38.142
    Jun 27 17:58:38.142: INFO: Pod "pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b" satisfied condition "Succeeded or Failed"
    Jun 27 17:58:38.153: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 17:58:38.209
    Jun 27 17:58:38.243: INFO: Waiting for pod pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b to disappear
    Jun 27 17:58:38.252: INFO: Pod pod-configmaps-288a0e40-17a1-4470-896f-b21502460f6b no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 17:58:38.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-323" for this suite. 06/27/23 17:58:38.267
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:58:38.301
Jun 27 17:58:38.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:58:38.303
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:38.352
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:38.36
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/27/23 17:58:38.37
Jun 27 17:58:38.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
Jun 27 17:58:48.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 17:59:26.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8041" for this suite. 06/27/23 17:59:26.459
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":328,"skipped":6161,"failed":0}
------------------------------
â€¢ [SLOW TEST] [48.184 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:58:38.301
    Jun 27 17:58:38.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 17:58:38.303
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:58:38.352
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:58:38.36
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 06/27/23 17:58:38.37
    Jun 27 17:58:38.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    Jun 27 17:58:48.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 17:59:26.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8041" for this suite. 06/27/23 17:59:26.459
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:59:26.491
Jun 27 17:59:26.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename statefulset 06/27/23 17:59:26.495
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:59:26.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:59:26.589
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1230 06/27/23 17:59:26.602
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-1230 06/27/23 17:59:26.618
Jun 27 17:59:26.721: INFO: Found 0 stateful pods, waiting for 1
Jun 27 17:59:36.748: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 06/27/23 17:59:36.797
STEP: updating a scale subresource 06/27/23 17:59:36.83
STEP: verifying the statefulset Spec.Replicas was modified 06/27/23 17:59:36.852
STEP: Patch a scale subresource 06/27/23 17:59:36.87
STEP: verifying the statefulset Spec.Replicas was modified 06/27/23 17:59:36.896
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Jun 27 17:59:36.911: INFO: Deleting all statefulset in ns statefulset-1230
Jun 27 17:59:36.929: INFO: Scaling statefulset ss to 0
Jun 27 17:59:47.064: INFO: Waiting for statefulset status.replicas updated to 0
Jun 27 17:59:47.100: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Jun 27 17:59:47.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1230" for this suite. 06/27/23 17:59:47.249
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":329,"skipped":6168,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.832 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:59:26.491
    Jun 27 17:59:26.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename statefulset 06/27/23 17:59:26.495
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:59:26.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:59:26.589
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1230 06/27/23 17:59:26.602
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-1230 06/27/23 17:59:26.618
    Jun 27 17:59:26.721: INFO: Found 0 stateful pods, waiting for 1
    Jun 27 17:59:36.748: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 06/27/23 17:59:36.797
    STEP: updating a scale subresource 06/27/23 17:59:36.83
    STEP: verifying the statefulset Spec.Replicas was modified 06/27/23 17:59:36.852
    STEP: Patch a scale subresource 06/27/23 17:59:36.87
    STEP: verifying the statefulset Spec.Replicas was modified 06/27/23 17:59:36.896
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Jun 27 17:59:36.911: INFO: Deleting all statefulset in ns statefulset-1230
    Jun 27 17:59:36.929: INFO: Scaling statefulset ss to 0
    Jun 27 17:59:47.064: INFO: Waiting for statefulset status.replicas updated to 0
    Jun 27 17:59:47.100: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Jun 27 17:59:47.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1230" for this suite. 06/27/23 17:59:47.249
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:59:47.324
Jun 27 17:59:47.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename secrets 06/27/23 17:59:47.325
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:59:47.506
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:59:47.543
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-2295c33a-a6b7-414f-9595-7ec5426395b2 06/27/23 17:59:47.602
STEP: Creating a pod to test consume secrets 06/27/23 17:59:47.632
Jun 27 17:59:47.732: INFO: Waiting up to 5m0s for pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260" in namespace "secrets-2435" to be "Succeeded or Failed"
Jun 27 17:59:47.794: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260": Phase="Pending", Reason="", readiness=false. Elapsed: 47.401535ms
Jun 27 17:59:49.812: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065610873s
Jun 27 17:59:51.838: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090726459s
Jun 27 17:59:53.812: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064851612s
STEP: Saw pod success 06/27/23 17:59:53.812
Jun 27 17:59:53.812: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260" satisfied condition "Succeeded or Failed"
Jun 27 17:59:53.834: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260 container secret-volume-test: <nil>
STEP: delete the pod 06/27/23 17:59:53.905
Jun 27 17:59:53.960: INFO: Waiting for pod pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260 to disappear
Jun 27 17:59:53.982: INFO: Pod pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Jun 27 17:59:53.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-2435" for this suite. 06/27/23 17:59:54.012
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":330,"skipped":6174,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.720 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:59:47.324
    Jun 27 17:59:47.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename secrets 06/27/23 17:59:47.325
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:59:47.506
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:59:47.543
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-2295c33a-a6b7-414f-9595-7ec5426395b2 06/27/23 17:59:47.602
    STEP: Creating a pod to test consume secrets 06/27/23 17:59:47.632
    Jun 27 17:59:47.732: INFO: Waiting up to 5m0s for pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260" in namespace "secrets-2435" to be "Succeeded or Failed"
    Jun 27 17:59:47.794: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260": Phase="Pending", Reason="", readiness=false. Elapsed: 47.401535ms
    Jun 27 17:59:49.812: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260": Phase="Pending", Reason="", readiness=false. Elapsed: 2.065610873s
    Jun 27 17:59:51.838: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260": Phase="Pending", Reason="", readiness=false. Elapsed: 4.090726459s
    Jun 27 17:59:53.812: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.064851612s
    STEP: Saw pod success 06/27/23 17:59:53.812
    Jun 27 17:59:53.812: INFO: Pod "pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260" satisfied condition "Succeeded or Failed"
    Jun 27 17:59:53.834: INFO: Trying to get logs from node 10.113.180.90 pod pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260 container secret-volume-test: <nil>
    STEP: delete the pod 06/27/23 17:59:53.905
    Jun 27 17:59:53.960: INFO: Waiting for pod pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260 to disappear
    Jun 27 17:59:53.982: INFO: Pod pod-secrets-91c1e7e5-05a9-48e5-9133-0ebcc084f260 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Jun 27 17:59:53.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-2435" for this suite. 06/27/23 17:59:54.012
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:59:54.05
Jun 27 17:59:54.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename runtimeclass 06/27/23 17:59:54.052
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:59:54.106
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:59:54.122
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Jun 27 17:59:54.224: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7600 to be scheduled
Jun 27 17:59:54.246: INFO: 1 pods are not scheduled: [runtimeclass-7600/test-runtimeclass-runtimeclass-7600-preconfigured-handler-j7l6s(26afadfe-1037-4460-9018-b3aa117a6dff)]
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Jun 27 17:59:56.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-7600" for this suite. 06/27/23 17:59:56.347
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":331,"skipped":6216,"failed":0}
------------------------------
â€¢ [2.323 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:59:54.05
    Jun 27 17:59:54.051: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename runtimeclass 06/27/23 17:59:54.052
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:59:54.106
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:59:54.122
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Jun 27 17:59:54.224: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-7600 to be scheduled
    Jun 27 17:59:54.246: INFO: 1 pods are not scheduled: [runtimeclass-7600/test-runtimeclass-runtimeclass-7600-preconfigured-handler-j7l6s(26afadfe-1037-4460-9018-b3aa117a6dff)]
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Jun 27 17:59:56.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-7600" for this suite. 06/27/23 17:59:56.347
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 17:59:56.381
Jun 27 17:59:56.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 17:59:56.383
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:59:56.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:59:56.485
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 06/27/23 17:59:56.506
STEP: submitting the pod to kubernetes 06/27/23 17:59:56.506
Jun 27 17:59:56.579: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365" in namespace "pods-4578" to be "running and ready"
Jun 27 17:59:56.602: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Pending", Reason="", readiness=false. Elapsed: 23.119448ms
Jun 27 17:59:56.602: INFO: The phase of Pod pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 17:59:58.621: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Running", Reason="", readiness=true. Elapsed: 2.04215401s
Jun 27 17:59:58.621: INFO: The phase of Pod pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365 is Running (Ready = true)
Jun 27 17:59:58.621: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 06/27/23 17:59:58.639
STEP: updating the pod 06/27/23 17:59:58.656
Jun 27 17:59:59.271: INFO: Successfully updated pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365"
Jun 27 17:59:59.271: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365" in namespace "pods-4578" to be "terminated with reason DeadlineExceeded"
Jun 27 17:59:59.289: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Running", Reason="", readiness=true. Elapsed: 18.444044ms
Jun 27 18:00:01.309: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Running", Reason="", readiness=true. Elapsed: 2.038611185s
Jun 27 18:00:03.313: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Running", Reason="", readiness=false. Elapsed: 4.042419494s
Jun 27 18:00:05.309: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.038204493s
Jun 27 18:00:05.309: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 18:00:05.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4578" for this suite. 06/27/23 18:00:05.335
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":332,"skipped":6227,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.980 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 17:59:56.381
    Jun 27 17:59:56.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 17:59:56.383
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 17:59:56.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 17:59:56.485
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 06/27/23 17:59:56.506
    STEP: submitting the pod to kubernetes 06/27/23 17:59:56.506
    Jun 27 17:59:56.579: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365" in namespace "pods-4578" to be "running and ready"
    Jun 27 17:59:56.602: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Pending", Reason="", readiness=false. Elapsed: 23.119448ms
    Jun 27 17:59:56.602: INFO: The phase of Pod pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 17:59:58.621: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Running", Reason="", readiness=true. Elapsed: 2.04215401s
    Jun 27 17:59:58.621: INFO: The phase of Pod pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365 is Running (Ready = true)
    Jun 27 17:59:58.621: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 06/27/23 17:59:58.639
    STEP: updating the pod 06/27/23 17:59:58.656
    Jun 27 17:59:59.271: INFO: Successfully updated pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365"
    Jun 27 17:59:59.271: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365" in namespace "pods-4578" to be "terminated with reason DeadlineExceeded"
    Jun 27 17:59:59.289: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Running", Reason="", readiness=true. Elapsed: 18.444044ms
    Jun 27 18:00:01.309: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Running", Reason="", readiness=true. Elapsed: 2.038611185s
    Jun 27 18:00:03.313: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Running", Reason="", readiness=false. Elapsed: 4.042419494s
    Jun 27 18:00:05.309: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.038204493s
    Jun 27 18:00:05.309: INFO: Pod "pod-update-activedeadlineseconds-73c21352-d327-490b-a514-84415d9df365" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 18:00:05.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4578" for this suite. 06/27/23 18:00:05.335
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:00:05.363
Jun 27 18:00:05.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 18:00:05.365
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:00:05.414
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:00:05.431
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
Jun 27 18:00:05.475: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
STEP: Creating configMap with name configmap-test-upd-cc33a4e7-f54d-479a-a405-a4a22d01ca3e 06/27/23 18:00:05.475
STEP: Creating the pod 06/27/23 18:00:05.501
Jun 27 18:00:05.561: INFO: Waiting up to 5m0s for pod "pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681" in namespace "configmap-635" to be "running and ready"
Jun 27 18:00:05.582: INFO: Pod "pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681": Phase="Pending", Reason="", readiness=false. Elapsed: 21.70532ms
Jun 27 18:00:05.582: INFO: The phase of Pod pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 18:00:07.602: INFO: Pod "pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681": Phase="Running", Reason="", readiness=true. Elapsed: 2.041598273s
Jun 27 18:00:07.603: INFO: The phase of Pod pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681 is Running (Ready = true)
Jun 27 18:00:07.603: INFO: Pod "pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-cc33a4e7-f54d-479a-a405-a4a22d01ca3e 06/27/23 18:00:07.673
STEP: waiting to observe update in volume 06/27/23 18:00:07.695
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 18:00:09.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-635" for this suite. 06/27/23 18:00:09.84
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":333,"skipped":6231,"failed":0}
------------------------------
â€¢ [4.505 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:00:05.363
    Jun 27 18:00:05.363: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 18:00:05.365
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:00:05.414
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:00:05.431
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    Jun 27 18:00:05.475: INFO: Couldn't get node TTL annotation (using default value of 0): No TTL annotation found on the node
    STEP: Creating configMap with name configmap-test-upd-cc33a4e7-f54d-479a-a405-a4a22d01ca3e 06/27/23 18:00:05.475
    STEP: Creating the pod 06/27/23 18:00:05.501
    Jun 27 18:00:05.561: INFO: Waiting up to 5m0s for pod "pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681" in namespace "configmap-635" to be "running and ready"
    Jun 27 18:00:05.582: INFO: Pod "pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681": Phase="Pending", Reason="", readiness=false. Elapsed: 21.70532ms
    Jun 27 18:00:05.582: INFO: The phase of Pod pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 18:00:07.602: INFO: Pod "pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681": Phase="Running", Reason="", readiness=true. Elapsed: 2.041598273s
    Jun 27 18:00:07.603: INFO: The phase of Pod pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681 is Running (Ready = true)
    Jun 27 18:00:07.603: INFO: Pod "pod-configmaps-c14c4c72-eaf5-4968-95f5-019a44302681" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-cc33a4e7-f54d-479a-a405-a4a22d01ca3e 06/27/23 18:00:07.673
    STEP: waiting to observe update in volume 06/27/23 18:00:07.695
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 18:00:09.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-635" for this suite. 06/27/23 18:00:09.84
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:00:09.874
Jun 27 18:00:09.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 18:00:09.876
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:00:09.936
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:00:09.964
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/27/23 18:00:10.013
Jun 27 18:00:10.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9137 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Jun 27 18:00:10.204: INFO: stderr: ""
Jun 27 18:00:10.204: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 06/27/23 18:00:10.204
Jun 27 18:00:10.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9137 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Jun 27 18:00:11.701: INFO: stderr: ""
Jun 27 18:00:11.701: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/27/23 18:00:11.701
Jun 27 18:00:11.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9137 delete pods e2e-test-httpd-pod'
Jun 27 18:00:14.449: INFO: stderr: ""
Jun 27 18:00:14.449: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 18:00:14.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9137" for this suite. 06/27/23 18:00:14.477
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":334,"skipped":6251,"failed":0}
------------------------------
â€¢ [4.632 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:00:09.874
    Jun 27 18:00:09.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 18:00:09.876
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:00:09.936
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:00:09.964
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/27/23 18:00:10.013
    Jun 27 18:00:10.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9137 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Jun 27 18:00:10.204: INFO: stderr: ""
    Jun 27 18:00:10.204: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 06/27/23 18:00:10.204
    Jun 27 18:00:10.204: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9137 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Jun 27 18:00:11.701: INFO: stderr: ""
    Jun 27 18:00:11.701: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 06/27/23 18:00:11.701
    Jun 27 18:00:11.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-9137 delete pods e2e-test-httpd-pod'
    Jun 27 18:00:14.449: INFO: stderr: ""
    Jun 27 18:00:14.449: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 18:00:14.449: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9137" for this suite. 06/27/23 18:00:14.477
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:00:14.506
Jun 27 18:00:14.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename job 06/27/23 18:00:14.507
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:00:14.555
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:00:14.578
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 06/27/23 18:00:14.595
STEP: Ensuring active pods == parallelism 06/27/23 18:00:14.632
STEP: delete a job 06/27/23 18:00:16.65
STEP: deleting Job.batch foo in namespace job-4771, will wait for the garbage collector to delete the pods 06/27/23 18:00:16.651
Jun 27 18:00:16.741: INFO: Deleting Job.batch foo took: 25.010245ms
Jun 27 18:00:16.842: INFO: Terminating Job.batch foo pods took: 100.719461ms
STEP: Ensuring job was deleted 06/27/23 18:00:50.842
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Jun 27 18:00:50.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4771" for this suite. 06/27/23 18:00:50.88
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":335,"skipped":6253,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.402 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:00:14.506
    Jun 27 18:00:14.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename job 06/27/23 18:00:14.507
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:00:14.555
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:00:14.578
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 06/27/23 18:00:14.595
    STEP: Ensuring active pods == parallelism 06/27/23 18:00:14.632
    STEP: delete a job 06/27/23 18:00:16.65
    STEP: deleting Job.batch foo in namespace job-4771, will wait for the garbage collector to delete the pods 06/27/23 18:00:16.651
    Jun 27 18:00:16.741: INFO: Deleting Job.batch foo took: 25.010245ms
    Jun 27 18:00:16.842: INFO: Terminating Job.batch foo pods took: 100.719461ms
    STEP: Ensuring job was deleted 06/27/23 18:00:50.842
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Jun 27 18:00:50.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4771" for this suite. 06/27/23 18:00:50.88
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:00:50.913
Jun 27 18:00:50.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 18:00:50.915
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:00:50.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:00:50.998
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 18:00:51.075
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 18:00:51.457
STEP: Deploying the webhook pod 06/27/23 18:00:51.497
STEP: Wait for the deployment to be ready 06/27/23 18:00:51.541
Jun 27 18:00:51.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
Jun 27 18:00:53.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 18:00:55.625
STEP: Verifying the service has paired with the endpoint 06/27/23 18:00:55.662
Jun 27 18:00:56.664: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Jun 27 18:00:56.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1510-crds.webhook.example.com via the AdmissionRegistration API 06/27/23 18:00:57.252
STEP: Creating a custom resource that should be mutated by the webhook 06/27/23 18:00:57.324
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 18:01:00.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2843" for this suite. 06/27/23 18:01:00.061
STEP: Destroying namespace "webhook-2843-markers" for this suite. 06/27/23 18:01:00.096
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":336,"skipped":6277,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.459 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:00:50.913
    Jun 27 18:00:50.913: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 18:00:50.915
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:00:50.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:00:50.998
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 18:00:51.075
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 18:00:51.457
    STEP: Deploying the webhook pod 06/27/23 18:00:51.497
    STEP: Wait for the deployment to be ready 06/27/23 18:00:51.541
    Jun 27 18:00:51.602: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-5d85dd8cdb\""}, v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}}, CollisionCount:(*int32)(nil)}
    Jun 27 18:00:53.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 0, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 18:00:55.625
    STEP: Verifying the service has paired with the endpoint 06/27/23 18:00:55.662
    Jun 27 18:00:56.664: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Jun 27 18:00:56.700: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1510-crds.webhook.example.com via the AdmissionRegistration API 06/27/23 18:00:57.252
    STEP: Creating a custom resource that should be mutated by the webhook 06/27/23 18:00:57.324
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 18:01:00.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2843" for this suite. 06/27/23 18:01:00.061
    STEP: Destroying namespace "webhook-2843-markers" for this suite. 06/27/23 18:01:00.096
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:01:00.374
Jun 27 18:01:00.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename svcaccounts 06/27/23 18:01:00.377
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:00.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:00.446
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Jun 27 18:01:00.620: INFO: Waiting up to 5m0s for pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c" in namespace "svcaccounts-7806" to be "running"
Jun 27 18:01:00.637: INFO: Pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.581239ms
Jun 27 18:01:02.659: INFO: Pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038506477s
Jun 27 18:01:04.661: INFO: Pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c": Phase="Running", Reason="", readiness=true. Elapsed: 4.040681203s
Jun 27 18:01:04.661: INFO: Pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c" satisfied condition "running"
STEP: reading a file in the container 06/27/23 18:01:04.661
Jun 27 18:01:04.661: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7806 pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 06/27/23 18:01:05.139
Jun 27 18:01:05.140: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7806 pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 06/27/23 18:01:05.54
Jun 27 18:01:05.540: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7806 pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Jun 27 18:01:05.919: INFO: Got root ca configmap in namespace "svcaccounts-7806"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Jun 27 18:01:05.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7806" for this suite. 06/27/23 18:01:05.958
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":337,"skipped":6290,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.610 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:01:00.374
    Jun 27 18:01:00.375: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename svcaccounts 06/27/23 18:01:00.377
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:00.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:00.446
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Jun 27 18:01:00.620: INFO: Waiting up to 5m0s for pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c" in namespace "svcaccounts-7806" to be "running"
    Jun 27 18:01:00.637: INFO: Pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 16.581239ms
    Jun 27 18:01:02.659: INFO: Pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.038506477s
    Jun 27 18:01:04.661: INFO: Pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c": Phase="Running", Reason="", readiness=true. Elapsed: 4.040681203s
    Jun 27 18:01:04.661: INFO: Pod "pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c" satisfied condition "running"
    STEP: reading a file in the container 06/27/23 18:01:04.661
    Jun 27 18:01:04.661: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7806 pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 06/27/23 18:01:05.139
    Jun 27 18:01:05.140: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7806 pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 06/27/23 18:01:05.54
    Jun 27 18:01:05.540: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-7806 pod-service-account-4e1dcf6c-15ec-4cc9-9d10-f5ddce5c8f7c -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Jun 27 18:01:05.919: INFO: Got root ca configmap in namespace "svcaccounts-7806"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Jun 27 18:01:05.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7806" for this suite. 06/27/23 18:01:05.958
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:01:05.991
Jun 27 18:01:05.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename disruption 06/27/23 18:01:05.993
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:06.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:06.063
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 06/27/23 18:01:06.075
STEP: Waiting for the pdb to be processed 06/27/23 18:01:06.094
STEP: First trying to evict a pod which shouldn't be evictable 06/27/23 18:01:06.132
STEP: Waiting for all pods to be running 06/27/23 18:01:06.132
Jun 27 18:01:06.153: INFO: pods: 0 < 3
Jun 27 18:01:08.176: INFO: running pods: 1 < 3
STEP: locating a running pod 06/27/23 18:01:10.175
STEP: Updating the pdb to allow a pod to be evicted 06/27/23 18:01:10.231
STEP: Waiting for the pdb to be processed 06/27/23 18:01:10.281
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/27/23 18:01:10.3
STEP: Waiting for all pods to be running 06/27/23 18:01:10.3
STEP: Waiting for the pdb to observed all healthy pods 06/27/23 18:01:10.319
STEP: Patching the pdb to disallow a pod to be evicted 06/27/23 18:01:10.408
STEP: Waiting for the pdb to be processed 06/27/23 18:01:10.464
STEP: Waiting for all pods to be running 06/27/23 18:01:10.486
Jun 27 18:01:10.504: INFO: running pods: 2 < 3
Jun 27 18:01:12.536: INFO: running pods: 2 < 3
STEP: locating a running pod 06/27/23 18:01:14.525
STEP: Deleting the pdb to allow a pod to be evicted 06/27/23 18:01:14.581
STEP: Waiting for the pdb to be deleted 06/27/23 18:01:14.605
STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/27/23 18:01:14.625
STEP: Waiting for all pods to be running 06/27/23 18:01:14.625
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Jun 27 18:01:14.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8645" for this suite. 06/27/23 18:01:14.723
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":338,"skipped":6333,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.788 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:01:05.991
    Jun 27 18:01:05.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename disruption 06/27/23 18:01:05.993
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:06.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:06.063
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 06/27/23 18:01:06.075
    STEP: Waiting for the pdb to be processed 06/27/23 18:01:06.094
    STEP: First trying to evict a pod which shouldn't be evictable 06/27/23 18:01:06.132
    STEP: Waiting for all pods to be running 06/27/23 18:01:06.132
    Jun 27 18:01:06.153: INFO: pods: 0 < 3
    Jun 27 18:01:08.176: INFO: running pods: 1 < 3
    STEP: locating a running pod 06/27/23 18:01:10.175
    STEP: Updating the pdb to allow a pod to be evicted 06/27/23 18:01:10.231
    STEP: Waiting for the pdb to be processed 06/27/23 18:01:10.281
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/27/23 18:01:10.3
    STEP: Waiting for all pods to be running 06/27/23 18:01:10.3
    STEP: Waiting for the pdb to observed all healthy pods 06/27/23 18:01:10.319
    STEP: Patching the pdb to disallow a pod to be evicted 06/27/23 18:01:10.408
    STEP: Waiting for the pdb to be processed 06/27/23 18:01:10.464
    STEP: Waiting for all pods to be running 06/27/23 18:01:10.486
    Jun 27 18:01:10.504: INFO: running pods: 2 < 3
    Jun 27 18:01:12.536: INFO: running pods: 2 < 3
    STEP: locating a running pod 06/27/23 18:01:14.525
    STEP: Deleting the pdb to allow a pod to be evicted 06/27/23 18:01:14.581
    STEP: Waiting for the pdb to be deleted 06/27/23 18:01:14.605
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 06/27/23 18:01:14.625
    STEP: Waiting for all pods to be running 06/27/23 18:01:14.625
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Jun 27 18:01:14.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8645" for this suite. 06/27/23 18:01:14.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:01:14.781
Jun 27 18:01:14.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 18:01:14.782
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:14.832
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:14.848
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 06/27/23 18:01:14.914
STEP: watching for Pod to be ready 06/27/23 18:01:14.991
Jun 27 18:01:14.999: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions []
Jun 27 18:01:15.014: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
Jun 27 18:01:15.044: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
Jun 27 18:01:15.935: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
Jun 27 18:01:16.016: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
Jun 27 18:01:16.839: INFO: Found Pod pod-test in namespace pods-7377 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 06/27/23 18:01:16.859
STEP: getting the Pod and ensuring that it's patched 06/27/23 18:01:16.925
STEP: replacing the Pod's status Ready condition to False 06/27/23 18:01:16.942
STEP: check the Pod again to ensure its Ready conditions are False 06/27/23 18:01:16.996
STEP: deleting the Pod via a Collection with a LabelSelector 06/27/23 18:01:16.996
STEP: watching for the Pod to be deleted 06/27/23 18:01:17.04
Jun 27 18:01:17.048: INFO: observed event type MODIFIED
Jun 27 18:01:18.850: INFO: observed event type MODIFIED
Jun 27 18:01:19.306: INFO: observed event type MODIFIED
Jun 27 18:01:20.869: INFO: observed event type MODIFIED
Jun 27 18:01:20.890: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 18:01:20.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7377" for this suite. 06/27/23 18:01:20.936
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":339,"skipped":6355,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.186 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:01:14.781
    Jun 27 18:01:14.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 18:01:14.782
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:14.832
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:14.848
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 06/27/23 18:01:14.914
    STEP: watching for Pod to be ready 06/27/23 18:01:14.991
    Jun 27 18:01:14.999: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Jun 27 18:01:15.014: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
    Jun 27 18:01:15.044: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
    Jun 27 18:01:15.935: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
    Jun 27 18:01:16.016: INFO: observed Pod pod-test in namespace pods-7377 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
    Jun 27 18:01:16.839: INFO: Found Pod pod-test in namespace pods-7377 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:15 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:16 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:16 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-06-27 18:01:14 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 06/27/23 18:01:16.859
    STEP: getting the Pod and ensuring that it's patched 06/27/23 18:01:16.925
    STEP: replacing the Pod's status Ready condition to False 06/27/23 18:01:16.942
    STEP: check the Pod again to ensure its Ready conditions are False 06/27/23 18:01:16.996
    STEP: deleting the Pod via a Collection with a LabelSelector 06/27/23 18:01:16.996
    STEP: watching for the Pod to be deleted 06/27/23 18:01:17.04
    Jun 27 18:01:17.048: INFO: observed event type MODIFIED
    Jun 27 18:01:18.850: INFO: observed event type MODIFIED
    Jun 27 18:01:19.306: INFO: observed event type MODIFIED
    Jun 27 18:01:20.869: INFO: observed event type MODIFIED
    Jun 27 18:01:20.890: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 18:01:20.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7377" for this suite. 06/27/23 18:01:20.936
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:01:20.969
Jun 27 18:01:20.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 18:01:20.971
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:21.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:21.034
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-51e8e191-13f1-4c31-b7dc-44b1e7f6503c 06/27/23 18:01:21.056
STEP: Creating a pod to test consume configMaps 06/27/23 18:01:21.085
Jun 27 18:01:21.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe" in namespace "configmap-1475" to be "Succeeded or Failed"
Jun 27 18:01:21.179: INFO: Pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe": Phase="Pending", Reason="", readiness=false. Elapsed: 24.888026ms
Jun 27 18:01:23.207: INFO: Pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052530924s
Jun 27 18:01:25.199: INFO: Pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044977959s
STEP: Saw pod success 06/27/23 18:01:25.199
Jun 27 18:01:25.200: INFO: Pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe" satisfied condition "Succeeded or Failed"
Jun 27 18:01:25.218: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe container agnhost-container: <nil>
STEP: delete the pod 06/27/23 18:01:25.277
Jun 27 18:01:25.339: INFO: Waiting for pod pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe to disappear
Jun 27 18:01:25.359: INFO: Pod pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 18:01:25.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1475" for this suite. 06/27/23 18:01:25.385
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":340,"skipped":6364,"failed":0}
------------------------------
â€¢ [4.441 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:01:20.969
    Jun 27 18:01:20.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 18:01:20.971
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:21.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:21.034
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-51e8e191-13f1-4c31-b7dc-44b1e7f6503c 06/27/23 18:01:21.056
    STEP: Creating a pod to test consume configMaps 06/27/23 18:01:21.085
    Jun 27 18:01:21.154: INFO: Waiting up to 5m0s for pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe" in namespace "configmap-1475" to be "Succeeded or Failed"
    Jun 27 18:01:21.179: INFO: Pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe": Phase="Pending", Reason="", readiness=false. Elapsed: 24.888026ms
    Jun 27 18:01:23.207: INFO: Pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe": Phase="Pending", Reason="", readiness=false. Elapsed: 2.052530924s
    Jun 27 18:01:25.199: INFO: Pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.044977959s
    STEP: Saw pod success 06/27/23 18:01:25.199
    Jun 27 18:01:25.200: INFO: Pod "pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe" satisfied condition "Succeeded or Failed"
    Jun 27 18:01:25.218: INFO: Trying to get logs from node 10.113.180.90 pod pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe container agnhost-container: <nil>
    STEP: delete the pod 06/27/23 18:01:25.277
    Jun 27 18:01:25.339: INFO: Waiting for pod pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe to disappear
    Jun 27 18:01:25.359: INFO: Pod pod-configmaps-1d74d217-170b-4d2d-966d-c280cba4fabe no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 18:01:25.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1475" for this suite. 06/27/23 18:01:25.385
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:01:25.414
Jun 27 18:01:25.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 18:01:25.416
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:25.463
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:25.484
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 06/27/23 18:01:42.514
STEP: Creating a ResourceQuota 06/27/23 18:01:47.54
STEP: Ensuring resource quota status is calculated 06/27/23 18:01:47.565
STEP: Creating a ConfigMap 06/27/23 18:01:49.579
STEP: Ensuring resource quota status captures configMap creation 06/27/23 18:01:49.618
STEP: Deleting a ConfigMap 06/27/23 18:01:51.631
STEP: Ensuring resource quota status released usage 06/27/23 18:01:51.659
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 18:01:53.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8216" for this suite. 06/27/23 18:01:53.701
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":341,"skipped":6389,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.316 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:01:25.414
    Jun 27 18:01:25.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 18:01:25.416
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:25.463
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:25.484
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 06/27/23 18:01:42.514
    STEP: Creating a ResourceQuota 06/27/23 18:01:47.54
    STEP: Ensuring resource quota status is calculated 06/27/23 18:01:47.565
    STEP: Creating a ConfigMap 06/27/23 18:01:49.579
    STEP: Ensuring resource quota status captures configMap creation 06/27/23 18:01:49.618
    STEP: Deleting a ConfigMap 06/27/23 18:01:51.631
    STEP: Ensuring resource quota status released usage 06/27/23 18:01:51.659
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 18:01:53.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8216" for this suite. 06/27/23 18:01:53.701
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:01:53.733
Jun 27 18:01:53.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename webhook 06/27/23 18:01:53.734
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:53.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:53.811
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 06/27/23 18:01:53.904
STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 18:01:54.438
STEP: Deploying the webhook pod 06/27/23 18:01:54.475
STEP: Wait for the deployment to be ready 06/27/23 18:01:54.519
Jun 27 18:01:54.557: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Jun 27 18:01:56.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 18, 1, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 1, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 18, 1, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 1, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 06/27/23 18:01:58.635
STEP: Verifying the service has paired with the endpoint 06/27/23 18:01:58.665
Jun 27 18:01:59.666: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/27/23 18:01:59.684
STEP: create a namespace for the webhook 06/27/23 18:01:59.795
STEP: create a configmap should be unconditionally rejected by the webhook 06/27/23 18:01:59.824
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 18:01:59.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-74" for this suite. 06/27/23 18:01:59.984
STEP: Destroying namespace "webhook-74-markers" for this suite. 06/27/23 18:02:00.014
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":342,"skipped":6413,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.458 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:01:53.733
    Jun 27 18:01:53.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename webhook 06/27/23 18:01:53.734
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:01:53.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:01:53.811
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 06/27/23 18:01:53.904
    STEP: Create role binding to let webhook read extension-apiserver-authentication 06/27/23 18:01:54.438
    STEP: Deploying the webhook pod 06/27/23 18:01:54.475
    STEP: Wait for the deployment to be ready 06/27/23 18:01:54.519
    Jun 27 18:01:54.557: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Jun 27 18:01:56.618: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.June, 27, 18, 1, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 1, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.June, 27, 18, 1, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.June, 27, 18, 1, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 06/27/23 18:01:58.635
    STEP: Verifying the service has paired with the endpoint 06/27/23 18:01:58.665
    Jun 27 18:01:59.666: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 06/27/23 18:01:59.684
    STEP: create a namespace for the webhook 06/27/23 18:01:59.795
    STEP: create a configmap should be unconditionally rejected by the webhook 06/27/23 18:01:59.824
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 18:01:59.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-74" for this suite. 06/27/23 18:01:59.984
    STEP: Destroying namespace "webhook-74-markers" for this suite. 06/27/23 18:02:00.014
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:02:00.191
Jun 27 18:02:00.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 18:02:00.196
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:02:00.24
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:02:00.258
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 06/27/23 18:02:00.271
Jun 27 18:02:00.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: rename a version 06/27/23 18:02:22.698
STEP: check the new version name is served 06/27/23 18:02:22.728
STEP: check the old version name is removed 06/27/23 18:02:33.085
STEP: check the other version is not changed 06/27/23 18:02:36.833
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Jun 27 18:02:53.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3711" for this suite. 06/27/23 18:02:53.883
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":343,"skipped":6426,"failed":0}
------------------------------
â€¢ [SLOW TEST] [53.713 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:02:00.191
    Jun 27 18:02:00.192: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename crd-publish-openapi 06/27/23 18:02:00.196
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:02:00.24
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:02:00.258
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 06/27/23 18:02:00.271
    Jun 27 18:02:00.272: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: rename a version 06/27/23 18:02:22.698
    STEP: check the new version name is served 06/27/23 18:02:22.728
    STEP: check the old version name is removed 06/27/23 18:02:33.085
    STEP: check the other version is not changed 06/27/23 18:02:36.833
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Jun 27 18:02:53.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3711" for this suite. 06/27/23 18:02:53.883
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:02:53.905
Jun 27 18:02:53.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename replicaset 06/27/23 18:02:53.907
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:02:53.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:02:53.955
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 06/27/23 18:02:53.968
STEP: Verify that the required pods have come up 06/27/23 18:02:53.986
Jun 27 18:02:54.007: INFO: Pod name sample-pod: Found 0 pods out of 3
Jun 27 18:02:59.026: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 06/27/23 18:02:59.026
Jun 27 18:02:59.037: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 06/27/23 18:02:59.037
STEP: DeleteCollection of the ReplicaSets 06/27/23 18:02:59.054
STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/27/23 18:02:59.087
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Jun 27 18:02:59.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-5033" for this suite. 06/27/23 18:02:59.18
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":344,"skipped":6426,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.310 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:02:53.905
    Jun 27 18:02:53.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename replicaset 06/27/23 18:02:53.907
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:02:53.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:02:53.955
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 06/27/23 18:02:53.968
    STEP: Verify that the required pods have come up 06/27/23 18:02:53.986
    Jun 27 18:02:54.007: INFO: Pod name sample-pod: Found 0 pods out of 3
    Jun 27 18:02:59.026: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 06/27/23 18:02:59.026
    Jun 27 18:02:59.037: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 06/27/23 18:02:59.037
    STEP: DeleteCollection of the ReplicaSets 06/27/23 18:02:59.054
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 06/27/23 18:02:59.087
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Jun 27 18:02:59.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-5033" for this suite. 06/27/23 18:02:59.18
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:02:59.216
Jun 27 18:02:59.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename emptydir 06/27/23 18:02:59.217
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:02:59.255
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:02:59.267
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 06/27/23 18:02:59.3
Jun 27 18:02:59.370: INFO: Waiting up to 5m0s for pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12" in namespace "emptydir-2957" to be "Succeeded or Failed"
Jun 27 18:02:59.385: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12": Phase="Pending", Reason="", readiness=false. Elapsed: 14.721056ms
Jun 27 18:03:01.399: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029089278s
Jun 27 18:03:03.399: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028172943s
Jun 27 18:03:05.397: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027074401s
STEP: Saw pod success 06/27/23 18:03:05.397
Jun 27 18:03:05.398: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12" satisfied condition "Succeeded or Failed"
Jun 27 18:03:05.409: INFO: Trying to get logs from node 10.113.180.90 pod pod-a460e44f-216c-4fda-8e4c-d487fb051d12 container test-container: <nil>
STEP: delete the pod 06/27/23 18:03:05.464
Jun 27 18:03:05.507: INFO: Waiting for pod pod-a460e44f-216c-4fda-8e4c-d487fb051d12 to disappear
Jun 27 18:03:05.521: INFO: Pod pod-a460e44f-216c-4fda-8e4c-d487fb051d12 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Jun 27 18:03:05.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2957" for this suite. 06/27/23 18:03:05.539
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":345,"skipped":6452,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.343 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:02:59.216
    Jun 27 18:02:59.216: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename emptydir 06/27/23 18:02:59.217
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:02:59.255
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:02:59.267
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 06/27/23 18:02:59.3
    Jun 27 18:02:59.370: INFO: Waiting up to 5m0s for pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12" in namespace "emptydir-2957" to be "Succeeded or Failed"
    Jun 27 18:02:59.385: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12": Phase="Pending", Reason="", readiness=false. Elapsed: 14.721056ms
    Jun 27 18:03:01.399: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12": Phase="Pending", Reason="", readiness=false. Elapsed: 2.029089278s
    Jun 27 18:03:03.399: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12": Phase="Pending", Reason="", readiness=false. Elapsed: 4.028172943s
    Jun 27 18:03:05.397: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027074401s
    STEP: Saw pod success 06/27/23 18:03:05.397
    Jun 27 18:03:05.398: INFO: Pod "pod-a460e44f-216c-4fda-8e4c-d487fb051d12" satisfied condition "Succeeded or Failed"
    Jun 27 18:03:05.409: INFO: Trying to get logs from node 10.113.180.90 pod pod-a460e44f-216c-4fda-8e4c-d487fb051d12 container test-container: <nil>
    STEP: delete the pod 06/27/23 18:03:05.464
    Jun 27 18:03:05.507: INFO: Waiting for pod pod-a460e44f-216c-4fda-8e4c-d487fb051d12 to disappear
    Jun 27 18:03:05.521: INFO: Pod pod-a460e44f-216c-4fda-8e4c-d487fb051d12 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Jun 27 18:03:05.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2957" for this suite. 06/27/23 18:03:05.539
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:03:05.563
Jun 27 18:03:05.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename kubectl 06/27/23 18:03:05.565
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:05.602
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:05.617
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Jun 27 18:03:05.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 create -f -'
Jun 27 18:03:07.649: INFO: stderr: ""
Jun 27 18:03:07.649: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Jun 27 18:03:07.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 create -f -'
Jun 27 18:03:08.249: INFO: stderr: ""
Jun 27 18:03:08.249: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 06/27/23 18:03:08.249
Jun 27 18:03:09.266: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 18:03:09.266: INFO: Found 0 / 1
Jun 27 18:03:10.274: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 18:03:10.274: INFO: Found 1 / 1
Jun 27 18:03:10.274: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Jun 27 18:03:10.288: INFO: Selector matched 1 pods for map[app:agnhost]
Jun 27 18:03:10.288: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Jun 27 18:03:10.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe pod agnhost-primary-d25hv'
Jun 27 18:03:10.465: INFO: stderr: ""
Jun 27 18:03:10.465: INFO: stdout: "Name:             agnhost-primary-d25hv\nNamespace:        kubectl-2841\nPriority:         0\nService Account:  default\nNode:             10.113.180.90/10.113.180.90\nStart Time:       Tue, 27 Jun 2023 18:03:07 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: c7a5f10ce0de484902418ce64e83d68264a662e2c4c5b34a18d86157822ab9b6\n                  cni.projectcalico.org/podIP: 172.30.250.239/32\n                  cni.projectcalico.org/podIPs: 172.30.250.239/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.250.239\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.250.239\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.250.239\nIPs:\n  IP:           172.30.250.239\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://2baa6c42aca3e1d3ebca12c808a6bbe541586f59b6c15f04cfb716c67c1d5b5c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 27 Jun 2023 18:03:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gtx8r (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-gtx8r:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-2841/agnhost-primary-d25hv to 10.113.180.90\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.250.239/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
Jun 27 18:03:10.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe rc agnhost-primary'
Jun 27 18:03:10.669: INFO: stderr: ""
Jun 27 18:03:10.669: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2841\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-d25hv\n"
Jun 27 18:03:10.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe service agnhost-primary'
Jun 27 18:03:10.918: INFO: stderr: ""
Jun 27 18:03:10.918: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2841\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.189.165\nIPs:               172.21.189.165\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.250.239:6379\nSession Affinity:  None\nEvents:            <none>\n"
Jun 27 18:03:10.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe node 10.113.180.89'
Jun 27 18:03:11.469: INFO: stderr: ""
Jun 27 18:03:11.469: INFO: stdout: "Name:               10.113.180.89\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=159.122.215.18\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.113.180.89\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cide9hpl0h921tvtv8i0-kubee2epvgo-default-000002ba\n                    ibm-cloud.kubernetes.io/worker-pool-id=cide9hpl0h921tvtv8i0-f760913\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.12.21_1547_openshift\n                    ibm-cloud.kubernetes.io/zone=lon02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.113.180.89\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2747290\n                    publicVLAN=2747292\n                    topology.kubernetes.io/region=eu-gb\n                    topology.kubernetes.io/zone=lon02\nAnnotations:        projectcalico.org/IPv4Address: 10.113.180.89/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.106.128\nCreationTimestamp:  Tue, 27 Jun 2023 13:53:31 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.113.180.89\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 27 Jun 2023 18:03:04 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 27 Jun 2023 13:55:07 +0000   Tue, 27 Jun 2023 13:55:07 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 27 Jun 2023 18:01:33 +0000   Tue, 27 Jun 2023 13:53:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 27 Jun 2023 18:01:33 +0000   Tue, 27 Jun 2023 13:53:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 27 Jun 2023 18:01:33 +0000   Tue, 27 Jun 2023 13:53:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 27 Jun 2023 18:01:33 +0000   Tue, 27 Jun 2023 13:55:24 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.113.180.89\n  ExternalIP:  159.122.215.18\n  Hostname:    10.113.180.89\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102609848Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16382396Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93913280025\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13594044Ki\n  pods:               110\nSystem Info:\n  Machine ID:                             3cc22ccd289d4e509b59cbdc16b99a7d\n  System UUID:                            d42c2d85-390d-8108-b2e7-e8dab3db3d1e\n  Boot ID:                                ea4fdafc-8cfe-455f-ba60-4a17fa449cfe\n  Kernel Version:                         4.18.0-477.13.1.el8_8.x86_64\n  OS Image:                               Red Hat Enterprise Linux 8.8 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.25.3-5.rhaos4.12.git44a2cb2.el8\n  Kubelet Version:                        v1.25.10+3fe2906\n  Kube-Proxy Version:                     v1.25.10+3fe2906\nPodCIDR:                                  172.30.1.0/24\nPodCIDRs:                                 172.30.1.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///cide9hpl0h921tvtv8i0/kube-cide9hpl0h921tvtv8i0-kubee2epvgo-default-000002ba\nNon-terminated Pods:                      (44 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-vplt6                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h8m\n  calico-system                           calico-typha-657df678d7-65kdh                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h8m\n  disruption-8645                         rs-jnz52                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         117s\n  ibm-system                              ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp        5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h59m\n  kube-system                             ibm-keepalived-watcher-9rf8d                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         4h9m\n  kube-system                             ibm-master-proxy-static-10.113.180.89                      26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      4h9m\n  kube-system                             ibmcloud-block-storage-driver-5ml2s                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     4h9m\n  kube-system                             vpn-546f6bf578-7ks4f                                       5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         3h56m\n  openshift-cluster-node-tuning-operator  tuned-mwm8w                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h3m\n  openshift-cluster-storage-operator      csi-snapshot-controller-5694c47cbb-9pvhd                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h7m\n  openshift-cluster-storage-operator      csi-snapshot-webhook-5d7cc7f6cb-lwgl9                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         4h7m\n  openshift-console                       console-6c8658586b-jlcfh                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h59m\n  openshift-console                       downloads-57bd479866-2fpqm                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h6m\n  openshift-dns                           dns-default-z76hf                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         4h3m\n  openshift-dns                           node-resolver-cwltw                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         4h3m\n  openshift-image-registry                image-registry-7f546fc5bb-n7w67                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         79m\n  openshift-image-registry                node-ca-d4q6s                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h3m\n  openshift-ingress-canary                ingress-canary-bmwtq                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         4h3m\n  openshift-ingress                       router-default-7f97cd5c5f-m5pcx                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         4h3m\n  openshift-kube-proxy                    openshift-kube-proxy-b4224                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         4h8m\n  openshift-marketplace                   certified-operators-799hx                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         78m\n  openshift-marketplace                   community-operators-9jptm                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         78m\n  openshift-marketplace                   redhat-marketplace-qzk6b                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h5m\n  openshift-marketplace                   redhat-operators-jqjb9                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         78m\n  openshift-monitoring                    alertmanager-main-0                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         4h\n  openshift-monitoring                    kube-state-metrics-685444f59b-5dlq4                        4m (0%)       0 (0%)      110Mi (0%)       0 (0%)         4h2m\n  openshift-monitoring                    node-exporter-lqlws                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                    openshift-state-metrics-6d79d8c586-9mgzr                   3m (0%)       0 (0%)      72Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                    prometheus-adapter-648f68fcc-h27rt                         1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         4h1m\n  openshift-monitoring                    prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         4h1m\n  openshift-monitoring                    prometheus-operator-68dfcc5c8-kn6tn                        6m (0%)       0 (0%)      165Mi (1%)       0 (0%)         79m\n  openshift-monitoring                    prometheus-operator-admission-webhook-6c667b594b-bds22     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         4h3m\n  openshift-monitoring                    telemeter-client-7dc8fdddc8-kpccw                          3m (0%)       0 (0%)      70Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                    thanos-querier-754f675f77-6nf9l                            15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         4h2m\n  openshift-multus                        multus-7zznf                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         4h9m\n  openshift-multus                        multus-additional-cni-plugins-pn2hb                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h9m\n  openshift-multus                        multus-admission-controller-754d449d79-strmv               20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         78m\n  openshift-multus                        network-metrics-daemon-p8dh4                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         4h9m\n  openshift-network-diagnostics           network-check-target-7qghz                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         4h8m\n  openshift-operator-lifecycle-manager    packageserver-8d78bf5dd-q8ct5                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         79m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  sonobuoy                                sonobuoy-e2e-job-0e516cd149fa4871                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  tigera-operator                         tigera-operator-687c49f5c8-qcfmm                           100m (2%)     0 (0%)      40Mi (0%)        0 (0%)         79m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1386m (35%)      800m (20%)\n  memory             3992083Ki (29%)  826572800 (5%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:              <none>\n"
Jun 27 18:03:11.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe namespace kubectl-2841'
Jun 27 18:03:11.617: INFO: stderr: ""
Jun 27 18:03:11.617: INFO: stdout: "Name:         kubectl-2841\nLabels:       e2e-framework=kubectl\n              e2e-run=05315eb1-c48b-405c-954b-4593116b0046\n              kubernetes.io/metadata.name=kubectl-2841\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c68,c17\n              openshift.io/sa.scc.supplemental-groups: 1004590000/10000\n              openshift.io/sa.scc.uid-range: 1004590000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Jun 27 18:03:11.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2841" for this suite. 06/27/23 18:03:11.638
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":346,"skipped":6468,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.097 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:03:05.563
    Jun 27 18:03:05.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename kubectl 06/27/23 18:03:05.565
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:05.602
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:05.617
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Jun 27 18:03:05.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 create -f -'
    Jun 27 18:03:07.649: INFO: stderr: ""
    Jun 27 18:03:07.649: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Jun 27 18:03:07.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 create -f -'
    Jun 27 18:03:08.249: INFO: stderr: ""
    Jun 27 18:03:08.249: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 06/27/23 18:03:08.249
    Jun 27 18:03:09.266: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 18:03:09.266: INFO: Found 0 / 1
    Jun 27 18:03:10.274: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 18:03:10.274: INFO: Found 1 / 1
    Jun 27 18:03:10.274: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Jun 27 18:03:10.288: INFO: Selector matched 1 pods for map[app:agnhost]
    Jun 27 18:03:10.288: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Jun 27 18:03:10.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe pod agnhost-primary-d25hv'
    Jun 27 18:03:10.465: INFO: stderr: ""
    Jun 27 18:03:10.465: INFO: stdout: "Name:             agnhost-primary-d25hv\nNamespace:        kubectl-2841\nPriority:         0\nService Account:  default\nNode:             10.113.180.90/10.113.180.90\nStart Time:       Tue, 27 Jun 2023 18:03:07 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: c7a5f10ce0de484902418ce64e83d68264a662e2c4c5b34a18d86157822ab9b6\n                  cni.projectcalico.org/podIP: 172.30.250.239/32\n                  cni.projectcalico.org/podIPs: 172.30.250.239/32\n                  k8s.v1.cni.cncf.io/network-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.250.239\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  k8s.v1.cni.cncf.io/networks-status:\n                    [{\n                        \"name\": \"k8s-pod-network\",\n                        \"ips\": [\n                            \"172.30.250.239\"\n                        ],\n                        \"default\": true,\n                        \"dns\": {}\n                    }]\n                  openshift.io/scc: anyuid\nStatus:           Running\nIP:               172.30.250.239\nIPs:\n  IP:           172.30.250.239\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://2baa6c42aca3e1d3ebca12c808a6bbe541586f59b6c15f04cfb716c67c1d5b5c\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:37dd9ad84b34913b4bc57e00b0a28e8b00f18ca6a5ecec69461aa57402e5c787\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Tue, 27 Jun 2023 18:03:09 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-gtx8r (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-gtx8r:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\n    ConfigMapName:           openshift-service-ca.crt\n    ConfigMapOptional:       <nil>\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason          Age   From               Message\n  ----    ------          ----  ----               -------\n  Normal  Scheduled       3s    default-scheduler  Successfully assigned kubectl-2841/agnhost-primary-d25hv to 10.113.180.90\n  Normal  AddedInterface  2s    multus             Add eth0 [172.30.250.239/32] from k8s-pod-network\n  Normal  Pulled          2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created         1s    kubelet            Created container agnhost-primary\n  Normal  Started         1s    kubelet            Started container agnhost-primary\n"
    Jun 27 18:03:10.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe rc agnhost-primary'
    Jun 27 18:03:10.669: INFO: stderr: ""
    Jun 27 18:03:10.669: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2841\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  3s    replication-controller  Created pod: agnhost-primary-d25hv\n"
    Jun 27 18:03:10.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe service agnhost-primary'
    Jun 27 18:03:10.918: INFO: stderr: ""
    Jun 27 18:03:10.918: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2841\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                172.21.189.165\nIPs:               172.21.189.165\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.30.250.239:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Jun 27 18:03:10.938: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe node 10.113.180.89'
    Jun 27 18:03:11.469: INFO: stderr: ""
    Jun 27 18:03:11.469: INFO: stdout: "Name:               10.113.180.89\nRoles:              master,worker\nLabels:             arch=amd64\n                    beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=eu-gb\n                    failure-domain.beta.kubernetes.io/zone=lon02\n                    ibm-cloud.kubernetes.io/encrypted-docker-data=true\n                    ibm-cloud.kubernetes.io/external-ip=159.122.215.18\n                    ibm-cloud.kubernetes.io/iaas-provider=softlayer\n                    ibm-cloud.kubernetes.io/internal-ip=10.113.180.89\n                    ibm-cloud.kubernetes.io/machine-type=b3c.4x16.encrypted\n                    ibm-cloud.kubernetes.io/os=REDHAT_8_64\n                    ibm-cloud.kubernetes.io/region=eu-gb\n                    ibm-cloud.kubernetes.io/sgx-enabled=false\n                    ibm-cloud.kubernetes.io/worker-id=kube-cide9hpl0h921tvtv8i0-kubee2epvgo-default-000002ba\n                    ibm-cloud.kubernetes.io/worker-pool-id=cide9hpl0h921tvtv8i0-f760913\n                    ibm-cloud.kubernetes.io/worker-pool-name=default\n                    ibm-cloud.kubernetes.io/worker-version=4.12.21_1547_openshift\n                    ibm-cloud.kubernetes.io/zone=lon02\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=10.113.180.89\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/master=\n                    node-role.kubernetes.io/worker=\n                    node.kubernetes.io/instance-type=b3c.4x16.encrypted\n                    node.openshift.io/os_id=rhel\n                    privateVLAN=2747290\n                    publicVLAN=2747292\n                    topology.kubernetes.io/region=eu-gb\n                    topology.kubernetes.io/zone=lon02\nAnnotations:        projectcalico.org/IPv4Address: 10.113.180.89/26\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.30.106.128\nCreationTimestamp:  Tue, 27 Jun 2023 13:53:31 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  10.113.180.89\n  AcquireTime:     <unset>\n  RenewTime:       Tue, 27 Jun 2023 18:03:04 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Tue, 27 Jun 2023 13:55:07 +0000   Tue, 27 Jun 2023 13:55:07 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Tue, 27 Jun 2023 18:01:33 +0000   Tue, 27 Jun 2023 13:53:31 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Tue, 27 Jun 2023 18:01:33 +0000   Tue, 27 Jun 2023 13:53:31 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Tue, 27 Jun 2023 18:01:33 +0000   Tue, 27 Jun 2023 13:53:31 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Tue, 27 Jun 2023 18:01:33 +0000   Tue, 27 Jun 2023 13:55:24 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.113.180.89\n  ExternalIP:  159.122.215.18\n  Hostname:    10.113.180.89\nCapacity:\n  cpu:                4\n  ephemeral-storage:  102609848Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16382396Ki\n  pods:               110\nAllocatable:\n  cpu:                3910m\n  ephemeral-storage:  93913280025\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             13594044Ki\n  pods:               110\nSystem Info:\n  Machine ID:                             3cc22ccd289d4e509b59cbdc16b99a7d\n  System UUID:                            d42c2d85-390d-8108-b2e7-e8dab3db3d1e\n  Boot ID:                                ea4fdafc-8cfe-455f-ba60-4a17fa449cfe\n  Kernel Version:                         4.18.0-477.13.1.el8_8.x86_64\n  OS Image:                               Red Hat Enterprise Linux 8.8 (Ootpa)\n  Operating System:                       linux\n  Architecture:                           amd64\n  Container Runtime Version:              cri-o://1.25.3-5.rhaos4.12.git44a2cb2.el8\n  Kubelet Version:                        v1.25.10+3fe2906\n  Kube-Proxy Version:                     v1.25.10+3fe2906\nPodCIDR:                                  172.30.1.0/24\nPodCIDRs:                                 172.30.1.0/24\nProviderID:                               ibm://fee034388aa6435883a1f720010ab3a2///cide9hpl0h921tvtv8i0/kube-cide9hpl0h921tvtv8i0-kubee2epvgo-default-000002ba\nNon-terminated Pods:                      (44 in total)\n  Namespace                               Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                               ----                                                       ------------  ----------  ---------------  -------------  ---\n  calico-system                           calico-node-vplt6                                          250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h8m\n  calico-system                           calico-typha-657df678d7-65kdh                              250m (6%)     0 (0%)      80Mi (0%)        0 (0%)         4h8m\n  disruption-8645                         rs-jnz52                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         117s\n  ibm-system                              ibm-cloud-provider-ip-5-10-126-211-5c8746498b-25hhp        5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         3h59m\n  kube-system                             ibm-keepalived-watcher-9rf8d                               5m (0%)       0 (0%)      10Mi (0%)        0 (0%)         4h9m\n  kube-system                             ibm-master-proxy-static-10.113.180.89                      26m (0%)      300m (7%)   32001024 (0%)    512M (3%)      4h9m\n  kube-system                             ibmcloud-block-storage-driver-5ml2s                        50m (1%)      500m (12%)  100Mi (0%)       300Mi (2%)     4h9m\n  kube-system                             vpn-546f6bf578-7ks4f                                       5m (0%)       0 (0%)      5Mi (0%)         0 (0%)         3h56m\n  openshift-cluster-node-tuning-operator  tuned-mwm8w                                                10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h3m\n  openshift-cluster-storage-operator      csi-snapshot-controller-5694c47cbb-9pvhd                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h7m\n  openshift-cluster-storage-operator      csi-snapshot-webhook-5d7cc7f6cb-lwgl9                      10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         4h7m\n  openshift-console                       console-6c8658586b-jlcfh                                   10m (0%)      0 (0%)      100Mi (0%)       0 (0%)         3h59m\n  openshift-console                       downloads-57bd479866-2fpqm                                 10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h6m\n  openshift-dns                           dns-default-z76hf                                          60m (1%)      0 (0%)      110Mi (0%)       0 (0%)         4h3m\n  openshift-dns                           node-resolver-cwltw                                        5m (0%)       0 (0%)      21Mi (0%)        0 (0%)         4h3m\n  openshift-image-registry                image-registry-7f546fc5bb-n7w67                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         79m\n  openshift-image-registry                node-ca-d4q6s                                              10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h3m\n  openshift-ingress-canary                ingress-canary-bmwtq                                       10m (0%)      0 (0%)      20Mi (0%)        0 (0%)         4h3m\n  openshift-ingress                       router-default-7f97cd5c5f-m5pcx                            100m (2%)     0 (0%)      256Mi (1%)       0 (0%)         4h3m\n  openshift-kube-proxy                    openshift-kube-proxy-b4224                                 110m (2%)     0 (0%)      220Mi (1%)       0 (0%)         4h8m\n  openshift-marketplace                   certified-operators-799hx                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         78m\n  openshift-marketplace                   community-operators-9jptm                                  10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         78m\n  openshift-marketplace                   redhat-marketplace-qzk6b                                   10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         4h5m\n  openshift-marketplace                   redhat-operators-jqjb9                                     10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         78m\n  openshift-monitoring                    alertmanager-main-0                                        9m (0%)       0 (0%)      120Mi (0%)       0 (0%)         4h\n  openshift-monitoring                    kube-state-metrics-685444f59b-5dlq4                        4m (0%)       0 (0%)      110Mi (0%)       0 (0%)         4h2m\n  openshift-monitoring                    node-exporter-lqlws                                        9m (0%)       0 (0%)      47Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                    openshift-state-metrics-6d79d8c586-9mgzr                   3m (0%)       0 (0%)      72Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                    prometheus-adapter-648f68fcc-h27rt                         1m (0%)       0 (0%)      40Mi (0%)        0 (0%)         4h1m\n  openshift-monitoring                    prometheus-k8s-1                                           75m (1%)      0 (0%)      1104Mi (8%)      0 (0%)         4h1m\n  openshift-monitoring                    prometheus-operator-68dfcc5c8-kn6tn                        6m (0%)       0 (0%)      165Mi (1%)       0 (0%)         79m\n  openshift-monitoring                    prometheus-operator-admission-webhook-6c667b594b-bds22     5m (0%)       0 (0%)      30Mi (0%)        0 (0%)         4h3m\n  openshift-monitoring                    telemeter-client-7dc8fdddc8-kpccw                          3m (0%)       0 (0%)      70Mi (0%)        0 (0%)         4h2m\n  openshift-monitoring                    thanos-querier-754f675f77-6nf9l                            15m (0%)      0 (0%)      92Mi (0%)        0 (0%)         4h2m\n  openshift-multus                        multus-7zznf                                               10m (0%)      0 (0%)      65Mi (0%)        0 (0%)         4h9m\n  openshift-multus                        multus-additional-cni-plugins-pn2hb                        10m (0%)      0 (0%)      10Mi (0%)        0 (0%)         4h9m\n  openshift-multus                        multus-admission-controller-754d449d79-strmv               20m (0%)      0 (0%)      70Mi (0%)        0 (0%)         78m\n  openshift-multus                        network-metrics-daemon-p8dh4                               20m (0%)      0 (0%)      120Mi (0%)       0 (0%)         4h9m\n  openshift-network-diagnostics           network-check-target-7qghz                                 10m (0%)      0 (0%)      15Mi (0%)        0 (0%)         4h8m\n  openshift-operator-lifecycle-manager    packageserver-8d78bf5dd-q8ct5                              10m (0%)      0 (0%)      50Mi (0%)        0 (0%)         79m\n  sonobuoy                                sonobuoy                                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  sonobuoy                                sonobuoy-e2e-job-0e516cd149fa4871                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  sonobuoy                                sonobuoy-systemd-logs-daemon-set-21facd0441c84618-pc4nf    0 (0%)        0 (0%)      0 (0%)           0 (0%)         101m\n  tigera-operator                         tigera-operator-687c49f5c8-qcfmm                           100m (2%)     0 (0%)      40Mi (0%)        0 (0%)         79m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests         Limits\n  --------           --------         ------\n  cpu                1386m (35%)      800m (20%)\n  memory             3992083Ki (29%)  826572800 (5%)\n  ephemeral-storage  0 (0%)           0 (0%)\n  hugepages-1Gi      0 (0%)           0 (0%)\n  hugepages-2Mi      0 (0%)           0 (0%)\nEvents:              <none>\n"
    Jun 27 18:03:11.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=kubectl-2841 describe namespace kubectl-2841'
    Jun 27 18:03:11.617: INFO: stderr: ""
    Jun 27 18:03:11.617: INFO: stdout: "Name:         kubectl-2841\nLabels:       e2e-framework=kubectl\n              e2e-run=05315eb1-c48b-405c-954b-4593116b0046\n              kubernetes.io/metadata.name=kubectl-2841\n              pod-security.kubernetes.io/audit=privileged\n              pod-security.kubernetes.io/audit-version=v1.24\n              pod-security.kubernetes.io/enforce=baseline\n              pod-security.kubernetes.io/warn=privileged\n              pod-security.kubernetes.io/warn-version=v1.24\nAnnotations:  openshift.io/sa.scc.mcs: s0:c68,c17\n              openshift.io/sa.scc.supplemental-groups: 1004590000/10000\n              openshift.io/sa.scc.uid-range: 1004590000/10000\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Jun 27 18:03:11.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2841" for this suite. 06/27/23 18:03:11.638
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:03:11.666
Jun 27 18:03:11.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 18:03:11.668
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:11.725
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:11.739
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-1025 06/27/23 18:03:11.756
STEP: creating service affinity-clusterip in namespace services-1025 06/27/23 18:03:11.756
STEP: creating replication controller affinity-clusterip in namespace services-1025 06/27/23 18:03:11.811
I0627 18:03:11.836170      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1025, replica count: 3
I0627 18:03:14.891883      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 18:03:14.926: INFO: Creating new exec pod
Jun 27 18:03:14.955: INFO: Waiting up to 5m0s for pod "execpod-affinityg4sh9" in namespace "services-1025" to be "running"
Jun 27 18:03:14.967: INFO: Pod "execpod-affinityg4sh9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.225539ms
Jun 27 18:03:17.003: INFO: Pod "execpod-affinityg4sh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.048152268s
Jun 27 18:03:17.003: INFO: Pod "execpod-affinityg4sh9" satisfied condition "running"
Jun 27 18:03:18.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1025 exec execpod-affinityg4sh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Jun 27 18:03:18.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Jun 27 18:03:18.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 18:03:18.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1025 exec execpod-affinityg4sh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.46.205 80'
Jun 27 18:03:18.825: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.46.205 80\nConnection to 172.21.46.205 80 port [tcp/http] succeeded!\n"
Jun 27 18:03:18.825: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 18:03:18.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1025 exec execpod-affinityg4sh9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.46.205:80/ ; done'
Jun 27 18:03:19.331: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n"
Jun 27 18:03:19.331: INFO: stdout: "\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc"
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
Jun 27 18:03:19.331: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-1025, will wait for the garbage collector to delete the pods 06/27/23 18:03:19.358
Jun 27 18:03:19.451: INFO: Deleting ReplicationController affinity-clusterip took: 27.875142ms
Jun 27 18:03:19.553: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.578119ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 18:03:22.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-1025" for this suite. 06/27/23 18:03:22.937
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":347,"skipped":6487,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.293 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:03:11.666
    Jun 27 18:03:11.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 18:03:11.668
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:11.725
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:11.739
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-1025 06/27/23 18:03:11.756
    STEP: creating service affinity-clusterip in namespace services-1025 06/27/23 18:03:11.756
    STEP: creating replication controller affinity-clusterip in namespace services-1025 06/27/23 18:03:11.811
    I0627 18:03:11.836170      23 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-1025, replica count: 3
    I0627 18:03:14.891883      23 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 27 18:03:14.926: INFO: Creating new exec pod
    Jun 27 18:03:14.955: INFO: Waiting up to 5m0s for pod "execpod-affinityg4sh9" in namespace "services-1025" to be "running"
    Jun 27 18:03:14.967: INFO: Pod "execpod-affinityg4sh9": Phase="Pending", Reason="", readiness=false. Elapsed: 11.225539ms
    Jun 27 18:03:17.003: INFO: Pod "execpod-affinityg4sh9": Phase="Running", Reason="", readiness=true. Elapsed: 2.048152268s
    Jun 27 18:03:17.003: INFO: Pod "execpod-affinityg4sh9" satisfied condition "running"
    Jun 27 18:03:18.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1025 exec execpod-affinityg4sh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Jun 27 18:03:18.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Jun 27 18:03:18.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 18:03:18.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1025 exec execpod-affinityg4sh9 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.46.205 80'
    Jun 27 18:03:18.825: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.46.205 80\nConnection to 172.21.46.205 80 port [tcp/http] succeeded!\n"
    Jun 27 18:03:18.825: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 18:03:18.825: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-1025 exec execpod-affinityg4sh9 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.46.205:80/ ; done'
    Jun 27 18:03:19.331: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.46.205:80/\n"
    Jun 27 18:03:19.331: INFO: stdout: "\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc\naffinity-clusterip-dtwgc"
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Received response from host: affinity-clusterip-dtwgc
    Jun 27 18:03:19.331: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-1025, will wait for the garbage collector to delete the pods 06/27/23 18:03:19.358
    Jun 27 18:03:19.451: INFO: Deleting ReplicationController affinity-clusterip took: 27.875142ms
    Jun 27 18:03:19.553: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.578119ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 18:03:22.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-1025" for this suite. 06/27/23 18:03:22.937
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:03:22.962
Jun 27 18:03:22.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 18:03:22.964
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:23.015
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 06/27/23 18:03:23.03
STEP: Getting a ResourceQuota 06/27/23 18:03:23.046
STEP: Updating a ResourceQuota 06/27/23 18:03:23.058
STEP: Verifying a ResourceQuota was modified 06/27/23 18:03:23.075
STEP: Deleting a ResourceQuota 06/27/23 18:03:23.088
STEP: Verifying the deleted ResourceQuota 06/27/23 18:03:23.106
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 18:03:23.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1133" for this suite. 06/27/23 18:03:23.146
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":348,"skipped":6489,"failed":0}
------------------------------
â€¢ [0.207 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:03:22.962
    Jun 27 18:03:22.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 18:03:22.964
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:23.015
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 06/27/23 18:03:23.03
    STEP: Getting a ResourceQuota 06/27/23 18:03:23.046
    STEP: Updating a ResourceQuota 06/27/23 18:03:23.058
    STEP: Verifying a ResourceQuota was modified 06/27/23 18:03:23.075
    STEP: Deleting a ResourceQuota 06/27/23 18:03:23.088
    STEP: Verifying the deleted ResourceQuota 06/27/23 18:03:23.106
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 18:03:23.129: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1133" for this suite. 06/27/23 18:03:23.146
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:03:23.174
Jun 27 18:03:23.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename services 06/27/23 18:03:23.176
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:23.223
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:23.24
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
STEP: creating service in namespace services-97 06/27/23 18:03:23.255
STEP: creating service affinity-clusterip-transition in namespace services-97 06/27/23 18:03:23.255
STEP: creating replication controller affinity-clusterip-transition in namespace services-97 06/27/23 18:03:23.299
I0627 18:03:23.326188      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-97, replica count: 3
I0627 18:03:26.379748      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Jun 27 18:03:26.418: INFO: Creating new exec pod
Jun 27 18:03:26.449: INFO: Waiting up to 5m0s for pod "execpod-affinityvncjb" in namespace "services-97" to be "running"
Jun 27 18:03:26.464: INFO: Pod "execpod-affinityvncjb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.124272ms
Jun 27 18:03:28.480: INFO: Pod "execpod-affinityvncjb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030372684s
Jun 27 18:03:30.479: INFO: Pod "execpod-affinityvncjb": Phase="Running", Reason="", readiness=true. Elapsed: 4.02929433s
Jun 27 18:03:30.479: INFO: Pod "execpod-affinityvncjb" satisfied condition "running"
Jun 27 18:03:31.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-97 exec execpod-affinityvncjb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Jun 27 18:03:31.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Jun 27 18:03:31.871: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 18:03:31.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-97 exec execpod-affinityvncjb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.100.246 80'
Jun 27 18:03:32.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.100.246 80\nConnection to 172.21.100.246 80 port [tcp/http] succeeded!\n"
Jun 27 18:03:32.303: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Jun 27 18:03:32.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-97 exec execpod-affinityvncjb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.100.246:80/ ; done'
Jun 27 18:03:32.919: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n"
Jun 27 18:03:32.919: INFO: stdout: "\naffinity-clusterip-transition-qzwks\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-qzwks\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-qzwks\naffinity-clusterip-transition-5ptr7"
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-qzwks
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-qzwks
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-qzwks
Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
Jun 27 18:03:32.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-97 exec execpod-affinityvncjb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.100.246:80/ ; done'
Jun 27 18:03:33.404: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n"
Jun 27 18:03:33.404: INFO: stdout: "\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9"
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
Jun 27 18:03:33.404: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-97, will wait for the garbage collector to delete the pods 06/27/23 18:03:33.429
Jun 27 18:03:33.519: INFO: Deleting ReplicationController affinity-clusterip-transition took: 24.32879ms
Jun 27 18:03:33.621: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.205063ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Jun 27 18:03:37.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-97" for this suite. 06/27/23 18:03:37.1
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":349,"skipped":6502,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.944 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:03:23.174
    Jun 27 18:03:23.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename services 06/27/23 18:03:23.176
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:23.223
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:23.24
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2179
    STEP: creating service in namespace services-97 06/27/23 18:03:23.255
    STEP: creating service affinity-clusterip-transition in namespace services-97 06/27/23 18:03:23.255
    STEP: creating replication controller affinity-clusterip-transition in namespace services-97 06/27/23 18:03:23.299
    I0627 18:03:23.326188      23 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-97, replica count: 3
    I0627 18:03:26.379748      23 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Jun 27 18:03:26.418: INFO: Creating new exec pod
    Jun 27 18:03:26.449: INFO: Waiting up to 5m0s for pod "execpod-affinityvncjb" in namespace "services-97" to be "running"
    Jun 27 18:03:26.464: INFO: Pod "execpod-affinityvncjb": Phase="Pending", Reason="", readiness=false. Elapsed: 15.124272ms
    Jun 27 18:03:28.480: INFO: Pod "execpod-affinityvncjb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030372684s
    Jun 27 18:03:30.479: INFO: Pod "execpod-affinityvncjb": Phase="Running", Reason="", readiness=true. Elapsed: 4.02929433s
    Jun 27 18:03:30.479: INFO: Pod "execpod-affinityvncjb" satisfied condition "running"
    Jun 27 18:03:31.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-97 exec execpod-affinityvncjb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Jun 27 18:03:31.871: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Jun 27 18:03:31.871: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 18:03:31.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-97 exec execpod-affinityvncjb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.21.100.246 80'
    Jun 27 18:03:32.303: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.21.100.246 80\nConnection to 172.21.100.246 80 port [tcp/http] succeeded!\n"
    Jun 27 18:03:32.303: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Jun 27 18:03:32.340: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-97 exec execpod-affinityvncjb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.100.246:80/ ; done'
    Jun 27 18:03:32.919: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n"
    Jun 27 18:03:32.919: INFO: stdout: "\naffinity-clusterip-transition-qzwks\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-qzwks\naffinity-clusterip-transition-5ptr7\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-qzwks\naffinity-clusterip-transition-5ptr7"
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-qzwks
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-qzwks
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-qzwks
    Jun 27 18:03:32.919: INFO: Received response from host: affinity-clusterip-transition-5ptr7
    Jun 27 18:03:32.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2210758088 --namespace=services-97 exec execpod-affinityvncjb -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.21.100.246:80/ ; done'
    Jun 27 18:03:33.404: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.21.100.246:80/\n"
    Jun 27 18:03:33.404: INFO: stdout: "\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9\naffinity-clusterip-transition-7nnf9"
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Received response from host: affinity-clusterip-transition-7nnf9
    Jun 27 18:03:33.404: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-97, will wait for the garbage collector to delete the pods 06/27/23 18:03:33.429
    Jun 27 18:03:33.519: INFO: Deleting ReplicationController affinity-clusterip-transition took: 24.32879ms
    Jun 27 18:03:33.621: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.205063ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Jun 27 18:03:37.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-97" for this suite. 06/27/23 18:03:37.1
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:03:37.123
Jun 27 18:03:37.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 18:03:37.126
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:37.167
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:37.18
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 06/27/23 18:03:37.202
Jun 27 18:03:37.258: INFO: Waiting up to 5m0s for pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e" in namespace "downward-api-4872" to be "running and ready"
Jun 27 18:03:37.273: INFO: Pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.675958ms
Jun 27 18:03:37.273: INFO: The phase of Pod annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e is Pending, waiting for it to be Running (with Ready = true)
Jun 27 18:03:39.286: INFO: Pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027966781s
Jun 27 18:03:39.286: INFO: The phase of Pod annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e is Pending, waiting for it to be Running (with Ready = true)
Jun 27 18:03:41.286: INFO: Pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e": Phase="Running", Reason="", readiness=true. Elapsed: 4.027949452s
Jun 27 18:03:41.286: INFO: The phase of Pod annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e is Running (Ready = true)
Jun 27 18:03:41.286: INFO: Pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e" satisfied condition "running and ready"
Jun 27 18:03:41.892: INFO: Successfully updated pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 18:03:44.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-4872" for this suite. 06/27/23 18:03:44.057
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":350,"skipped":6531,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.963 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:03:37.123
    Jun 27 18:03:37.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 18:03:37.126
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:37.167
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:37.18
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 06/27/23 18:03:37.202
    Jun 27 18:03:37.258: INFO: Waiting up to 5m0s for pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e" in namespace "downward-api-4872" to be "running and ready"
    Jun 27 18:03:37.273: INFO: Pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 14.675958ms
    Jun 27 18:03:37.273: INFO: The phase of Pod annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 18:03:39.286: INFO: Pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027966781s
    Jun 27 18:03:39.286: INFO: The phase of Pod annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 18:03:41.286: INFO: Pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e": Phase="Running", Reason="", readiness=true. Elapsed: 4.027949452s
    Jun 27 18:03:41.286: INFO: The phase of Pod annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e is Running (Ready = true)
    Jun 27 18:03:41.286: INFO: Pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e" satisfied condition "running and ready"
    Jun 27 18:03:41.892: INFO: Successfully updated pod "annotationupdate56065b34-81f5-4348-bfc1-0ae4c13acd4e"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 18:03:44.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-4872" for this suite. 06/27/23 18:03:44.057
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:03:44.091
Jun 27 18:03:44.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename resourcequota 06/27/23 18:03:44.093
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:44.134
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:44.149
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 06/27/23 18:03:44.168
STEP: Creating a ResourceQuota 06/27/23 18:03:49.182
STEP: Ensuring resource quota status is calculated 06/27/23 18:03:49.198
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Jun 27 18:03:51.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8018" for this suite. 06/27/23 18:03:51.243
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":351,"skipped":6534,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.193 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:03:44.091
    Jun 27 18:03:44.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename resourcequota 06/27/23 18:03:44.093
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:44.134
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:44.149
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 06/27/23 18:03:44.168
    STEP: Creating a ResourceQuota 06/27/23 18:03:49.182
    STEP: Ensuring resource quota status is calculated 06/27/23 18:03:49.198
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Jun 27 18:03:51.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8018" for this suite. 06/27/23 18:03:51.243
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:03:51.287
Jun 27 18:03:51.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename cronjob 06/27/23 18:03:51.29
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:51.347
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:51.364
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 06/27/23 18:03:51.378
STEP: Ensuring more than one job is running at a time 06/27/23 18:03:51.397
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/27/23 18:05:01.415
STEP: Removing cronjob 06/27/23 18:05:01.426
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Jun 27 18:05:01.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7591" for this suite. 06/27/23 18:05:01.468
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":352,"skipped":6538,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.203 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:03:51.287
    Jun 27 18:03:51.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename cronjob 06/27/23 18:03:51.29
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:03:51.347
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:03:51.364
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 06/27/23 18:03:51.378
    STEP: Ensuring more than one job is running at a time 06/27/23 18:03:51.397
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 06/27/23 18:05:01.415
    STEP: Removing cronjob 06/27/23 18:05:01.426
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Jun 27 18:05:01.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7591" for this suite. 06/27/23 18:05:01.468
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:05:01.495
Jun 27 18:05:01.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-runtime 06/27/23 18:05:01.497
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:01.54
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:01.555
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 06/27/23 18:05:01.58
STEP: wait for the container to reach Succeeded 06/27/23 18:05:01.693
STEP: get the container status 06/27/23 18:05:06.798
STEP: the container should be terminated 06/27/23 18:05:06.809
STEP: the termination message should be set 06/27/23 18:05:06.809
Jun 27 18:05:06.809: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 06/27/23 18:05:06.809
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Jun 27 18:05:06.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-1518" for this suite. 06/27/23 18:05:06.873
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":353,"skipped":6546,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.403 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:05:01.495
    Jun 27 18:05:01.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-runtime 06/27/23 18:05:01.497
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:01.54
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:01.555
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 06/27/23 18:05:01.58
    STEP: wait for the container to reach Succeeded 06/27/23 18:05:01.693
    STEP: get the container status 06/27/23 18:05:06.798
    STEP: the container should be terminated 06/27/23 18:05:06.809
    STEP: the termination message should be set 06/27/23 18:05:06.809
    Jun 27 18:05:06.809: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 06/27/23 18:05:06.809
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Jun 27 18:05:06.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-1518" for this suite. 06/27/23 18:05:06.873
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:05:06.904
Jun 27 18:05:06.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename configmap 06/27/23 18:05:06.906
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:06.947
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:06.966
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 06/27/23 18:05:06.981
STEP: fetching the ConfigMap 06/27/23 18:05:06.997
STEP: patching the ConfigMap 06/27/23 18:05:07.012
STEP: listing all ConfigMaps in all namespaces with a label selector 06/27/23 18:05:07.03
STEP: deleting the ConfigMap by collection with a label selector 06/27/23 18:05:07.184
STEP: listing all ConfigMaps in test namespace 06/27/23 18:05:07.218
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Jun 27 18:05:07.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-8618" for this suite. 06/27/23 18:05:07.246
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":354,"skipped":6556,"failed":0}
------------------------------
â€¢ [0.382 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:05:06.904
    Jun 27 18:05:06.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename configmap 06/27/23 18:05:06.906
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:06.947
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:06.966
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 06/27/23 18:05:06.981
    STEP: fetching the ConfigMap 06/27/23 18:05:06.997
    STEP: patching the ConfigMap 06/27/23 18:05:07.012
    STEP: listing all ConfigMaps in all namespaces with a label selector 06/27/23 18:05:07.03
    STEP: deleting the ConfigMap by collection with a label selector 06/27/23 18:05:07.184
    STEP: listing all ConfigMaps in test namespace 06/27/23 18:05:07.218
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Jun 27 18:05:07.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-8618" for this suite. 06/27/23 18:05:07.246
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:05:07.299
Jun 27 18:05:07.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-lifecycle-hook 06/27/23 18:05:07.309
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:07.344
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:07.356
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 06/27/23 18:05:07.39
Jun 27 18:05:07.436: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6395" to be "running and ready"
Jun 27 18:05:07.449: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.457948ms
Jun 27 18:05:07.449: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 27 18:05:09.463: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027200611s
Jun 27 18:05:09.463: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Jun 27 18:05:11.463: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.026466426s
Jun 27 18:05:11.463: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Jun 27 18:05:11.463: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 06/27/23 18:05:11.474
Jun 27 18:05:11.537: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6395" to be "running and ready"
Jun 27 18:05:11.549: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.845697ms
Jun 27 18:05:11.549: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Jun 27 18:05:13.565: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.027883821s
Jun 27 18:05:13.566: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Jun 27 18:05:13.566: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 06/27/23 18:05:13.581
STEP: delete the pod with lifecycle hook 06/27/23 18:05:13.655
Jun 27 18:05:13.695: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 27 18:05:13.722: INFO: Pod pod-with-poststart-http-hook still exists
Jun 27 18:05:15.722: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 27 18:05:15.734: INFO: Pod pod-with-poststart-http-hook still exists
Jun 27 18:05:17.722: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Jun 27 18:05:17.734: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Jun 27 18:05:17.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-6395" for this suite. 06/27/23 18:05:17.784
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":355,"skipped":6557,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.505 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:05:07.299
    Jun 27 18:05:07.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-lifecycle-hook 06/27/23 18:05:07.309
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:07.344
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:07.356
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 06/27/23 18:05:07.39
    Jun 27 18:05:07.436: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-6395" to be "running and ready"
    Jun 27 18:05:07.449: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 12.457948ms
    Jun 27 18:05:07.449: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 18:05:09.463: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 2.027200611s
    Jun 27 18:05:09.463: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 18:05:11.463: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 4.026466426s
    Jun 27 18:05:11.463: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Jun 27 18:05:11.463: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 06/27/23 18:05:11.474
    Jun 27 18:05:11.537: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-6395" to be "running and ready"
    Jun 27 18:05:11.549: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 11.845697ms
    Jun 27 18:05:11.549: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 18:05:13.565: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.027883821s
    Jun 27 18:05:13.566: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Jun 27 18:05:13.566: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 06/27/23 18:05:13.581
    STEP: delete the pod with lifecycle hook 06/27/23 18:05:13.655
    Jun 27 18:05:13.695: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 27 18:05:13.722: INFO: Pod pod-with-poststart-http-hook still exists
    Jun 27 18:05:15.722: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 27 18:05:15.734: INFO: Pod pod-with-poststart-http-hook still exists
    Jun 27 18:05:17.722: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Jun 27 18:05:17.734: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Jun 27 18:05:17.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-6395" for this suite. 06/27/23 18:05:17.784
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:05:17.817
Jun 27 18:05:17.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename pods 06/27/23 18:05:17.82
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:17.86
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:17.876
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 06/27/23 18:05:17.899
Jun 27 18:05:17.942: INFO: Waiting up to 5m0s for pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2" in namespace "pods-1532" to be "running and ready"
Jun 27 18:05:17.956: INFO: Pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.656791ms
Jun 27 18:05:17.956: INFO: The phase of Pod pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 18:05:19.973: INFO: Pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030344571s
Jun 27 18:05:19.973: INFO: The phase of Pod pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2 is Pending, waiting for it to be Running (with Ready = true)
Jun 27 18:05:21.968: INFO: Pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2": Phase="Running", Reason="", readiness=true. Elapsed: 4.026009616s
Jun 27 18:05:21.968: INFO: The phase of Pod pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2 is Running (Ready = true)
Jun 27 18:05:21.968: INFO: Pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2" satisfied condition "running and ready"
Jun 27 18:05:21.992: INFO: Pod pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2 has hostIP: 10.113.180.90
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Jun 27 18:05:21.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-1532" for this suite. 06/27/23 18:05:22.018
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":356,"skipped":6582,"failed":0}
------------------------------
â€¢ [4.225 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:05:17.817
    Jun 27 18:05:17.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename pods 06/27/23 18:05:17.82
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:17.86
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:17.876
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 06/27/23 18:05:17.899
    Jun 27 18:05:17.942: INFO: Waiting up to 5m0s for pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2" in namespace "pods-1532" to be "running and ready"
    Jun 27 18:05:17.956: INFO: Pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2": Phase="Pending", Reason="", readiness=false. Elapsed: 13.656791ms
    Jun 27 18:05:17.956: INFO: The phase of Pod pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 18:05:19.973: INFO: Pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030344571s
    Jun 27 18:05:19.973: INFO: The phase of Pod pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2 is Pending, waiting for it to be Running (with Ready = true)
    Jun 27 18:05:21.968: INFO: Pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2": Phase="Running", Reason="", readiness=true. Elapsed: 4.026009616s
    Jun 27 18:05:21.968: INFO: The phase of Pod pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2 is Running (Ready = true)
    Jun 27 18:05:21.968: INFO: Pod "pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2" satisfied condition "running and ready"
    Jun 27 18:05:21.992: INFO: Pod pod-hostip-583b30d1-5991-447f-af54-c8f9680ddcd2 has hostIP: 10.113.180.90
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Jun 27 18:05:21.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-1532" for this suite. 06/27/23 18:05:22.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:05:22.048
Jun 27 18:05:22.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename watch 06/27/23 18:05:22.05
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:22.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:22.108
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 06/27/23 18:05:22.126
STEP: modifying the configmap once 06/27/23 18:05:22.151
STEP: modifying the configmap a second time 06/27/23 18:05:22.182
STEP: deleting the configmap 06/27/23 18:05:22.232
STEP: creating a watch on configmaps from the resource version returned by the first update 06/27/23 18:05:22.252
STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/27/23 18:05:22.259
Jun 27 18:05:22.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-892  1d3297ce-9370-4424-b678-283ea8d96807 146377 0 2023-06-27 18:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-27 18:05:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Jun 27 18:05:22.259: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-892  1d3297ce-9370-4424-b678-283ea8d96807 146379 0 2023-06-27 18:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-27 18:05:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Jun 27 18:05:22.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-892" for this suite. 06/27/23 18:05:22.273
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":357,"skipped":6598,"failed":0}
------------------------------
â€¢ [0.268 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:05:22.048
    Jun 27 18:05:22.048: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename watch 06/27/23 18:05:22.05
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:22.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:22.108
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 06/27/23 18:05:22.126
    STEP: modifying the configmap once 06/27/23 18:05:22.151
    STEP: modifying the configmap a second time 06/27/23 18:05:22.182
    STEP: deleting the configmap 06/27/23 18:05:22.232
    STEP: creating a watch on configmaps from the resource version returned by the first update 06/27/23 18:05:22.252
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 06/27/23 18:05:22.259
    Jun 27 18:05:22.259: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-892  1d3297ce-9370-4424-b678-283ea8d96807 146377 0 2023-06-27 18:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-27 18:05:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Jun 27 18:05:22.259: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-892  1d3297ce-9370-4424-b678-283ea8d96807 146379 0 2023-06-27 18:05:22 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-06-27 18:05:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Jun 27 18:05:22.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-892" for this suite. 06/27/23 18:05:22.273
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:05:22.33
Jun 27 18:05:22.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename sysctl 06/27/23 18:05:22.332
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:22.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:22.387
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 06/27/23 18:05:22.401
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Jun 27 18:05:22.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-5294" for this suite. 06/27/23 18:05:22.449
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":358,"skipped":6617,"failed":0}
------------------------------
â€¢ [0.167 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:05:22.33
    Jun 27 18:05:22.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename sysctl 06/27/23 18:05:22.332
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:22.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:22.387
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 06/27/23 18:05:22.401
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Jun 27 18:05:22.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-5294" for this suite. 06/27/23 18:05:22.449
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:05:22.517
Jun 27 18:05:22.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename downward-api 06/27/23 18:05:22.518
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:22.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:22.573
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 06/27/23 18:05:22.589
Jun 27 18:05:22.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383" in namespace "downward-api-9208" to be "Succeeded or Failed"
Jun 27 18:05:22.650: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383": Phase="Pending", Reason="", readiness=false. Elapsed: 11.93408ms
Jun 27 18:05:24.678: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04057254s
Jun 27 18:05:26.663: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02562416s
Jun 27 18:05:28.665: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027212702s
STEP: Saw pod success 06/27/23 18:05:28.665
Jun 27 18:05:28.665: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383" satisfied condition "Succeeded or Failed"
Jun 27 18:05:28.679: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383 container client-container: <nil>
STEP: delete the pod 06/27/23 18:05:28.74
Jun 27 18:05:28.778: INFO: Waiting for pod downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383 to disappear
Jun 27 18:05:28.809: INFO: Pod downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Jun 27 18:05:28.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9208" for this suite. 06/27/23 18:05:28.83
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":359,"skipped":6659,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.334 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:05:22.517
    Jun 27 18:05:22.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename downward-api 06/27/23 18:05:22.518
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:22.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:22.573
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 06/27/23 18:05:22.589
    Jun 27 18:05:22.638: INFO: Waiting up to 5m0s for pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383" in namespace "downward-api-9208" to be "Succeeded or Failed"
    Jun 27 18:05:22.650: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383": Phase="Pending", Reason="", readiness=false. Elapsed: 11.93408ms
    Jun 27 18:05:24.678: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383": Phase="Pending", Reason="", readiness=false. Elapsed: 2.04057254s
    Jun 27 18:05:26.663: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383": Phase="Pending", Reason="", readiness=false. Elapsed: 4.02562416s
    Jun 27 18:05:28.665: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.027212702s
    STEP: Saw pod success 06/27/23 18:05:28.665
    Jun 27 18:05:28.665: INFO: Pod "downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383" satisfied condition "Succeeded or Failed"
    Jun 27 18:05:28.679: INFO: Trying to get logs from node 10.113.180.90 pod downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383 container client-container: <nil>
    STEP: delete the pod 06/27/23 18:05:28.74
    Jun 27 18:05:28.778: INFO: Waiting for pod downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383 to disappear
    Jun 27 18:05:28.809: INFO: Pod downwardapi-volume-def4babf-8e83-47b0-a201-de18821f1383 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Jun 27 18:05:28.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9208" for this suite. 06/27/23 18:05:28.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 06/27/23 18:05:28.852
Jun 27 18:05:28.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
STEP: Building a namespace api object, basename container-probe 06/27/23 18:05:28.853
STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:28.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:28.908
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-a2880167-6686-4978-9733-326ac3a8e18b in namespace container-probe-8037 06/27/23 18:05:28.922
Jun 27 18:05:28.982: INFO: Waiting up to 5m0s for pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b" in namespace "container-probe-8037" to be "not pending"
Jun 27 18:05:28.998: INFO: Pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.19792ms
Jun 27 18:05:31.013: INFO: Pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030268108s
Jun 27 18:05:33.010: INFO: Pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b": Phase="Running", Reason="", readiness=true. Elapsed: 4.027418652s
Jun 27 18:05:33.010: INFO: Pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b" satisfied condition "not pending"
Jun 27 18:05:33.010: INFO: Started pod busybox-a2880167-6686-4978-9733-326ac3a8e18b in namespace container-probe-8037
STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 18:05:33.01
Jun 27 18:05:33.036: INFO: Initial restart count of pod busybox-a2880167-6686-4978-9733-326ac3a8e18b is 0
STEP: deleting the pod 06/27/23 18:09:34.934
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Jun 27 18:09:34.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-8037" for this suite. 06/27/23 18:09:35.004
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":360,"skipped":6667,"failed":0}
------------------------------
â€¢ [SLOW TEST] [246.172 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 06/27/23 18:05:28.852
    Jun 27 18:05:28.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2210758088
    STEP: Building a namespace api object, basename container-probe 06/27/23 18:05:28.853
    STEP: Waiting for a default service account to be provisioned in namespace 06/27/23 18:05:28.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 06/27/23 18:05:28.908
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-a2880167-6686-4978-9733-326ac3a8e18b in namespace container-probe-8037 06/27/23 18:05:28.922
    Jun 27 18:05:28.982: INFO: Waiting up to 5m0s for pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b" in namespace "container-probe-8037" to be "not pending"
    Jun 27 18:05:28.998: INFO: Pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b": Phase="Pending", Reason="", readiness=false. Elapsed: 15.19792ms
    Jun 27 18:05:31.013: INFO: Pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.030268108s
    Jun 27 18:05:33.010: INFO: Pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b": Phase="Running", Reason="", readiness=true. Elapsed: 4.027418652s
    Jun 27 18:05:33.010: INFO: Pod "busybox-a2880167-6686-4978-9733-326ac3a8e18b" satisfied condition "not pending"
    Jun 27 18:05:33.010: INFO: Started pod busybox-a2880167-6686-4978-9733-326ac3a8e18b in namespace container-probe-8037
    STEP: checking the pod's current state and verifying that restartCount is present 06/27/23 18:05:33.01
    Jun 27 18:05:33.036: INFO: Initial restart count of pod busybox-a2880167-6686-4978-9733-326ac3a8e18b is 0
    STEP: deleting the pod 06/27/23 18:09:34.934
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Jun 27 18:09:34.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-8037" for this suite. 06/27/23 18:09:35.004
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":360,"skipped":6706,"failed":0}
Jun 27 18:09:35.031: INFO: Running AfterSuite actions on all nodes
Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Jun 27 18:09:35.031: INFO: Running AfterSuite actions on node 1
Jun 27 18:09:35.031: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jun 27 18:09:35.031: INFO: Running AfterSuite actions on all nodes
    Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Jun 27 18:09:35.031: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Jun 27 18:09:35.031: INFO: Running AfterSuite actions on node 1
    Jun 27 18:09:35.031: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.088 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 360 of 7066 Specs in 6449.663 seconds
SUCCESS! -- 360 Passed | 0 Failed | 0 Pending | 6706 Skipped
PASS

Ginkgo ran 1 suite in 1h47m30.152729716s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m


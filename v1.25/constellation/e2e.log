I0811 14:02:16.383238      22 e2e.go:116] Starting e2e run "d9e28574-a8c3-4403-a356-b1e4da7eef54" on Ginkgo node 1
Aug 11 14:02:16.396: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1691762536 - will randomize all specs

Will run 360 of 7066 specs
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
{"msg":"Test Suite starting","completed":0,"skipped":0,"failed":0}
Aug 11 14:02:16.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:02:16.490: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
Aug 11 14:02:16.509: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
Aug 11 14:02:16.550: INFO: 65 / 65 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
Aug 11 14:02:16.550: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
Aug 11 14:02:16.550: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
Aug 11 14:02:16.557: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'csi-gce-pd-node' (0 seconds elapsed)
Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'gcp-guest-agent' (0 seconds elapsed)
Aug 11 14:02:16.557: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'join-service' (0 seconds elapsed)
Aug 11 14:02:16.557: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'key-service' (0 seconds elapsed)
Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'verification-service' (0 seconds elapsed)
Aug 11 14:02:16.557: INFO: e2e test version: v1.25.11
Aug 11 14:02:16.558: INFO: kube-apiserver version: v1.25.11
[SynchronizedBeforeSuite] TOP-LEVEL
  test/e2e/e2e.go:76
Aug 11 14:02:16.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:02:16.563: INFO: Cluster IP family: ipv4
------------------------------
[SynchronizedBeforeSuite] PASSED [0.074 seconds]
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:76

  Begin Captured GinkgoWriter Output >>
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Aug 11 14:02:16.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:02:16.490: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
    Aug 11 14:02:16.509: INFO: Waiting up to 10m0s for all pods (need at least 0) in namespace 'kube-system' to be running and ready
    Aug 11 14:02:16.550: INFO: 65 / 65 pods in namespace 'kube-system' are running and ready (0 seconds elapsed)
    Aug 11 14:02:16.550: INFO: expected 11 pod replicas in namespace 'kube-system', 11 are Running and Ready.
    Aug 11 14:02:16.550: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
    Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
    Aug 11 14:02:16.557: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cloud-controller-manager' (0 seconds elapsed)
    Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'csi-gce-pd-node' (0 seconds elapsed)
    Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'gcp-guest-agent' (0 seconds elapsed)
    Aug 11 14:02:16.557: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'join-service' (0 seconds elapsed)
    Aug 11 14:02:16.557: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'key-service' (0 seconds elapsed)
    Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'konnectivity-agent' (0 seconds elapsed)
    Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
    Aug 11 14:02:16.557: INFO: 5 / 5 pods ready in namespace 'kube-system' in daemonset 'verification-service' (0 seconds elapsed)
    Aug 11 14:02:16.557: INFO: e2e test version: v1.25.11
    Aug 11 14:02:16.558: INFO: kube-apiserver version: v1.25.11
    [SynchronizedBeforeSuite] TOP-LEVEL
      test/e2e/e2e.go:76
    Aug 11 14:02:16.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:02:16.563: INFO: Cluster IP family: ipv4
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:02:16.586
Aug 11 14:02:16.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename containers 08/11/23 14:02:16.587
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:16.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:16.607
[It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72
STEP: Creating a pod to test override command 08/11/23 14:02:16.61
Aug 11 14:02:16.619: INFO: Waiting up to 5m0s for pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9" in namespace "containers-1433" to be "Succeeded or Failed"
Aug 11 14:02:16.623: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377476ms
Aug 11 14:02:18.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010016638s
Aug 11 14:02:20.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009935863s
Aug 11 14:02:22.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Running", Reason="", readiness=true. Elapsed: 6.010058823s
Aug 11 14:02:24.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Running", Reason="", readiness=true. Elapsed: 8.010279416s
Aug 11 14:02:26.630: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Running", Reason="", readiness=false. Elapsed: 10.01048027s
Aug 11 14:02:28.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.0102599s
STEP: Saw pod success 08/11/23 14:02:28.629
Aug 11 14:02:28.630: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9" satisfied condition "Succeeded or Failed"
Aug 11 14:02:28.633: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:02:28.657
Aug 11 14:02:28.672: INFO: Waiting for pod client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9 to disappear
Aug 11 14:02:28.676: INFO: Pod client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Aug 11 14:02:28.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-1433" for this suite. 08/11/23 14:02:28.68
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]","completed":1,"skipped":5,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.106 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:02:16.586
    Aug 11 14:02:16.586: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename containers 08/11/23 14:02:16.587
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:16.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:16.607
    [It] should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:72
    STEP: Creating a pod to test override command 08/11/23 14:02:16.61
    Aug 11 14:02:16.619: INFO: Waiting up to 5m0s for pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9" in namespace "containers-1433" to be "Succeeded or Failed"
    Aug 11 14:02:16.623: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.377476ms
    Aug 11 14:02:18.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010016638s
    Aug 11 14:02:20.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009935863s
    Aug 11 14:02:22.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Running", Reason="", readiness=true. Elapsed: 6.010058823s
    Aug 11 14:02:24.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Running", Reason="", readiness=true. Elapsed: 8.010279416s
    Aug 11 14:02:26.630: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Running", Reason="", readiness=false. Elapsed: 10.01048027s
    Aug 11 14:02:28.629: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 12.0102599s
    STEP: Saw pod success 08/11/23 14:02:28.629
    Aug 11 14:02:28.630: INFO: Pod "client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9" satisfied condition "Succeeded or Failed"
    Aug 11 14:02:28.633: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:02:28.657
    Aug 11 14:02:28.672: INFO: Waiting for pod client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9 to disappear
    Aug 11 14:02:28.676: INFO: Pod client-containers-b6f43de6-9e2c-47af-b082-6dc5792766a9 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Aug 11 14:02:28.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-1433" for this suite. 08/11/23 14:02:28.68
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:02:28.693
Aug 11 14:02:28.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:02:28.694
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:28.711
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:28.714
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:02:28.731
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:02:29.103
STEP: Deploying the webhook pod 08/11/23 14:02:29.112
STEP: Wait for the deployment to be ready 08/11/23 14:02:29.128
Aug 11 14:02:29.135: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:02:31.149
STEP: Verifying the service has paired with the endpoint 08/11/23 14:02:31.166
Aug 11 14:02:32.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238
STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/11/23 14:02:32.173
STEP: create a namespace for the webhook 08/11/23 14:02:32.202
STEP: create a configmap should be unconditionally rejected by the webhook 08/11/23 14:02:32.211
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:02:32.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7842" for this suite. 08/11/23 14:02:32.282
STEP: Destroying namespace "webhook-7842-markers" for this suite. 08/11/23 14:02:32.289
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]","completed":2,"skipped":11,"failed":0}
------------------------------
â€¢ [3.660 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should unconditionally reject operations on fail closed webhook [Conformance]
  test/e2e/apimachinery/webhook.go:238

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:02:28.693
    Aug 11 14:02:28.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:02:28.694
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:28.711
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:28.714
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:02:28.731
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:02:29.103
    STEP: Deploying the webhook pod 08/11/23 14:02:29.112
    STEP: Wait for the deployment to be ready 08/11/23 14:02:29.128
    Aug 11 14:02:29.135: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:02:31.149
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:02:31.166
    Aug 11 14:02:32.168: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should unconditionally reject operations on fail closed webhook [Conformance]
      test/e2e/apimachinery/webhook.go:238
    STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API 08/11/23 14:02:32.173
    STEP: create a namespace for the webhook 08/11/23 14:02:32.202
    STEP: create a configmap should be unconditionally rejected by the webhook 08/11/23 14:02:32.211
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:02:32.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7842" for this suite. 08/11/23 14:02:32.282
    STEP: Destroying namespace "webhook-7842-markers" for this suite. 08/11/23 14:02:32.289
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:02:32.356
Aug 11 14:02:32.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:02:32.356
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:32.374
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:32.376
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108
STEP: Creating configMap with name configmap-test-volume-map-1f2f949b-203a-4ff4-989e-d604ce401c36 08/11/23 14:02:32.379
STEP: Creating a pod to test consume configMaps 08/11/23 14:02:32.385
Aug 11 14:02:32.394: INFO: Waiting up to 5m0s for pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19" in namespace "configmap-6171" to be "Succeeded or Failed"
Aug 11 14:02:32.400: INFO: Pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162572ms
Aug 11 14:02:34.406: INFO: Pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011547046s
Aug 11 14:02:36.406: INFO: Pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01148282s
STEP: Saw pod success 08/11/23 14:02:36.406
Aug 11 14:02:36.406: INFO: Pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19" satisfied condition "Succeeded or Failed"
Aug 11 14:02:36.409: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:02:36.421
Aug 11 14:02:36.436: INFO: Waiting for pod pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19 to disappear
Aug 11 14:02:36.439: INFO: Pod pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:02:36.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6171" for this suite. 08/11/23 14:02:36.443
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":3,"skipped":26,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:02:32.356
    Aug 11 14:02:32.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:02:32.356
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:32.374
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:32.376
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:108
    STEP: Creating configMap with name configmap-test-volume-map-1f2f949b-203a-4ff4-989e-d604ce401c36 08/11/23 14:02:32.379
    STEP: Creating a pod to test consume configMaps 08/11/23 14:02:32.385
    Aug 11 14:02:32.394: INFO: Waiting up to 5m0s for pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19" in namespace "configmap-6171" to be "Succeeded or Failed"
    Aug 11 14:02:32.400: INFO: Pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19": Phase="Pending", Reason="", readiness=false. Elapsed: 6.162572ms
    Aug 11 14:02:34.406: INFO: Pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011547046s
    Aug 11 14:02:36.406: INFO: Pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01148282s
    STEP: Saw pod success 08/11/23 14:02:36.406
    Aug 11 14:02:36.406: INFO: Pod "pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19" satisfied condition "Succeeded or Failed"
    Aug 11 14:02:36.409: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:02:36.421
    Aug 11 14:02:36.436: INFO: Waiting for pod pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19 to disappear
    Aug 11 14:02:36.439: INFO: Pod pod-configmaps-5bbe78bd-f63f-4dab-be4e-279af1d1df19 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:02:36.439: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6171" for this suite. 08/11/23 14:02:36.443
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:02:36.451
Aug 11 14:02:36.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:02:36.452
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:36.469
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:36.472
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should scale a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:350
STEP: creating a replication controller 08/11/23 14:02:36.475
Aug 11 14:02:36.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 create -f -'
Aug 11 14:02:37.280: INFO: stderr: ""
Aug 11 14:02:37.280: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:02:37.28
Aug 11 14:02:37.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:02:37.346: INFO: stderr: ""
Aug 11 14:02:37.346: INFO: stdout: "update-demo-nautilus-rcsjx update-demo-nautilus-wcnvr "
Aug 11 14:02:37.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-rcsjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:02:37.400: INFO: stderr: ""
Aug 11 14:02:37.400: INFO: stdout: ""
Aug 11 14:02:37.400: INFO: update-demo-nautilus-rcsjx is created but not running
Aug 11 14:02:42.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:02:43.721: INFO: stderr: ""
Aug 11 14:02:43.721: INFO: stdout: "update-demo-nautilus-rcsjx update-demo-nautilus-wcnvr "
Aug 11 14:02:43.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-rcsjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:02:44.631: INFO: stderr: ""
Aug 11 14:02:44.631: INFO: stdout: ""
Aug 11 14:02:44.631: INFO: update-demo-nautilus-rcsjx is created but not running
Aug 11 14:02:49.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:02:49.690: INFO: stderr: ""
Aug 11 14:02:49.690: INFO: stdout: "update-demo-nautilus-rcsjx update-demo-nautilus-wcnvr "
Aug 11 14:02:49.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-rcsjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:02:49.745: INFO: stderr: ""
Aug 11 14:02:49.745: INFO: stdout: "true"
Aug 11 14:02:49.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-rcsjx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:02:49.796: INFO: stderr: ""
Aug 11 14:02:49.796: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 11 14:02:49.796: INFO: validating pod update-demo-nautilus-rcsjx
Aug 11 14:02:49.811: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:02:49.811: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:02:49.811: INFO: update-demo-nautilus-rcsjx is verified up and running
Aug 11 14:02:49.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:02:49.865: INFO: stderr: ""
Aug 11 14:02:49.865: INFO: stdout: "true"
Aug 11 14:02:49.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:02:49.920: INFO: stderr: ""
Aug 11 14:02:49.920: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 11 14:02:49.920: INFO: validating pod update-demo-nautilus-wcnvr
Aug 11 14:02:49.933: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:02:49.933: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:02:49.933: INFO: update-demo-nautilus-wcnvr is verified up and running
STEP: scaling down the replication controller 08/11/23 14:02:49.933
Aug 11 14:02:49.934: INFO: scanned /root for discovery docs: <nil>
Aug 11 14:02:49.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
Aug 11 14:02:51.008: INFO: stderr: ""
Aug 11 14:02:51.008: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:02:51.008
Aug 11 14:02:51.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:02:51.073: INFO: stderr: ""
Aug 11 14:02:51.073: INFO: stdout: "update-demo-nautilus-rcsjx update-demo-nautilus-wcnvr "
STEP: Replicas for name=update-demo: expected=1 actual=2 08/11/23 14:02:51.073
Aug 11 14:02:56.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:02:56.136: INFO: stderr: ""
Aug 11 14:02:56.136: INFO: stdout: "update-demo-nautilus-wcnvr "
Aug 11 14:02:56.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:02:56.192: INFO: stderr: ""
Aug 11 14:02:56.192: INFO: stdout: "true"
Aug 11 14:02:56.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:02:56.246: INFO: stderr: ""
Aug 11 14:02:56.246: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 11 14:02:56.246: INFO: validating pod update-demo-nautilus-wcnvr
Aug 11 14:02:56.265: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:02:56.265: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:02:56.265: INFO: update-demo-nautilus-wcnvr is verified up and running
STEP: scaling up the replication controller 08/11/23 14:02:56.265
Aug 11 14:02:56.266: INFO: scanned /root for discovery docs: <nil>
Aug 11 14:02:56.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
Aug 11 14:02:57.346: INFO: stderr: ""
Aug 11 14:02:57.346: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:02:57.346
Aug 11 14:02:57.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:02:57.407: INFO: stderr: ""
Aug 11 14:02:57.407: INFO: stdout: "update-demo-nautilus-jnpg5 update-demo-nautilus-wcnvr "
Aug 11 14:02:57.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-jnpg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:02:57.462: INFO: stderr: ""
Aug 11 14:02:57.462: INFO: stdout: ""
Aug 11 14:02:57.462: INFO: update-demo-nautilus-jnpg5 is created but not running
Aug 11 14:03:02.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:03:02.523: INFO: stderr: ""
Aug 11 14:03:02.523: INFO: stdout: "update-demo-nautilus-jnpg5 update-demo-nautilus-wcnvr "
Aug 11 14:03:02.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-jnpg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:03:02.581: INFO: stderr: ""
Aug 11 14:03:02.581: INFO: stdout: "true"
Aug 11 14:03:02.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-jnpg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:03:02.642: INFO: stderr: ""
Aug 11 14:03:02.642: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 11 14:03:02.642: INFO: validating pod update-demo-nautilus-jnpg5
Aug 11 14:03:02.656: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:03:02.656: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:03:02.656: INFO: update-demo-nautilus-jnpg5 is verified up and running
Aug 11 14:03:02.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:03:02.714: INFO: stderr: ""
Aug 11 14:03:02.714: INFO: stdout: "true"
Aug 11 14:03:02.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:03:02.773: INFO: stderr: ""
Aug 11 14:03:02.773: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 11 14:03:02.773: INFO: validating pod update-demo-nautilus-wcnvr
Aug 11 14:03:02.782: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:03:02.782: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:03:02.782: INFO: update-demo-nautilus-wcnvr is verified up and running
STEP: using delete to clean up resources 08/11/23 14:03:02.782
Aug 11 14:03:02.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 delete --grace-period=0 --force -f -'
Aug 11 14:03:02.844: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 14:03:02.844: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 11 14:03:02.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get rc,svc -l name=update-demo --no-headers'
Aug 11 14:03:02.921: INFO: stderr: "No resources found in kubectl-5660 namespace.\n"
Aug 11 14:03:02.921: INFO: stdout: ""
Aug 11 14:03:02.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 11 14:03:02.992: INFO: stderr: ""
Aug 11 14:03:02.992: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:03:02.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5660" for this suite. 08/11/23 14:03:02.998
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]","completed":4,"skipped":37,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.564 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should scale a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:350

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:02:36.451
    Aug 11 14:02:36.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:02:36.452
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:02:36.469
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:02:36.472
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should scale a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:350
    STEP: creating a replication controller 08/11/23 14:02:36.475
    Aug 11 14:02:36.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 create -f -'
    Aug 11 14:02:37.280: INFO: stderr: ""
    Aug 11 14:02:37.280: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:02:37.28
    Aug 11 14:02:37.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:02:37.346: INFO: stderr: ""
    Aug 11 14:02:37.346: INFO: stdout: "update-demo-nautilus-rcsjx update-demo-nautilus-wcnvr "
    Aug 11 14:02:37.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-rcsjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:02:37.400: INFO: stderr: ""
    Aug 11 14:02:37.400: INFO: stdout: ""
    Aug 11 14:02:37.400: INFO: update-demo-nautilus-rcsjx is created but not running
    Aug 11 14:02:42.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:02:43.721: INFO: stderr: ""
    Aug 11 14:02:43.721: INFO: stdout: "update-demo-nautilus-rcsjx update-demo-nautilus-wcnvr "
    Aug 11 14:02:43.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-rcsjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:02:44.631: INFO: stderr: ""
    Aug 11 14:02:44.631: INFO: stdout: ""
    Aug 11 14:02:44.631: INFO: update-demo-nautilus-rcsjx is created but not running
    Aug 11 14:02:49.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:02:49.690: INFO: stderr: ""
    Aug 11 14:02:49.690: INFO: stdout: "update-demo-nautilus-rcsjx update-demo-nautilus-wcnvr "
    Aug 11 14:02:49.690: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-rcsjx -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:02:49.745: INFO: stderr: ""
    Aug 11 14:02:49.745: INFO: stdout: "true"
    Aug 11 14:02:49.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-rcsjx -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:02:49.796: INFO: stderr: ""
    Aug 11 14:02:49.796: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 11 14:02:49.796: INFO: validating pod update-demo-nautilus-rcsjx
    Aug 11 14:02:49.811: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:02:49.811: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:02:49.811: INFO: update-demo-nautilus-rcsjx is verified up and running
    Aug 11 14:02:49.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:02:49.865: INFO: stderr: ""
    Aug 11 14:02:49.865: INFO: stdout: "true"
    Aug 11 14:02:49.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:02:49.920: INFO: stderr: ""
    Aug 11 14:02:49.920: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 11 14:02:49.920: INFO: validating pod update-demo-nautilus-wcnvr
    Aug 11 14:02:49.933: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:02:49.933: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:02:49.933: INFO: update-demo-nautilus-wcnvr is verified up and running
    STEP: scaling down the replication controller 08/11/23 14:02:49.933
    Aug 11 14:02:49.934: INFO: scanned /root for discovery docs: <nil>
    Aug 11 14:02:49.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
    Aug 11 14:02:51.008: INFO: stderr: ""
    Aug 11 14:02:51.008: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:02:51.008
    Aug 11 14:02:51.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:02:51.073: INFO: stderr: ""
    Aug 11 14:02:51.073: INFO: stdout: "update-demo-nautilus-rcsjx update-demo-nautilus-wcnvr "
    STEP: Replicas for name=update-demo: expected=1 actual=2 08/11/23 14:02:51.073
    Aug 11 14:02:56.077: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:02:56.136: INFO: stderr: ""
    Aug 11 14:02:56.136: INFO: stdout: "update-demo-nautilus-wcnvr "
    Aug 11 14:02:56.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:02:56.192: INFO: stderr: ""
    Aug 11 14:02:56.192: INFO: stdout: "true"
    Aug 11 14:02:56.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:02:56.246: INFO: stderr: ""
    Aug 11 14:02:56.246: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 11 14:02:56.246: INFO: validating pod update-demo-nautilus-wcnvr
    Aug 11 14:02:56.265: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:02:56.265: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:02:56.265: INFO: update-demo-nautilus-wcnvr is verified up and running
    STEP: scaling up the replication controller 08/11/23 14:02:56.265
    Aug 11 14:02:56.266: INFO: scanned /root for discovery docs: <nil>
    Aug 11 14:02:56.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
    Aug 11 14:02:57.346: INFO: stderr: ""
    Aug 11 14:02:57.346: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:02:57.346
    Aug 11 14:02:57.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:02:57.407: INFO: stderr: ""
    Aug 11 14:02:57.407: INFO: stdout: "update-demo-nautilus-jnpg5 update-demo-nautilus-wcnvr "
    Aug 11 14:02:57.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-jnpg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:02:57.462: INFO: stderr: ""
    Aug 11 14:02:57.462: INFO: stdout: ""
    Aug 11 14:02:57.462: INFO: update-demo-nautilus-jnpg5 is created but not running
    Aug 11 14:03:02.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:03:02.523: INFO: stderr: ""
    Aug 11 14:03:02.523: INFO: stdout: "update-demo-nautilus-jnpg5 update-demo-nautilus-wcnvr "
    Aug 11 14:03:02.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-jnpg5 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:03:02.581: INFO: stderr: ""
    Aug 11 14:03:02.581: INFO: stdout: "true"
    Aug 11 14:03:02.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-jnpg5 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:03:02.642: INFO: stderr: ""
    Aug 11 14:03:02.642: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 11 14:03:02.642: INFO: validating pod update-demo-nautilus-jnpg5
    Aug 11 14:03:02.656: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:03:02.656: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:03:02.656: INFO: update-demo-nautilus-jnpg5 is verified up and running
    Aug 11 14:03:02.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:03:02.714: INFO: stderr: ""
    Aug 11 14:03:02.714: INFO: stdout: "true"
    Aug 11 14:03:02.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods update-demo-nautilus-wcnvr -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:03:02.773: INFO: stderr: ""
    Aug 11 14:03:02.773: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 11 14:03:02.773: INFO: validating pod update-demo-nautilus-wcnvr
    Aug 11 14:03:02.782: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:03:02.782: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:03:02.782: INFO: update-demo-nautilus-wcnvr is verified up and running
    STEP: using delete to clean up resources 08/11/23 14:03:02.782
    Aug 11 14:03:02.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 delete --grace-period=0 --force -f -'
    Aug 11 14:03:02.844: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 14:03:02.844: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 11 14:03:02.844: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get rc,svc -l name=update-demo --no-headers'
    Aug 11 14:03:02.921: INFO: stderr: "No resources found in kubectl-5660 namespace.\n"
    Aug 11 14:03:02.921: INFO: stdout: ""
    Aug 11 14:03:02.921: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5660 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 11 14:03:02.992: INFO: stderr: ""
    Aug 11 14:03:02.992: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:03:02.992: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5660" for this suite. 08/11/23 14:03:02.998
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:03:03.016
Aug 11 14:03:03.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename gc 08/11/23 14:03:03.017
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:03.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:03.042
[It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650
STEP: create the rc 08/11/23 14:03:03.05
STEP: delete the rc 08/11/23 14:03:08.063
STEP: wait for the rc to be deleted 08/11/23 14:03:08.073
Aug 11 14:03:09.088: INFO: 85 pods remaining
Aug 11 14:03:09.088: INFO: 80 pods has nil DeletionTimestamp
Aug 11 14:03:09.088: INFO: 
Aug 11 14:03:10.089: INFO: 79 pods remaining
Aug 11 14:03:10.089: INFO: 70 pods has nil DeletionTimestamp
Aug 11 14:03:10.089: INFO: 
Aug 11 14:03:11.084: INFO: 71 pods remaining
Aug 11 14:03:11.084: INFO: 60 pods has nil DeletionTimestamp
Aug 11 14:03:11.084: INFO: 
Aug 11 14:03:12.085: INFO: 60 pods remaining
Aug 11 14:03:12.085: INFO: 40 pods has nil DeletionTimestamp
Aug 11 14:03:12.085: INFO: 
Aug 11 14:03:13.089: INFO: 53 pods remaining
Aug 11 14:03:13.089: INFO: 30 pods has nil DeletionTimestamp
Aug 11 14:03:13.089: INFO: 
Aug 11 14:03:14.083: INFO: 48 pods remaining
Aug 11 14:03:14.084: INFO: 20 pods has nil DeletionTimestamp
Aug 11 14:03:14.084: INFO: 
Aug 11 14:03:15.085: INFO: 39 pods remaining
Aug 11 14:03:15.085: INFO: 0 pods has nil DeletionTimestamp
Aug 11 14:03:15.085: INFO: 
Aug 11 14:03:16.083: INFO: 35 pods remaining
Aug 11 14:03:16.083: INFO: 0 pods has nil DeletionTimestamp
Aug 11 14:03:16.083: INFO: 
Aug 11 14:03:17.084: INFO: 24 pods remaining
Aug 11 14:03:17.084: INFO: 0 pods has nil DeletionTimestamp
Aug 11 14:03:17.084: INFO: 
Aug 11 14:03:18.082: INFO: 17 pods remaining
Aug 11 14:03:18.082: INFO: 0 pods has nil DeletionTimestamp
Aug 11 14:03:18.082: INFO: 
Aug 11 14:03:19.083: INFO: 10 pods remaining
Aug 11 14:03:19.083: INFO: 0 pods has nil DeletionTimestamp
Aug 11 14:03:19.083: INFO: 
Aug 11 14:03:20.081: INFO: 4 pods remaining
Aug 11 14:03:20.081: INFO: 0 pods has nil DeletionTimestamp
Aug 11 14:03:20.081: INFO: 
Aug 11 14:03:21.084: INFO: 0 pods remaining
Aug 11 14:03:21.084: INFO: 0 pods has nil DeletionTimestamp
Aug 11 14:03:21.084: INFO: 
STEP: Gathering metrics 08/11/23 14:03:22.08
Aug 11 14:03:22.115: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
Aug 11 14:03:22.119: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.524129ms
Aug 11 14:03:22.119: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
Aug 11 14:03:22.119: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
Aug 11 14:03:22.190: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 11 14:03:22.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7886" for this suite. 08/11/23 14:03:22.195
{"msg":"PASSED [sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]","completed":5,"skipped":50,"failed":0}
------------------------------
â€¢ [SLOW TEST] [19.186 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:03:03.016
    Aug 11 14:03:03.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename gc 08/11/23 14:03:03.017
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:03.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:03.042
    [It] should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:650
    STEP: create the rc 08/11/23 14:03:03.05
    STEP: delete the rc 08/11/23 14:03:08.063
    STEP: wait for the rc to be deleted 08/11/23 14:03:08.073
    Aug 11 14:03:09.088: INFO: 85 pods remaining
    Aug 11 14:03:09.088: INFO: 80 pods has nil DeletionTimestamp
    Aug 11 14:03:09.088: INFO: 
    Aug 11 14:03:10.089: INFO: 79 pods remaining
    Aug 11 14:03:10.089: INFO: 70 pods has nil DeletionTimestamp
    Aug 11 14:03:10.089: INFO: 
    Aug 11 14:03:11.084: INFO: 71 pods remaining
    Aug 11 14:03:11.084: INFO: 60 pods has nil DeletionTimestamp
    Aug 11 14:03:11.084: INFO: 
    Aug 11 14:03:12.085: INFO: 60 pods remaining
    Aug 11 14:03:12.085: INFO: 40 pods has nil DeletionTimestamp
    Aug 11 14:03:12.085: INFO: 
    Aug 11 14:03:13.089: INFO: 53 pods remaining
    Aug 11 14:03:13.089: INFO: 30 pods has nil DeletionTimestamp
    Aug 11 14:03:13.089: INFO: 
    Aug 11 14:03:14.083: INFO: 48 pods remaining
    Aug 11 14:03:14.084: INFO: 20 pods has nil DeletionTimestamp
    Aug 11 14:03:14.084: INFO: 
    Aug 11 14:03:15.085: INFO: 39 pods remaining
    Aug 11 14:03:15.085: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 14:03:15.085: INFO: 
    Aug 11 14:03:16.083: INFO: 35 pods remaining
    Aug 11 14:03:16.083: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 14:03:16.083: INFO: 
    Aug 11 14:03:17.084: INFO: 24 pods remaining
    Aug 11 14:03:17.084: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 14:03:17.084: INFO: 
    Aug 11 14:03:18.082: INFO: 17 pods remaining
    Aug 11 14:03:18.082: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 14:03:18.082: INFO: 
    Aug 11 14:03:19.083: INFO: 10 pods remaining
    Aug 11 14:03:19.083: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 14:03:19.083: INFO: 
    Aug 11 14:03:20.081: INFO: 4 pods remaining
    Aug 11 14:03:20.081: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 14:03:20.081: INFO: 
    Aug 11 14:03:21.084: INFO: 0 pods remaining
    Aug 11 14:03:21.084: INFO: 0 pods has nil DeletionTimestamp
    Aug 11 14:03:21.084: INFO: 
    STEP: Gathering metrics 08/11/23 14:03:22.08
    Aug 11 14:03:22.115: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
    Aug 11 14:03:22.119: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.524129ms
    Aug 11 14:03:22.119: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
    Aug 11 14:03:22.119: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
    Aug 11 14:03:22.190: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 11 14:03:22.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7886" for this suite. 08/11/23 14:03:22.195
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:03:22.204
Aug 11 14:03:22.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:03:22.205
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:22.22
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:22.223
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:03:22.24
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:03:22.489
STEP: Deploying the webhook pod 08/11/23 14:03:22.498
STEP: Wait for the deployment to be ready 08/11/23 14:03:22.512
Aug 11 14:03:22.525: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 11 14:03:24.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:03:26.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:03:28.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/11/23 14:03:30.542
STEP: Verifying the service has paired with the endpoint 08/11/23 14:03:30.561
Aug 11 14:03:31.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276
STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/11/23 14:03:31.565
STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/11/23 14:03:31.593
STEP: Creating a dummy validating-webhook-configuration object 08/11/23 14:03:31.619
STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/11/23 14:03:31.636
STEP: Creating a dummy mutating-webhook-configuration object 08/11/23 14:03:31.647
STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/11/23 14:03:31.666
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:03:31.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8747" for this suite. 08/11/23 14:03:31.693
STEP: Destroying namespace "webhook-8747-markers" for this suite. 08/11/23 14:03:31.699
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]","completed":6,"skipped":71,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.548 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
  test/e2e/apimachinery/webhook.go:276

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:03:22.204
    Aug 11 14:03:22.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:03:22.205
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:22.22
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:22.223
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:03:22.24
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:03:22.489
    STEP: Deploying the webhook pod 08/11/23 14:03:22.498
    STEP: Wait for the deployment to be ready 08/11/23 14:03:22.512
    Aug 11 14:03:22.525: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 11 14:03:24.536: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:03:26.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:03:28.541: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 3, 22, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/11/23 14:03:30.542
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:03:30.561
    Aug 11 14:03:31.561: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
      test/e2e/apimachinery/webhook.go:276
    STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/11/23 14:03:31.565
    STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API 08/11/23 14:03:31.593
    STEP: Creating a dummy validating-webhook-configuration object 08/11/23 14:03:31.619
    STEP: Deleting the validating-webhook-configuration, which should be possible to remove 08/11/23 14:03:31.636
    STEP: Creating a dummy mutating-webhook-configuration object 08/11/23 14:03:31.647
    STEP: Deleting the mutating-webhook-configuration, which should be possible to remove 08/11/23 14:03:31.666
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:03:31.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8747" for this suite. 08/11/23 14:03:31.693
    STEP: Destroying namespace "webhook-8747-markers" for this suite. 08/11/23 14:03:31.699
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:03:31.758
Aug 11 14:03:31.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 14:03:31.759
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:31.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:31.778
[It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165
STEP: Creating a pod to test downward api env vars 08/11/23 14:03:31.782
Aug 11 14:03:31.790: INFO: Waiting up to 5m0s for pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc" in namespace "downward-api-1100" to be "Succeeded or Failed"
Aug 11 14:03:31.796: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.304935ms
Aug 11 14:03:33.801: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010944776s
Aug 11 14:03:35.802: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012200002s
Aug 11 14:03:37.802: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011723825s
STEP: Saw pod success 08/11/23 14:03:37.802
Aug 11 14:03:37.802: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc" satisfied condition "Succeeded or Failed"
Aug 11 14:03:37.805: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-f1330052-3f81-4e2c-8971-f947124c69bc container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:03:37.814
Aug 11 14:03:37.832: INFO: Waiting for pod downward-api-f1330052-3f81-4e2c-8971-f947124c69bc to disappear
Aug 11 14:03:37.835: INFO: Pod downward-api-f1330052-3f81-4e2c-8971-f947124c69bc no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 11 14:03:37.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1100" for this suite. 08/11/23 14:03:37.839
{"msg":"PASSED [sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]","completed":7,"skipped":95,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.087 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:03:31.758
    Aug 11 14:03:31.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:03:31.759
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:31.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:31.778
    [It] should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:165
    STEP: Creating a pod to test downward api env vars 08/11/23 14:03:31.782
    Aug 11 14:03:31.790: INFO: Waiting up to 5m0s for pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc" in namespace "downward-api-1100" to be "Succeeded or Failed"
    Aug 11 14:03:31.796: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc": Phase="Pending", Reason="", readiness=false. Elapsed: 6.304935ms
    Aug 11 14:03:33.801: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010944776s
    Aug 11 14:03:35.802: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012200002s
    Aug 11 14:03:37.802: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.011723825s
    STEP: Saw pod success 08/11/23 14:03:37.802
    Aug 11 14:03:37.802: INFO: Pod "downward-api-f1330052-3f81-4e2c-8971-f947124c69bc" satisfied condition "Succeeded or Failed"
    Aug 11 14:03:37.805: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-f1330052-3f81-4e2c-8971-f947124c69bc container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:03:37.814
    Aug 11 14:03:37.832: INFO: Waiting for pod downward-api-f1330052-3f81-4e2c-8971-f947124c69bc to disappear
    Aug 11 14:03:37.835: INFO: Pod downward-api-f1330052-3f81-4e2c-8971-f947124c69bc no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 11 14:03:37.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1100" for this suite. 08/11/23 14:03:37.839
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:03:37.846
Aug 11 14:03:37.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:03:37.847
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:37.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:37.866
[It] should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272
STEP: Creating a pod to test service account token:  08/11/23 14:03:37.868
Aug 11 14:03:37.878: INFO: Waiting up to 5m0s for pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148" in namespace "svcaccounts-2473" to be "Succeeded or Failed"
Aug 11 14:03:37.882: INFO: Pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765757ms
Aug 11 14:03:39.887: INFO: Pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008439871s
Aug 11 14:03:41.888: INFO: Pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009279281s
STEP: Saw pod success 08/11/23 14:03:41.888
Aug 11 14:03:41.888: INFO: Pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148" satisfied condition "Succeeded or Failed"
Aug 11 14:03:41.892: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod test-pod-39e2a961-cdb7-4090-a143-2038b2121148 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:03:41.904
Aug 11 14:03:41.919: INFO: Waiting for pod test-pod-39e2a961-cdb7-4090-a143-2038b2121148 to disappear
Aug 11 14:03:41.922: INFO: Pod test-pod-39e2a961-cdb7-4090-a143-2038b2121148 no longer exists
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 11 14:03:41.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-2473" for this suite. 08/11/23 14:03:41.926
{"msg":"PASSED [sig-auth] ServiceAccounts should mount projected service account token [Conformance]","completed":8,"skipped":97,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount projected service account token [Conformance]
  test/e2e/auth/service_accounts.go:272

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:03:37.846
    Aug 11 14:03:37.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:03:37.847
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:37.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:37.866
    [It] should mount projected service account token [Conformance]
      test/e2e/auth/service_accounts.go:272
    STEP: Creating a pod to test service account token:  08/11/23 14:03:37.868
    Aug 11 14:03:37.878: INFO: Waiting up to 5m0s for pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148" in namespace "svcaccounts-2473" to be "Succeeded or Failed"
    Aug 11 14:03:37.882: INFO: Pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148": Phase="Pending", Reason="", readiness=false. Elapsed: 3.765757ms
    Aug 11 14:03:39.887: INFO: Pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008439871s
    Aug 11 14:03:41.888: INFO: Pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009279281s
    STEP: Saw pod success 08/11/23 14:03:41.888
    Aug 11 14:03:41.888: INFO: Pod "test-pod-39e2a961-cdb7-4090-a143-2038b2121148" satisfied condition "Succeeded or Failed"
    Aug 11 14:03:41.892: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod test-pod-39e2a961-cdb7-4090-a143-2038b2121148 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:03:41.904
    Aug 11 14:03:41.919: INFO: Waiting for pod test-pod-39e2a961-cdb7-4090-a143-2038b2121148 to disappear
    Aug 11 14:03:41.922: INFO: Pod test-pod-39e2a961-cdb7-4090-a143-2038b2121148 no longer exists
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 11 14:03:41.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-2473" for this suite. 08/11/23 14:03:41.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:03:41.934
Aug 11 14:03:41.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-probe 08/11/23 14:03:41.935
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:41.95
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:41.954
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 11 14:04:41.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9636" for this suite. 08/11/23 14:04:41.977
{"msg":"PASSED [sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]","completed":9,"skipped":107,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.051 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:03:41.934
    Aug 11 14:03:41.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:03:41.935
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:03:41.95
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:03:41.954
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:104
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 11 14:04:41.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9636" for this suite. 08/11/23 14:04:41.977
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:04:41.986
Aug 11 14:04:41.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename podtemplate 08/11/23 14:04:41.987
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:42.002
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:42.005
[It] should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Aug 11 14:04:42.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-7382" for this suite. 08/11/23 14:04:42.046
{"msg":"PASSED [sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]","completed":10,"skipped":116,"failed":0}
------------------------------
â€¢ [0.067 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should run the lifecycle of PodTemplates [Conformance]
  test/e2e/common/node/podtemplates.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:04:41.986
    Aug 11 14:04:41.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename podtemplate 08/11/23 14:04:41.987
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:42.002
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:42.005
    [It] should run the lifecycle of PodTemplates [Conformance]
      test/e2e/common/node/podtemplates.go:53
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Aug 11 14:04:42.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-7382" for this suite. 08/11/23 14:04:42.046
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:04:42.053
Aug 11 14:04:42.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 14:04:42.054
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:42.069
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:42.073
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225
STEP: creating the pod 08/11/23 14:04:42.076
STEP: setting up watch 08/11/23 14:04:42.076
STEP: submitting the pod to kubernetes 08/11/23 14:04:42.18
STEP: verifying the pod is in kubernetes 08/11/23 14:04:42.19
STEP: verifying pod creation was observed 08/11/23 14:04:42.195
Aug 11 14:04:42.196: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45" in namespace "pods-9638" to be "running"
Aug 11 14:04:42.200: INFO: Pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774778ms
Aug 11 14:04:44.205: INFO: Pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008999537s
Aug 11 14:04:46.205: INFO: Pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45": Phase="Running", Reason="", readiness=true. Elapsed: 4.009487139s
Aug 11 14:04:46.205: INFO: Pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45" satisfied condition "running"
STEP: deleting the pod gracefully 08/11/23 14:04:46.208
STEP: verifying pod deletion was observed 08/11/23 14:04:46.217
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 14:04:47.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9638" for this suite. 08/11/23 14:04:47.532
{"msg":"PASSED [sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]","completed":11,"skipped":119,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.486 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be submitted and removed [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:225

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:04:42.053
    Aug 11 14:04:42.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 14:04:42.054
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:42.069
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:42.073
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be submitted and removed [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:225
    STEP: creating the pod 08/11/23 14:04:42.076
    STEP: setting up watch 08/11/23 14:04:42.076
    STEP: submitting the pod to kubernetes 08/11/23 14:04:42.18
    STEP: verifying the pod is in kubernetes 08/11/23 14:04:42.19
    STEP: verifying pod creation was observed 08/11/23 14:04:42.195
    Aug 11 14:04:42.196: INFO: Waiting up to 5m0s for pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45" in namespace "pods-9638" to be "running"
    Aug 11 14:04:42.200: INFO: Pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45": Phase="Pending", Reason="", readiness=false. Elapsed: 3.774778ms
    Aug 11 14:04:44.205: INFO: Pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008999537s
    Aug 11 14:04:46.205: INFO: Pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45": Phase="Running", Reason="", readiness=true. Elapsed: 4.009487139s
    Aug 11 14:04:46.205: INFO: Pod "pod-submit-remove-e41dbff9-59b8-4c7e-8d27-ba2cfae6fa45" satisfied condition "running"
    STEP: deleting the pod gracefully 08/11/23 14:04:46.208
    STEP: verifying pod deletion was observed 08/11/23 14:04:46.217
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 14:04:47.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9638" for this suite. 08/11/23 14:04:47.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:04:47.542
Aug 11 14:04:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replication-controller 08/11/23 14:04:47.543
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:47.562
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:47.565
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66
STEP: Creating replication controller my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690 08/11/23 14:04:47.567
Aug 11 14:04:47.579: INFO: Pod name my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690: Found 0 pods out of 1
Aug 11 14:04:52.584: INFO: Pod name my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690: Found 1 pods out of 1
Aug 11 14:04:52.584: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690" are running
Aug 11 14:04:52.584: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw" in namespace "replication-controller-136" to be "running"
Aug 11 14:04:52.587: INFO: Pod "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw": Phase="Running", Reason="", readiness=true. Elapsed: 3.266252ms
Aug 11 14:04:52.587: INFO: Pod "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw" satisfied condition "running"
Aug 11 14:04:52.587: INFO: Pod "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:04:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:04:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:04:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:04:47 +0000 UTC Reason: Message:}])
Aug 11 14:04:52.587: INFO: Trying to dial the pod
Aug 11 14:04:57.607: INFO: Controller my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690: Got expected result from replica 1 [my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw]: "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 11 14:04:57.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-136" for this suite. 08/11/23 14:04:57.612
{"msg":"PASSED [sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]","completed":12,"skipped":191,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.078 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/rc.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:04:47.542
    Aug 11 14:04:47.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replication-controller 08/11/23 14:04:47.543
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:47.562
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:47.565
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/rc.go:66
    STEP: Creating replication controller my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690 08/11/23 14:04:47.567
    Aug 11 14:04:47.579: INFO: Pod name my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690: Found 0 pods out of 1
    Aug 11 14:04:52.584: INFO: Pod name my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690: Found 1 pods out of 1
    Aug 11 14:04:52.584: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690" are running
    Aug 11 14:04:52.584: INFO: Waiting up to 5m0s for pod "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw" in namespace "replication-controller-136" to be "running"
    Aug 11 14:04:52.587: INFO: Pod "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw": Phase="Running", Reason="", readiness=true. Elapsed: 3.266252ms
    Aug 11 14:04:52.587: INFO: Pod "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw" satisfied condition "running"
    Aug 11 14:04:52.587: INFO: Pod "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:04:47 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:04:51 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:04:51 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:04:47 +0000 UTC Reason: Message:}])
    Aug 11 14:04:52.587: INFO: Trying to dial the pod
    Aug 11 14:04:57.607: INFO: Controller my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690: Got expected result from replica 1 [my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw]: "my-hostname-basic-4c5ac0f6-afbb-49a9-8b66-c568453a3690-6pmnw", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 11 14:04:57.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-136" for this suite. 08/11/23 14:04:57.612
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Secrets
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:04:57.62
Aug 11 14:04:57.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:04:57.622
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:57.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:57.64
[It] should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153
STEP: creating a secret 08/11/23 14:04:57.643
STEP: listing secrets in all namespaces to ensure that there are more than zero 08/11/23 14:04:57.649
STEP: patching the secret 08/11/23 14:04:57.655
STEP: deleting the secret using a LabelSelector 08/11/23 14:04:57.7
STEP: listing secrets in all namespaces, searching for label name and value in patch 08/11/23 14:04:57.71
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:04:57.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9601" for this suite. 08/11/23 14:04:57.721
{"msg":"PASSED [sig-node] Secrets should patch a secret [Conformance]","completed":13,"skipped":193,"failed":0}
------------------------------
â€¢ [0.108 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should patch a secret [Conformance]
  test/e2e/common/node/secrets.go:153

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:04:57.62
    Aug 11 14:04:57.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:04:57.622
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:57.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:57.64
    [It] should patch a secret [Conformance]
      test/e2e/common/node/secrets.go:153
    STEP: creating a secret 08/11/23 14:04:57.643
    STEP: listing secrets in all namespaces to ensure that there are more than zero 08/11/23 14:04:57.649
    STEP: patching the secret 08/11/23 14:04:57.655
    STEP: deleting the secret using a LabelSelector 08/11/23 14:04:57.7
    STEP: listing secrets in all namespaces, searching for label name and value in patch 08/11/23 14:04:57.71
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:04:57.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9601" for this suite. 08/11/23 14:04:57.721
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:04:57.73
Aug 11 14:04:57.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-runtime 08/11/23 14:04:57.731
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:57.747
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:57.75
[It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:231
STEP: create the container 08/11/23 14:04:57.753
STEP: wait for the container to reach Succeeded 08/11/23 14:04:57.761
STEP: get the container status 08/11/23 14:05:01.785
STEP: the container should be terminated 08/11/23 14:05:01.788
STEP: the termination message should be set 08/11/23 14:05:01.788
Aug 11 14:05:01.788: INFO: Expected: &{} to match Container's Termination Message:  --
STEP: delete the container 08/11/23 14:05:01.788
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 11 14:05:01.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-7824" for this suite. 08/11/23 14:05:01.813
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":14,"skipped":201,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:04:57.73
    Aug 11 14:04:57.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-runtime 08/11/23 14:04:57.731
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:04:57.747
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:04:57.75
    [It] should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:231
    STEP: create the container 08/11/23 14:04:57.753
    STEP: wait for the container to reach Succeeded 08/11/23 14:04:57.761
    STEP: get the container status 08/11/23 14:05:01.785
    STEP: the container should be terminated 08/11/23 14:05:01.788
    STEP: the termination message should be set 08/11/23 14:05:01.788
    Aug 11 14:05:01.788: INFO: Expected: &{} to match Container's Termination Message:  --
    STEP: delete the container 08/11/23 14:05:01.788
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 11 14:05:01.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-7824" for this suite. 08/11/23 14:05:01.813
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:05:01.824
Aug 11 14:05:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:05:01.825
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:05:01.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:05:01.843
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431
Aug 11 14:05:01.868: INFO: Create a RollingUpdate DaemonSet
Aug 11 14:05:01.874: INFO: Check that daemon pods launch on every node of the cluster
Aug 11 14:05:01.880: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:01.880: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:01.880: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:01.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:05:01.887: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:05:02.892: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:02.892: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:02.892: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:02.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:05:02.896: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:05:03.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:03.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:03.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:03.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:05:03.897: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:05:04.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:04.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:04.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:04.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:05:04.897: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:05:05.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:05.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:05.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:05.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:05:05.896: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:05:06.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:06.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:06.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:06.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:05:06.896: INFO: Node constell-d93e7e1d-worker-d314547c-wzlp is running 0 daemon pod, expected 1
Aug 11 14:05:07.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:07.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:07.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:07.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:05:07.896: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
Aug 11 14:05:07.896: INFO: Update the DaemonSet to trigger a rollout
Aug 11 14:05:07.906: INFO: Updating DaemonSet daemon-set
Aug 11 14:05:11.926: INFO: Roll back the DaemonSet before rollout is complete
Aug 11 14:05:11.936: INFO: Updating DaemonSet daemon-set
Aug 11 14:05:11.936: INFO: Make sure DaemonSet rollback is complete
Aug 11 14:05:11.939: INFO: Wrong image for pod: daemon-set-pz8kh. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
Aug 11 14:05:11.939: INFO: Pod daemon-set-pz8kh is not available
Aug 11 14:05:11.945: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:11.946: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:11.946: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:12.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:12.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:12.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:13.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:13.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:13.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:14.954: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:14.954: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:14.954: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:15.951: INFO: Pod daemon-set-wf2kt is not available
Aug 11 14:05:15.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:15.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:05:15.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:05:15.961
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9568, will wait for the garbage collector to delete the pods 08/11/23 14:05:15.961
Aug 11 14:05:16.022: INFO: Deleting DaemonSet.extensions daemon-set took: 7.601156ms
Aug 11 14:05:16.122: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.803287ms
Aug 11 14:05:19.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:05:19.128: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:05:19.132: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8168"},"items":null}

Aug 11 14:05:19.135: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8168"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:05:19.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9568" for this suite. 08/11/23 14:05:19.15
{"msg":"PASSED [sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]","completed":15,"skipped":217,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.332 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should rollback without unnecessary restarts [Conformance]
  test/e2e/apps/daemon_set.go:431

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:05:01.824
    Aug 11 14:05:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:05:01.825
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:05:01.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:05:01.843
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should rollback without unnecessary restarts [Conformance]
      test/e2e/apps/daemon_set.go:431
    Aug 11 14:05:01.868: INFO: Create a RollingUpdate DaemonSet
    Aug 11 14:05:01.874: INFO: Check that daemon pods launch on every node of the cluster
    Aug 11 14:05:01.880: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:01.880: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:01.880: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:01.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:05:01.887: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:05:02.892: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:02.892: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:02.892: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:02.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:05:02.896: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:05:03.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:03.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:03.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:03.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:05:03.897: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:05:04.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:04.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:04.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:04.897: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:05:04.897: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:05:05.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:05.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:05.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:05.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:05:05.896: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:05:06.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:06.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:06.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:06.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:05:06.896: INFO: Node constell-d93e7e1d-worker-d314547c-wzlp is running 0 daemon pod, expected 1
    Aug 11 14:05:07.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:07.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:07.893: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:07.896: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:05:07.896: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    Aug 11 14:05:07.896: INFO: Update the DaemonSet to trigger a rollout
    Aug 11 14:05:07.906: INFO: Updating DaemonSet daemon-set
    Aug 11 14:05:11.926: INFO: Roll back the DaemonSet before rollout is complete
    Aug 11 14:05:11.936: INFO: Updating DaemonSet daemon-set
    Aug 11 14:05:11.936: INFO: Make sure DaemonSet rollback is complete
    Aug 11 14:05:11.939: INFO: Wrong image for pod: daemon-set-pz8kh. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-2, got: foo:non-existent.
    Aug 11 14:05:11.939: INFO: Pod daemon-set-pz8kh is not available
    Aug 11 14:05:11.945: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:11.946: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:11.946: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:12.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:12.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:12.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:13.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:13.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:13.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:14.954: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:14.954: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:14.954: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:15.951: INFO: Pod daemon-set-wf2kt is not available
    Aug 11 14:05:15.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:15.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:05:15.955: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:05:15.961
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9568, will wait for the garbage collector to delete the pods 08/11/23 14:05:15.961
    Aug 11 14:05:16.022: INFO: Deleting DaemonSet.extensions daemon-set took: 7.601156ms
    Aug 11 14:05:16.122: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.803287ms
    Aug 11 14:05:19.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:05:19.128: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:05:19.132: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8168"},"items":null}

    Aug 11 14:05:19.135: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8168"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:05:19.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9568" for this suite. 08/11/23 14:05:19.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:05:19.158
Aug 11 14:05:19.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:05:19.159
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:05:19.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:05:19.175
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204
STEP: Creating secret with name s-test-opt-del-ef8bdd66-d3f7-4964-852e-a1b0b92b9137 08/11/23 14:05:19.183
STEP: Creating secret with name s-test-opt-upd-b3f65d76-60ee-4182-b718-ab0997882ecc 08/11/23 14:05:19.188
STEP: Creating the pod 08/11/23 14:05:19.194
Aug 11 14:05:19.207: INFO: Waiting up to 5m0s for pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79" in namespace "secrets-3957" to be "running and ready"
Aug 11 14:05:19.212: INFO: Pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.992276ms
Aug 11 14:05:19.212: INFO: The phase of Pod pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:05:21.217: INFO: Pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010122353s
Aug 11 14:05:21.217: INFO: The phase of Pod pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:05:23.218: INFO: Pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79": Phase="Running", Reason="", readiness=true. Elapsed: 4.011053799s
Aug 11 14:05:23.218: INFO: The phase of Pod pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79 is Running (Ready = true)
Aug 11 14:05:23.218: INFO: Pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-ef8bdd66-d3f7-4964-852e-a1b0b92b9137 08/11/23 14:05:23.26
STEP: Updating secret s-test-opt-upd-b3f65d76-60ee-4182-b718-ab0997882ecc 08/11/23 14:05:23.268
STEP: Creating secret with name s-test-opt-create-35202f1d-9971-4645-a7b1-d0cd47f17b3b 08/11/23 14:05:23.274
STEP: waiting to observe update in volume 08/11/23 14:05:23.283
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:06:41.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-3957" for this suite. 08/11/23 14:06:41.769
{"msg":"PASSED [sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":16,"skipped":222,"failed":0}
------------------------------
â€¢ [SLOW TEST] [82.619 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:05:19.158
    Aug 11 14:05:19.158: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:05:19.159
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:05:19.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:05:19.175
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:204
    STEP: Creating secret with name s-test-opt-del-ef8bdd66-d3f7-4964-852e-a1b0b92b9137 08/11/23 14:05:19.183
    STEP: Creating secret with name s-test-opt-upd-b3f65d76-60ee-4182-b718-ab0997882ecc 08/11/23 14:05:19.188
    STEP: Creating the pod 08/11/23 14:05:19.194
    Aug 11 14:05:19.207: INFO: Waiting up to 5m0s for pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79" in namespace "secrets-3957" to be "running and ready"
    Aug 11 14:05:19.212: INFO: Pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.992276ms
    Aug 11 14:05:19.212: INFO: The phase of Pod pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:05:21.217: INFO: Pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010122353s
    Aug 11 14:05:21.217: INFO: The phase of Pod pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:05:23.218: INFO: Pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79": Phase="Running", Reason="", readiness=true. Elapsed: 4.011053799s
    Aug 11 14:05:23.218: INFO: The phase of Pod pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79 is Running (Ready = true)
    Aug 11 14:05:23.218: INFO: Pod "pod-secrets-ee6260a9-d68a-4f48-8e1d-31eba2e55b79" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-ef8bdd66-d3f7-4964-852e-a1b0b92b9137 08/11/23 14:05:23.26
    STEP: Updating secret s-test-opt-upd-b3f65d76-60ee-4182-b718-ab0997882ecc 08/11/23 14:05:23.268
    STEP: Creating secret with name s-test-opt-create-35202f1d-9971-4645-a7b1-d0cd47f17b3b 08/11/23 14:05:23.274
    STEP: waiting to observe update in volume 08/11/23 14:05:23.283
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:06:41.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-3957" for this suite. 08/11/23 14:06:41.769
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:06:41.778
Aug 11 14:06:41.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:06:41.779
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:06:41.794
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:06:41.798
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165
STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:06:41.819
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:06:41.826
Aug 11 14:06:41.831: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:41.831: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:41.831: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:41.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:06:41.837: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:06:42.843: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:42.843: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:42.843: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:42.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:06:42.847: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Stop a daemon pod, check that the daemon pod is revived. 08/11/23 14:06:42.85
Aug 11 14:06:42.867: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:42.867: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:42.867: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:42.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:06:42.870: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:06:43.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:43.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:43.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:43.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:06:43.880: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:06:44.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:44.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:44.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:44.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:06:44.881: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:06:45.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:45.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:45.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:45.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:06:45.881: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:06:46.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:46.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:46.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:06:46.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:06:46.880: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:06:46.884
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7083, will wait for the garbage collector to delete the pods 08/11/23 14:06:46.884
Aug 11 14:06:46.946: INFO: Deleting DaemonSet.extensions daemon-set took: 7.831983ms
Aug 11 14:06:47.047: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.169366ms
Aug 11 14:06:49.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:06:49.752: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:06:49.755: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8810"},"items":null}

Aug 11 14:06:49.758: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8810"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:06:49.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-7083" for this suite. 08/11/23 14:06:49.773
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]","completed":17,"skipped":235,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.003 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop simple daemon [Conformance]
  test/e2e/apps/daemon_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:06:41.778
    Aug 11 14:06:41.778: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:06:41.779
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:06:41.794
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:06:41.798
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop simple daemon [Conformance]
      test/e2e/apps/daemon_set.go:165
    STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:06:41.819
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:06:41.826
    Aug 11 14:06:41.831: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:41.831: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:41.831: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:41.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:06:41.837: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:06:42.843: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:42.843: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:42.843: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:42.846: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:06:42.847: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Stop a daemon pod, check that the daemon pod is revived. 08/11/23 14:06:42.85
    Aug 11 14:06:42.867: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:42.867: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:42.867: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:42.870: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:06:42.870: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:06:43.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:43.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:43.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:43.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:06:43.880: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:06:44.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:44.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:44.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:44.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:06:44.881: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:06:45.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:45.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:45.877: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:45.881: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:06:45.881: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:06:46.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:46.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:46.876: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:06:46.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:06:46.880: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:06:46.884
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7083, will wait for the garbage collector to delete the pods 08/11/23 14:06:46.884
    Aug 11 14:06:46.946: INFO: Deleting DaemonSet.extensions daemon-set took: 7.831983ms
    Aug 11 14:06:47.047: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.169366ms
    Aug 11 14:06:49.752: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:06:49.752: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:06:49.755: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"8810"},"items":null}

    Aug 11 14:06:49.758: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"8810"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:06:49.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-7083" for this suite. 08/11/23 14:06:49.773
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:06:49.782
Aug 11 14:06:49.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:06:49.783
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:06:49.798
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:06:49.801
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:06:49.804
Aug 11 14:06:49.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53" in namespace "projected-3039" to be "Succeeded or Failed"
Aug 11 14:06:49.823: INFO: Pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53": Phase="Pending", Reason="", readiness=false. Elapsed: 7.125572ms
Aug 11 14:06:51.828: INFO: Pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011454123s
Aug 11 14:06:53.829: INFO: Pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012498453s
STEP: Saw pod success 08/11/23 14:06:53.829
Aug 11 14:06:53.829: INFO: Pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53" satisfied condition "Succeeded or Failed"
Aug 11 14:06:53.832: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53 container client-container: <nil>
STEP: delete the pod 08/11/23 14:06:53.842
Aug 11 14:06:53.854: INFO: Waiting for pod downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53 to disappear
Aug 11 14:06:53.857: INFO: Pod downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 14:06:53.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3039" for this suite. 08/11/23 14:06:53.862
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]","completed":18,"skipped":277,"failed":0}
------------------------------
â€¢ [4.099 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:06:49.782
    Aug 11 14:06:49.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:06:49.783
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:06:49.798
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:06:49.801
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:192
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:06:49.804
    Aug 11 14:06:49.816: INFO: Waiting up to 5m0s for pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53" in namespace "projected-3039" to be "Succeeded or Failed"
    Aug 11 14:06:49.823: INFO: Pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53": Phase="Pending", Reason="", readiness=false. Elapsed: 7.125572ms
    Aug 11 14:06:51.828: INFO: Pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011454123s
    Aug 11 14:06:53.829: INFO: Pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012498453s
    STEP: Saw pod success 08/11/23 14:06:53.829
    Aug 11 14:06:53.829: INFO: Pod "downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53" satisfied condition "Succeeded or Failed"
    Aug 11 14:06:53.832: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:06:53.842
    Aug 11 14:06:53.854: INFO: Waiting for pod downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53 to disappear
    Aug 11 14:06:53.857: INFO: Pod downwardapi-volume-34d8830a-70ba-4f52-95af-efa20b77bb53 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 14:06:53.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3039" for this suite. 08/11/23 14:06:53.862
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:06:53.883
Aug 11 14:06:53.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:06:53.883
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:06:53.904
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:06:53.908
[It] should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296
STEP: creating the pod 08/11/23 14:06:53.911
STEP: waiting for pod running 08/11/23 14:06:53.922
Aug 11 14:06:53.922: INFO: Waiting up to 2m0s for pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" in namespace "var-expansion-797" to be "running"
Aug 11 14:06:53.926: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481309ms
Aug 11 14:06:55.932: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83": Phase="Running", Reason="", readiness=true. Elapsed: 2.009765129s
Aug 11 14:06:55.932: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" satisfied condition "running"
STEP: creating a file in subpath 08/11/23 14:06:55.932
Aug 11 14:06:55.935: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-797 PodName:var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:06:55.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:06:55.935: INFO: ExecWithOptions: Clientset creation
Aug 11 14:06:55.936: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-797/pods/var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: test for file in mounted path 08/11/23 14:06:56.017
Aug 11 14:06:56.021: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-797 PodName:var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:06:56.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:06:56.022: INFO: ExecWithOptions: Clientset creation
Aug 11 14:06:56.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-797/pods/var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
STEP: updating the annotation value 08/11/23 14:06:56.099
Aug 11 14:06:56.612: INFO: Successfully updated pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83"
STEP: waiting for annotated pod running 08/11/23 14:06:56.612
Aug 11 14:06:56.612: INFO: Waiting up to 2m0s for pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" in namespace "var-expansion-797" to be "running"
Aug 11 14:06:56.615: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83": Phase="Running", Reason="", readiness=true. Elapsed: 3.174219ms
Aug 11 14:06:56.615: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" satisfied condition "running"
STEP: deleting the pod gracefully 08/11/23 14:06:56.615
Aug 11 14:06:56.615: INFO: Deleting pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" in namespace "var-expansion-797"
Aug 11 14:06:56.623: INFO: Wait up to 5m0s for pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 11 14:07:30.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-797" for this suite. 08/11/23 14:07:30.637
{"msg":"PASSED [sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]","completed":19,"skipped":309,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.765 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should succeed in writing subpaths in container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:296

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:06:53.883
    Aug 11 14:06:53.883: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:06:53.883
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:06:53.904
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:06:53.908
    [It] should succeed in writing subpaths in container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:296
    STEP: creating the pod 08/11/23 14:06:53.911
    STEP: waiting for pod running 08/11/23 14:06:53.922
    Aug 11 14:06:53.922: INFO: Waiting up to 2m0s for pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" in namespace "var-expansion-797" to be "running"
    Aug 11 14:06:53.926: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83": Phase="Pending", Reason="", readiness=false. Elapsed: 4.481309ms
    Aug 11 14:06:55.932: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83": Phase="Running", Reason="", readiness=true. Elapsed: 2.009765129s
    Aug 11 14:06:55.932: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" satisfied condition "running"
    STEP: creating a file in subpath 08/11/23 14:06:55.932
    Aug 11 14:06:55.935: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-797 PodName:var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:06:55.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:06:55.935: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:06:55.936: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-797/pods/var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: test for file in mounted path 08/11/23 14:06:56.017
    Aug 11 14:06:56.021: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-797 PodName:var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:06:56.021: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:06:56.022: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:06:56.022: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-797/pods/var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
    STEP: updating the annotation value 08/11/23 14:06:56.099
    Aug 11 14:06:56.612: INFO: Successfully updated pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83"
    STEP: waiting for annotated pod running 08/11/23 14:06:56.612
    Aug 11 14:06:56.612: INFO: Waiting up to 2m0s for pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" in namespace "var-expansion-797" to be "running"
    Aug 11 14:06:56.615: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83": Phase="Running", Reason="", readiness=true. Elapsed: 3.174219ms
    Aug 11 14:06:56.615: INFO: Pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" satisfied condition "running"
    STEP: deleting the pod gracefully 08/11/23 14:06:56.615
    Aug 11 14:06:56.615: INFO: Deleting pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" in namespace "var-expansion-797"
    Aug 11 14:06:56.623: INFO: Wait up to 5m0s for pod "var-expansion-456ace9e-9c72-43f0-9b02-60afd2a27c83" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 11 14:07:30.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-797" for this suite. 08/11/23 14:07:30.637
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:07:30.65
Aug 11 14:07:30.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pod-network-test 08/11/23 14:07:30.651
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:07:30.666
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:07:30.668
[It] should function for intra-pod communication: http [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:82
STEP: Performing setup for networking test in namespace pod-network-test-3885 08/11/23 14:07:30.671
STEP: creating a selector 08/11/23 14:07:30.671
STEP: Creating the service pods in kubernetes 08/11/23 14:07:30.671
Aug 11 14:07:30.671: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 11 14:07:30.694: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3885" to be "running and ready"
Aug 11 14:07:30.699: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.057437ms
Aug 11 14:07:30.699: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:07:32.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010370019s
Aug 11 14:07:32.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:34.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010595514s
Aug 11 14:07:34.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:36.703: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008973939s
Aug 11 14:07:36.703: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:38.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010711169s
Aug 11 14:07:38.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:40.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009939683s
Aug 11 14:07:40.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:42.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01025088s
Aug 11 14:07:42.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:44.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010917737s
Aug 11 14:07:44.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:46.703: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009740907s
Aug 11 14:07:46.703: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:48.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011086457s
Aug 11 14:07:48.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:50.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011015812s
Aug 11 14:07:50.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:07:52.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010148991s
Aug 11 14:07:52.704: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 11 14:07:52.704: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 11 14:07:52.709: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3885" to be "running and ready"
Aug 11 14:07:52.712: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.014994ms
Aug 11 14:07:52.712: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 11 14:07:52.712: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/11/23 14:07:52.715
Aug 11 14:07:52.721: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3885" to be "running"
Aug 11 14:07:52.727: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154682ms
Aug 11 14:07:54.732: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011169795s
Aug 11 14:07:54.732: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 11 14:07:54.736: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 11 14:07:54.736: INFO: Breadth first check of 10.10.0.230 on host 192.168.178.2...
Aug 11 14:07:54.739: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.1:9080/dial?request=hostname&protocol=http&host=10.10.0.230&port=8083&tries=1'] Namespace:pod-network-test-3885 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:07:54.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:07:54.740: INFO: ExecWithOptions: Clientset creation
Aug 11 14:07:54.740: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3885/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.1%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.0.230%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 11 14:07:54.816: INFO: Waiting for responses: map[]
Aug 11 14:07:54.816: INFO: reached 10.10.0.230 after 0/1 tries
Aug 11 14:07:54.816: INFO: Breadth first check of 10.10.1.237 on host 192.168.178.3...
Aug 11 14:07:54.820: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.1:9080/dial?request=hostname&protocol=http&host=10.10.1.237&port=8083&tries=1'] Namespace:pod-network-test-3885 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:07:54.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:07:54.821: INFO: ExecWithOptions: Clientset creation
Aug 11 14:07:54.821: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3885/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.1%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.1.237%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 11 14:07:54.892: INFO: Waiting for responses: map[]
Aug 11 14:07:54.892: INFO: reached 10.10.1.237 after 0/1 tries
Aug 11 14:07:54.892: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Aug 11 14:07:54.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-3885" for this suite. 08/11/23 14:07:54.898
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]","completed":20,"skipped":354,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.254 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: http [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:07:30.65
    Aug 11 14:07:30.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pod-network-test 08/11/23 14:07:30.651
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:07:30.666
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:07:30.668
    [It] should function for intra-pod communication: http [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:82
    STEP: Performing setup for networking test in namespace pod-network-test-3885 08/11/23 14:07:30.671
    STEP: creating a selector 08/11/23 14:07:30.671
    STEP: Creating the service pods in kubernetes 08/11/23 14:07:30.671
    Aug 11 14:07:30.671: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 11 14:07:30.694: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-3885" to be "running and ready"
    Aug 11 14:07:30.699: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 5.057437ms
    Aug 11 14:07:30.699: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:07:32.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010370019s
    Aug 11 14:07:32.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:34.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.010595514s
    Aug 11 14:07:34.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:36.703: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.008973939s
    Aug 11 14:07:36.703: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:38.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010711169s
    Aug 11 14:07:38.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:40.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.009939683s
    Aug 11 14:07:40.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:42.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 12.01025088s
    Aug 11 14:07:42.704: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:44.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 14.010917737s
    Aug 11 14:07:44.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:46.703: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 16.009740907s
    Aug 11 14:07:46.703: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:48.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 18.011086457s
    Aug 11 14:07:48.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:50.705: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 20.011015812s
    Aug 11 14:07:50.705: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:07:52.704: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 22.010148991s
    Aug 11 14:07:52.704: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 11 14:07:52.704: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 11 14:07:52.709: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-3885" to be "running and ready"
    Aug 11 14:07:52.712: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.014994ms
    Aug 11 14:07:52.712: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 11 14:07:52.712: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/11/23 14:07:52.715
    Aug 11 14:07:52.721: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-3885" to be "running"
    Aug 11 14:07:52.727: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.154682ms
    Aug 11 14:07:54.732: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011169795s
    Aug 11 14:07:54.732: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 11 14:07:54.736: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 11 14:07:54.736: INFO: Breadth first check of 10.10.0.230 on host 192.168.178.2...
    Aug 11 14:07:54.739: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.1:9080/dial?request=hostname&protocol=http&host=10.10.0.230&port=8083&tries=1'] Namespace:pod-network-test-3885 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:07:54.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:07:54.740: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:07:54.740: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3885/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.1%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.0.230%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 11 14:07:54.816: INFO: Waiting for responses: map[]
    Aug 11 14:07:54.816: INFO: reached 10.10.0.230 after 0/1 tries
    Aug 11 14:07:54.816: INFO: Breadth first check of 10.10.1.237 on host 192.168.178.3...
    Aug 11 14:07:54.820: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.1:9080/dial?request=hostname&protocol=http&host=10.10.1.237&port=8083&tries=1'] Namespace:pod-network-test-3885 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:07:54.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:07:54.821: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:07:54.821: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3885/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.1%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.10.1.237%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 11 14:07:54.892: INFO: Waiting for responses: map[]
    Aug 11 14:07:54.892: INFO: reached 10.10.1.237 after 0/1 tries
    Aug 11 14:07:54.892: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Aug 11 14:07:54.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-3885" for this suite. 08/11/23 14:07:54.898
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:07:54.907
Aug 11 14:07:54.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir-wrapper 08/11/23 14:07:54.908
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:07:54.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:07:54.928
[It] should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189
STEP: Creating 50 configmaps 08/11/23 14:07:54.931
STEP: Creating RC which spawns configmap-volume pods 08/11/23 14:07:55.211
Aug 11 14:07:55.269: INFO: Pod name wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e: Found 3 pods out of 5
Aug 11 14:08:00.279: INFO: Pod name wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/11/23 14:08:00.279
Aug 11 14:08:00.279: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:00.283: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.821288ms
Aug 11 14:08:02.288: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008665486s
Aug 11 14:08:04.287: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008128096s
Aug 11 14:08:06.289: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009855897s
Aug 11 14:08:08.288: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008801481s
Aug 11 14:08:10.290: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Running", Reason="", readiness=true. Elapsed: 10.010435389s
Aug 11 14:08:10.290: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q" satisfied condition "running"
Aug 11 14:08:10.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-56499" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:10.294: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-56499": Phase="Running", Reason="", readiness=true. Elapsed: 4.001374ms
Aug 11 14:08:10.294: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-56499" satisfied condition "running"
Aug 11 14:08:10.294: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-d68nn" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:10.298: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-d68nn": Phase="Running", Reason="", readiness=true. Elapsed: 3.826759ms
Aug 11 14:08:10.298: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-d68nn" satisfied condition "running"
Aug 11 14:08:10.298: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-gqpbd" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:10.302: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-gqpbd": Phase="Running", Reason="", readiness=true. Elapsed: 3.85801ms
Aug 11 14:08:10.302: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-gqpbd" satisfied condition "running"
Aug 11 14:08:10.302: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-m4z8l" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:10.306: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-m4z8l": Phase="Running", Reason="", readiness=true. Elapsed: 4.044306ms
Aug 11 14:08:10.306: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-m4z8l" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e in namespace emptydir-wrapper-9197, will wait for the garbage collector to delete the pods 08/11/23 14:08:10.306
Aug 11 14:08:10.370: INFO: Deleting ReplicationController wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e took: 9.382042ms
Aug 11 14:08:10.471: INFO: Terminating ReplicationController wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e pods took: 100.371505ms
STEP: Creating RC which spawns configmap-volume pods 08/11/23 14:08:14.976
Aug 11 14:08:14.989: INFO: Pod name wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8: Found 0 pods out of 5
Aug 11 14:08:19.999: INFO: Pod name wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/11/23 14:08:19.999
Aug 11 14:08:19.999: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:20.003: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 3.455257ms
Aug 11 14:08:22.008: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008210379s
Aug 11 14:08:24.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009853228s
Aug 11 14:08:26.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009354479s
Aug 11 14:08:28.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009371397s
Aug 11 14:08:30.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Running", Reason="", readiness=true. Elapsed: 10.009680473s
Aug 11 14:08:30.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j" satisfied condition "running"
Aug 11 14:08:30.009: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gdmk8" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:30.013: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gdmk8": Phase="Running", Reason="", readiness=true. Elapsed: 4.247423ms
Aug 11 14:08:30.013: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gdmk8" satisfied condition "running"
Aug 11 14:08:30.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-ht9rb" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:30.017: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-ht9rb": Phase="Running", Reason="", readiness=true. Elapsed: 3.888681ms
Aug 11 14:08:30.017: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-ht9rb" satisfied condition "running"
Aug 11 14:08:30.017: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-jtbr7" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:30.021: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-jtbr7": Phase="Running", Reason="", readiness=true. Elapsed: 3.662524ms
Aug 11 14:08:30.021: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-jtbr7" satisfied condition "running"
Aug 11 14:08:30.021: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-tvfz8" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:30.025: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-tvfz8": Phase="Running", Reason="", readiness=true. Elapsed: 3.747046ms
Aug 11 14:08:30.025: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-tvfz8" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8 in namespace emptydir-wrapper-9197, will wait for the garbage collector to delete the pods 08/11/23 14:08:30.025
Aug 11 14:08:30.086: INFO: Deleting ReplicationController wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8 took: 7.891236ms
Aug 11 14:08:30.186: INFO: Terminating ReplicationController wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8 pods took: 100.481197ms
STEP: Creating RC which spawns configmap-volume pods 08/11/23 14:08:32.892
Aug 11 14:08:32.909: INFO: Pod name wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce: Found 0 pods out of 5
Aug 11 14:08:37.916: INFO: Pod name wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce: Found 5 pods out of 5
STEP: Ensuring each pod is running 08/11/23 14:08:37.916
Aug 11 14:08:37.917: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:37.920: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.688024ms
Aug 11 14:08:39.926: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009364917s
Aug 11 14:08:41.926: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009009674s
Aug 11 14:08:43.925: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008824464s
Aug 11 14:08:45.925: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008915795s
Aug 11 14:08:47.925: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Running", Reason="", readiness=true. Elapsed: 10.008502939s
Aug 11 14:08:47.925: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg" satisfied condition "running"
Aug 11 14:08:47.925: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-fb7pz" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:47.929: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-fb7pz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439187ms
Aug 11 14:08:49.934: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-fb7pz": Phase="Running", Reason="", readiness=true. Elapsed: 2.008984596s
Aug 11 14:08:49.934: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-fb7pz" satisfied condition "running"
Aug 11 14:08:49.934: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-s8w5l" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:49.939: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-s8w5l": Phase="Running", Reason="", readiness=true. Elapsed: 4.638714ms
Aug 11 14:08:49.939: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-s8w5l" satisfied condition "running"
Aug 11 14:08:49.939: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-skpxf" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:49.947: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-skpxf": Phase="Running", Reason="", readiness=true. Elapsed: 7.688749ms
Aug 11 14:08:49.947: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-skpxf" satisfied condition "running"
Aug 11 14:08:49.947: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-xl9jt" in namespace "emptydir-wrapper-9197" to be "running"
Aug 11 14:08:49.951: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-xl9jt": Phase="Running", Reason="", readiness=true. Elapsed: 4.029646ms
Aug 11 14:08:49.951: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-xl9jt" satisfied condition "running"
STEP: deleting ReplicationController wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce in namespace emptydir-wrapper-9197, will wait for the garbage collector to delete the pods 08/11/23 14:08:49.951
Aug 11 14:08:50.013: INFO: Deleting ReplicationController wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce took: 8.120702ms
Aug 11 14:08:50.114: INFO: Terminating ReplicationController wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce pods took: 100.731014ms
STEP: Cleaning up the configMaps 08/11/23 14:08:53.115
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Aug 11 14:08:53.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-9197" for this suite. 08/11/23 14:08:53.458
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]","completed":21,"skipped":416,"failed":0}
------------------------------
â€¢ [SLOW TEST] [58.557 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not cause race condition when used for configmaps [Serial] [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:07:54.907
    Aug 11 14:07:54.907: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir-wrapper 08/11/23 14:07:54.908
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:07:54.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:07:54.928
    [It] should not cause race condition when used for configmaps [Serial] [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:189
    STEP: Creating 50 configmaps 08/11/23 14:07:54.931
    STEP: Creating RC which spawns configmap-volume pods 08/11/23 14:07:55.211
    Aug 11 14:07:55.269: INFO: Pod name wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e: Found 3 pods out of 5
    Aug 11 14:08:00.279: INFO: Pod name wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/11/23 14:08:00.279
    Aug 11 14:08:00.279: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:00.283: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 3.821288ms
    Aug 11 14:08:02.288: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008665486s
    Aug 11 14:08:04.287: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 4.008128096s
    Aug 11 14:08:06.289: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009855897s
    Aug 11 14:08:08.288: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008801481s
    Aug 11 14:08:10.290: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q": Phase="Running", Reason="", readiness=true. Elapsed: 10.010435389s
    Aug 11 14:08:10.290: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-2jg9q" satisfied condition "running"
    Aug 11 14:08:10.290: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-56499" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:10.294: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-56499": Phase="Running", Reason="", readiness=true. Elapsed: 4.001374ms
    Aug 11 14:08:10.294: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-56499" satisfied condition "running"
    Aug 11 14:08:10.294: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-d68nn" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:10.298: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-d68nn": Phase="Running", Reason="", readiness=true. Elapsed: 3.826759ms
    Aug 11 14:08:10.298: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-d68nn" satisfied condition "running"
    Aug 11 14:08:10.298: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-gqpbd" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:10.302: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-gqpbd": Phase="Running", Reason="", readiness=true. Elapsed: 3.85801ms
    Aug 11 14:08:10.302: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-gqpbd" satisfied condition "running"
    Aug 11 14:08:10.302: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-m4z8l" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:10.306: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-m4z8l": Phase="Running", Reason="", readiness=true. Elapsed: 4.044306ms
    Aug 11 14:08:10.306: INFO: Pod "wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e-m4z8l" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e in namespace emptydir-wrapper-9197, will wait for the garbage collector to delete the pods 08/11/23 14:08:10.306
    Aug 11 14:08:10.370: INFO: Deleting ReplicationController wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e took: 9.382042ms
    Aug 11 14:08:10.471: INFO: Terminating ReplicationController wrapped-volume-race-c2ad75dd-56d3-4419-b1a4-09ea2e34850e pods took: 100.371505ms
    STEP: Creating RC which spawns configmap-volume pods 08/11/23 14:08:14.976
    Aug 11 14:08:14.989: INFO: Pod name wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8: Found 0 pods out of 5
    Aug 11 14:08:19.999: INFO: Pod name wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/11/23 14:08:19.999
    Aug 11 14:08:19.999: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:20.003: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 3.455257ms
    Aug 11 14:08:22.008: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008210379s
    Aug 11 14:08:24.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009853228s
    Aug 11 14:08:26.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009354479s
    Aug 11 14:08:28.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009371397s
    Aug 11 14:08:30.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j": Phase="Running", Reason="", readiness=true. Elapsed: 10.009680473s
    Aug 11 14:08:30.009: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gb29j" satisfied condition "running"
    Aug 11 14:08:30.009: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gdmk8" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:30.013: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gdmk8": Phase="Running", Reason="", readiness=true. Elapsed: 4.247423ms
    Aug 11 14:08:30.013: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-gdmk8" satisfied condition "running"
    Aug 11 14:08:30.013: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-ht9rb" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:30.017: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-ht9rb": Phase="Running", Reason="", readiness=true. Elapsed: 3.888681ms
    Aug 11 14:08:30.017: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-ht9rb" satisfied condition "running"
    Aug 11 14:08:30.017: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-jtbr7" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:30.021: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-jtbr7": Phase="Running", Reason="", readiness=true. Elapsed: 3.662524ms
    Aug 11 14:08:30.021: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-jtbr7" satisfied condition "running"
    Aug 11 14:08:30.021: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-tvfz8" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:30.025: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-tvfz8": Phase="Running", Reason="", readiness=true. Elapsed: 3.747046ms
    Aug 11 14:08:30.025: INFO: Pod "wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8-tvfz8" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8 in namespace emptydir-wrapper-9197, will wait for the garbage collector to delete the pods 08/11/23 14:08:30.025
    Aug 11 14:08:30.086: INFO: Deleting ReplicationController wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8 took: 7.891236ms
    Aug 11 14:08:30.186: INFO: Terminating ReplicationController wrapped-volume-race-bbed7245-5e0a-4f46-9433-654cd57186c8 pods took: 100.481197ms
    STEP: Creating RC which spawns configmap-volume pods 08/11/23 14:08:32.892
    Aug 11 14:08:32.909: INFO: Pod name wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce: Found 0 pods out of 5
    Aug 11 14:08:37.916: INFO: Pod name wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce: Found 5 pods out of 5
    STEP: Ensuring each pod is running 08/11/23 14:08:37.916
    Aug 11 14:08:37.917: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:37.920: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 3.688024ms
    Aug 11 14:08:39.926: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009364917s
    Aug 11 14:08:41.926: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009009674s
    Aug 11 14:08:43.925: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 6.008824464s
    Aug 11 14:08:45.925: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Pending", Reason="", readiness=false. Elapsed: 8.008915795s
    Aug 11 14:08:47.925: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg": Phase="Running", Reason="", readiness=true. Elapsed: 10.008502939s
    Aug 11 14:08:47.925: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-8n7dg" satisfied condition "running"
    Aug 11 14:08:47.925: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-fb7pz" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:47.929: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-fb7pz": Phase="Pending", Reason="", readiness=false. Elapsed: 3.439187ms
    Aug 11 14:08:49.934: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-fb7pz": Phase="Running", Reason="", readiness=true. Elapsed: 2.008984596s
    Aug 11 14:08:49.934: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-fb7pz" satisfied condition "running"
    Aug 11 14:08:49.934: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-s8w5l" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:49.939: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-s8w5l": Phase="Running", Reason="", readiness=true. Elapsed: 4.638714ms
    Aug 11 14:08:49.939: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-s8w5l" satisfied condition "running"
    Aug 11 14:08:49.939: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-skpxf" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:49.947: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-skpxf": Phase="Running", Reason="", readiness=true. Elapsed: 7.688749ms
    Aug 11 14:08:49.947: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-skpxf" satisfied condition "running"
    Aug 11 14:08:49.947: INFO: Waiting up to 5m0s for pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-xl9jt" in namespace "emptydir-wrapper-9197" to be "running"
    Aug 11 14:08:49.951: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-xl9jt": Phase="Running", Reason="", readiness=true. Elapsed: 4.029646ms
    Aug 11 14:08:49.951: INFO: Pod "wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce-xl9jt" satisfied condition "running"
    STEP: deleting ReplicationController wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce in namespace emptydir-wrapper-9197, will wait for the garbage collector to delete the pods 08/11/23 14:08:49.951
    Aug 11 14:08:50.013: INFO: Deleting ReplicationController wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce took: 8.120702ms
    Aug 11 14:08:50.114: INFO: Terminating ReplicationController wrapped-volume-race-c4d59381-274c-4b69-a4d6-350b4d6ab7ce pods took: 100.731014ms
    STEP: Cleaning up the configMaps 08/11/23 14:08:53.115
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:08:53.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-9197" for this suite. 08/11/23 14:08:53.458
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl patch
  should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:08:53.465
Aug 11 14:08:53.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:08:53.466
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:08:53.482
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:08:53.484
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should add annotations for pods in rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1650
STEP: creating Agnhost RC 08/11/23 14:08:53.487
Aug 11 14:08:53.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8430 create -f -'
Aug 11 14:08:54.267: INFO: stderr: ""
Aug 11 14:08:54.267: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/11/23 14:08:54.267
Aug 11 14:08:55.273: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:08:55.273: INFO: Found 0 / 1
Aug 11 14:08:56.272: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:08:56.272: INFO: Found 1 / 1
Aug 11 14:08:56.272: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
STEP: patching all pods 08/11/23 14:08:56.272
Aug 11 14:08:56.276: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:08:56.276: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 11 14:08:56.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8430 patch pod agnhost-primary-pp5jx -p {"metadata":{"annotations":{"x":"y"}}}'
Aug 11 14:08:56.338: INFO: stderr: ""
Aug 11 14:08:56.338: INFO: stdout: "pod/agnhost-primary-pp5jx patched\n"
STEP: checking annotations 08/11/23 14:08:56.338
Aug 11 14:08:56.341: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:08:56.341: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:08:56.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8430" for this suite. 08/11/23 14:08:56.345
{"msg":"PASSED [sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]","completed":22,"skipped":418,"failed":0}
------------------------------
â€¢ [2.888 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl patch
  test/e2e/kubectl/kubectl.go:1644
    should add annotations for pods in rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1650

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:08:53.465
    Aug 11 14:08:53.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:08:53.466
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:08:53.482
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:08:53.484
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should add annotations for pods in rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1650
    STEP: creating Agnhost RC 08/11/23 14:08:53.487
    Aug 11 14:08:53.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8430 create -f -'
    Aug 11 14:08:54.267: INFO: stderr: ""
    Aug 11 14:08:54.267: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/11/23 14:08:54.267
    Aug 11 14:08:55.273: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:08:55.273: INFO: Found 0 / 1
    Aug 11 14:08:56.272: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:08:56.272: INFO: Found 1 / 1
    Aug 11 14:08:56.272: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    STEP: patching all pods 08/11/23 14:08:56.272
    Aug 11 14:08:56.276: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:08:56.276: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 11 14:08:56.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8430 patch pod agnhost-primary-pp5jx -p {"metadata":{"annotations":{"x":"y"}}}'
    Aug 11 14:08:56.338: INFO: stderr: ""
    Aug 11 14:08:56.338: INFO: stdout: "pod/agnhost-primary-pp5jx patched\n"
    STEP: checking annotations 08/11/23 14:08:56.338
    Aug 11 14:08:56.341: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:08:56.341: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:08:56.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8430" for this suite. 08/11/23 14:08:56.345
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:08:56.353
Aug 11 14:08:56.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:08:56.354
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:08:56.368
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:08:56.371
[It] works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68
Aug 11 14:08:56.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/11/23 14:08:59.221
Aug 11 14:08:59.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 create -f -'
Aug 11 14:08:59.858: INFO: stderr: ""
Aug 11 14:08:59.858: INFO: stdout: "e2e-test-crd-publish-openapi-6327-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 11 14:08:59.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 delete e2e-test-crd-publish-openapi-6327-crds test-foo'
Aug 11 14:08:59.939: INFO: stderr: ""
Aug 11 14:08:59.939: INFO: stdout: "e2e-test-crd-publish-openapi-6327-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
Aug 11 14:08:59.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 apply -f -'
Aug 11 14:09:00.484: INFO: stderr: ""
Aug 11 14:09:00.484: INFO: stdout: "e2e-test-crd-publish-openapi-6327-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
Aug 11 14:09:00.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 delete e2e-test-crd-publish-openapi-6327-crds test-foo'
Aug 11 14:09:00.544: INFO: stderr: ""
Aug 11 14:09:00.544: INFO: stdout: "e2e-test-crd-publish-openapi-6327-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/11/23 14:09:00.544
Aug 11 14:09:00.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 create -f -'
Aug 11 14:09:00.748: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/11/23 14:09:00.748
Aug 11 14:09:00.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 create -f -'
Aug 11 14:09:00.952: INFO: rc: 1
Aug 11 14:09:00.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 apply -f -'
Aug 11 14:09:01.533: INFO: rc: 1
STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/11/23 14:09:01.533
Aug 11 14:09:01.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 create -f -'
Aug 11 14:09:01.741: INFO: rc: 1
Aug 11 14:09:01.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 apply -f -'
Aug 11 14:09:01.944: INFO: rc: 1
STEP: kubectl explain works to explain CR properties 08/11/23 14:09:01.944
Aug 11 14:09:01.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds'
Aug 11 14:09:02.145: INFO: stderr: ""
Aug 11 14:09:02.145: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6327-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
STEP: kubectl explain works to explain CR properties recursively 08/11/23 14:09:02.146
Aug 11 14:09:02.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds.metadata'
Aug 11 14:09:02.347: INFO: stderr: ""
Aug 11 14:09:02.347: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6327-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
Aug 11 14:09:02.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds.spec'
Aug 11 14:09:02.556: INFO: stderr: ""
Aug 11 14:09:02.556: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6327-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
Aug 11 14:09:02.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds.spec.bars'
Aug 11 14:09:02.765: INFO: stderr: ""
Aug 11 14:09:02.765: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6327-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/11/23 14:09:02.765
Aug 11 14:09:02.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds.spec.bars2'
Aug 11 14:09:02.952: INFO: rc: 1
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:09:05.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-1216" for this suite. 08/11/23 14:09:05.639
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]","completed":23,"skipped":418,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.294 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD with validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:08:56.353
    Aug 11 14:08:56.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:08:56.354
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:08:56.368
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:08:56.371
    [It] works for CRD with validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:68
    Aug 11 14:08:56.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: kubectl validation (kubectl create and apply) allows request with known and required properties 08/11/23 14:08:59.221
    Aug 11 14:08:59.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 create -f -'
    Aug 11 14:08:59.858: INFO: stderr: ""
    Aug 11 14:08:59.858: INFO: stdout: "e2e-test-crd-publish-openapi-6327-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 11 14:08:59.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 delete e2e-test-crd-publish-openapi-6327-crds test-foo'
    Aug 11 14:08:59.939: INFO: stderr: ""
    Aug 11 14:08:59.939: INFO: stdout: "e2e-test-crd-publish-openapi-6327-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    Aug 11 14:08:59.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 apply -f -'
    Aug 11 14:09:00.484: INFO: stderr: ""
    Aug 11 14:09:00.484: INFO: stdout: "e2e-test-crd-publish-openapi-6327-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
    Aug 11 14:09:00.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 delete e2e-test-crd-publish-openapi-6327-crds test-foo'
    Aug 11 14:09:00.544: INFO: stderr: ""
    Aug 11 14:09:00.544: INFO: stdout: "e2e-test-crd-publish-openapi-6327-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
    STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values 08/11/23 14:09:00.544
    Aug 11 14:09:00.544: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 create -f -'
    Aug 11 14:09:00.748: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema 08/11/23 14:09:00.748
    Aug 11 14:09:00.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 create -f -'
    Aug 11 14:09:00.952: INFO: rc: 1
    Aug 11 14:09:00.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 apply -f -'
    Aug 11 14:09:01.533: INFO: rc: 1
    STEP: kubectl validation (kubectl create and apply) rejects request without required properties 08/11/23 14:09:01.533
    Aug 11 14:09:01.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 create -f -'
    Aug 11 14:09:01.741: INFO: rc: 1
    Aug 11 14:09:01.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 --namespace=crd-publish-openapi-1216 apply -f -'
    Aug 11 14:09:01.944: INFO: rc: 1
    STEP: kubectl explain works to explain CR properties 08/11/23 14:09:01.944
    Aug 11 14:09:01.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds'
    Aug 11 14:09:02.145: INFO: stderr: ""
    Aug 11 14:09:02.145: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6327-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nDESCRIPTION:\n     Foo CRD for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<Object>\n     Specification of Foo\n\n   status\t<Object>\n     Status of Foo\n\n"
    STEP: kubectl explain works to explain CR properties recursively 08/11/23 14:09:02.146
    Aug 11 14:09:02.146: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds.metadata'
    Aug 11 14:09:02.347: INFO: stderr: ""
    Aug 11 14:09:02.347: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6327-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: metadata <Object>\n\nDESCRIPTION:\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n     ObjectMeta is metadata that all persisted resources must have, which\n     includes all objects users must create.\n\nFIELDS:\n   annotations\t<map[string]string>\n     Annotations is an unstructured key value map stored with a resource that\n     may be set by external tools to store and retrieve arbitrary metadata. They\n     are not queryable and should be preserved when modifying objects. More\n     info: http://kubernetes.io/docs/user-guide/annotations\n\n   creationTimestamp\t<string>\n     CreationTimestamp is a timestamp representing the server time when this\n     object was created. It is not guaranteed to be set in happens-before order\n     across separate operations. Clients may not set this value. It is\n     represented in RFC3339 form and is in UTC.\n\n     Populated by the system. Read-only. Null for lists. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   deletionGracePeriodSeconds\t<integer>\n     Number of seconds allowed for this object to gracefully terminate before it\n     will be removed from the system. Only set when deletionTimestamp is also\n     set. May only be shortened. Read-only.\n\n   deletionTimestamp\t<string>\n     DeletionTimestamp is RFC 3339 date and time at which this resource will be\n     deleted. This field is set by the server when a graceful deletion is\n     requested by the user, and is not directly settable by a client. The\n     resource is expected to be deleted (no longer visible from resource lists,\n     and not reachable by name) after the time in this field, once the\n     finalizers list is empty. As long as the finalizers list contains items,\n     deletion is blocked. Once the deletionTimestamp is set, this value may not\n     be unset or be set further into the future, although it may be shortened or\n     the resource may be deleted prior to this time. For example, a user may\n     request that a pod is deleted in 30 seconds. The Kubelet will react by\n     sending a graceful termination signal to the containers in the pod. After\n     that 30 seconds, the Kubelet will send a hard termination signal (SIGKILL)\n     to the container and after cleanup, remove the pod from the API. In the\n     presence of network partitions, this object may still exist after this\n     timestamp, until an administrator or automated process can determine the\n     resource is fully terminated. If not set, graceful deletion of the object\n     has not been requested.\n\n     Populated by the system when a graceful deletion is requested. Read-only.\n     More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   finalizers\t<[]string>\n     Must be empty before the object is deleted from the registry. Each entry is\n     an identifier for the responsible component that will remove the entry from\n     the list. If the deletionTimestamp of the object is non-nil, entries in\n     this list can only be removed. Finalizers may be processed and removed in\n     any order. Order is NOT enforced because it introduces significant risk of\n     stuck finalizers. finalizers is a shared field, any actor with permission\n     can reorder it. If the finalizer list is processed in order, then this can\n     lead to a situation in which the component responsible for the first\n     finalizer in the list is waiting for a signal (field value, external\n     system, or other) produced by a component responsible for a finalizer later\n     in the list, resulting in a deadlock. Without enforced ordering finalizers\n     are free to order amongst themselves and are not vulnerable to ordering\n     changes in the list.\n\n   generateName\t<string>\n     GenerateName is an optional prefix, used by the server, to generate a\n     unique name ONLY IF the Name field has not been provided. If this field is\n     used, the name returned to the client will be different than the name\n     passed. This value will also be combined with a unique suffix. The provided\n     value has the same validation rules as the Name field, and may be truncated\n     by the length of the suffix required to make the value unique on the\n     server.\n\n     If this field is specified and the generated name exists, the server will\n     return a 409.\n\n     Applied only if Name is not specified. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n   generation\t<integer>\n     A sequence number representing a specific generation of the desired state.\n     Populated by the system. Read-only.\n\n   labels\t<map[string]string>\n     Map of string keys and values that can be used to organize and categorize\n     (scope and select) objects. May match selectors of replication controllers\n     and services. More info: http://kubernetes.io/docs/user-guide/labels\n\n   managedFields\t<[]Object>\n     ManagedFields maps workflow-id and version to the set of fields that are\n     managed by that workflow. This is mostly for internal housekeeping, and\n     users typically shouldn't need to set or understand this field. A workflow\n     can be the user's name, a controller's name, or the name of a specific\n     apply path like \"ci-cd\". The set of fields is always in the version that\n     the workflow used when modifying the object.\n\n   name\t<string>\n     Name must be unique within a namespace. Is required when creating\n     resources, although some resources may allow a client to request the\n     generation of an appropriate name automatically. Name is primarily intended\n     for creation idempotence and configuration definition. Cannot be updated.\n     More info: http://kubernetes.io/docs/user-guide/identifiers#names\n\n   namespace\t<string>\n     Namespace defines the space within which each name must be unique. An empty\n     namespace is equivalent to the \"default\" namespace, but \"default\" is the\n     canonical representation. Not all objects are required to be scoped to a\n     namespace - the value of this field for those objects will be empty.\n\n     Must be a DNS_LABEL. Cannot be updated. More info:\n     http://kubernetes.io/docs/user-guide/namespaces\n\n   ownerReferences\t<[]Object>\n     List of objects depended by this object. If ALL objects in the list have\n     been deleted, this object will be garbage collected. If this object is\n     managed by a controller, then an entry in this list will point to this\n     controller, with the controller field set to true. There cannot be more\n     than one managing controller.\n\n   resourceVersion\t<string>\n     An opaque value that represents the internal version of this object that\n     can be used by clients to determine when objects have changed. May be used\n     for optimistic concurrency, change detection, and the watch operation on a\n     resource or set of resources. Clients must treat these values as opaque and\n     passed unmodified back to the server. They may only be valid for a\n     particular resource or set of resources.\n\n     Populated by the system. Read-only. Value must be treated as opaque by\n     clients and . More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n   selfLink\t<string>\n     Deprecated: selfLink is a legacy read-only field that is no longer\n     populated by the system.\n\n   uid\t<string>\n     UID is the unique in time and space value for this object. It is typically\n     generated by the server on successful creation of a resource and is not\n     allowed to change on PUT operations.\n\n     Populated by the system. Read-only. More info:\n     http://kubernetes.io/docs/user-guide/identifiers#uids\n\n"
    Aug 11 14:09:02.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds.spec'
    Aug 11 14:09:02.556: INFO: stderr: ""
    Aug 11 14:09:02.556: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6327-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: spec <Object>\n\nDESCRIPTION:\n     Specification of Foo\n\nFIELDS:\n   bars\t<[]Object>\n     List of Bars and their specs.\n\n"
    Aug 11 14:09:02.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds.spec.bars'
    Aug 11 14:09:02.765: INFO: stderr: ""
    Aug 11 14:09:02.765: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-6327-crd\nVERSION:  crd-publish-openapi-test-foo.example.com/v1\n\nRESOURCE: bars <[]Object>\n\nDESCRIPTION:\n     List of Bars and their specs.\n\nFIELDS:\n   age\t<string>\n     Age of Bar.\n\n   bazs\t<[]string>\n     List of Bazs.\n\n   feeling\t<string>\n     Whether Bar is feeling great.\n\n   name\t<string> -required-\n     Name of Bar.\n\n"
    STEP: kubectl explain works to return error when explain is called on property that doesn't exist 08/11/23 14:09:02.765
    Aug 11 14:09:02.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-1216 explain e2e-test-crd-publish-openapi-6327-crds.spec.bars2'
    Aug 11 14:09:02.952: INFO: rc: 1
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:09:05.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-1216" for this suite. 08/11/23 14:09:05.639
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:09:05.647
Aug 11 14:09:05.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename subpath 08/11/23 14:09:05.648
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:05.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:05.669
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 14:09:05.672
[It] should support subpaths with secret pod [Conformance]
  test/e2e/storage/subpath.go:60
STEP: Creating pod pod-subpath-test-secret-bzbb 08/11/23 14:09:05.683
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:09:05.684
Aug 11 14:09:05.693: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bzbb" in namespace "subpath-3909" to be "Succeeded or Failed"
Aug 11 14:09:05.699: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033287ms
Aug 11 14:09:07.703: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010474843s
Aug 11 14:09:09.705: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 4.011699508s
Aug 11 14:09:11.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 6.011091475s
Aug 11 14:09:13.705: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 8.011783783s
Aug 11 14:09:15.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 10.011575143s
Aug 11 14:09:17.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 12.011299432s
Aug 11 14:09:19.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 14.011253196s
Aug 11 14:09:21.703: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 16.010269513s
Aug 11 14:09:23.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 18.011428396s
Aug 11 14:09:25.705: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 20.011732203s
Aug 11 14:09:27.705: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=false. Elapsed: 22.011911296s
Aug 11 14:09:29.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011118756s
STEP: Saw pod success 08/11/23 14:09:29.704
Aug 11 14:09:29.704: INFO: Pod "pod-subpath-test-secret-bzbb" satisfied condition "Succeeded or Failed"
Aug 11 14:09:29.708: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-secret-bzbb container test-container-subpath-secret-bzbb: <nil>
STEP: delete the pod 08/11/23 14:09:29.731
Aug 11 14:09:29.747: INFO: Waiting for pod pod-subpath-test-secret-bzbb to disappear
Aug 11 14:09:29.750: INFO: Pod pod-subpath-test-secret-bzbb no longer exists
STEP: Deleting pod pod-subpath-test-secret-bzbb 08/11/23 14:09:29.75
Aug 11 14:09:29.750: INFO: Deleting pod "pod-subpath-test-secret-bzbb" in namespace "subpath-3909"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 11 14:09:29.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-3909" for this suite. 08/11/23 14:09:29.758
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]","completed":24,"skipped":441,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.119 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with secret pod [Conformance]
    test/e2e/storage/subpath.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:09:05.647
    Aug 11 14:09:05.647: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename subpath 08/11/23 14:09:05.648
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:05.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:05.669
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 14:09:05.672
    [It] should support subpaths with secret pod [Conformance]
      test/e2e/storage/subpath.go:60
    STEP: Creating pod pod-subpath-test-secret-bzbb 08/11/23 14:09:05.683
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:09:05.684
    Aug 11 14:09:05.693: INFO: Waiting up to 5m0s for pod "pod-subpath-test-secret-bzbb" in namespace "subpath-3909" to be "Succeeded or Failed"
    Aug 11 14:09:05.699: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.033287ms
    Aug 11 14:09:07.703: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010474843s
    Aug 11 14:09:09.705: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 4.011699508s
    Aug 11 14:09:11.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 6.011091475s
    Aug 11 14:09:13.705: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 8.011783783s
    Aug 11 14:09:15.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 10.011575143s
    Aug 11 14:09:17.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 12.011299432s
    Aug 11 14:09:19.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 14.011253196s
    Aug 11 14:09:21.703: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 16.010269513s
    Aug 11 14:09:23.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 18.011428396s
    Aug 11 14:09:25.705: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=true. Elapsed: 20.011732203s
    Aug 11 14:09:27.705: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Running", Reason="", readiness=false. Elapsed: 22.011911296s
    Aug 11 14:09:29.704: INFO: Pod "pod-subpath-test-secret-bzbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011118756s
    STEP: Saw pod success 08/11/23 14:09:29.704
    Aug 11 14:09:29.704: INFO: Pod "pod-subpath-test-secret-bzbb" satisfied condition "Succeeded or Failed"
    Aug 11 14:09:29.708: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-secret-bzbb container test-container-subpath-secret-bzbb: <nil>
    STEP: delete the pod 08/11/23 14:09:29.731
    Aug 11 14:09:29.747: INFO: Waiting for pod pod-subpath-test-secret-bzbb to disappear
    Aug 11 14:09:29.750: INFO: Pod pod-subpath-test-secret-bzbb no longer exists
    STEP: Deleting pod pod-subpath-test-secret-bzbb 08/11/23 14:09:29.75
    Aug 11 14:09:29.750: INFO: Deleting pod "pod-subpath-test-secret-bzbb" in namespace "subpath-3909"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 11 14:09:29.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-3909" for this suite. 08/11/23 14:09:29.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:09:29.767
Aug 11 14:09:29.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename security-context 08/11/23 14:09:29.768
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:29.784
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:29.787
[It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/11/23 14:09:29.789
Aug 11 14:09:29.799: INFO: Waiting up to 5m0s for pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab" in namespace "security-context-9728" to be "Succeeded or Failed"
Aug 11 14:09:29.803: INFO: Pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099287ms
Aug 11 14:09:31.808: INFO: Pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009181632s
Aug 11 14:09:33.809: INFO: Pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010393736s
STEP: Saw pod success 08/11/23 14:09:33.809
Aug 11 14:09:33.809: INFO: Pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab" satisfied condition "Succeeded or Failed"
Aug 11 14:09:33.813: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab container test-container: <nil>
STEP: delete the pod 08/11/23 14:09:33.824
Aug 11 14:09:33.840: INFO: Waiting for pod security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab to disappear
Aug 11 14:09:33.843: INFO: Pod security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 11 14:09:33.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-9728" for this suite. 08/11/23 14:09:33.847
{"msg":"PASSED [sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":25,"skipped":448,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:09:29.767
    Aug 11 14:09:29.767: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename security-context 08/11/23 14:09:29.768
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:29.784
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:29.787
    [It] should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:97
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/11/23 14:09:29.789
    Aug 11 14:09:29.799: INFO: Waiting up to 5m0s for pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab" in namespace "security-context-9728" to be "Succeeded or Failed"
    Aug 11 14:09:29.803: INFO: Pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab": Phase="Pending", Reason="", readiness=false. Elapsed: 4.099287ms
    Aug 11 14:09:31.808: INFO: Pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009181632s
    Aug 11 14:09:33.809: INFO: Pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010393736s
    STEP: Saw pod success 08/11/23 14:09:33.809
    Aug 11 14:09:33.809: INFO: Pod "security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab" satisfied condition "Succeeded or Failed"
    Aug 11 14:09:33.813: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab container test-container: <nil>
    STEP: delete the pod 08/11/23 14:09:33.824
    Aug 11 14:09:33.840: INFO: Waiting for pod security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab to disappear
    Aug 11 14:09:33.843: INFO: Pod security-context-add10aba-4471-4bd6-8dec-237e3e57a0ab no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 11 14:09:33.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-9728" for this suite. 08/11/23 14:09:33.847
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:09:33.854
Aug 11 14:09:33.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename statefulset 08/11/23 14:09:33.855
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:33.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:33.875
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1145 08/11/23 14:09:33.878
[It] should have a working scale subresource [Conformance]
  test/e2e/apps/statefulset.go:846
STEP: Creating statefulset ss in namespace statefulset-1145 08/11/23 14:09:33.884
Aug 11 14:09:33.896: INFO: Found 0 stateful pods, waiting for 1
Aug 11 14:09:43.901: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: getting scale subresource 08/11/23 14:09:43.908
STEP: updating a scale subresource 08/11/23 14:09:43.911
STEP: verifying the statefulset Spec.Replicas was modified 08/11/23 14:09:43.917
STEP: Patch a scale subresource 08/11/23 14:09:43.923
STEP: verifying the statefulset Spec.Replicas was modified 08/11/23 14:09:43.938
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 11 14:09:43.943: INFO: Deleting all statefulset in ns statefulset-1145
Aug 11 14:09:43.946: INFO: Scaling statefulset ss to 0
Aug 11 14:09:53.969: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:09:53.971: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 11 14:09:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1145" for this suite. 08/11/23 14:09:53.993
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]","completed":26,"skipped":450,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.145 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should have a working scale subresource [Conformance]
    test/e2e/apps/statefulset.go:846

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:09:33.854
    Aug 11 14:09:33.854: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename statefulset 08/11/23 14:09:33.855
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:33.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:33.875
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1145 08/11/23 14:09:33.878
    [It] should have a working scale subresource [Conformance]
      test/e2e/apps/statefulset.go:846
    STEP: Creating statefulset ss in namespace statefulset-1145 08/11/23 14:09:33.884
    Aug 11 14:09:33.896: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 14:09:43.901: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: getting scale subresource 08/11/23 14:09:43.908
    STEP: updating a scale subresource 08/11/23 14:09:43.911
    STEP: verifying the statefulset Spec.Replicas was modified 08/11/23 14:09:43.917
    STEP: Patch a scale subresource 08/11/23 14:09:43.923
    STEP: verifying the statefulset Spec.Replicas was modified 08/11/23 14:09:43.938
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 11 14:09:43.943: INFO: Deleting all statefulset in ns statefulset-1145
    Aug 11 14:09:43.946: INFO: Scaling statefulset ss to 0
    Aug 11 14:09:53.969: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:09:53.971: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 11 14:09:53.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1145" for this suite. 08/11/23 14:09:53.993
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:09:54
Aug 11 14:09:54.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:09:54.001
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:54.015
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:54.018
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861
STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:09:54.044
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:09:54.049
Aug 11 14:09:54.054: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:09:54.055: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:09:54.055: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:09:54.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:09:54.057: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:09:55.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:09:55.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:09:55.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:09:55.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:09:55.066: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:09:56.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:09:56.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:09:56.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:09:56.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:09:56.066: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Getting /status 08/11/23 14:09:56.069
Aug 11 14:09:56.073: INFO: Daemon Set daemon-set has Conditions: []
STEP: updating the DaemonSet Status 08/11/23 14:09:56.073
Aug 11 14:09:56.083: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the daemon set status to be updated 08/11/23 14:09:56.083
Aug 11 14:09:56.085: INFO: Observed &DaemonSet event: ADDED
Aug 11 14:09:56.085: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:09:56.086: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:09:56.086: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:09:56.086: INFO: Found daemon set daemon-set in namespace daemonsets-9719 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 11 14:09:56.086: INFO: Daemon set daemon-set has an updated status
STEP: patching the DaemonSet Status 08/11/23 14:09:56.086
STEP: watching for the daemon set status to be patched 08/11/23 14:09:56.093
Aug 11 14:09:56.095: INFO: Observed &DaemonSet event: ADDED
Aug 11 14:09:56.095: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:09:56.095: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:09:56.095: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:09:56.095: INFO: Observed daemon set daemon-set in namespace daemonsets-9719 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 11 14:09:56.096: INFO: Observed &DaemonSet event: MODIFIED
Aug 11 14:09:56.096: INFO: Found daemon set daemon-set in namespace daemonsets-9719 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
Aug 11 14:09:56.096: INFO: Daemon set daemon-set has a patched status
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:09:56.099
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9719, will wait for the garbage collector to delete the pods 08/11/23 14:09:56.099
Aug 11 14:09:56.162: INFO: Deleting DaemonSet.extensions daemon-set took: 9.838236ms
Aug 11 14:09:56.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.553849ms
Aug 11 14:09:58.468: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:09:58.468: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:09:58.472: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11097"},"items":null}

Aug 11 14:09:58.475: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11097"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:09:58.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9719" for this suite. 08/11/23 14:09:58.49
{"msg":"PASSED [sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]","completed":27,"skipped":457,"failed":0}
------------------------------
â€¢ [4.498 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should verify changes to a daemon set status [Conformance]
  test/e2e/apps/daemon_set.go:861

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:09:54
    Aug 11 14:09:54.000: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:09:54.001
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:54.015
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:54.018
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should verify changes to a daemon set status [Conformance]
      test/e2e/apps/daemon_set.go:861
    STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:09:54.044
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:09:54.049
    Aug 11 14:09:54.054: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:09:54.055: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:09:54.055: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:09:54.057: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:09:54.057: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:09:55.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:09:55.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:09:55.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:09:55.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:09:55.066: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:09:56.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:09:56.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:09:56.062: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:09:56.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:09:56.066: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Getting /status 08/11/23 14:09:56.069
    Aug 11 14:09:56.073: INFO: Daemon Set daemon-set has Conditions: []
    STEP: updating the DaemonSet Status 08/11/23 14:09:56.073
    Aug 11 14:09:56.083: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the daemon set status to be updated 08/11/23 14:09:56.083
    Aug 11 14:09:56.085: INFO: Observed &DaemonSet event: ADDED
    Aug 11 14:09:56.085: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:09:56.086: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:09:56.086: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:09:56.086: INFO: Found daemon set daemon-set in namespace daemonsets-9719 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 11 14:09:56.086: INFO: Daemon set daemon-set has an updated status
    STEP: patching the DaemonSet Status 08/11/23 14:09:56.086
    STEP: watching for the daemon set status to be patched 08/11/23 14:09:56.093
    Aug 11 14:09:56.095: INFO: Observed &DaemonSet event: ADDED
    Aug 11 14:09:56.095: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:09:56.095: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:09:56.095: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:09:56.095: INFO: Observed daemon set daemon-set in namespace daemonsets-9719 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 11 14:09:56.096: INFO: Observed &DaemonSet event: MODIFIED
    Aug 11 14:09:56.096: INFO: Found daemon set daemon-set in namespace daemonsets-9719 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
    Aug 11 14:09:56.096: INFO: Daemon set daemon-set has a patched status
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:09:56.099
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9719, will wait for the garbage collector to delete the pods 08/11/23 14:09:56.099
    Aug 11 14:09:56.162: INFO: Deleting DaemonSet.extensions daemon-set took: 9.838236ms
    Aug 11 14:09:56.263: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.553849ms
    Aug 11 14:09:58.468: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:09:58.468: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:09:58.472: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11097"},"items":null}

    Aug 11 14:09:58.475: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11097"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:09:58.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9719" for this suite. 08/11/23 14:09:58.49
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:09:58.499
Aug 11 14:09:58.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename disruption 08/11/23 14:09:58.5
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:58.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:58.521
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140
STEP: Waiting for the pdb to be processed 08/11/23 14:09:58.53
STEP: Waiting for all pods to be running 08/11/23 14:09:58.562
Aug 11 14:09:58.571: INFO: running pods: 0 < 3
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 11 14:10:00.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-6312" for this suite. 08/11/23 14:10:00.582
{"msg":"PASSED [sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]","completed":28,"skipped":486,"failed":0}
------------------------------
â€¢ [2.089 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should observe PodDisruptionBudget status updated [Conformance]
  test/e2e/apps/disruption.go:140

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:09:58.499
    Aug 11 14:09:58.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename disruption 08/11/23 14:09:58.5
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:09:58.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:09:58.521
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should observe PodDisruptionBudget status updated [Conformance]
      test/e2e/apps/disruption.go:140
    STEP: Waiting for the pdb to be processed 08/11/23 14:09:58.53
    STEP: Waiting for all pods to be running 08/11/23 14:09:58.562
    Aug 11 14:09:58.571: INFO: running pods: 0 < 3
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 11 14:10:00.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-6312" for this suite. 08/11/23 14:10:00.582
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:00.588
Aug 11 14:10:00.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:10:00.589
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:00.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:00.607
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73
STEP: Creating configMap with name configmap-test-volume-05a88ce8-616e-42b3-afbb-0b5663ab878f 08/11/23 14:10:00.61
STEP: Creating a pod to test consume configMaps 08/11/23 14:10:00.615
Aug 11 14:10:00.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e" in namespace "configmap-921" to be "Succeeded or Failed"
Aug 11 14:10:00.629: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.380228ms
Aug 11 14:10:02.634: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010711801s
Aug 11 14:10:04.635: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e": Phase="Running", Reason="", readiness=false. Elapsed: 4.010851891s
Aug 11 14:10:06.634: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010523237s
STEP: Saw pod success 08/11/23 14:10:06.634
Aug 11 14:10:06.634: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e" satisfied condition "Succeeded or Failed"
Aug 11 14:10:06.638: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:10:06.664
Aug 11 14:10:06.678: INFO: Waiting for pod pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e to disappear
Aug 11 14:10:06.682: INFO: Pod pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:10:06.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-921" for this suite. 08/11/23 14:10:06.686
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":29,"skipped":487,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.106 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:00.588
    Aug 11 14:10:00.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:10:00.589
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:00.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:00.607
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:73
    STEP: Creating configMap with name configmap-test-volume-05a88ce8-616e-42b3-afbb-0b5663ab878f 08/11/23 14:10:00.61
    STEP: Creating a pod to test consume configMaps 08/11/23 14:10:00.615
    Aug 11 14:10:00.624: INFO: Waiting up to 5m0s for pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e" in namespace "configmap-921" to be "Succeeded or Failed"
    Aug 11 14:10:00.629: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.380228ms
    Aug 11 14:10:02.634: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010711801s
    Aug 11 14:10:04.635: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e": Phase="Running", Reason="", readiness=false. Elapsed: 4.010851891s
    Aug 11 14:10:06.634: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.010523237s
    STEP: Saw pod success 08/11/23 14:10:06.634
    Aug 11 14:10:06.634: INFO: Pod "pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:06.638: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:10:06.664
    Aug 11 14:10:06.678: INFO: Waiting for pod pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e to disappear
    Aug 11 14:10:06.682: INFO: Pod pod-configmaps-e862636a-4a5e-4524-a3b0-8663ef8efc7e no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:10:06.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-921" for this suite. 08/11/23 14:10:06.686
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:06.695
Aug 11 14:10:06.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:10:06.696
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:06.71
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:06.713
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:10:06.729
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:10:07.236
STEP: Deploying the webhook pod 08/11/23 14:10:07.246
STEP: Wait for the deployment to be ready 08/11/23 14:10:07.258
Aug 11 14:10:07.276: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:10:09.288
STEP: Verifying the service has paired with the endpoint 08/11/23 14:10:09.305
Aug 11 14:10:10.305: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263
STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/11/23 14:10:10.309
STEP: create a pod that should be updated by the webhook 08/11/23 14:10:10.33
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:10:10.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4460" for this suite. 08/11/23 14:10:10.375
STEP: Destroying namespace "webhook-4460-markers" for this suite. 08/11/23 14:10:10.385
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]","completed":30,"skipped":488,"failed":0}
------------------------------
â€¢ [3.748 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate pod and apply defaults after mutation [Conformance]
  test/e2e/apimachinery/webhook.go:263

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:06.695
    Aug 11 14:10:06.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:10:06.696
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:06.71
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:06.713
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:10:06.729
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:10:07.236
    STEP: Deploying the webhook pod 08/11/23 14:10:07.246
    STEP: Wait for the deployment to be ready 08/11/23 14:10:07.258
    Aug 11 14:10:07.276: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:10:09.288
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:10:09.305
    Aug 11 14:10:10.305: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate pod and apply defaults after mutation [Conformance]
      test/e2e/apimachinery/webhook.go:263
    STEP: Registering the mutating pod webhook via the AdmissionRegistration API 08/11/23 14:10:10.309
    STEP: create a pod that should be updated by the webhook 08/11/23 14:10:10.33
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:10:10.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4460" for this suite. 08/11/23 14:10:10.375
    STEP: Destroying namespace "webhook-4460-markers" for this suite. 08/11/23 14:10:10.385
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:10.445
Aug 11 14:10:10.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 14:10:10.446
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:10.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:10.469
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194
STEP: creating service in namespace services-2378 08/11/23 14:10:10.471
STEP: creating service affinity-nodeport in namespace services-2378 08/11/23 14:10:10.472
STEP: creating replication controller affinity-nodeport in namespace services-2378 08/11/23 14:10:10.496
I0811 14:10:10.503363      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2378, replica count: 3
I0811 14:10:13.555418      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:10:13.566: INFO: Creating new exec pod
Aug 11 14:10:13.575: INFO: Waiting up to 5m0s for pod "execpod-affinityhm5pr" in namespace "services-2378" to be "running"
Aug 11 14:10:13.577: INFO: Pod "execpod-affinityhm5pr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.912071ms
Aug 11 14:10:15.582: INFO: Pod "execpod-affinityhm5pr": Phase="Running", Reason="", readiness=true. Elapsed: 2.00717519s
Aug 11 14:10:15.582: INFO: Pod "execpod-affinityhm5pr" satisfied condition "running"
Aug 11 14:10:16.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
Aug 11 14:10:16.723: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
Aug 11 14:10:16.723: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 14:10:16.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.29.196 80'
Aug 11 14:10:16.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.29.196 80\nConnection to 10.96.29.196 80 port [tcp/http] succeeded!\n"
Aug 11 14:10:16.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 14:10:16.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.2 32253'
Aug 11 14:10:16.976: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.2 32253\nConnection to 192.168.178.2 32253 port [tcp/*] succeeded!\n"
Aug 11 14:10:16.976: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 14:10:16.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.3 32253'
Aug 11 14:10:17.096: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.3 32253\nConnection to 192.168.178.3 32253 port [tcp/*] succeeded!\n"
Aug 11 14:10:17.096: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 14:10:17.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:32253/ ; done'
Aug 11 14:10:17.267: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n"
Aug 11 14:10:17.267: INFO: stdout: "\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9"
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
Aug 11 14:10:17.267: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport in namespace services-2378, will wait for the garbage collector to delete the pods 08/11/23 14:10:17.282
Aug 11 14:10:17.342: INFO: Deleting ReplicationController affinity-nodeport took: 6.729309ms
Aug 11 14:10:17.443: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.763836ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 14:10:19.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2378" for this suite. 08/11/23 14:10:19.579
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]","completed":31,"skipped":500,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.141 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:10.445
    Aug 11 14:10:10.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 14:10:10.446
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:10.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:10.469
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2194
    STEP: creating service in namespace services-2378 08/11/23 14:10:10.471
    STEP: creating service affinity-nodeport in namespace services-2378 08/11/23 14:10:10.472
    STEP: creating replication controller affinity-nodeport in namespace services-2378 08/11/23 14:10:10.496
    I0811 14:10:10.503363      22 runners.go:193] Created replication controller with name: affinity-nodeport, namespace: services-2378, replica count: 3
    I0811 14:10:13.555418      22 runners.go:193] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:10:13.566: INFO: Creating new exec pod
    Aug 11 14:10:13.575: INFO: Waiting up to 5m0s for pod "execpod-affinityhm5pr" in namespace "services-2378" to be "running"
    Aug 11 14:10:13.577: INFO: Pod "execpod-affinityhm5pr": Phase="Pending", Reason="", readiness=false. Elapsed: 2.912071ms
    Aug 11 14:10:15.582: INFO: Pod "execpod-affinityhm5pr": Phase="Running", Reason="", readiness=true. Elapsed: 2.00717519s
    Aug 11 14:10:15.582: INFO: Pod "execpod-affinityhm5pr" satisfied condition "running"
    Aug 11 14:10:16.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
    Aug 11 14:10:16.723: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
    Aug 11 14:10:16.723: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 14:10:16.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.96.29.196 80'
    Aug 11 14:10:16.840: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.96.29.196 80\nConnection to 10.96.29.196 80 port [tcp/http] succeeded!\n"
    Aug 11 14:10:16.840: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 14:10:16.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.2 32253'
    Aug 11 14:10:16.976: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.2 32253\nConnection to 192.168.178.2 32253 port [tcp/*] succeeded!\n"
    Aug 11 14:10:16.976: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 14:10:16.976: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.3 32253'
    Aug 11 14:10:17.096: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.3 32253\nConnection to 192.168.178.3 32253 port [tcp/*] succeeded!\n"
    Aug 11 14:10:17.096: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 14:10:17.096: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2378 exec execpod-affinityhm5pr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:32253/ ; done'
    Aug 11 14:10:17.267: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:32253/\n"
    Aug 11 14:10:17.267: INFO: stdout: "\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9\naffinity-nodeport-2jjn9"
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Received response from host: affinity-nodeport-2jjn9
    Aug 11 14:10:17.267: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport in namespace services-2378, will wait for the garbage collector to delete the pods 08/11/23 14:10:17.282
    Aug 11 14:10:17.342: INFO: Deleting ReplicationController affinity-nodeport took: 6.729309ms
    Aug 11 14:10:17.443: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.763836ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 14:10:19.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2378" for this suite. 08/11/23 14:10:19.579
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:19.589
Aug 11 14:10:19.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:10:19.59
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:19.604
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:19.607
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support --unix-socket=/path  [Conformance]
  test/e2e/kubectl/kubectl.go:1810
STEP: Starting the proxy 08/11/23 14:10:19.609
Aug 11 14:10:19.610: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-38 proxy --unix-socket=/tmp/kubectl-proxy-unix4233365373/test'
STEP: retrieving proxy /api/ output 08/11/23 14:10:19.646
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:10:19.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-38" for this suite. 08/11/23 14:10:19.653
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]","completed":32,"skipped":555,"failed":0}
------------------------------
â€¢ [0.071 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support --unix-socket=/path  [Conformance]
    test/e2e/kubectl/kubectl.go:1810

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:19.589
    Aug 11 14:10:19.589: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:10:19.59
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:19.604
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:19.607
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support --unix-socket=/path  [Conformance]
      test/e2e/kubectl/kubectl.go:1810
    STEP: Starting the proxy 08/11/23 14:10:19.609
    Aug 11 14:10:19.610: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-38 proxy --unix-socket=/tmp/kubectl-proxy-unix4233365373/test'
    STEP: retrieving proxy /api/ output 08/11/23 14:10:19.646
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:10:19.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-38" for this suite. 08/11/23 14:10:19.653
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:19.661
Aug 11 14:10:19.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:10:19.662
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:19.678
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:19.681
[It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156
STEP: Deleting RuntimeClass runtimeclass-455-delete-me 08/11/23 14:10:19.689
STEP: Waiting for the RuntimeClass to disappear 08/11/23 14:10:19.699
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 11 14:10:19.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-455" for this suite. 08/11/23 14:10:19.713
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]","completed":33,"skipped":572,"failed":0}
------------------------------
â€¢ [0.058 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:19.661
    Aug 11 14:10:19.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:10:19.662
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:19.678
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:19.681
    [It] should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:156
    STEP: Deleting RuntimeClass runtimeclass-455-delete-me 08/11/23 14:10:19.689
    STEP: Waiting for the RuntimeClass to disappear 08/11/23 14:10:19.699
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 11 14:10:19.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-455" for this suite. 08/11/23 14:10:19.713
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:19.719
Aug 11 14:10:19.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:10:19.72
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:19.734
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:19.737
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56
STEP: Creating configMap with name projected-configmap-test-volume-ab9faf83-3bc8-46ca-9243-b775e896cde0 08/11/23 14:10:19.74
STEP: Creating a pod to test consume configMaps 08/11/23 14:10:19.745
Aug 11 14:10:19.754: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a" in namespace "projected-7083" to be "Succeeded or Failed"
Aug 11 14:10:19.759: INFO: Pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.916133ms
Aug 11 14:10:21.764: INFO: Pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010118423s
Aug 11 14:10:23.765: INFO: Pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011262484s
STEP: Saw pod success 08/11/23 14:10:23.765
Aug 11 14:10:23.765: INFO: Pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a" satisfied condition "Succeeded or Failed"
Aug 11 14:10:23.769: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:10:23.778
Aug 11 14:10:23.793: INFO: Waiting for pod pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a to disappear
Aug 11 14:10:23.796: INFO: Pod pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 11 14:10:23.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7083" for this suite. 08/11/23 14:10:23.8
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":34,"skipped":577,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:19.719
    Aug 11 14:10:19.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:10:19.72
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:19.734
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:19.737
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:56
    STEP: Creating configMap with name projected-configmap-test-volume-ab9faf83-3bc8-46ca-9243-b775e896cde0 08/11/23 14:10:19.74
    STEP: Creating a pod to test consume configMaps 08/11/23 14:10:19.745
    Aug 11 14:10:19.754: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a" in namespace "projected-7083" to be "Succeeded or Failed"
    Aug 11 14:10:19.759: INFO: Pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.916133ms
    Aug 11 14:10:21.764: INFO: Pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010118423s
    Aug 11 14:10:23.765: INFO: Pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011262484s
    STEP: Saw pod success 08/11/23 14:10:23.765
    Aug 11 14:10:23.765: INFO: Pod "pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:23.769: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:10:23.778
    Aug 11 14:10:23.793: INFO: Waiting for pod pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a to disappear
    Aug 11 14:10:23.796: INFO: Pod pod-projected-configmaps-4079959a-2cf0-4f70-bd00-a8e4019ca75a no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 11 14:10:23.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7083" for this suite. 08/11/23 14:10:23.8
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:23.808
Aug 11 14:10:23.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:10:23.809
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:23.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:23.828
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46
STEP: Creating configMap with name projected-configmap-test-volume-92d6b39f-c7a3-4220-b43d-2f4ad6fb5155 08/11/23 14:10:23.831
STEP: Creating a pod to test consume configMaps 08/11/23 14:10:23.838
Aug 11 14:10:23.847: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c" in namespace "projected-4418" to be "Succeeded or Failed"
Aug 11 14:10:23.854: INFO: Pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.650347ms
Aug 11 14:10:25.860: INFO: Pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013109774s
Aug 11 14:10:27.859: INFO: Pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012240433s
STEP: Saw pod success 08/11/23 14:10:27.859
Aug 11 14:10:27.859: INFO: Pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c" satisfied condition "Succeeded or Failed"
Aug 11 14:10:27.862: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:10:27.871
Aug 11 14:10:27.885: INFO: Waiting for pod pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c to disappear
Aug 11 14:10:27.888: INFO: Pod pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 11 14:10:27.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4418" for this suite. 08/11/23 14:10:27.892
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":35,"skipped":602,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:23.808
    Aug 11 14:10:23.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:10:23.809
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:23.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:23.828
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:46
    STEP: Creating configMap with name projected-configmap-test-volume-92d6b39f-c7a3-4220-b43d-2f4ad6fb5155 08/11/23 14:10:23.831
    STEP: Creating a pod to test consume configMaps 08/11/23 14:10:23.838
    Aug 11 14:10:23.847: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c" in namespace "projected-4418" to be "Succeeded or Failed"
    Aug 11 14:10:23.854: INFO: Pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c": Phase="Pending", Reason="", readiness=false. Elapsed: 7.650347ms
    Aug 11 14:10:25.860: INFO: Pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.013109774s
    Aug 11 14:10:27.859: INFO: Pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012240433s
    STEP: Saw pod success 08/11/23 14:10:27.859
    Aug 11 14:10:27.859: INFO: Pod "pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:27.862: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:10:27.871
    Aug 11 14:10:27.885: INFO: Waiting for pod pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c to disappear
    Aug 11 14:10:27.888: INFO: Pod pod-projected-configmaps-fc5b8ac2-b10e-4373-8895-864a6f19f04c no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 11 14:10:27.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4418" for this suite. 08/11/23 14:10:27.892
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Ingress API
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
[BeforeEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:27.9
Aug 11 14:10:27.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename ingress 08/11/23 14:10:27.901
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:27.915
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:27.917
[It] should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552
STEP: getting /apis 08/11/23 14:10:27.92
STEP: getting /apis/networking.k8s.io 08/11/23 14:10:27.922
STEP: getting /apis/networking.k8s.iov1 08/11/23 14:10:27.923
STEP: creating 08/11/23 14:10:27.923
STEP: getting 08/11/23 14:10:27.942
STEP: listing 08/11/23 14:10:27.945
STEP: watching 08/11/23 14:10:27.948
Aug 11 14:10:27.948: INFO: starting watch
STEP: cluster-wide listing 08/11/23 14:10:27.949
STEP: cluster-wide watching 08/11/23 14:10:27.952
Aug 11 14:10:27.952: INFO: starting watch
STEP: patching 08/11/23 14:10:27.953
STEP: updating 08/11/23 14:10:27.958
Aug 11 14:10:27.966: INFO: waiting for watch events with expected annotations
Aug 11 14:10:27.966: INFO: saw patched and updated annotations
STEP: patching /status 08/11/23 14:10:27.966
STEP: updating /status 08/11/23 14:10:27.973
STEP: get /status 08/11/23 14:10:27.984
STEP: deleting 08/11/23 14:10:27.987
STEP: deleting a collection 08/11/23 14:10:28
[AfterEach] [sig-network] Ingress API
  test/e2e/framework/framework.go:187
Aug 11 14:10:28.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingress-8674" for this suite. 08/11/23 14:10:28.018
{"msg":"PASSED [sig-network] Ingress API should support creating Ingress API operations [Conformance]","completed":36,"skipped":603,"failed":0}
------------------------------
â€¢ [0.124 seconds]
[sig-network] Ingress API
test/e2e/network/common/framework.go:23
  should support creating Ingress API operations [Conformance]
  test/e2e/network/ingress.go:552

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:27.9
    Aug 11 14:10:27.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename ingress 08/11/23 14:10:27.901
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:27.915
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:27.917
    [It] should support creating Ingress API operations [Conformance]
      test/e2e/network/ingress.go:552
    STEP: getting /apis 08/11/23 14:10:27.92
    STEP: getting /apis/networking.k8s.io 08/11/23 14:10:27.922
    STEP: getting /apis/networking.k8s.iov1 08/11/23 14:10:27.923
    STEP: creating 08/11/23 14:10:27.923
    STEP: getting 08/11/23 14:10:27.942
    STEP: listing 08/11/23 14:10:27.945
    STEP: watching 08/11/23 14:10:27.948
    Aug 11 14:10:27.948: INFO: starting watch
    STEP: cluster-wide listing 08/11/23 14:10:27.949
    STEP: cluster-wide watching 08/11/23 14:10:27.952
    Aug 11 14:10:27.952: INFO: starting watch
    STEP: patching 08/11/23 14:10:27.953
    STEP: updating 08/11/23 14:10:27.958
    Aug 11 14:10:27.966: INFO: waiting for watch events with expected annotations
    Aug 11 14:10:27.966: INFO: saw patched and updated annotations
    STEP: patching /status 08/11/23 14:10:27.966
    STEP: updating /status 08/11/23 14:10:27.973
    STEP: get /status 08/11/23 14:10:27.984
    STEP: deleting 08/11/23 14:10:27.987
    STEP: deleting a collection 08/11/23 14:10:28
    [AfterEach] [sig-network] Ingress API
      test/e2e/framework/framework.go:187
    Aug 11 14:10:28.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingress-8674" for this suite. 08/11/23 14:10:28.018
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:28.025
Aug 11 14:10:28.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:10:28.026
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:28.042
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:28.045
[It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166
STEP: Creating a pod to test emptydir 0644 on node default medium 08/11/23 14:10:28.047
Aug 11 14:10:28.056: INFO: Waiting up to 5m0s for pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc" in namespace "emptydir-3662" to be "Succeeded or Failed"
Aug 11 14:10:28.061: INFO: Pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.360946ms
Aug 11 14:10:30.067: INFO: Pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010988728s
Aug 11 14:10:32.065: INFO: Pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009712186s
STEP: Saw pod success 08/11/23 14:10:32.065
Aug 11 14:10:32.065: INFO: Pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc" satisfied condition "Succeeded or Failed"
Aug 11 14:10:32.069: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-d941dfd8-482c-4e0e-b125-822d728566dc container test-container: <nil>
STEP: delete the pod 08/11/23 14:10:32.078
Aug 11 14:10:32.095: INFO: Waiting for pod pod-d941dfd8-482c-4e0e-b125-822d728566dc to disappear
Aug 11 14:10:32.098: INFO: Pod pod-d941dfd8-482c-4e0e-b125-822d728566dc no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 14:10:32.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3662" for this suite. 08/11/23 14:10:32.102
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":37,"skipped":610,"failed":0}
------------------------------
â€¢ [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:166

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:28.025
    Aug 11 14:10:28.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:10:28.026
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:28.042
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:28.045
    [It] should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:166
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/11/23 14:10:28.047
    Aug 11 14:10:28.056: INFO: Waiting up to 5m0s for pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc" in namespace "emptydir-3662" to be "Succeeded or Failed"
    Aug 11 14:10:28.061: INFO: Pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc": Phase="Pending", Reason="", readiness=false. Elapsed: 5.360946ms
    Aug 11 14:10:30.067: INFO: Pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010988728s
    Aug 11 14:10:32.065: INFO: Pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009712186s
    STEP: Saw pod success 08/11/23 14:10:32.065
    Aug 11 14:10:32.065: INFO: Pod "pod-d941dfd8-482c-4e0e-b125-822d728566dc" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:32.069: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-d941dfd8-482c-4e0e-b125-822d728566dc container test-container: <nil>
    STEP: delete the pod 08/11/23 14:10:32.078
    Aug 11 14:10:32.095: INFO: Waiting for pod pod-d941dfd8-482c-4e0e-b125-822d728566dc to disappear
    Aug 11 14:10:32.098: INFO: Pod pod-d941dfd8-482c-4e0e-b125-822d728566dc no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:10:32.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3662" for this suite. 08/11/23 14:10:32.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:32.116
Aug 11 14:10:32.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename tables 08/11/23 14:10:32.117
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:32.133
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:32.135
[BeforeEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/apimachinery/table_conversion.go:49
[It] should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154
[AfterEach] [sig-api-machinery] Servers with support for Table transformation
  test/e2e/framework/framework.go:187
Aug 11 14:10:32.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "tables-7156" for this suite. 08/11/23 14:10:32.144
{"msg":"PASSED [sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]","completed":38,"skipped":667,"failed":0}
------------------------------
â€¢ [0.035 seconds]
[sig-api-machinery] Servers with support for Table transformation
test/e2e/apimachinery/framework.go:23
  should return a 406 for a backend which does not implement metadata [Conformance]
  test/e2e/apimachinery/table_conversion.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:32.116
    Aug 11 14:10:32.116: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename tables 08/11/23 14:10:32.117
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:32.133
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:32.135
    [BeforeEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/apimachinery/table_conversion.go:49
    [It] should return a 406 for a backend which does not implement metadata [Conformance]
      test/e2e/apimachinery/table_conversion.go:154
    [AfterEach] [sig-api-machinery] Servers with support for Table transformation
      test/e2e/framework/framework.go:187
    Aug 11 14:10:32.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "tables-7156" for this suite. 08/11/23 14:10:32.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:32.152
Aug 11 14:10:32.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:10:32.153
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:32.168
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:32.171
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56
STEP: Creating secret with name secret-test-0161a027-c8c6-4014-a6d3-60dc6323e858 08/11/23 14:10:32.173
STEP: Creating a pod to test consume secrets 08/11/23 14:10:32.179
Aug 11 14:10:32.188: INFO: Waiting up to 5m0s for pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44" in namespace "secrets-6093" to be "Succeeded or Failed"
Aug 11 14:10:32.194: INFO: Pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44": Phase="Pending", Reason="", readiness=false. Elapsed: 5.408208ms
Aug 11 14:10:34.199: INFO: Pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010517532s
Aug 11 14:10:36.200: INFO: Pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011259463s
STEP: Saw pod success 08/11/23 14:10:36.2
Aug 11 14:10:36.200: INFO: Pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44" satisfied condition "Succeeded or Failed"
Aug 11 14:10:36.203: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:10:36.212
Aug 11 14:10:36.228: INFO: Waiting for pod pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44 to disappear
Aug 11 14:10:36.231: INFO: Pod pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:10:36.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-6093" for this suite. 08/11/23 14:10:36.235
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":39,"skipped":676,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:32.152
    Aug 11 14:10:32.152: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:10:32.153
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:32.168
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:32.171
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:56
    STEP: Creating secret with name secret-test-0161a027-c8c6-4014-a6d3-60dc6323e858 08/11/23 14:10:32.173
    STEP: Creating a pod to test consume secrets 08/11/23 14:10:32.179
    Aug 11 14:10:32.188: INFO: Waiting up to 5m0s for pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44" in namespace "secrets-6093" to be "Succeeded or Failed"
    Aug 11 14:10:32.194: INFO: Pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44": Phase="Pending", Reason="", readiness=false. Elapsed: 5.408208ms
    Aug 11 14:10:34.199: INFO: Pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010517532s
    Aug 11 14:10:36.200: INFO: Pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011259463s
    STEP: Saw pod success 08/11/23 14:10:36.2
    Aug 11 14:10:36.200: INFO: Pod "pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:36.203: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:10:36.212
    Aug 11 14:10:36.228: INFO: Waiting for pod pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44 to disappear
    Aug 11 14:10:36.231: INFO: Pod pod-secrets-a496a700-b803-40fb-81ec-fbe332e0ca44 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:10:36.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-6093" for this suite. 08/11/23 14:10:36.235
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Garbage collector
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:36.241
Aug 11 14:10:36.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename gc 08/11/23 14:10:36.242
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:36.256
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:36.259
[It] should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312
STEP: create the rc 08/11/23 14:10:36.261
STEP: delete the rc 08/11/23 14:10:41.273
STEP: wait for all pods to be garbage collected 08/11/23 14:10:41.281
STEP: Gathering metrics 08/11/23 14:10:46.289
Aug 11 14:10:46.325: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
Aug 11 14:10:46.329: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.85565ms
Aug 11 14:10:46.329: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
Aug 11 14:10:46.329: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
Aug 11 14:10:46.386: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 11 14:10:46.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2021" for this suite. 08/11/23 14:10:46.39
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]","completed":40,"skipped":676,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.157 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete pods created by rc when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:312

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:36.241
    Aug 11 14:10:36.241: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename gc 08/11/23 14:10:36.242
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:36.256
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:36.259
    [It] should delete pods created by rc when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:312
    STEP: create the rc 08/11/23 14:10:36.261
    STEP: delete the rc 08/11/23 14:10:41.273
    STEP: wait for all pods to be garbage collected 08/11/23 14:10:41.281
    STEP: Gathering metrics 08/11/23 14:10:46.289
    Aug 11 14:10:46.325: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
    Aug 11 14:10:46.329: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.85565ms
    Aug 11 14:10:46.329: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
    Aug 11 14:10:46.329: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
    Aug 11 14:10:46.386: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 11 14:10:46.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2021" for this suite. 08/11/23 14:10:46.39
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:46.398
Aug 11 14:10:46.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:10:46.399
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:46.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:46.417
[It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129
Aug 11 14:10:46.434: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2278 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 11 14:10:46.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-2278" for this suite. 08/11/23 14:10:46.452
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]","completed":41,"skipped":689,"failed":0}
------------------------------
â€¢ [0.061 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:46.398
    Aug 11 14:10:46.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:10:46.399
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:46.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:46.417
    [It] should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:129
    Aug 11 14:10:46.434: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-2278 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 11 14:10:46.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-2278" for this suite. 08/11/23 14:10:46.452
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:46.462
Aug 11 14:10:46.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:10:46.462
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:46.479
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:46.481
[It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739
Aug 11 14:10:46.488: INFO: Got root ca configmap in namespace "svcaccounts-3244"
Aug 11 14:10:46.494: INFO: Deleted root ca configmap in namespace "svcaccounts-3244"
STEP: waiting for a new root ca configmap created 08/11/23 14:10:46.995
Aug 11 14:10:46.999: INFO: Recreated root ca configmap in namespace "svcaccounts-3244"
Aug 11 14:10:47.004: INFO: Updated root ca configmap in namespace "svcaccounts-3244"
STEP: waiting for the root ca configmap reconciled 08/11/23 14:10:47.505
Aug 11 14:10:47.509: INFO: Reconciled root ca configmap in namespace "svcaccounts-3244"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 11 14:10:47.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-3244" for this suite. 08/11/23 14:10:47.513
{"msg":"PASSED [sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]","completed":42,"skipped":752,"failed":0}
------------------------------
â€¢ [1.059 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should guarantee kube-root-ca.crt exist in any namespace [Conformance]
  test/e2e/auth/service_accounts.go:739

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:46.462
    Aug 11 14:10:46.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:10:46.462
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:46.479
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:46.481
    [It] should guarantee kube-root-ca.crt exist in any namespace [Conformance]
      test/e2e/auth/service_accounts.go:739
    Aug 11 14:10:46.488: INFO: Got root ca configmap in namespace "svcaccounts-3244"
    Aug 11 14:10:46.494: INFO: Deleted root ca configmap in namespace "svcaccounts-3244"
    STEP: waiting for a new root ca configmap created 08/11/23 14:10:46.995
    Aug 11 14:10:46.999: INFO: Recreated root ca configmap in namespace "svcaccounts-3244"
    Aug 11 14:10:47.004: INFO: Updated root ca configmap in namespace "svcaccounts-3244"
    STEP: waiting for the root ca configmap reconciled 08/11/23 14:10:47.505
    Aug 11 14:10:47.509: INFO: Reconciled root ca configmap in namespace "svcaccounts-3244"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 11 14:10:47.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-3244" for this suite. 08/11/23 14:10:47.513
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:47.521
Aug 11 14:10:47.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:10:47.522
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:47.537
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:47.541
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98
STEP: Creating configMap with name configmap-test-volume-map-4efe1cf8-644d-43aa-aa72-1936a1ca1459 08/11/23 14:10:47.543
STEP: Creating a pod to test consume configMaps 08/11/23 14:10:47.549
Aug 11 14:10:47.559: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63" in namespace "configmap-6006" to be "Succeeded or Failed"
Aug 11 14:10:47.563: INFO: Pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.463929ms
Aug 11 14:10:49.568: INFO: Pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008970307s
Aug 11 14:10:51.568: INFO: Pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009303492s
STEP: Saw pod success 08/11/23 14:10:51.568
Aug 11 14:10:51.568: INFO: Pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63" satisfied condition "Succeeded or Failed"
Aug 11 14:10:51.572: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:10:51.581
Aug 11 14:10:51.597: INFO: Waiting for pod pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63 to disappear
Aug 11 14:10:51.605: INFO: Pod pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:10:51.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6006" for this suite. 08/11/23 14:10:51.609
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":43,"skipped":760,"failed":0}
------------------------------
â€¢ [4.098 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:47.521
    Aug 11 14:10:47.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:10:47.522
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:47.537
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:47.541
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:98
    STEP: Creating configMap with name configmap-test-volume-map-4efe1cf8-644d-43aa-aa72-1936a1ca1459 08/11/23 14:10:47.543
    STEP: Creating a pod to test consume configMaps 08/11/23 14:10:47.549
    Aug 11 14:10:47.559: INFO: Waiting up to 5m0s for pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63" in namespace "configmap-6006" to be "Succeeded or Failed"
    Aug 11 14:10:47.563: INFO: Pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63": Phase="Pending", Reason="", readiness=false. Elapsed: 4.463929ms
    Aug 11 14:10:49.568: INFO: Pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008970307s
    Aug 11 14:10:51.568: INFO: Pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009303492s
    STEP: Saw pod success 08/11/23 14:10:51.568
    Aug 11 14:10:51.568: INFO: Pod "pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63" satisfied condition "Succeeded or Failed"
    Aug 11 14:10:51.572: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:10:51.581
    Aug 11 14:10:51.597: INFO: Waiting for pod pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63 to disappear
    Aug 11 14:10:51.605: INFO: Pod pod-configmaps-8fab9cba-a2c1-4370-833b-110301d06e63 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:10:51.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6006" for this suite. 08/11/23 14:10:51.609
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
[BeforeEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:51.621
Aug 11 14:10:51.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename csistoragecapacity 08/11/23 14:10:51.621
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:51.636
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:51.639
[It]  should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49
STEP: getting /apis 08/11/23 14:10:51.641
STEP: getting /apis/storage.k8s.io 08/11/23 14:10:51.643
STEP: getting /apis/storage.k8s.io/v1 08/11/23 14:10:51.644
STEP: creating 08/11/23 14:10:51.645
STEP: watching 08/11/23 14:10:51.665
Aug 11 14:10:51.665: INFO: starting watch
STEP: getting 08/11/23 14:10:51.672
STEP: listing in namespace 08/11/23 14:10:51.675
STEP: listing across namespaces 08/11/23 14:10:51.677
STEP: patching 08/11/23 14:10:51.68
STEP: updating 08/11/23 14:10:51.685
Aug 11 14:10:51.691: INFO: waiting for watch events with expected annotations in namespace
Aug 11 14:10:51.691: INFO: waiting for watch events with expected annotations across namespace
STEP: deleting 08/11/23 14:10:51.691
STEP: deleting a collection 08/11/23 14:10:51.702
[AfterEach] [sig-storage] CSIStorageCapacity
  test/e2e/framework/framework.go:187
Aug 11 14:10:51.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "csistoragecapacity-8930" for this suite. 08/11/23 14:10:51.723
{"msg":"PASSED [sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]","completed":44,"skipped":779,"failed":0}
------------------------------
â€¢ [0.109 seconds]
[sig-storage] CSIStorageCapacity
test/e2e/storage/utils/framework.go:23
   should support CSIStorageCapacities API operations [Conformance]
  test/e2e/storage/csistoragecapacity.go:49

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:51.621
    Aug 11 14:10:51.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename csistoragecapacity 08/11/23 14:10:51.621
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:51.636
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:51.639
    [It]  should support CSIStorageCapacities API operations [Conformance]
      test/e2e/storage/csistoragecapacity.go:49
    STEP: getting /apis 08/11/23 14:10:51.641
    STEP: getting /apis/storage.k8s.io 08/11/23 14:10:51.643
    STEP: getting /apis/storage.k8s.io/v1 08/11/23 14:10:51.644
    STEP: creating 08/11/23 14:10:51.645
    STEP: watching 08/11/23 14:10:51.665
    Aug 11 14:10:51.665: INFO: starting watch
    STEP: getting 08/11/23 14:10:51.672
    STEP: listing in namespace 08/11/23 14:10:51.675
    STEP: listing across namespaces 08/11/23 14:10:51.677
    STEP: patching 08/11/23 14:10:51.68
    STEP: updating 08/11/23 14:10:51.685
    Aug 11 14:10:51.691: INFO: waiting for watch events with expected annotations in namespace
    Aug 11 14:10:51.691: INFO: waiting for watch events with expected annotations across namespace
    STEP: deleting 08/11/23 14:10:51.691
    STEP: deleting a collection 08/11/23 14:10:51.702
    [AfterEach] [sig-storage] CSIStorageCapacity
      test/e2e/framework/framework.go:187
    Aug 11 14:10:51.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "csistoragecapacity-8930" for this suite. 08/11/23 14:10:51.723
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:10:51.73
Aug 11 14:10:51.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename subpath 08/11/23 14:10:51.731
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:51.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:51.749
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 14:10:51.751
[It] should support subpaths with projected pod [Conformance]
  test/e2e/storage/subpath.go:106
STEP: Creating pod pod-subpath-test-projected-dqx5 08/11/23 14:10:51.762
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:10:51.762
Aug 11 14:10:51.771: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dqx5" in namespace "subpath-9832" to be "Succeeded or Failed"
Aug 11 14:10:51.777: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.636145ms
Aug 11 14:10:53.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010764191s
Aug 11 14:10:55.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 4.010657495s
Aug 11 14:10:57.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 6.010872499s
Aug 11 14:10:59.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 8.010935707s
Aug 11 14:11:01.781: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 10.010015087s
Aug 11 14:11:03.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 12.011145127s
Aug 11 14:11:05.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 14.011082511s
Aug 11 14:11:07.783: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 16.011535752s
Aug 11 14:11:09.783: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 18.011793877s
Aug 11 14:11:11.781: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 20.010162853s
Aug 11 14:11:13.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=false. Elapsed: 22.010440798s
Aug 11 14:11:15.783: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011462568s
STEP: Saw pod success 08/11/23 14:11:15.783
Aug 11 14:11:15.783: INFO: Pod "pod-subpath-test-projected-dqx5" satisfied condition "Succeeded or Failed"
Aug 11 14:11:15.786: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-projected-dqx5 container test-container-subpath-projected-dqx5: <nil>
STEP: delete the pod 08/11/23 14:11:15.796
Aug 11 14:11:15.813: INFO: Waiting for pod pod-subpath-test-projected-dqx5 to disappear
Aug 11 14:11:15.816: INFO: Pod pod-subpath-test-projected-dqx5 no longer exists
STEP: Deleting pod pod-subpath-test-projected-dqx5 08/11/23 14:11:15.816
Aug 11 14:11:15.816: INFO: Deleting pod "pod-subpath-test-projected-dqx5" in namespace "subpath-9832"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 11 14:11:15.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-9832" for this suite. 08/11/23 14:11:15.824
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]","completed":45,"skipped":798,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.100 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with projected pod [Conformance]
    test/e2e/storage/subpath.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:10:51.73
    Aug 11 14:10:51.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename subpath 08/11/23 14:10:51.731
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:10:51.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:10:51.749
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 14:10:51.751
    [It] should support subpaths with projected pod [Conformance]
      test/e2e/storage/subpath.go:106
    STEP: Creating pod pod-subpath-test-projected-dqx5 08/11/23 14:10:51.762
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:10:51.762
    Aug 11 14:10:51.771: INFO: Waiting up to 5m0s for pod "pod-subpath-test-projected-dqx5" in namespace "subpath-9832" to be "Succeeded or Failed"
    Aug 11 14:10:51.777: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Pending", Reason="", readiness=false. Elapsed: 5.636145ms
    Aug 11 14:10:53.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010764191s
    Aug 11 14:10:55.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 4.010657495s
    Aug 11 14:10:57.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 6.010872499s
    Aug 11 14:10:59.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 8.010935707s
    Aug 11 14:11:01.781: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 10.010015087s
    Aug 11 14:11:03.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 12.011145127s
    Aug 11 14:11:05.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 14.011082511s
    Aug 11 14:11:07.783: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 16.011535752s
    Aug 11 14:11:09.783: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 18.011793877s
    Aug 11 14:11:11.781: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=true. Elapsed: 20.010162853s
    Aug 11 14:11:13.782: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Running", Reason="", readiness=false. Elapsed: 22.010440798s
    Aug 11 14:11:15.783: INFO: Pod "pod-subpath-test-projected-dqx5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.011462568s
    STEP: Saw pod success 08/11/23 14:11:15.783
    Aug 11 14:11:15.783: INFO: Pod "pod-subpath-test-projected-dqx5" satisfied condition "Succeeded or Failed"
    Aug 11 14:11:15.786: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-projected-dqx5 container test-container-subpath-projected-dqx5: <nil>
    STEP: delete the pod 08/11/23 14:11:15.796
    Aug 11 14:11:15.813: INFO: Waiting for pod pod-subpath-test-projected-dqx5 to disappear
    Aug 11 14:11:15.816: INFO: Pod pod-subpath-test-projected-dqx5 no longer exists
    STEP: Deleting pod pod-subpath-test-projected-dqx5 08/11/23 14:11:15.816
    Aug 11 14:11:15.816: INFO: Deleting pod "pod-subpath-test-projected-dqx5" in namespace "subpath-9832"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 11 14:11:15.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-9832" for this suite. 08/11/23 14:11:15.824
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:11:15.832
Aug 11 14:11:15.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename watch 08/11/23 14:11:15.833
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:15.847
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:15.85
[It] should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60
STEP: creating a watch on configmaps with label A 08/11/23 14:11:15.853
STEP: creating a watch on configmaps with label B 08/11/23 14:11:15.854
STEP: creating a watch on configmaps with label A or B 08/11/23 14:11:15.855
STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/11/23 14:11:15.856
Aug 11 14:11:15.860: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12221 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:11:15.860: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12221 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/11/23 14:11:15.861
Aug 11 14:11:15.869: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12222 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:11:15.869: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12222 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/11/23 14:11:15.869
Aug 11 14:11:15.878: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12223 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:11:15.878: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12223 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/11/23 14:11:15.878
Aug 11 14:11:15.884: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12224 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:11:15.884: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12224 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/11/23 14:11:15.884
Aug 11 14:11:15.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-592  3439683e-2c96-4d23-a324-1e3525240c37 12225 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:11:15.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-592  3439683e-2c96-4d23-a324-1e3525240c37 12225 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/11/23 14:11:25.889
Aug 11 14:11:25.896: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-592  3439683e-2c96-4d23-a324-1e3525240c37 12290 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:11:25.896: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-592  3439683e-2c96-4d23-a324-1e3525240c37 12290 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 11 14:11:35.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-592" for this suite. 08/11/23 14:11:35.906
{"msg":"PASSED [sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]","completed":46,"skipped":800,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.082 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe add, update, and delete watch notifications on configmaps [Conformance]
  test/e2e/apimachinery/watch.go:60

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:11:15.832
    Aug 11 14:11:15.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename watch 08/11/23 14:11:15.833
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:15.847
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:15.85
    [It] should observe add, update, and delete watch notifications on configmaps [Conformance]
      test/e2e/apimachinery/watch.go:60
    STEP: creating a watch on configmaps with label A 08/11/23 14:11:15.853
    STEP: creating a watch on configmaps with label B 08/11/23 14:11:15.854
    STEP: creating a watch on configmaps with label A or B 08/11/23 14:11:15.855
    STEP: creating a configmap with label A and ensuring the correct watchers observe the notification 08/11/23 14:11:15.856
    Aug 11 14:11:15.860: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12221 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:11:15.860: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12221 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A and ensuring the correct watchers observe the notification 08/11/23 14:11:15.861
    Aug 11 14:11:15.869: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12222 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:11:15.869: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12222 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying configmap A again and ensuring the correct watchers observe the notification 08/11/23 14:11:15.869
    Aug 11 14:11:15.878: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12223 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:11:15.878: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12223 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap A and ensuring the correct watchers observe the notification 08/11/23 14:11:15.878
    Aug 11 14:11:15.884: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12224 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:11:15.884: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-592  7deda232-7880-4044-8ed0-79d76556e760 12224 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: creating a configmap with label B and ensuring the correct watchers observe the notification 08/11/23 14:11:15.884
    Aug 11 14:11:15.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-592  3439683e-2c96-4d23-a324-1e3525240c37 12225 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:11:15.889: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-592  3439683e-2c96-4d23-a324-1e3525240c37 12225 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: deleting configmap B and ensuring the correct watchers observe the notification 08/11/23 14:11:25.889
    Aug 11 14:11:25.896: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-592  3439683e-2c96-4d23-a324-1e3525240c37 12290 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:11:25.896: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-592  3439683e-2c96-4d23-a324-1e3525240c37 12290 0 2023-08-11 14:11:15 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-11 14:11:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 11 14:11:35.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-592" for this suite. 08/11/23 14:11:35.906
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:11:35.915
Aug 11 14:11:35.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:11:35.916
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:35.931
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:35.934
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293
STEP: Creating a simple DaemonSet "daemon-set" 08/11/23 14:11:35.953
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:11:35.958
Aug 11 14:11:35.964: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:35.965: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:35.965: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:35.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:11:35.967: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:11:36.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:36.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:36.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:36.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:11:36.977: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:11:37.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:37.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:37.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:37.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:11:37.977: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/11/23 14:11:37.981
Aug 11 14:11:37.998: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:37.998: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:37.998: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:38.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:11:38.002: INFO: Node constell-d93e7e1d-worker-d314547c-wzlp is running 0 daemon pod, expected 1
Aug 11 14:11:39.010: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:39.010: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:39.010: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:39.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:11:39.014: INFO: Node constell-d93e7e1d-worker-d314547c-wzlp is running 0 daemon pod, expected 1
Aug 11 14:11:40.008: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:40.009: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:40.009: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:11:40.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:11:40.012: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Wait for the failed daemon pod to be completely deleted. 08/11/23 14:11:40.012
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:11:40.018
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1338, will wait for the garbage collector to delete the pods 08/11/23 14:11:40.018
Aug 11 14:11:40.081: INFO: Deleting DaemonSet.extensions daemon-set took: 8.575218ms
Aug 11 14:11:40.182: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.085536ms
Aug 11 14:11:42.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:11:42.686: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:11:42.689: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12451"},"items":null}

Aug 11 14:11:42.692: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12451"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:11:42.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-1338" for this suite. 08/11/23 14:11:42.711
{"msg":"PASSED [sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]","completed":47,"skipped":816,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.802 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should retry creating failed daemon pods [Conformance]
  test/e2e/apps/daemon_set.go:293

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:11:35.915
    Aug 11 14:11:35.915: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:11:35.916
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:35.931
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:35.934
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should retry creating failed daemon pods [Conformance]
      test/e2e/apps/daemon_set.go:293
    STEP: Creating a simple DaemonSet "daemon-set" 08/11/23 14:11:35.953
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:11:35.958
    Aug 11 14:11:35.964: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:35.965: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:35.965: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:35.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:11:35.967: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:11:36.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:36.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:36.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:36.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:11:36.977: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:11:37.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:37.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:37.973: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:37.977: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:11:37.977: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. 08/11/23 14:11:37.981
    Aug 11 14:11:37.998: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:37.998: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:37.998: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:38.002: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:11:38.002: INFO: Node constell-d93e7e1d-worker-d314547c-wzlp is running 0 daemon pod, expected 1
    Aug 11 14:11:39.010: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:39.010: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:39.010: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:39.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:11:39.014: INFO: Node constell-d93e7e1d-worker-d314547c-wzlp is running 0 daemon pod, expected 1
    Aug 11 14:11:40.008: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:40.009: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:40.009: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:11:40.012: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:11:40.012: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Wait for the failed daemon pod to be completely deleted. 08/11/23 14:11:40.012
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:11:40.018
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1338, will wait for the garbage collector to delete the pods 08/11/23 14:11:40.018
    Aug 11 14:11:40.081: INFO: Deleting DaemonSet.extensions daemon-set took: 8.575218ms
    Aug 11 14:11:40.182: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.085536ms
    Aug 11 14:11:42.686: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:11:42.686: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:11:42.689: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12451"},"items":null}

    Aug 11 14:11:42.692: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12451"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:11:42.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-1338" for this suite. 08/11/23 14:11:42.711
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl diff
  should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:11:42.718
Aug 11 14:11:42.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:11:42.719
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:42.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:42.738
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl diff finds a difference for Deployments [Conformance]
  test/e2e/kubectl/kubectl.go:929
STEP: create deployment with httpd image 08/11/23 14:11:42.741
Aug 11 14:11:42.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-4909 create -f -'
Aug 11 14:11:43.325: INFO: stderr: ""
Aug 11 14:11:43.325: INFO: stdout: "deployment.apps/httpd-deployment created\n"
STEP: verify diff finds difference between live and declared image 08/11/23 14:11:43.325
Aug 11 14:11:43.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-4909 diff -f -'
Aug 11 14:11:43.868: INFO: rc: 1
Aug 11 14:11:43.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-4909 delete -f -'
Aug 11 14:11:43.925: INFO: stderr: ""
Aug 11 14:11:43.925: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:11:43.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4909" for this suite. 08/11/23 14:11:43.931
{"msg":"PASSED [sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]","completed":48,"skipped":817,"failed":0}
------------------------------
â€¢ [1.223 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl diff
  test/e2e/kubectl/kubectl.go:923
    should check if kubectl diff finds a difference for Deployments [Conformance]
    test/e2e/kubectl/kubectl.go:929

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:11:42.718
    Aug 11 14:11:42.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:11:42.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:42.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:42.738
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl diff finds a difference for Deployments [Conformance]
      test/e2e/kubectl/kubectl.go:929
    STEP: create deployment with httpd image 08/11/23 14:11:42.741
    Aug 11 14:11:42.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-4909 create -f -'
    Aug 11 14:11:43.325: INFO: stderr: ""
    Aug 11 14:11:43.325: INFO: stdout: "deployment.apps/httpd-deployment created\n"
    STEP: verify diff finds difference between live and declared image 08/11/23 14:11:43.325
    Aug 11 14:11:43.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-4909 diff -f -'
    Aug 11 14:11:43.868: INFO: rc: 1
    Aug 11 14:11:43.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-4909 delete -f -'
    Aug 11 14:11:43.925: INFO: stderr: ""
    Aug 11 14:11:43.925: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:11:43.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4909" for this suite. 08/11/23 14:11:43.931
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-network] Services
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:11:43.941
Aug 11 14:11:43.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 14:11:43.942
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:43.958
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:43.962
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852
STEP: creating service multi-endpoint-test in namespace services-5636 08/11/23 14:11:43.965
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[] 08/11/23 14:11:43.98
Aug 11 14:11:43.991: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-5636 08/11/23 14:11:43.991
Aug 11 14:11:44.001: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5636" to be "running and ready"
Aug 11 14:11:44.006: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.45268ms
Aug 11 14:11:44.006: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:11:46.011: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01040236s
Aug 11 14:11:46.011: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 11 14:11:46.011: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[pod1:[100]] 08/11/23 14:11:46.015
Aug 11 14:11:46.025: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[pod1:[100]]
STEP: Creating pod pod2 in namespace services-5636 08/11/23 14:11:46.025
Aug 11 14:11:46.032: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5636" to be "running and ready"
Aug 11 14:11:46.036: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.264303ms
Aug 11 14:11:46.036: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:11:48.041: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009527263s
Aug 11 14:11:48.041: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 11 14:11:48.041: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[pod1:[100] pod2:[101]] 08/11/23 14:11:48.045
Aug 11 14:11:48.060: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[pod1:[100] pod2:[101]]
STEP: Checking if the Service forwards traffic to pods 08/11/23 14:11:48.06
Aug 11 14:11:48.060: INFO: Creating new exec pod
Aug 11 14:11:48.065: INFO: Waiting up to 5m0s for pod "execpodlxw8z" in namespace "services-5636" to be "running"
Aug 11 14:11:48.071: INFO: Pod "execpodlxw8z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.640856ms
Aug 11 14:11:50.075: INFO: Pod "execpodlxw8z": Phase="Running", Reason="", readiness=true. Elapsed: 2.010058611s
Aug 11 14:11:50.075: INFO: Pod "execpodlxw8z" satisfied condition "running"
Aug 11 14:11:51.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5636 exec execpodlxw8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
Aug 11 14:11:51.213: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
Aug 11 14:11:51.213: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 14:11:51.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5636 exec execpodlxw8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.140.50 80'
Aug 11 14:11:51.339: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.140.50 80\nConnection to 10.104.140.50 80 port [tcp/http] succeeded!\n"
Aug 11 14:11:51.339: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 14:11:51.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5636 exec execpodlxw8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
Aug 11 14:11:51.483: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
Aug 11 14:11:51.483: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 14:11:51.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5636 exec execpodlxw8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.140.50 81'
Aug 11 14:11:51.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.140.50 81\nConnection to 10.104.140.50 81 port [tcp/*] succeeded!\n"
Aug 11 14:11:51.611: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-5636 08/11/23 14:11:51.611
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[pod2:[101]] 08/11/23 14:11:51.63
Aug 11 14:11:51.650: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[pod2:[101]]
STEP: Deleting pod pod2 in namespace services-5636 08/11/23 14:11:51.65
STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[] 08/11/23 14:11:51.669
Aug 11 14:11:52.694: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 14:11:52.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5636" for this suite. 08/11/23 14:11:52.724
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve multiport endpoints from pods  [Conformance]","completed":49,"skipped":820,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.791 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve multiport endpoints from pods  [Conformance]
  test/e2e/network/service.go:852

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:11:43.941
    Aug 11 14:11:43.941: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 14:11:43.942
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:43.958
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:43.962
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve multiport endpoints from pods  [Conformance]
      test/e2e/network/service.go:852
    STEP: creating service multi-endpoint-test in namespace services-5636 08/11/23 14:11:43.965
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[] 08/11/23 14:11:43.98
    Aug 11 14:11:43.991: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-5636 08/11/23 14:11:43.991
    Aug 11 14:11:44.001: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-5636" to be "running and ready"
    Aug 11 14:11:44.006: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 5.45268ms
    Aug 11 14:11:44.006: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:11:46.011: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.01040236s
    Aug 11 14:11:46.011: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 11 14:11:46.011: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[pod1:[100]] 08/11/23 14:11:46.015
    Aug 11 14:11:46.025: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[pod1:[100]]
    STEP: Creating pod pod2 in namespace services-5636 08/11/23 14:11:46.025
    Aug 11 14:11:46.032: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-5636" to be "running and ready"
    Aug 11 14:11:46.036: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.264303ms
    Aug 11 14:11:46.036: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:11:48.041: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009527263s
    Aug 11 14:11:48.041: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 11 14:11:48.041: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[pod1:[100] pod2:[101]] 08/11/23 14:11:48.045
    Aug 11 14:11:48.060: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[pod1:[100] pod2:[101]]
    STEP: Checking if the Service forwards traffic to pods 08/11/23 14:11:48.06
    Aug 11 14:11:48.060: INFO: Creating new exec pod
    Aug 11 14:11:48.065: INFO: Waiting up to 5m0s for pod "execpodlxw8z" in namespace "services-5636" to be "running"
    Aug 11 14:11:48.071: INFO: Pod "execpodlxw8z": Phase="Pending", Reason="", readiness=false. Elapsed: 5.640856ms
    Aug 11 14:11:50.075: INFO: Pod "execpodlxw8z": Phase="Running", Reason="", readiness=true. Elapsed: 2.010058611s
    Aug 11 14:11:50.075: INFO: Pod "execpodlxw8z" satisfied condition "running"
    Aug 11 14:11:51.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5636 exec execpodlxw8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
    Aug 11 14:11:51.213: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
    Aug 11 14:11:51.213: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 14:11:51.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5636 exec execpodlxw8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.140.50 80'
    Aug 11 14:11:51.339: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.140.50 80\nConnection to 10.104.140.50 80 port [tcp/http] succeeded!\n"
    Aug 11 14:11:51.339: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 14:11:51.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5636 exec execpodlxw8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
    Aug 11 14:11:51.483: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
    Aug 11 14:11:51.483: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 14:11:51.483: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5636 exec execpodlxw8z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.140.50 81'
    Aug 11 14:11:51.611: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.140.50 81\nConnection to 10.104.140.50 81 port [tcp/*] succeeded!\n"
    Aug 11 14:11:51.611: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-5636 08/11/23 14:11:51.611
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[pod2:[101]] 08/11/23 14:11:51.63
    Aug 11 14:11:51.650: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[pod2:[101]]
    STEP: Deleting pod pod2 in namespace services-5636 08/11/23 14:11:51.65
    STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5636 to expose endpoints map[] 08/11/23 14:11:51.669
    Aug 11 14:11:52.694: INFO: successfully validated that service multi-endpoint-test in namespace services-5636 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 14:11:52.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5636" for this suite. 08/11/23 14:11:52.724
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:11:52.733
Aug 11 14:11:52.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename namespaces 08/11/23 14:11:52.734
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:52.749
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:52.751
[It] should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250
STEP: Creating a test namespace 08/11/23 14:11:52.754
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:52.767
STEP: Creating a service in the namespace 08/11/23 14:11:52.77
STEP: Deleting the namespace 08/11/23 14:11:52.785
STEP: Waiting for the namespace to be removed. 08/11/23 14:11:52.794
STEP: Recreating the namespace 08/11/23 14:11:58.799
STEP: Verifying there is no service in the namespace 08/11/23 14:11:58.817
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:11:58.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-7777" for this suite. 08/11/23 14:11:58.825
STEP: Destroying namespace "nsdeletetest-2999" for this suite. 08/11/23 14:11:58.832
Aug 11 14:11:58.835: INFO: Namespace nsdeletetest-2999 was already deleted
STEP: Destroying namespace "nsdeletetest-2132" for this suite. 08/11/23 14:11:58.836
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]","completed":50,"skipped":848,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.110 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all services are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:250

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:11:52.733
    Aug 11 14:11:52.733: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename namespaces 08/11/23 14:11:52.734
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:52.749
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:52.751
    [It] should ensure that all services are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:250
    STEP: Creating a test namespace 08/11/23 14:11:52.754
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:52.767
    STEP: Creating a service in the namespace 08/11/23 14:11:52.77
    STEP: Deleting the namespace 08/11/23 14:11:52.785
    STEP: Waiting for the namespace to be removed. 08/11/23 14:11:52.794
    STEP: Recreating the namespace 08/11/23 14:11:58.799
    STEP: Verifying there is no service in the namespace 08/11/23 14:11:58.817
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:11:58.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-7777" for this suite. 08/11/23 14:11:58.825
    STEP: Destroying namespace "nsdeletetest-2999" for this suite. 08/11/23 14:11:58.832
    Aug 11 14:11:58.835: INFO: Namespace nsdeletetest-2999 was already deleted
    STEP: Destroying namespace "nsdeletetest-2132" for this suite. 08/11/23 14:11:58.836
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:11:58.846
Aug 11 14:11:58.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename dns 08/11/23 14:11:58.847
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:58.867
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:58.869
[It] should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248
STEP: Creating a test headless service 08/11/23 14:11:58.872
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9705.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9705.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
 08/11/23 14:11:58.879
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9705.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9705.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
 08/11/23 14:11:58.879
STEP: creating a pod to probe DNS 08/11/23 14:11:58.879
STEP: submitting the pod to kubernetes 08/11/23 14:11:58.879
Aug 11 14:11:58.891: INFO: Waiting up to 15m0s for pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0" in namespace "dns-9705" to be "running"
Aug 11 14:11:58.898: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.810322ms
Aug 11 14:12:00.903: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012451995s
Aug 11 14:12:02.903: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011909093s
Aug 11 14:12:04.904: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013206151s
Aug 11 14:12:06.902: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Running", Reason="", readiness=true. Elapsed: 8.011687021s
Aug 11 14:12:06.903: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0" satisfied condition "running"
STEP: retrieving the pod 08/11/23 14:12:06.903
STEP: looking for the results for each expected name from probers 08/11/23 14:12:06.907
Aug 11 14:12:06.946: INFO: DNS probes using dns-9705/dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0 succeeded

STEP: deleting the pod 08/11/23 14:12:06.946
STEP: deleting the test headless service 08/11/23 14:12:06.963
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 11 14:12:06.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9705" for this suite. 08/11/23 14:12:06.982
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Hostname [Conformance]","completed":51,"skipped":922,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.144 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Hostname [Conformance]
  test/e2e/network/dns.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:11:58.846
    Aug 11 14:11:58.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename dns 08/11/23 14:11:58.847
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:11:58.867
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:11:58.869
    [It] should provide DNS for pods for Hostname [Conformance]
      test/e2e/network/dns.go:248
    STEP: Creating a test headless service 08/11/23 14:11:58.872
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9705.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-9705.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
     08/11/23 14:11:58.879
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-9705.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-9705.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
     08/11/23 14:11:58.879
    STEP: creating a pod to probe DNS 08/11/23 14:11:58.879
    STEP: submitting the pod to kubernetes 08/11/23 14:11:58.879
    Aug 11 14:11:58.891: INFO: Waiting up to 15m0s for pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0" in namespace "dns-9705" to be "running"
    Aug 11 14:11:58.898: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.810322ms
    Aug 11 14:12:00.903: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012451995s
    Aug 11 14:12:02.903: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.011909093s
    Aug 11 14:12:04.904: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.013206151s
    Aug 11 14:12:06.902: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0": Phase="Running", Reason="", readiness=true. Elapsed: 8.011687021s
    Aug 11 14:12:06.903: INFO: Pod "dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 14:12:06.903
    STEP: looking for the results for each expected name from probers 08/11/23 14:12:06.907
    Aug 11 14:12:06.946: INFO: DNS probes using dns-9705/dns-test-c9cba9ef-5bb8-42eb-9a2a-f838a00f23e0 succeeded

    STEP: deleting the pod 08/11/23 14:12:06.946
    STEP: deleting the test headless service 08/11/23 14:12:06.963
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 11 14:12:06.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9705" for this suite. 08/11/23 14:12:06.982
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:12:06.992
Aug 11 14:12:06.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:12:06.993
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:07.01
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:07.013
[It] removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441
STEP: set up a multi version CRD 08/11/23 14:12:07.016
Aug 11 14:12:07.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: mark a version not serverd 08/11/23 14:12:15.002
STEP: check the unserved version gets removed 08/11/23 14:12:15.019
STEP: check the other version is not changed 08/11/23 14:12:18.131
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:12:24.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-8728" for this suite. 08/11/23 14:12:24.621
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]","completed":52,"skipped":940,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.635 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  removes definition from spec when one version gets changed to not be served [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:441

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:12:06.992
    Aug 11 14:12:06.993: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:12:06.993
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:07.01
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:07.013
    [It] removes definition from spec when one version gets changed to not be served [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:441
    STEP: set up a multi version CRD 08/11/23 14:12:07.016
    Aug 11 14:12:07.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: mark a version not serverd 08/11/23 14:12:15.002
    STEP: check the unserved version gets removed 08/11/23 14:12:15.019
    STEP: check the other version is not changed 08/11/23 14:12:18.131
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:12:24.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-8728" for this suite. 08/11/23 14:12:24.621
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:12:24.628
Aug 11 14:12:24.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename init-container 08/11/23 14:12:24.63
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:24.644
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:24.646
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176
STEP: creating the pod 08/11/23 14:12:24.649
Aug 11 14:12:24.649: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 11 14:12:28.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-564" for this suite. 08/11/23 14:12:28.943
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]","completed":53,"skipped":946,"failed":0}
------------------------------
â€¢ [4.321 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:12:24.628
    Aug 11 14:12:24.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename init-container 08/11/23 14:12:24.63
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:24.644
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:24.646
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:176
    STEP: creating the pod 08/11/23 14:12:24.649
    Aug 11 14:12:24.649: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 11 14:12:28.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-564" for this suite. 08/11/23 14:12:28.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:12:28.952
Aug 11 14:12:28.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:12:28.953
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:28.968
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:28.972
[It] works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308
STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/11/23 14:12:28.975
Aug 11 14:12:28.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/11/23 14:12:40.906
Aug 11 14:12:40.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:12:44.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:12:55.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7162" for this suite. 08/11/23 14:12:55.308
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]","completed":54,"skipped":1015,"failed":0}
------------------------------
â€¢ [SLOW TEST] [26.364 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group but different versions [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:308

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:12:28.952
    Aug 11 14:12:28.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:12:28.953
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:28.968
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:28.972
    [It] works for multiple CRDs of same group but different versions [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:308
    STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation 08/11/23 14:12:28.975
    Aug 11 14:12:28.975: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation 08/11/23 14:12:40.906
    Aug 11 14:12:40.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:12:44.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:12:55.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7162" for this suite. 08/11/23 14:12:55.308
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class
  should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
[BeforeEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:12:55.318
Aug 11 14:12:55.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 14:12:55.319
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:55.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:55.339
[BeforeEach] Pods Set QOS Class
  test/e2e/node/pods.go:152
[It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
  test/e2e/node/pods.go:161
STEP: creating the pod 08/11/23 14:12:55.341
STEP: submitting the pod to kubernetes 08/11/23 14:12:55.341
STEP: verifying QOS class is set on the pod 08/11/23 14:12:55.352
[AfterEach] [sig-node] Pods Extended
  test/e2e/framework/framework.go:187
Aug 11 14:12:55.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6705" for this suite. 08/11/23 14:12:55.363
{"msg":"PASSED [sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]","completed":55,"skipped":1048,"failed":0}
------------------------------
â€¢ [0.054 seconds]
[sig-node] Pods Extended
test/e2e/node/framework.go:23
  Pods Set QOS Class
  test/e2e/node/pods.go:150
    should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
    test/e2e/node/pods.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:12:55.318
    Aug 11 14:12:55.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 14:12:55.319
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:55.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:55.339
    [BeforeEach] Pods Set QOS Class
      test/e2e/node/pods.go:152
    [It] should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
      test/e2e/node/pods.go:161
    STEP: creating the pod 08/11/23 14:12:55.341
    STEP: submitting the pod to kubernetes 08/11/23 14:12:55.341
    STEP: verifying QOS class is set on the pod 08/11/23 14:12:55.352
    [AfterEach] [sig-node] Pods Extended
      test/e2e/framework/framework.go:187
    Aug 11 14:12:55.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6705" for this suite. 08/11/23 14:12:55.363
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:12:55.372
Aug 11 14:12:55.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:12:55.373
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:55.388
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:55.391
[It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/11/23 14:12:55.393
Aug 11 14:12:55.403: INFO: Waiting up to 5m0s for pod "pod-c336c133-825d-4030-be96-acab684f0115" in namespace "emptydir-2004" to be "Succeeded or Failed"
Aug 11 14:12:55.408: INFO: Pod "pod-c336c133-825d-4030-be96-acab684f0115": Phase="Pending", Reason="", readiness=false. Elapsed: 4.81727ms
Aug 11 14:12:57.412: INFO: Pod "pod-c336c133-825d-4030-be96-acab684f0115": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008880923s
Aug 11 14:12:59.414: INFO: Pod "pod-c336c133-825d-4030-be96-acab684f0115": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01114408s
STEP: Saw pod success 08/11/23 14:12:59.414
Aug 11 14:12:59.415: INFO: Pod "pod-c336c133-825d-4030-be96-acab684f0115" satisfied condition "Succeeded or Failed"
Aug 11 14:12:59.419: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-c336c133-825d-4030-be96-acab684f0115 container test-container: <nil>
STEP: delete the pod 08/11/23 14:12:59.438
Aug 11 14:12:59.454: INFO: Waiting for pod pod-c336c133-825d-4030-be96-acab684f0115 to disappear
Aug 11 14:12:59.457: INFO: Pod pod-c336c133-825d-4030-be96-acab684f0115 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 14:12:59.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2004" for this suite. 08/11/23 14:12:59.461
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":56,"skipped":1052,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:12:55.372
    Aug 11 14:12:55.372: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:12:55.373
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:55.388
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:55.391
    [It] should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:116
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/11/23 14:12:55.393
    Aug 11 14:12:55.403: INFO: Waiting up to 5m0s for pod "pod-c336c133-825d-4030-be96-acab684f0115" in namespace "emptydir-2004" to be "Succeeded or Failed"
    Aug 11 14:12:55.408: INFO: Pod "pod-c336c133-825d-4030-be96-acab684f0115": Phase="Pending", Reason="", readiness=false. Elapsed: 4.81727ms
    Aug 11 14:12:57.412: INFO: Pod "pod-c336c133-825d-4030-be96-acab684f0115": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008880923s
    Aug 11 14:12:59.414: INFO: Pod "pod-c336c133-825d-4030-be96-acab684f0115": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01114408s
    STEP: Saw pod success 08/11/23 14:12:59.414
    Aug 11 14:12:59.415: INFO: Pod "pod-c336c133-825d-4030-be96-acab684f0115" satisfied condition "Succeeded or Failed"
    Aug 11 14:12:59.419: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-c336c133-825d-4030-be96-acab684f0115 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:12:59.438
    Aug 11 14:12:59.454: INFO: Waiting for pod pod-c336c133-825d-4030-be96-acab684f0115 to disappear
    Aug 11 14:12:59.457: INFO: Pod pod-c336c133-825d-4030-be96-acab684f0115 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:12:59.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2004" for this suite. 08/11/23 14:12:59.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:12:59.468
Aug 11 14:12:59.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename containers 08/11/23 14:12:59.469
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:59.483
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:59.486
[It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38
Aug 11 14:12:59.499: INFO: Waiting up to 5m0s for pod "client-containers-f33e39d9-9dd2-4045-9f32-9c9261bda82f" in namespace "containers-3366" to be "running"
Aug 11 14:12:59.504: INFO: Pod "client-containers-f33e39d9-9dd2-4045-9f32-9c9261bda82f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.695686ms
Aug 11 14:13:01.508: INFO: Pod "client-containers-f33e39d9-9dd2-4045-9f32-9c9261bda82f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009063037s
Aug 11 14:13:01.508: INFO: Pod "client-containers-f33e39d9-9dd2-4045-9f32-9c9261bda82f" satisfied condition "running"
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Aug 11 14:13:01.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-3366" for this suite. 08/11/23 14:13:01.525
{"msg":"PASSED [sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]","completed":57,"skipped":1069,"failed":0}
------------------------------
â€¢ [2.063 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should use the image defaults if command and args are blank [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:12:59.468
    Aug 11 14:12:59.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename containers 08/11/23 14:12:59.469
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:12:59.483
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:12:59.486
    [It] should use the image defaults if command and args are blank [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:38
    Aug 11 14:12:59.499: INFO: Waiting up to 5m0s for pod "client-containers-f33e39d9-9dd2-4045-9f32-9c9261bda82f" in namespace "containers-3366" to be "running"
    Aug 11 14:12:59.504: INFO: Pod "client-containers-f33e39d9-9dd2-4045-9f32-9c9261bda82f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.695686ms
    Aug 11 14:13:01.508: INFO: Pod "client-containers-f33e39d9-9dd2-4045-9f32-9c9261bda82f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009063037s
    Aug 11 14:13:01.508: INFO: Pod "client-containers-f33e39d9-9dd2-4045-9f32-9c9261bda82f" satisfied condition "running"
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Aug 11 14:13:01.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-3366" for this suite. 08/11/23 14:13:01.525
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:13:01.536
Aug 11 14:13:01.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:13:01.536
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:13:01.553
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:13:01.555
[It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:145
Aug 11 14:13:01.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:13:02.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-1316" for this suite. 08/11/23 14:13:02.114
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]","completed":58,"skipped":1186,"failed":0}
------------------------------
â€¢ [0.588 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    getting/updating/patching custom resource definition status sub-resource works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:145

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:13:01.536
    Aug 11 14:13:01.536: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:13:01.536
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:13:01.553
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:13:01.555
    [It] getting/updating/patching custom resource definition status sub-resource works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:145
    Aug 11 14:13:01.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:13:02.104: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-1316" for this suite. 08/11/23 14:13:02.114
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:13:02.124
Aug 11 14:13:02.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:13:02.125
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:13:02.139
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:13:02.142
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123
STEP: Creating configMap with name configmap-test-upd-da4f3827-0488-42c7-956f-76bb0abf6a2a 08/11/23 14:13:02.148
STEP: Creating the pod 08/11/23 14:13:02.154
Aug 11 14:13:02.164: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409" in namespace "configmap-9343" to be "running and ready"
Aug 11 14:13:02.169: INFO: Pod "pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877542ms
Aug 11 14:13:02.169: INFO: The phase of Pod pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:13:04.174: INFO: Pod "pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409": Phase="Running", Reason="", readiness=true. Elapsed: 2.010188554s
Aug 11 14:13:04.174: INFO: The phase of Pod pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409 is Running (Ready = true)
Aug 11 14:13:04.174: INFO: Pod "pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409" satisfied condition "running and ready"
STEP: Updating configmap configmap-test-upd-da4f3827-0488-42c7-956f-76bb0abf6a2a 08/11/23 14:13:04.199
STEP: waiting to observe update in volume 08/11/23 14:13:04.205
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:13:06.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9343" for this suite. 08/11/23 14:13:06.23
{"msg":"PASSED [sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":59,"skipped":1195,"failed":0}
------------------------------
â€¢ [4.113 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:13:02.124
    Aug 11 14:13:02.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:13:02.125
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:13:02.139
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:13:02.142
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:123
    STEP: Creating configMap with name configmap-test-upd-da4f3827-0488-42c7-956f-76bb0abf6a2a 08/11/23 14:13:02.148
    STEP: Creating the pod 08/11/23 14:13:02.154
    Aug 11 14:13:02.164: INFO: Waiting up to 5m0s for pod "pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409" in namespace "configmap-9343" to be "running and ready"
    Aug 11 14:13:02.169: INFO: Pod "pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409": Phase="Pending", Reason="", readiness=false. Elapsed: 4.877542ms
    Aug 11 14:13:02.169: INFO: The phase of Pod pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:13:04.174: INFO: Pod "pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409": Phase="Running", Reason="", readiness=true. Elapsed: 2.010188554s
    Aug 11 14:13:04.174: INFO: The phase of Pod pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409 is Running (Ready = true)
    Aug 11 14:13:04.174: INFO: Pod "pod-configmaps-a3bccaa6-3f74-4458-b4fd-6426903b6409" satisfied condition "running and ready"
    STEP: Updating configmap configmap-test-upd-da4f3827-0488-42c7-956f-76bb0abf6a2a 08/11/23 14:13:04.199
    STEP: waiting to observe update in volume 08/11/23 14:13:04.205
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:13:06.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9343" for this suite. 08/11/23 14:13:06.23
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:13:06.24
Aug 11 14:13:06.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:13:06.24
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:13:06.257
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:13:06.26
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 11 14:13:06.279: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 14:14:06.327: INFO: Waiting for terminating namespaces to be deleted...
[It] validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125
STEP: Create pods that use 4/5 of node resources. 08/11/23 14:14:06.331
Aug 11 14:14:06.354: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 11 14:14:06.365: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 11 14:14:06.382: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 11 14:14:06.391: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/11/23 14:14:06.391
Aug 11 14:14:06.391: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9855" to be "running"
Aug 11 14:14:06.398: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07464ms
Aug 11 14:14:08.403: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012340931s
Aug 11 14:14:10.402: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.011584644s
Aug 11 14:14:10.402: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 11 14:14:10.402: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9855" to be "running"
Aug 11 14:14:10.406: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.346935ms
Aug 11 14:14:10.406: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 11 14:14:10.406: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9855" to be "running"
Aug 11 14:14:10.410: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.795978ms
Aug 11 14:14:10.410: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 11 14:14:10.410: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9855" to be "running"
Aug 11 14:14:10.413: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.323944ms
Aug 11 14:14:10.413: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/11/23 14:14:10.413
Aug 11 14:14:10.420: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9855" to be "running"
Aug 11 14:14:10.423: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.955032ms
Aug 11 14:14:12.428: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007109137s
Aug 11 14:14:14.430: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009193898s
Aug 11 14:14:14.430: INFO: Pod "preemptor-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:14:14.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-9855" for this suite. 08/11/23 14:14:14.448
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]","completed":60,"skipped":1233,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.252 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates basic preemption works [Conformance]
  test/e2e/scheduling/preemption.go:125

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:13:06.24
    Aug 11 14:13:06.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:13:06.24
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:13:06.257
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:13:06.26
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Aug 11 14:13:06.279: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 14:14:06.327: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates basic preemption works [Conformance]
      test/e2e/scheduling/preemption.go:125
    STEP: Create pods that use 4/5 of node resources. 08/11/23 14:14:06.331
    Aug 11 14:14:06.354: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 11 14:14:06.365: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 11 14:14:06.382: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 11 14:14:06.391: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/11/23 14:14:06.391
    Aug 11 14:14:06.391: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-9855" to be "running"
    Aug 11 14:14:06.398: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 7.07464ms
    Aug 11 14:14:08.403: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012340931s
    Aug 11 14:14:10.402: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.011584644s
    Aug 11 14:14:10.402: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 11 14:14:10.402: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-9855" to be "running"
    Aug 11 14:14:10.406: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.346935ms
    Aug 11 14:14:10.406: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 11 14:14:10.406: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-9855" to be "running"
    Aug 11 14:14:10.410: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.795978ms
    Aug 11 14:14:10.410: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 11 14:14:10.410: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-9855" to be "running"
    Aug 11 14:14:10.413: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.323944ms
    Aug 11 14:14:10.413: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a high priority pod that has same requirements as that of lower priority pod 08/11/23 14:14:10.413
    Aug 11 14:14:10.420: INFO: Waiting up to 2m0s for pod "preemptor-pod" in namespace "sched-preemption-9855" to be "running"
    Aug 11 14:14:10.423: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.955032ms
    Aug 11 14:14:12.428: INFO: Pod "preemptor-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007109137s
    Aug 11 14:14:14.430: INFO: Pod "preemptor-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.009193898s
    Aug 11 14:14:14.430: INFO: Pod "preemptor-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:14:14.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-9855" for this suite. 08/11/23 14:14:14.448
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:14:14.493
Aug 11 14:14:14.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-probe 08/11/23 14:14:14.494
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:14:14.509
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:14:14.511
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195
STEP: Creating pod liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 in namespace container-probe-9702 08/11/23 14:14:14.514
Aug 11 14:14:14.522: INFO: Waiting up to 5m0s for pod "liveness-946d2da6-dd41-4c67-8f80-3e9b66316949" in namespace "container-probe-9702" to be "not pending"
Aug 11 14:14:14.527: INFO: Pod "liveness-946d2da6-dd41-4c67-8f80-3e9b66316949": Phase="Pending", Reason="", readiness=false. Elapsed: 4.147489ms
Aug 11 14:14:16.531: INFO: Pod "liveness-946d2da6-dd41-4c67-8f80-3e9b66316949": Phase="Running", Reason="", readiness=true. Elapsed: 2.008539312s
Aug 11 14:14:16.531: INFO: Pod "liveness-946d2da6-dd41-4c67-8f80-3e9b66316949" satisfied condition "not pending"
Aug 11 14:14:16.531: INFO: Started pod liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 in namespace container-probe-9702
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:14:16.531
Aug 11 14:14:16.534: INFO: Initial restart count of pod liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is 0
Aug 11 14:14:36.590: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 1 (20.05571259s elapsed)
Aug 11 14:14:56.642: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 2 (40.107331215s elapsed)
Aug 11 14:15:16.696: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 3 (1m0.162280869s elapsed)
Aug 11 14:15:36.752: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 4 (1m20.218119084s elapsed)
Aug 11 14:16:46.934: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 5 (2m30.399958487s elapsed)
STEP: deleting the pod 08/11/23 14:16:46.934
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 11 14:16:46.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9702" for this suite. 08/11/23 14:16:46.957
{"msg":"PASSED [sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]","completed":61,"skipped":1240,"failed":0}
------------------------------
â€¢ [SLOW TEST] [152.473 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should have monotonically increasing restart count [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:195

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:14:14.493
    Aug 11 14:14:14.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:14:14.494
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:14:14.509
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:14:14.511
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should have monotonically increasing restart count [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:195
    STEP: Creating pod liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 in namespace container-probe-9702 08/11/23 14:14:14.514
    Aug 11 14:14:14.522: INFO: Waiting up to 5m0s for pod "liveness-946d2da6-dd41-4c67-8f80-3e9b66316949" in namespace "container-probe-9702" to be "not pending"
    Aug 11 14:14:14.527: INFO: Pod "liveness-946d2da6-dd41-4c67-8f80-3e9b66316949": Phase="Pending", Reason="", readiness=false. Elapsed: 4.147489ms
    Aug 11 14:14:16.531: INFO: Pod "liveness-946d2da6-dd41-4c67-8f80-3e9b66316949": Phase="Running", Reason="", readiness=true. Elapsed: 2.008539312s
    Aug 11 14:14:16.531: INFO: Pod "liveness-946d2da6-dd41-4c67-8f80-3e9b66316949" satisfied condition "not pending"
    Aug 11 14:14:16.531: INFO: Started pod liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 in namespace container-probe-9702
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:14:16.531
    Aug 11 14:14:16.534: INFO: Initial restart count of pod liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is 0
    Aug 11 14:14:36.590: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 1 (20.05571259s elapsed)
    Aug 11 14:14:56.642: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 2 (40.107331215s elapsed)
    Aug 11 14:15:16.696: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 3 (1m0.162280869s elapsed)
    Aug 11 14:15:36.752: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 4 (1m20.218119084s elapsed)
    Aug 11 14:16:46.934: INFO: Restart count of pod container-probe-9702/liveness-946d2da6-dd41-4c67-8f80-3e9b66316949 is now 5 (2m30.399958487s elapsed)
    STEP: deleting the pod 08/11/23 14:16:46.934
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 11 14:16:46.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9702" for this suite. 08/11/23 14:16:46.957
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:16:46.967
Aug 11 14:16:46.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replication-controller 08/11/23 14:16:46.968
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:46.986
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:46.99
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100
STEP: Given a ReplicationController is created 08/11/23 14:16:46.992
STEP: When the matched label of one of its pods change 08/11/23 14:16:46.999
Aug 11 14:16:47.003: INFO: Pod name pod-release: Found 0 pods out of 1
Aug 11 14:16:52.008: INFO: Pod name pod-release: Found 1 pods out of 1
STEP: Then the pod is released 08/11/23 14:16:52.019
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 11 14:16:53.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-212" for this suite. 08/11/23 14:16:53.031
{"msg":"PASSED [sig-apps] ReplicationController should release no longer matching pods [Conformance]","completed":62,"skipped":1252,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.071 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should release no longer matching pods [Conformance]
  test/e2e/apps/rc.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:16:46.967
    Aug 11 14:16:46.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replication-controller 08/11/23 14:16:46.968
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:46.986
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:46.99
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should release no longer matching pods [Conformance]
      test/e2e/apps/rc.go:100
    STEP: Given a ReplicationController is created 08/11/23 14:16:46.992
    STEP: When the matched label of one of its pods change 08/11/23 14:16:46.999
    Aug 11 14:16:47.003: INFO: Pod name pod-release: Found 0 pods out of 1
    Aug 11 14:16:52.008: INFO: Pod name pod-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/11/23 14:16:52.019
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 11 14:16:53.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-212" for this suite. 08/11/23 14:16:53.031
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:16:53.04
Aug 11 14:16:53.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:16:53.041
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:53.057
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:53.06
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822
STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:16:53.08
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:16:53.086
Aug 11 14:16:53.092: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:16:53.092: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:16:53.093: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:16:53.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:16:53.096: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:16:54.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:16:54.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:16:54.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:16:54.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:16:54.106: INFO: Node constell-d93e7e1d-worker-d314547c-wzlp is running 0 daemon pod, expected 1
Aug 11 14:16:55.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:16:55.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:16:55.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:16:55.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:16:55.106: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: listing all DeamonSets 08/11/23 14:16:55.109
STEP: DeleteCollection of the DaemonSets 08/11/23 14:16:55.113
STEP: Verify that ReplicaSets have been deleted 08/11/23 14:16:55.122
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
Aug 11 14:16:55.136: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14864"},"items":null}

Aug 11 14:16:55.140: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14864"},"items":[{"metadata":{"name":"daemon-set-6mkjj","generateName":"daemon-set-","namespace":"daemonsets-2","uid":"5836534b-8a70-41a6-8e84-dd85ffd7aeaf","resourceVersion":"14864","creationTimestamp":"2023-08-11T14:16:53Z","deletionTimestamp":"2023-08-11T14:17:25Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"da6932d7-39f0-477c-87d0-04dbeb51c384","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:16:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da6932d7-39f0-477c-87d0-04dbeb51c384\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:16:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5t5sw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5t5sw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"constell-d93e7e1d-worker-d314547c-0lc3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["constell-d93e7e1d-worker-d314547c-0lc3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"}],"hostIP":"192.168.178.2","podIP":"10.10.0.231","podIPs":[{"ip":"10.10.0.231"}],"startTime":"2023-08-11T14:16:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-11T14:16:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f10a9fe0384edaff6378bfa5b711da8112f8b5422975f811fca081c93d339b2f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w8rlm","generateName":"daemon-set-","namespace":"daemonsets-2","uid":"e52aa809-c62e-4b9e-9fea-abfb5a1c77ac","resourceVersion":"14863","creationTimestamp":"2023-08-11T14:16:53Z","deletionTimestamp":"2023-08-11T14:17:25Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"da6932d7-39f0-477c-87d0-04dbeb51c384","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:16:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da6932d7-39f0-477c-87d0-04dbeb51c384\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:16:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kdkxx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kdkxx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"constell-d93e7e1d-worker-d314547c-wzlp","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["constell-d93e7e1d-worker-d314547c-wzlp"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"}],"hostIP":"192.168.178.3","podIP":"10.10.1.1","podIPs":[{"ip":"10.10.1.1"}],"startTime":"2023-08-11T14:16:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-11T14:16:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b571ba07158f293227cb41cdfc8f71b4fe1ea002b246126f92e6ab5f5207d533","started":true}],"qosClass":"BestEffort"}}]}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:16:55.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-2" for this suite. 08/11/23 14:16:55.154
{"msg":"PASSED [sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]","completed":63,"skipped":1293,"failed":0}
------------------------------
â€¢ [2.121 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should list and delete a collection of DaemonSets [Conformance]
  test/e2e/apps/daemon_set.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:16:53.04
    Aug 11 14:16:53.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:16:53.041
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:53.057
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:53.06
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should list and delete a collection of DaemonSets [Conformance]
      test/e2e/apps/daemon_set.go:822
    STEP: Creating simple DaemonSet "daemon-set" 08/11/23 14:16:53.08
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:16:53.086
    Aug 11 14:16:53.092: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:16:53.092: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:16:53.093: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:16:53.096: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:16:53.096: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:16:54.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:16:54.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:16:54.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:16:54.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:16:54.106: INFO: Node constell-d93e7e1d-worker-d314547c-wzlp is running 0 daemon pod, expected 1
    Aug 11 14:16:55.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:16:55.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:16:55.102: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:16:55.106: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:16:55.106: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: listing all DeamonSets 08/11/23 14:16:55.109
    STEP: DeleteCollection of the DaemonSets 08/11/23 14:16:55.113
    STEP: Verify that ReplicaSets have been deleted 08/11/23 14:16:55.122
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    Aug 11 14:16:55.136: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"14864"},"items":null}

    Aug 11 14:16:55.140: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"14864"},"items":[{"metadata":{"name":"daemon-set-6mkjj","generateName":"daemon-set-","namespace":"daemonsets-2","uid":"5836534b-8a70-41a6-8e84-dd85ffd7aeaf","resourceVersion":"14864","creationTimestamp":"2023-08-11T14:16:53Z","deletionTimestamp":"2023-08-11T14:17:25Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"da6932d7-39f0-477c-87d0-04dbeb51c384","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:16:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da6932d7-39f0-477c-87d0-04dbeb51c384\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:16:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.231\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5t5sw","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5t5sw","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"constell-d93e7e1d-worker-d314547c-0lc3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["constell-d93e7e1d-worker-d314547c-0lc3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"}],"hostIP":"192.168.178.2","podIP":"10.10.0.231","podIPs":[{"ip":"10.10.0.231"}],"startTime":"2023-08-11T14:16:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-11T14:16:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://f10a9fe0384edaff6378bfa5b711da8112f8b5422975f811fca081c93d339b2f","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-w8rlm","generateName":"daemon-set-","namespace":"daemonsets-2","uid":"e52aa809-c62e-4b9e-9fea-abfb5a1c77ac","resourceVersion":"14863","creationTimestamp":"2023-08-11T14:16:53Z","deletionTimestamp":"2023-08-11T14:17:25Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"7f7ffb4fcc","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"da6932d7-39f0-477c-87d0-04dbeb51c384","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:16:53Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da6932d7-39f0-477c-87d0-04dbeb51c384\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-11T14:16:54Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.1\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-kdkxx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-kdkxx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"constell-d93e7e1d-worker-d314547c-wzlp","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["constell-d93e7e1d-worker-d314547c-wzlp"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:54Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:54Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-11T14:16:53Z"}],"hostIP":"192.168.178.3","podIP":"10.10.1.1","podIPs":[{"ip":"10.10.1.1"}],"startTime":"2023-08-11T14:16:53Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-11T14:16:53Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-2","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3","containerID":"containerd://b571ba07158f293227cb41cdfc8f71b4fe1ea002b246126f92e6ab5f5207d533","started":true}],"qosClass":"BestEffort"}}]}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:16:55.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-2" for this suite. 08/11/23 14:16:55.154
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] PodTemplates
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:16:55.161
Aug 11 14:16:55.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename podtemplate 08/11/23 14:16:55.162
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:55.178
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:55.181
[It] should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176
STEP: Create a pod template 08/11/23 14:16:55.183
STEP: Replace a pod template 08/11/23 14:16:55.189
Aug 11 14:16:55.199: INFO: Found updated podtemplate annotation: "true"

[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Aug 11 14:16:55.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-1588" for this suite. 08/11/23 14:16:55.203
{"msg":"PASSED [sig-node] PodTemplates should replace a pod template [Conformance]","completed":64,"skipped":1293,"failed":0}
------------------------------
â€¢ [0.050 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should replace a pod template [Conformance]
  test/e2e/common/node/podtemplates.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:16:55.161
    Aug 11 14:16:55.161: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename podtemplate 08/11/23 14:16:55.162
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:55.178
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:55.181
    [It] should replace a pod template [Conformance]
      test/e2e/common/node/podtemplates.go:176
    STEP: Create a pod template 08/11/23 14:16:55.183
    STEP: Replace a pod template 08/11/23 14:16:55.189
    Aug 11 14:16:55.199: INFO: Found updated podtemplate annotation: "true"

    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Aug 11 14:16:55.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-1588" for this suite. 08/11/23 14:16:55.203
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:16:55.213
Aug 11 14:16:55.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename job 08/11/23 14:16:55.214
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:55.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:55.233
[It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254
STEP: Creating a job 08/11/23 14:16:55.235
STEP: Ensuring job reaches completions 08/11/23 14:16:55.243
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 11 14:17:07.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7466" for this suite. 08/11/23 14:17:07.252
{"msg":"PASSED [sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]","completed":65,"skipped":1312,"failed":0}
------------------------------
â€¢ [SLOW TEST] [12.046 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
  test/e2e/apps/job.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:16:55.213
    Aug 11 14:16:55.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename job 08/11/23 14:16:55.214
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:16:55.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:16:55.233
    [It] should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
      test/e2e/apps/job.go:254
    STEP: Creating a job 08/11/23 14:16:55.235
    STEP: Ensuring job reaches completions 08/11/23 14:16:55.243
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 11 14:17:07.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7466" for this suite. 08/11/23 14:17:07.252
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:17:07.262
Aug 11 14:17:07.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-probe 08/11/23 14:17:07.263
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:07.278
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:07.281
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148
STEP: Creating pod busybox-56026f8c-3709-4e9f-a318-3230256e6f79 in namespace container-probe-9269 08/11/23 14:17:07.283
Aug 11 14:17:07.293: INFO: Waiting up to 5m0s for pod "busybox-56026f8c-3709-4e9f-a318-3230256e6f79" in namespace "container-probe-9269" to be "not pending"
Aug 11 14:17:07.297: INFO: Pod "busybox-56026f8c-3709-4e9f-a318-3230256e6f79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.096907ms
Aug 11 14:17:09.303: INFO: Pod "busybox-56026f8c-3709-4e9f-a318-3230256e6f79": Phase="Running", Reason="", readiness=true. Elapsed: 2.009647257s
Aug 11 14:17:09.303: INFO: Pod "busybox-56026f8c-3709-4e9f-a318-3230256e6f79" satisfied condition "not pending"
Aug 11 14:17:09.303: INFO: Started pod busybox-56026f8c-3709-4e9f-a318-3230256e6f79 in namespace container-probe-9269
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:17:09.303
Aug 11 14:17:09.306: INFO: Initial restart count of pod busybox-56026f8c-3709-4e9f-a318-3230256e6f79 is 0
STEP: deleting the pod 08/11/23 14:21:09.918
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 11 14:21:09.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-9269" for this suite. 08/11/23 14:21:09.945
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":66,"skipped":1364,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.691 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:17:07.262
    Aug 11 14:17:07.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:17:07.263
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:17:07.278
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:17:07.281
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:148
    STEP: Creating pod busybox-56026f8c-3709-4e9f-a318-3230256e6f79 in namespace container-probe-9269 08/11/23 14:17:07.283
    Aug 11 14:17:07.293: INFO: Waiting up to 5m0s for pod "busybox-56026f8c-3709-4e9f-a318-3230256e6f79" in namespace "container-probe-9269" to be "not pending"
    Aug 11 14:17:07.297: INFO: Pod "busybox-56026f8c-3709-4e9f-a318-3230256e6f79": Phase="Pending", Reason="", readiness=false. Elapsed: 4.096907ms
    Aug 11 14:17:09.303: INFO: Pod "busybox-56026f8c-3709-4e9f-a318-3230256e6f79": Phase="Running", Reason="", readiness=true. Elapsed: 2.009647257s
    Aug 11 14:17:09.303: INFO: Pod "busybox-56026f8c-3709-4e9f-a318-3230256e6f79" satisfied condition "not pending"
    Aug 11 14:17:09.303: INFO: Started pod busybox-56026f8c-3709-4e9f-a318-3230256e6f79 in namespace container-probe-9269
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 14:17:09.303
    Aug 11 14:17:09.306: INFO: Initial restart count of pod busybox-56026f8c-3709-4e9f-a318-3230256e6f79 is 0
    STEP: deleting the pod 08/11/23 14:21:09.918
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 11 14:21:09.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-9269" for this suite. 08/11/23 14:21:09.945
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:21:09.955
Aug 11 14:21:09.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:21:09.955
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:21:09.972
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:21:09.976
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:21:09.992
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:21:10.361
STEP: Deploying the webhook pod 08/11/23 14:21:10.37
STEP: Wait for the deployment to be ready 08/11/23 14:21:10.386
Aug 11 14:21:10.396: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:21:12.411
STEP: Verifying the service has paired with the endpoint 08/11/23 14:21:12.429
Aug 11 14:21:13.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220
Aug 11 14:21:13.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/11/23 14:21:13.947
STEP: Creating a custom resource that should be denied by the webhook 08/11/23 14:21:13.975
STEP: Creating a custom resource whose deletion would be denied by the webhook 08/11/23 14:21:16.018
STEP: Updating the custom resource with disallowed data should be denied 08/11/23 14:21:16.03
STEP: Deleting the custom resource should be denied 08/11/23 14:21:16.043
STEP: Remove the offending key and value from the custom resource data 08/11/23 14:21:16.054
STEP: Deleting the updated custom resource should be successful 08/11/23 14:21:16.068
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:21:16.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8079" for this suite. 08/11/23 14:21:16.605
STEP: Destroying namespace "webhook-8079-markers" for this suite. 08/11/23 14:21:16.611
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]","completed":67,"skipped":1393,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.739 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny custom resource creation, update and deletion [Conformance]
  test/e2e/apimachinery/webhook.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:21:09.955
    Aug 11 14:21:09.955: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:21:09.955
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:21:09.972
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:21:09.976
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:21:09.992
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:21:10.361
    STEP: Deploying the webhook pod 08/11/23 14:21:10.37
    STEP: Wait for the deployment to be ready 08/11/23 14:21:10.386
    Aug 11 14:21:10.396: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:21:12.411
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:21:12.429
    Aug 11 14:21:13.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny custom resource creation, update and deletion [Conformance]
      test/e2e/apimachinery/webhook.go:220
    Aug 11 14:21:13.433: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Registering the custom resource webhook via the AdmissionRegistration API 08/11/23 14:21:13.947
    STEP: Creating a custom resource that should be denied by the webhook 08/11/23 14:21:13.975
    STEP: Creating a custom resource whose deletion would be denied by the webhook 08/11/23 14:21:16.018
    STEP: Updating the custom resource with disallowed data should be denied 08/11/23 14:21:16.03
    STEP: Deleting the custom resource should be denied 08/11/23 14:21:16.043
    STEP: Remove the offending key and value from the custom resource data 08/11/23 14:21:16.054
    STEP: Deleting the updated custom resource should be successful 08/11/23 14:21:16.068
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:21:16.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8079" for this suite. 08/11/23 14:21:16.605
    STEP: Destroying namespace "webhook-8079-markers" for this suite. 08/11/23 14:21:16.611
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:21:16.695
Aug 11 14:21:16.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename dns 08/11/23 14:21:16.696
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:21:16.717
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:21:16.72
[It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193
STEP: Creating a test headless service 08/11/23 14:21:16.724
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3122 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3122;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3122 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3122;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3122.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3122.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3122.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3122.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3122.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3122.svc;check="$$(dig +notcp +noall +answer +search 251.112.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.112.251_udp@PTR;check="$$(dig +tcp +noall +answer +search 251.112.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.112.251_tcp@PTR;sleep 1; done
 08/11/23 14:21:16.753
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3122 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3122;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3122 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3122;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3122.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3122.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3122.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3122.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3122.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3122.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3122.svc;check="$$(dig +notcp +noall +answer +search 251.112.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.112.251_udp@PTR;check="$$(dig +tcp +noall +answer +search 251.112.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.112.251_tcp@PTR;sleep 1; done
 08/11/23 14:21:16.753
STEP: creating a pod to probe DNS 08/11/23 14:21:16.753
STEP: submitting the pod to kubernetes 08/11/23 14:21:16.754
Aug 11 14:21:16.766: INFO: Waiting up to 15m0s for pod "dns-test-78940e77-9088-492d-849e-f7bd6fc084e8" in namespace "dns-3122" to be "running"
Aug 11 14:21:16.769: INFO: Pod "dns-test-78940e77-9088-492d-849e-f7bd6fc084e8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108827ms
Aug 11 14:21:18.775: INFO: Pod "dns-test-78940e77-9088-492d-849e-f7bd6fc084e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008799021s
Aug 11 14:21:18.775: INFO: Pod "dns-test-78940e77-9088-492d-849e-f7bd6fc084e8" satisfied condition "running"
STEP: retrieving the pod 08/11/23 14:21:18.775
STEP: looking for the results for each expected name from probers 08/11/23 14:21:18.779
Aug 11 14:21:18.794: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.801: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.808: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.815: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.822: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.830: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.837: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.846: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.885: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.893: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.900: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.907: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.915: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.924: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.932: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.941: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:18.968: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

Aug 11 14:21:23.979: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:23.986: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:23.992: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:23.999: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.005: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.012: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.018: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.024: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.056: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.062: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.068: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.075: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.082: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.088: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.094: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.101: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:24.127: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

Aug 11 14:21:28.978: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:28.984: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:28.990: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:28.998: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.005: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.013: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.020: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.057: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.065: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.071: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.083: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.089: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.095: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.102: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.108: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:29.133: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

Aug 11 14:21:33.977: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:33.984: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:33.991: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:33.997: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.003: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.010: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.016: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.059: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.065: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.071: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.078: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.084: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.091: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.097: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.103: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:34.129: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

Aug 11 14:21:38.976: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:38.983: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:38.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:38.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.002: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.009: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.016: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.055: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.062: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.072: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.079: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.087: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.100: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.106: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:39.134: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

Aug 11 14:21:43.979: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:43.986: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:43.992: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:43.998: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.005: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.012: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.019: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.060: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.066: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.073: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.086: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.101: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.107: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
Aug 11 14:21:44.132: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

Aug 11 14:21:49.134: INFO: DNS probes using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 succeeded

STEP: deleting the pod 08/11/23 14:21:49.134
STEP: deleting the test service 08/11/23 14:21:49.157
STEP: deleting the test headless service 08/11/23 14:21:49.192
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 11 14:21:49.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-3122" for this suite. 08/11/23 14:21:49.216
{"msg":"PASSED [sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]","completed":68,"skipped":1401,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.530 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
  test/e2e/network/dns.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:21:16.695
    Aug 11 14:21:16.695: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename dns 08/11/23 14:21:16.696
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:21:16.717
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:21:16.72
    [It] should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
      test/e2e/network/dns.go:193
    STEP: Creating a test headless service 08/11/23 14:21:16.724
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3122 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3122;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3122 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3122;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3122.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3122.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3122.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3122.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3122.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3122.svc;check="$$(dig +notcp +noall +answer +search 251.112.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.112.251_udp@PTR;check="$$(dig +tcp +noall +answer +search 251.112.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.112.251_tcp@PTR;sleep 1; done
     08/11/23 14:21:16.753
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3122 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3122;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3122 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3122;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3122.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3122.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3122.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3122.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3122.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3122.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3122.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3122.svc;check="$$(dig +notcp +noall +answer +search 251.112.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.112.251_udp@PTR;check="$$(dig +tcp +noall +answer +search 251.112.96.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.96.112.251_tcp@PTR;sleep 1; done
     08/11/23 14:21:16.753
    STEP: creating a pod to probe DNS 08/11/23 14:21:16.753
    STEP: submitting the pod to kubernetes 08/11/23 14:21:16.754
    Aug 11 14:21:16.766: INFO: Waiting up to 15m0s for pod "dns-test-78940e77-9088-492d-849e-f7bd6fc084e8" in namespace "dns-3122" to be "running"
    Aug 11 14:21:16.769: INFO: Pod "dns-test-78940e77-9088-492d-849e-f7bd6fc084e8": Phase="Pending", Reason="", readiness=false. Elapsed: 3.108827ms
    Aug 11 14:21:18.775: INFO: Pod "dns-test-78940e77-9088-492d-849e-f7bd6fc084e8": Phase="Running", Reason="", readiness=true. Elapsed: 2.008799021s
    Aug 11 14:21:18.775: INFO: Pod "dns-test-78940e77-9088-492d-849e-f7bd6fc084e8" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 14:21:18.775
    STEP: looking for the results for each expected name from probers 08/11/23 14:21:18.779
    Aug 11 14:21:18.794: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.801: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.808: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.815: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.822: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.830: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.837: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.846: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.885: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.893: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.900: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.907: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.915: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.924: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.932: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.941: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:18.968: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

    Aug 11 14:21:23.979: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:23.986: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:23.992: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:23.999: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.005: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.012: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.018: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.024: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.056: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.062: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.068: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.075: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.082: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.088: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.094: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.101: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:24.127: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

    Aug 11 14:21:28.978: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:28.984: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:28.990: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:28.998: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.005: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.013: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.020: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.057: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.065: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.071: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.083: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.089: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.095: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.102: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.108: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:29.133: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

    Aug 11 14:21:33.977: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:33.984: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:33.991: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:33.997: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.003: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.010: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.016: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.059: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.065: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.071: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.078: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.084: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.091: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.097: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.103: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:34.129: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

    Aug 11 14:21:38.976: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:38.983: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:38.989: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:38.995: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.002: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.009: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.016: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.023: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.055: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.062: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.072: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.079: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.087: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.100: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.106: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:39.134: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

    Aug 11 14:21:43.979: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:43.986: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:43.992: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:43.998: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.005: INFO: Unable to read wheezy_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.012: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.019: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.026: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.060: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.066: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.073: INFO: Unable to read jessie_udp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.080: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122 from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.086: INFO: Unable to read jessie_udp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.093: INFO: Unable to read jessie_tcp@dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.101: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.107: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc from pod dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8: the server could not find the requested resource (get pods dns-test-78940e77-9088-492d-849e-f7bd6fc084e8)
    Aug 11 14:21:44.132: INFO: Lookups using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3122 wheezy_tcp@dns-test-service.dns-3122 wheezy_udp@dns-test-service.dns-3122.svc wheezy_tcp@dns-test-service.dns-3122.svc wheezy_udp@_http._tcp.dns-test-service.dns-3122.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3122.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3122 jessie_tcp@dns-test-service.dns-3122 jessie_udp@dns-test-service.dns-3122.svc jessie_tcp@dns-test-service.dns-3122.svc jessie_udp@_http._tcp.dns-test-service.dns-3122.svc jessie_tcp@_http._tcp.dns-test-service.dns-3122.svc]

    Aug 11 14:21:49.134: INFO: DNS probes using dns-3122/dns-test-78940e77-9088-492d-849e-f7bd6fc084e8 succeeded

    STEP: deleting the pod 08/11/23 14:21:49.134
    STEP: deleting the test service 08/11/23 14:21:49.157
    STEP: deleting the test headless service 08/11/23 14:21:49.192
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 11 14:21:49.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-3122" for this suite. 08/11/23 14:21:49.216
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:21:49.227
Aug 11 14:21:49.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename statefulset 08/11/23 14:21:49.228
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:21:49.248
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:21:49.251
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-1480 08/11/23 14:21:49.254
[It] should list, patch and delete a collection of StatefulSets [Conformance]
  test/e2e/apps/statefulset.go:906
Aug 11 14:21:49.274: INFO: Found 0 stateful pods, waiting for 1
Aug 11 14:21:59.279: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: patching the StatefulSet 08/11/23 14:21:59.287
W0811 14:21:59.300136      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 11 14:21:59.309: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:21:59.309: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
Aug 11 14:22:09.314: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:22:09.314: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
STEP: Listing all StatefulSets 08/11/23 14:22:09.32
STEP: Delete all of the StatefulSets 08/11/23 14:22:09.323
STEP: Verify that StatefulSets have been deleted 08/11/23 14:22:09.331
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 11 14:22:09.334: INFO: Deleting all statefulset in ns statefulset-1480
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 11 14:22:09.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-1480" for this suite. 08/11/23 14:22:09.356
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]","completed":69,"skipped":1422,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.143 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should list, patch and delete a collection of StatefulSets [Conformance]
    test/e2e/apps/statefulset.go:906

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:21:49.227
    Aug 11 14:21:49.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename statefulset 08/11/23 14:21:49.228
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:21:49.248
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:21:49.251
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-1480 08/11/23 14:21:49.254
    [It] should list, patch and delete a collection of StatefulSets [Conformance]
      test/e2e/apps/statefulset.go:906
    Aug 11 14:21:49.274: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 14:21:59.279: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: patching the StatefulSet 08/11/23 14:21:59.287
    W0811 14:21:59.300136      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 11 14:21:59.309: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:21:59.309: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Pending - Ready=false
    Aug 11 14:22:09.314: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:22:09.314: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Listing all StatefulSets 08/11/23 14:22:09.32
    STEP: Delete all of the StatefulSets 08/11/23 14:22:09.323
    STEP: Verify that StatefulSets have been deleted 08/11/23 14:22:09.331
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 11 14:22:09.334: INFO: Deleting all statefulset in ns statefulset-1480
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 11 14:22:09.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-1480" for this suite. 08/11/23 14:22:09.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe
  should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:22:09.37
Aug 11 14:22:09.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:22:09.371
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:09.391
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:09.394
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
  test/e2e/kubectl/kubectl.go:1274
Aug 11 14:22:09.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 create -f -'
Aug 11 14:22:10.257: INFO: stderr: ""
Aug 11 14:22:10.258: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
Aug 11 14:22:10.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 create -f -'
Aug 11 14:22:11.062: INFO: stderr: ""
Aug 11 14:22:11.062: INFO: stdout: "service/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/11/23 14:22:11.062
Aug 11 14:22:12.067: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:22:12.067: INFO: Found 1 / 1
Aug 11 14:22:12.067: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 11 14:22:12.070: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 14:22:12.070: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 11 14:22:12.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe pod agnhost-primary-s8tmd'
Aug 11 14:22:12.139: INFO: stderr: ""
Aug 11 14:22:12.139: INFO: stdout: "Name:             agnhost-primary-s8tmd\nNamespace:        kubectl-9857\nPriority:         0\nService Account:  default\nNode:             constell-d93e7e1d-worker-d314547c-0lc3/192.168.178.2\nStart Time:       Fri, 11 Aug 2023 14:22:10 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.10.0.9\nIPs:\n  IP:           10.10.0.9\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://9cb5d5f571d29fdf257af7d38ae77365f0456984e4e4eb0a99e37abeb0e04731\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 11 Aug 2023 14:22:10 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jdf6x (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-jdf6x:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9857/agnhost-primary-s8tmd to constell-d93e7e1d-worker-d314547c-0lc3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
Aug 11 14:22:12.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe rc agnhost-primary'
Aug 11 14:22:12.210: INFO: stderr: ""
Aug 11 14:22:12.210: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9857\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-s8tmd\n"
Aug 11 14:22:12.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe service agnhost-primary'
Aug 11 14:22:12.275: INFO: stderr: ""
Aug 11 14:22:12.275: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9857\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.97.239.107\nIPs:               10.97.239.107\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.0.9:6379\nSession Affinity:  None\nEvents:            <none>\n"
Aug 11 14:22:12.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe node constell-d93e7e1d-control-plane-6ef148f4-1v0j'
Aug 11 14:22:12.378: INFO: stderr: ""
Aug 11 14:22:12.378: INFO: stdout: "Name:               constell-d93e7e1d-control-plane-6ef148f4-1v0j\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n2d-standard-4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=europe-west3\n                    failure-domain.beta.kubernetes.io/zone=europe-west3-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=constell-d93e7e1d-control-plane-6ef148f4-1v0j\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=n2d-standard-4\n                    topology.gke.io/zone=europe-west3-b\n                    topology.kubernetes.io/region=europe-west3\n                    topology.kubernetes.io/zone=europe-west3-b\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.178.4\n                    constellation.edgeless.systems/kubernetes-components: k8s-components-sha256-d3a935d3136652120d85d0378e2007802af82d65780d35f5b2d7841b6dcfe612\n                    constellation.edgeless.systems/node-image: projects/constellation-images/global/images/v2-9-1-gcp-sev-es-stable\n                    constellation.edgeless.systems/scaling-group-id:\n                      projects/constellation-331613/zones/europe-west3-b/instanceGroupManagers/constell-d93e7e1d-control-plane-6ef148f4\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"gcp.csi.confidential.cloud\":\"projects/constellation-331613/zones/europe-west3-b/instances/constell-d93e7e1d-control-plane-6ef148f4-1v0j\"...\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 11 Aug 2023 13:52:21 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  constell-d93e7e1d-control-plane-6ef148f4-1v0j\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 11 Aug 2023 14:22:07 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 11 Aug 2023 13:53:23 +0000   Fri, 11 Aug 2023 13:53:23 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 11 Aug 2023 14:17:44 +0000   Fri, 11 Aug 2023 13:52:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 11 Aug 2023 14:17:44 +0000   Fri, 11 Aug 2023 13:52:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 11 Aug 2023 14:17:44 +0000   Fri, 11 Aug 2023 13:52:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 11 Aug 2023 14:17:44 +0000   Fri, 11 Aug 2023 13:52:44 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.178.4\n  Hostname:    constell-d93e7e1d-control-plane-6ef148f4-1v0j\nCapacity:\n  cpu:                4\n  ephemeral-storage:  30467368Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15365128Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  28078726303\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15262728Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 d52bab49ec247613fd264c92a40ac401\n  System UUID:                d52bab49-ec24-7613-fd26-4c92a40ac401\n  Boot ID:                    28058fc7-53b2-4593-acc0-66e637d7de2c\n  Kernel Version:             6.3.12-200.fc38.x86_64\n  OS Image:                   Fedora Linux 38 (Thirty Eight)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19\n  Kubelet Version:            v1.25.11\n  Kube-Proxy Version:         v1.25.11\nPodCIDR:                      10.10.4.0/24\nPodCIDRs:                     10.10.4.0/24\nProviderID:                   gce://constellation-331613/europe-west3-b/constell-d93e7e1d-control-plane-6ef148f4-1v0j\nNon-terminated Pods:          (24 in total)\n  Namespace                   Name                                                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                     ------------  ----------  ---------------  -------------  ---\n  kube-system                 cert-manager-555c986f89-wfbfd                                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 cert-manager-cainjector-844dff5bf5-rrfrf                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 cert-manager-webhook-86848457cc-gn5f9                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 cilium-kqd9l                                                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         29m\n  kube-system                 cilium-operator-5bfff4c47c-wz992                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 cloud-controller-manager-mlvn9                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 constellation-cluster-autoscaler-f844d465b-82kms                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 constellation-operator-controller-manager-b487b5955-th7xf                15m (0%)      1 (25%)     128Mi (0%)       256Mi (1%)     28m\n  kube-system                 coredns-6c49cf4575-6zmp4                                                 100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     29m\n  kube-system                 csi-gce-pd-controller-787585bbcd-zl4bw                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 csi-gce-pd-node-cljcz                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 etcd-constell-d93e7e1d-control-plane-6ef148f4-1v0j                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         29m\n  kube-system                 gcp-guest-agent-vmlhb                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 join-service-64bmb                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 key-service-ghkkl                                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 konnectivity-agent-9bdkl                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 konnectivity-server-constell-d93e7e1d-control-plane-6ef148f4-1v0j        0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-apiserver-constell-d93e7e1d-control-plane-6ef148f4-1v0j             250m (6%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-1v0j    200m (5%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-proxy-bh8nr                                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-scheduler-constell-d93e7e1d-control-plane-6ef148f4-1v0j             100m (2%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 node-maintenance-operator-controller-manager-5585fff6d4-pcxb5            100m (2%)     200m (5%)   20Mi (0%)        100Mi (0%)     28m\n  kube-system                 verification-service-r5756                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-kw9r6                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                965m (24%)  1200m (30%)\n  memory             418Mi (2%)  526Mi (3%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From                   Message\n  ----     ------                   ----               ----                   -------\n  Normal   Starting                 29m                kube-proxy             \n  Normal   Starting                 29m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      29m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  29m (x8 over 29m)  kubelet                Node constell-d93e7e1d-control-plane-6ef148f4-1v0j status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    29m (x7 over 29m)  kubelet                Node constell-d93e7e1d-control-plane-6ef148f4-1v0j status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     29m (x7 over 29m)  kubelet                Node constell-d93e7e1d-control-plane-6ef148f4-1v0j status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  29m                kubelet                Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           29m                node-controller        Node constell-d93e7e1d-control-plane-6ef148f4-1v0j event: Registered Node constell-d93e7e1d-control-plane-6ef148f4-1v0j in Controller\n  Normal   Synced                   28m (x2 over 28m)  cloud-node-controller  Node synced successfully\n"
Aug 11 14:22:12.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe namespace kubectl-9857'
Aug 11 14:22:12.441: INFO: stderr: ""
Aug 11 14:22:12.441: INFO: stdout: "Name:         kubectl-9857\nLabels:       e2e-framework=kubectl\n              e2e-run=d9e28574-a8c3-4403-a356-b1e4da7eef54\n              kubernetes.io/metadata.name=kubectl-9857\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:22:12.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-9857" for this suite. 08/11/23 14:22:12.445
{"msg":"PASSED [sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]","completed":70,"skipped":1433,"failed":0}
------------------------------
â€¢ [3.082 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl describe
  test/e2e/kubectl/kubectl.go:1268
    should check if kubectl describe prints relevant information for rc and pods  [Conformance]
    test/e2e/kubectl/kubectl.go:1274

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:22:09.37
    Aug 11 14:22:09.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:22:09.371
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:09.391
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:09.394
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl describe prints relevant information for rc and pods  [Conformance]
      test/e2e/kubectl/kubectl.go:1274
    Aug 11 14:22:09.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 create -f -'
    Aug 11 14:22:10.257: INFO: stderr: ""
    Aug 11 14:22:10.258: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    Aug 11 14:22:10.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 create -f -'
    Aug 11 14:22:11.062: INFO: stderr: ""
    Aug 11 14:22:11.062: INFO: stdout: "service/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/11/23 14:22:11.062
    Aug 11 14:22:12.067: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:22:12.067: INFO: Found 1 / 1
    Aug 11 14:22:12.067: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 11 14:22:12.070: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 14:22:12.070: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 11 14:22:12.070: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe pod agnhost-primary-s8tmd'
    Aug 11 14:22:12.139: INFO: stderr: ""
    Aug 11 14:22:12.139: INFO: stdout: "Name:             agnhost-primary-s8tmd\nNamespace:        kubectl-9857\nPriority:         0\nService Account:  default\nNode:             constell-d93e7e1d-worker-d314547c-0lc3/192.168.178.2\nStart Time:       Fri, 11 Aug 2023 14:22:10 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.10.0.9\nIPs:\n  IP:           10.10.0.9\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://9cb5d5f571d29fdf257af7d38ae77365f0456984e4e4eb0a99e37abeb0e04731\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.40\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 11 Aug 2023 14:22:10 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jdf6x (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-jdf6x:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-9857/agnhost-primary-s8tmd to constell-d93e7e1d-worker-d314547c-0lc3\n  Normal  Pulled     2s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.40\" already present on machine\n  Normal  Created    2s    kubelet            Created container agnhost-primary\n  Normal  Started    2s    kubelet            Started container agnhost-primary\n"
    Aug 11 14:22:12.139: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe rc agnhost-primary'
    Aug 11 14:22:12.210: INFO: stderr: ""
    Aug 11 14:22:12.210: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-9857\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.40\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-s8tmd\n"
    Aug 11 14:22:12.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe service agnhost-primary'
    Aug 11 14:22:12.275: INFO: stderr: ""
    Aug 11 14:22:12.275: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-9857\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.97.239.107\nIPs:               10.97.239.107\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.10.0.9:6379\nSession Affinity:  None\nEvents:            <none>\n"
    Aug 11 14:22:12.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe node constell-d93e7e1d-control-plane-6ef148f4-1v0j'
    Aug 11 14:22:12.378: INFO: stderr: ""
    Aug 11 14:22:12.378: INFO: stdout: "Name:               constell-d93e7e1d-control-plane-6ef148f4-1v0j\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=n2d-standard-4\n                    beta.kubernetes.io/os=linux\n                    failure-domain.beta.kubernetes.io/region=europe-west3\n                    failure-domain.beta.kubernetes.io/zone=europe-west3-b\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=constell-d93e7e1d-control-plane-6ef148f4-1v0j\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\n                    node.kubernetes.io/instance-type=n2d-standard-4\n                    topology.gke.io/zone=europe-west3-b\n                    topology.kubernetes.io/region=europe-west3\n                    topology.kubernetes.io/zone=europe-west3-b\nAnnotations:        alpha.kubernetes.io/provided-node-ip: 192.168.178.4\n                    constellation.edgeless.systems/kubernetes-components: k8s-components-sha256-d3a935d3136652120d85d0378e2007802af82d65780d35f5b2d7841b6dcfe612\n                    constellation.edgeless.systems/node-image: projects/constellation-images/global/images/v2-9-1-gcp-sev-es-stable\n                    constellation.edgeless.systems/scaling-group-id:\n                      projects/constellation-331613/zones/europe-west3-b/instanceGroupManagers/constell-d93e7e1d-control-plane-6ef148f4\n                    csi.volume.kubernetes.io/nodeid:\n                      {\"gcp.csi.confidential.cloud\":\"projects/constellation-331613/zones/europe-west3-b/instances/constell-d93e7e1d-control-plane-6ef148f4-1v0j\"...\n                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 11 Aug 2023 13:52:21 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  constell-d93e7e1d-control-plane-6ef148f4-1v0j\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 11 Aug 2023 14:22:07 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 11 Aug 2023 13:53:23 +0000   Fri, 11 Aug 2023 13:53:23 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Fri, 11 Aug 2023 14:17:44 +0000   Fri, 11 Aug 2023 13:52:21 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 11 Aug 2023 14:17:44 +0000   Fri, 11 Aug 2023 13:52:21 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 11 Aug 2023 14:17:44 +0000   Fri, 11 Aug 2023 13:52:21 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 11 Aug 2023 14:17:44 +0000   Fri, 11 Aug 2023 13:52:44 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.178.4\n  Hostname:    constell-d93e7e1d-control-plane-6ef148f4-1v0j\nCapacity:\n  cpu:                4\n  ephemeral-storage:  30467368Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15365128Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  28078726303\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             15262728Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 d52bab49ec247613fd264c92a40ac401\n  System UUID:                d52bab49-ec24-7613-fd26-4c92a40ac401\n  Boot ID:                    28058fc7-53b2-4593-acc0-66e637d7de2c\n  Kernel Version:             6.3.12-200.fc38.x86_64\n  OS Image:                   Fedora Linux 38 (Thirty Eight)\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19\n  Kubelet Version:            v1.25.11\n  Kube-Proxy Version:         v1.25.11\nPodCIDR:                      10.10.4.0/24\nPodCIDRs:                     10.10.4.0/24\nProviderID:                   gce://constellation-331613/europe-west3-b/constell-d93e7e1d-control-plane-6ef148f4-1v0j\nNon-terminated Pods:          (24 in total)\n  Namespace                   Name                                                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                                     ------------  ----------  ---------------  -------------  ---\n  kube-system                 cert-manager-555c986f89-wfbfd                                            0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 cert-manager-cainjector-844dff5bf5-rrfrf                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 cert-manager-webhook-86848457cc-gn5f9                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 cilium-kqd9l                                                             100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         29m\n  kube-system                 cilium-operator-5bfff4c47c-wz992                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 cloud-controller-manager-mlvn9                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 constellation-cluster-autoscaler-f844d465b-82kms                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         26m\n  kube-system                 constellation-operator-controller-manager-b487b5955-th7xf                15m (0%)      1 (25%)     128Mi (0%)       256Mi (1%)     28m\n  kube-system                 coredns-6c49cf4575-6zmp4                                                 100m (2%)     0 (0%)      70Mi (0%)        170Mi (1%)     29m\n  kube-system                 csi-gce-pd-controller-787585bbcd-zl4bw                                   0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 csi-gce-pd-node-cljcz                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 etcd-constell-d93e7e1d-control-plane-6ef148f4-1v0j                       100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         29m\n  kube-system                 gcp-guest-agent-vmlhb                                                    0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 join-service-64bmb                                                       0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 key-service-ghkkl                                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 konnectivity-agent-9bdkl                                                 0 (0%)        0 (0%)      0 (0%)           0 (0%)         28m\n  kube-system                 konnectivity-server-constell-d93e7e1d-control-plane-6ef148f4-1v0j        0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-apiserver-constell-d93e7e1d-control-plane-6ef148f4-1v0j             250m (6%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-1v0j    200m (5%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-proxy-bh8nr                                                         0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 kube-scheduler-constell-d93e7e1d-control-plane-6ef148f4-1v0j             100m (2%)     0 (0%)      0 (0%)           0 (0%)         29m\n  kube-system                 node-maintenance-operator-controller-manager-5585fff6d4-pcxb5            100m (2%)     200m (5%)   20Mi (0%)        100Mi (0%)     28m\n  kube-system                 verification-service-r5756                                               0 (0%)        0 (0%)      0 (0%)           0 (0%)         29m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-kw9r6                  0 (0%)        0 (0%)      0 (0%)           0 (0%)         20m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                965m (24%)  1200m (30%)\n  memory             418Mi (2%)  526Mi (3%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:\n  Type     Reason                   Age                From                   Message\n  ----     ------                   ----               ----                   -------\n  Normal   Starting                 29m                kube-proxy             \n  Normal   Starting                 29m                kubelet                Starting kubelet.\n  Warning  InvalidDiskCapacity      29m                kubelet                invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  29m (x8 over 29m)  kubelet                Node constell-d93e7e1d-control-plane-6ef148f4-1v0j status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    29m (x7 over 29m)  kubelet                Node constell-d93e7e1d-control-plane-6ef148f4-1v0j status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     29m (x7 over 29m)  kubelet                Node constell-d93e7e1d-control-plane-6ef148f4-1v0j status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  29m                kubelet                Updated Node Allocatable limit across pods\n  Normal   RegisteredNode           29m                node-controller        Node constell-d93e7e1d-control-plane-6ef148f4-1v0j event: Registered Node constell-d93e7e1d-control-plane-6ef148f4-1v0j in Controller\n  Normal   Synced                   28m (x2 over 28m)  cloud-node-controller  Node synced successfully\n"
    Aug 11 14:22:12.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-9857 describe namespace kubectl-9857'
    Aug 11 14:22:12.441: INFO: stderr: ""
    Aug 11 14:22:12.441: INFO: stdout: "Name:         kubectl-9857\nLabels:       e2e-framework=kubectl\n              e2e-run=d9e28574-a8c3-4403-a356-b1e4da7eef54\n              kubernetes.io/metadata.name=kubectl-9857\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:22:12.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-9857" for this suite. 08/11/23 14:22:12.445
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:22:12.454
Aug 11 14:22:12.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:22:12.455
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:12.471
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:12.474
[It] updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123
STEP: Creating projection with configMap that has name projected-configmap-test-upd-e46cac65-ff1a-4462-9e93-e918da84e569 08/11/23 14:22:12.48
STEP: Creating the pod 08/11/23 14:22:12.485
Aug 11 14:22:12.495: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f" in namespace "projected-1408" to be "running and ready"
Aug 11 14:22:12.500: INFO: Pod "pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.766708ms
Aug 11 14:22:12.500: INFO: The phase of Pod pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:22:14.505: INFO: Pod "pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010108931s
Aug 11 14:22:14.505: INFO: The phase of Pod pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f is Running (Ready = true)
Aug 11 14:22:14.505: INFO: Pod "pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f" satisfied condition "running and ready"
STEP: Updating configmap projected-configmap-test-upd-e46cac65-ff1a-4462-9e93-e918da84e569 08/11/23 14:22:14.535
STEP: waiting to observe update in volume 08/11/23 14:22:14.542
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 11 14:22:18.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1408" for this suite. 08/11/23 14:22:18.582
{"msg":"PASSED [sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]","completed":71,"skipped":1452,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.136 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:22:12.454
    Aug 11 14:22:12.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:22:12.455
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:12.471
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:12.474
    [It] updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:123
    STEP: Creating projection with configMap that has name projected-configmap-test-upd-e46cac65-ff1a-4462-9e93-e918da84e569 08/11/23 14:22:12.48
    STEP: Creating the pod 08/11/23 14:22:12.485
    Aug 11 14:22:12.495: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f" in namespace "projected-1408" to be "running and ready"
    Aug 11 14:22:12.500: INFO: Pod "pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.766708ms
    Aug 11 14:22:12.500: INFO: The phase of Pod pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:22:14.505: INFO: Pod "pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.010108931s
    Aug 11 14:22:14.505: INFO: The phase of Pod pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f is Running (Ready = true)
    Aug 11 14:22:14.505: INFO: Pod "pod-projected-configmaps-c4ab9f68-1421-4c81-9df7-45123f686a7f" satisfied condition "running and ready"
    STEP: Updating configmap projected-configmap-test-upd-e46cac65-ff1a-4462-9e93-e918da84e569 08/11/23 14:22:14.535
    STEP: waiting to observe update in volume 08/11/23 14:22:14.542
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 11 14:22:18.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1408" for this suite. 08/11/23 14:22:18.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:22:18.591
Aug 11 14:22:18.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename disruption 08/11/23 14:22:18.592
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:18.61
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:18.614
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163
STEP: Waiting for the pdb to be processed 08/11/23 14:22:18.623
STEP: Updating PodDisruptionBudget status 08/11/23 14:22:20.636
STEP: Waiting for all pods to be running 08/11/23 14:22:20.645
Aug 11 14:22:20.655: INFO: running pods: 0 < 1
STEP: locating a running pod 08/11/23 14:22:22.66
STEP: Waiting for the pdb to be processed 08/11/23 14:22:22.673
STEP: Patching PodDisruptionBudget status 08/11/23 14:22:22.681
STEP: Waiting for the pdb to be processed 08/11/23 14:22:22.689
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 11 14:22:22.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-8585" for this suite. 08/11/23 14:22:22.699
{"msg":"PASSED [sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]","completed":72,"skipped":1482,"failed":0}
------------------------------
â€¢ [4.115 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should update/patch PodDisruptionBudget status [Conformance]
  test/e2e/apps/disruption.go:163

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:22:18.591
    Aug 11 14:22:18.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename disruption 08/11/23 14:22:18.592
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:18.61
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:18.614
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should update/patch PodDisruptionBudget status [Conformance]
      test/e2e/apps/disruption.go:163
    STEP: Waiting for the pdb to be processed 08/11/23 14:22:18.623
    STEP: Updating PodDisruptionBudget status 08/11/23 14:22:20.636
    STEP: Waiting for all pods to be running 08/11/23 14:22:20.645
    Aug 11 14:22:20.655: INFO: running pods: 0 < 1
    STEP: locating a running pod 08/11/23 14:22:22.66
    STEP: Waiting for the pdb to be processed 08/11/23 14:22:22.673
    STEP: Patching PodDisruptionBudget status 08/11/23 14:22:22.681
    STEP: Waiting for the pdb to be processed 08/11/23 14:22:22.689
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 11 14:22:22.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-8585" for this suite. 08/11/23 14:22:22.699
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:22:22.706
Aug 11 14:22:22.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:22:22.707
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:22.722
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:22.725
[It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/11/23 14:22:22.729
Aug 11 14:22:22.741: INFO: Waiting up to 5m0s for pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b" in namespace "emptydir-2176" to be "Succeeded or Failed"
Aug 11 14:22:22.745: INFO: Pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.473388ms
Aug 11 14:22:24.750: INFO: Pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008259664s
Aug 11 14:22:26.751: INFO: Pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009178179s
STEP: Saw pod success 08/11/23 14:22:26.751
Aug 11 14:22:26.751: INFO: Pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b" satisfied condition "Succeeded or Failed"
Aug 11 14:22:26.755: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b container test-container: <nil>
STEP: delete the pod 08/11/23 14:22:26.779
Aug 11 14:22:26.791: INFO: Waiting for pod pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b to disappear
Aug 11 14:22:26.794: INFO: Pod pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 14:22:26.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-2176" for this suite. 08/11/23 14:22:26.798
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":73,"skipped":1491,"failed":0}
------------------------------
â€¢ [4.099 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:136

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:22:22.706
    Aug 11 14:22:22.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:22:22.707
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:22.722
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:22.725
    [It] should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:136
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/11/23 14:22:22.729
    Aug 11 14:22:22.741: INFO: Waiting up to 5m0s for pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b" in namespace "emptydir-2176" to be "Succeeded or Failed"
    Aug 11 14:22:22.745: INFO: Pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b": Phase="Pending", Reason="", readiness=false. Elapsed: 3.473388ms
    Aug 11 14:22:24.750: INFO: Pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008259664s
    Aug 11 14:22:26.751: INFO: Pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009178179s
    STEP: Saw pod success 08/11/23 14:22:26.751
    Aug 11 14:22:26.751: INFO: Pod "pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b" satisfied condition "Succeeded or Failed"
    Aug 11 14:22:26.755: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b container test-container: <nil>
    STEP: delete the pod 08/11/23 14:22:26.779
    Aug 11 14:22:26.791: INFO: Waiting for pod pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b to disappear
    Aug 11 14:22:26.794: INFO: Pod pod-622ac5e3-2f87-4a5b-911f-1e74ab17576b no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:22:26.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-2176" for this suite. 08/11/23 14:22:26.798
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:22:26.809
Aug 11 14:22:26.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename statefulset 08/11/23 14:22:26.81
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:26.826
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:26.829
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-7330 08/11/23 14:22:26.832
[It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
  test/e2e/apps/statefulset.go:695
STEP: Creating stateful set ss in namespace statefulset-7330 08/11/23 14:22:26.839
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7330 08/11/23 14:22:26.849
Aug 11 14:22:26.853: INFO: Found 0 stateful pods, waiting for 1
Aug 11 14:22:36.858: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/11/23 14:22:36.858
Aug 11 14:22:36.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:22:37.006: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:22:37.006: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:22:37.006: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 14:22:37.010: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 11 14:22:47.015: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 14:22:47.015: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:22:47.034: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Aug 11 14:22:47.034: INFO: ss-0  constell-d93e7e1d-worker-d314547c-wzlp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:26 +0000 UTC  }]
Aug 11 14:22:47.034: INFO: 
Aug 11 14:22:47.034: INFO: StatefulSet ss has not reached scale 3, at 1
Aug 11 14:22:48.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993681325s
Aug 11 14:22:49.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988437874s
Aug 11 14:22:50.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983966176s
Aug 11 14:22:51.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979166289s
Aug 11 14:22:52.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975607839s
Aug 11 14:22:53.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.971597466s
Aug 11 14:22:54.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967229162s
Aug 11 14:22:55.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962370373s
Aug 11 14:22:56.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.149413ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7330 08/11/23 14:22:57.075
Aug 11 14:22:57.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 14:22:57.218: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 14:22:57.218: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 14:22:57.218: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 14:22:57.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 14:22:57.362: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 11 14:22:57.362: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 14:22:57.362: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 14:22:57.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 14:22:57.498: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
Aug 11 14:22:57.499: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 14:22:57.499: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 14:22:57.503: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
Aug 11 14:23:07.508: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:23:07.508: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 14:23:07.508: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Scale down will not halt with unhealthy stateful pod 08/11/23 14:23:07.508
Aug 11 14:23:07.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:23:07.641: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:23:07.641: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:23:07.641: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 14:23:07.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:23:07.771: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:23:07.771: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:23:07.771: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 14:23:07.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 14:23:07.905: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 14:23:07.905: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 14:23:07.905: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 14:23:07.905: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:23:07.909: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
Aug 11 14:23:17.918: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 14:23:17.918: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 14:23:17.918: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 14:23:17.930: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
Aug 11 14:23:17.930: INFO: ss-0  constell-d93e7e1d-worker-d314547c-wzlp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:26 +0000 UTC  }]
Aug 11 14:23:17.930: INFO: ss-1  constell-d93e7e1d-worker-d314547c-0lc3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:47 +0000 UTC  }]
Aug 11 14:23:17.930: INFO: ss-2  constell-d93e7e1d-worker-d314547c-0lc3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:47 +0000 UTC  }]
Aug 11 14:23:17.930: INFO: 
Aug 11 14:23:17.930: INFO: StatefulSet ss has not reached scale 0, at 3
Aug 11 14:23:18.934: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.996249623s
Aug 11 14:23:19.938: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.991810896s
Aug 11 14:23:20.943: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987758923s
Aug 11 14:23:21.947: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98280622s
Aug 11 14:23:22.951: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.979283913s
Aug 11 14:23:23.955: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.975598229s
Aug 11 14:23:24.959: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.971619649s
Aug 11 14:23:25.963: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.967275035s
Aug 11 14:23:26.969: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.720636ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7330 08/11/23 14:23:27.97
Aug 11 14:23:27.974: INFO: Scaling statefulset ss to 0
Aug 11 14:23:27.986: INFO: Waiting for statefulset status.replicas updated to 0
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 11 14:23:27.990: INFO: Deleting all statefulset in ns statefulset-7330
Aug 11 14:23:27.992: INFO: Scaling statefulset ss to 0
Aug 11 14:23:28.002: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 14:23:28.005: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 11 14:23:28.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-7330" for this suite. 08/11/23 14:23:28.022
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]","completed":74,"skipped":1566,"failed":0}
------------------------------
â€¢ [SLOW TEST] [61.221 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
    test/e2e/apps/statefulset.go:695

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:22:26.809
    Aug 11 14:22:26.809: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename statefulset 08/11/23 14:22:26.81
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:22:26.826
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:22:26.829
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-7330 08/11/23 14:22:26.832
    [It] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
      test/e2e/apps/statefulset.go:695
    STEP: Creating stateful set ss in namespace statefulset-7330 08/11/23 14:22:26.839
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-7330 08/11/23 14:22:26.849
    Aug 11 14:22:26.853: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 14:22:36.858: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod 08/11/23 14:22:36.858
    Aug 11 14:22:36.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:22:37.006: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:22:37.006: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:22:37.006: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 14:22:37.010: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 11 14:22:47.015: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 14:22:47.015: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:22:47.034: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Aug 11 14:22:47.034: INFO: ss-0  constell-d93e7e1d-worker-d314547c-wzlp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:37 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:26 +0000 UTC  }]
    Aug 11 14:22:47.034: INFO: 
    Aug 11 14:22:47.034: INFO: StatefulSet ss has not reached scale 3, at 1
    Aug 11 14:22:48.039: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993681325s
    Aug 11 14:22:49.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.988437874s
    Aug 11 14:22:50.049: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.983966176s
    Aug 11 14:22:51.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.979166289s
    Aug 11 14:22:52.056: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.975607839s
    Aug 11 14:22:53.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.971597466s
    Aug 11 14:22:54.065: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.967229162s
    Aug 11 14:22:55.071: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.962370373s
    Aug 11 14:22:56.075: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.149413ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-7330 08/11/23 14:22:57.075
    Aug 11 14:22:57.081: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 14:22:57.218: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 14:22:57.218: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 14:22:57.218: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 14:22:57.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 14:22:57.362: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 11 14:22:57.362: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 14:22:57.362: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 14:22:57.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 14:22:57.498: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
    Aug 11 14:22:57.499: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 14:22:57.499: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 14:22:57.503: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
    Aug 11 14:23:07.508: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:23:07.508: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 14:23:07.508: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Scale down will not halt with unhealthy stateful pod 08/11/23 14:23:07.508
    Aug 11 14:23:07.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:23:07.641: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:23:07.641: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:23:07.641: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 14:23:07.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:23:07.771: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:23:07.771: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:23:07.771: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 14:23:07.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-7330 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 14:23:07.905: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 14:23:07.905: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 14:23:07.905: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 14:23:07.905: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:23:07.909: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
    Aug 11 14:23:17.918: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 14:23:17.918: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 14:23:17.918: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 14:23:17.930: INFO: POD   NODE                                    PHASE    GRACE  CONDITIONS
    Aug 11 14:23:17.930: INFO: ss-0  constell-d93e7e1d-worker-d314547c-wzlp  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:26 +0000 UTC  }]
    Aug 11 14:23:17.930: INFO: ss-1  constell-d93e7e1d-worker-d314547c-0lc3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:46 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:47 +0000 UTC  }]
    Aug 11 14:23:17.930: INFO: ss-2  constell-d93e7e1d-worker-d314547c-0lc3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:47 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:23:08 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:22:47 +0000 UTC  }]
    Aug 11 14:23:17.930: INFO: 
    Aug 11 14:23:17.930: INFO: StatefulSet ss has not reached scale 0, at 3
    Aug 11 14:23:18.934: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.996249623s
    Aug 11 14:23:19.938: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.991810896s
    Aug 11 14:23:20.943: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.987758923s
    Aug 11 14:23:21.947: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98280622s
    Aug 11 14:23:22.951: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.979283913s
    Aug 11 14:23:23.955: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.975598229s
    Aug 11 14:23:24.959: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.971619649s
    Aug 11 14:23:25.963: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.967275035s
    Aug 11 14:23:26.969: INFO: Verifying statefulset ss doesn't scale past 0 for another 962.720636ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-7330 08/11/23 14:23:27.97
    Aug 11 14:23:27.974: INFO: Scaling statefulset ss to 0
    Aug 11 14:23:27.986: INFO: Waiting for statefulset status.replicas updated to 0
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 11 14:23:27.990: INFO: Deleting all statefulset in ns statefulset-7330
    Aug 11 14:23:27.992: INFO: Scaling statefulset ss to 0
    Aug 11 14:23:28.002: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 14:23:28.005: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 11 14:23:28.018: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-7330" for this suite. 08/11/23 14:23:28.022
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:23:28.031
Aug 11 14:23:28.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:23:28.032
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:28.048
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:28.051
[It] should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90
STEP: Counting existing ResourceQuota 08/11/23 14:23:28.054
STEP: Creating a ResourceQuota 08/11/23 14:23:33.061
STEP: Ensuring resource quota status is calculated 08/11/23 14:23:33.068
STEP: Creating a Service 08/11/23 14:23:35.072
STEP: Creating a NodePort Service 08/11/23 14:23:35.099
STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/11/23 14:23:35.129
STEP: Ensuring resource quota status captures service creation 08/11/23 14:23:35.162
STEP: Deleting Services 08/11/23 14:23:37.167
STEP: Ensuring resource quota status released usage 08/11/23 14:23:37.22
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 14:23:39.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-163" for this suite. 08/11/23 14:23:39.231
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]","completed":75,"skipped":1600,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.207 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a service. [Conformance]
  test/e2e/apimachinery/resource_quota.go:90

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:23:28.031
    Aug 11 14:23:28.032: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:23:28.032
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:28.048
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:28.051
    [It] should create a ResourceQuota and capture the life of a service. [Conformance]
      test/e2e/apimachinery/resource_quota.go:90
    STEP: Counting existing ResourceQuota 08/11/23 14:23:28.054
    STEP: Creating a ResourceQuota 08/11/23 14:23:33.061
    STEP: Ensuring resource quota status is calculated 08/11/23 14:23:33.068
    STEP: Creating a Service 08/11/23 14:23:35.072
    STEP: Creating a NodePort Service 08/11/23 14:23:35.099
    STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota 08/11/23 14:23:35.129
    STEP: Ensuring resource quota status captures service creation 08/11/23 14:23:35.162
    STEP: Deleting Services 08/11/23 14:23:37.167
    STEP: Ensuring resource quota status released usage 08/11/23 14:23:37.22
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 14:23:39.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-163" for this suite. 08/11/23 14:23:39.231
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:23:39.24
Aug 11 14:23:39.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:23:39.241
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:39.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:39.263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:23:39.28
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:23:39.446
STEP: Deploying the webhook pod 08/11/23 14:23:39.455
STEP: Wait for the deployment to be ready 08/11/23 14:23:39.469
Aug 11 14:23:39.484: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:23:41.495
STEP: Verifying the service has paired with the endpoint 08/11/23 14:23:41.512
Aug 11 14:23:42.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340
Aug 11 14:23:42.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4265-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 14:23:43.03
STEP: Creating a custom resource that should be mutated by the webhook 08/11/23 14:23:43.06
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:23:45.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-368" for this suite. 08/11/23 14:23:45.659
STEP: Destroying namespace "webhook-368-markers" for this suite. 08/11/23 14:23:45.67
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]","completed":76,"skipped":1609,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.496 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with pruning [Conformance]
  test/e2e/apimachinery/webhook.go:340

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:23:39.24
    Aug 11 14:23:39.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:23:39.241
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:39.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:39.263
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:23:39.28
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:23:39.446
    STEP: Deploying the webhook pod 08/11/23 14:23:39.455
    STEP: Wait for the deployment to be ready 08/11/23 14:23:39.469
    Aug 11 14:23:39.484: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:23:41.495
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:23:41.512
    Aug 11 14:23:42.512: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with pruning [Conformance]
      test/e2e/apimachinery/webhook.go:340
    Aug 11 14:23:42.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4265-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 14:23:43.03
    STEP: Creating a custom resource that should be mutated by the webhook 08/11/23 14:23:43.06
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:23:45.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-368" for this suite. 08/11/23 14:23:45.659
    STEP: Destroying namespace "webhook-368-markers" for this suite. 08/11/23 14:23:45.67
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:23:45.736
Aug 11 14:23:45.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:23:45.737
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:45.762
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:45.765
[It] should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874
STEP: Creating a ResourceQuota 08/11/23 14:23:45.77
STEP: Getting a ResourceQuota 08/11/23 14:23:45.775
STEP: Updating a ResourceQuota 08/11/23 14:23:45.783
STEP: Verifying a ResourceQuota was modified 08/11/23 14:23:45.789
STEP: Deleting a ResourceQuota 08/11/23 14:23:45.794
STEP: Verifying the deleted ResourceQuota 08/11/23 14:23:45.802
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 14:23:45.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-4230" for this suite. 08/11/23 14:23:45.81
{"msg":"PASSED [sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]","completed":77,"skipped":1609,"failed":0}
------------------------------
â€¢ [0.084 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should be able to update and delete ResourceQuota. [Conformance]
  test/e2e/apimachinery/resource_quota.go:874

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:23:45.736
    Aug 11 14:23:45.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:23:45.737
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:45.762
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:45.765
    [It] should be able to update and delete ResourceQuota. [Conformance]
      test/e2e/apimachinery/resource_quota.go:874
    STEP: Creating a ResourceQuota 08/11/23 14:23:45.77
    STEP: Getting a ResourceQuota 08/11/23 14:23:45.775
    STEP: Updating a ResourceQuota 08/11/23 14:23:45.783
    STEP: Verifying a ResourceQuota was modified 08/11/23 14:23:45.789
    STEP: Deleting a ResourceQuota 08/11/23 14:23:45.794
    STEP: Verifying the deleted ResourceQuota 08/11/23 14:23:45.802
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 14:23:45.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-4230" for this suite. 08/11/23 14:23:45.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:23:45.823
Aug 11 14:23:45.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename watch 08/11/23 14:23:45.824
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:45.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:45.844
[It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191
STEP: creating a watch on configmaps 08/11/23 14:23:45.847
STEP: creating a new configmap 08/11/23 14:23:45.848
STEP: modifying the configmap once 08/11/23 14:23:45.855
STEP: closing the watch once it receives two notifications 08/11/23 14:23:45.863
Aug 11 14:23:45.863: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2891  182e8dc9-144f-4aae-b736-5a719a77578b 18037 0 2023-08-11 14:23:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:23:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:23:45.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2891  182e8dc9-144f-4aae-b736-5a719a77578b 18038 0 2023-08-11 14:23:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time, while the watch is closed 08/11/23 14:23:45.864
STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/11/23 14:23:45.872
STEP: deleting the configmap 08/11/23 14:23:45.874
STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/11/23 14:23:45.881
Aug 11 14:23:45.881: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2891  182e8dc9-144f-4aae-b736-5a719a77578b 18039 0 2023-08-11 14:23:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:23:45.881: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2891  182e8dc9-144f-4aae-b736-5a719a77578b 18040 0 2023-08-11 14:23:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 11 14:23:45.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2891" for this suite. 08/11/23 14:23:45.886
{"msg":"PASSED [sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]","completed":78,"skipped":1649,"failed":0}
------------------------------
â€¢ [0.070 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to restart watching from the last resource version observed by the previous watch [Conformance]
  test/e2e/apimachinery/watch.go:191

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:23:45.823
    Aug 11 14:23:45.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename watch 08/11/23 14:23:45.824
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:45.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:45.844
    [It] should be able to restart watching from the last resource version observed by the previous watch [Conformance]
      test/e2e/apimachinery/watch.go:191
    STEP: creating a watch on configmaps 08/11/23 14:23:45.847
    STEP: creating a new configmap 08/11/23 14:23:45.848
    STEP: modifying the configmap once 08/11/23 14:23:45.855
    STEP: closing the watch once it receives two notifications 08/11/23 14:23:45.863
    Aug 11 14:23:45.863: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2891  182e8dc9-144f-4aae-b736-5a719a77578b 18037 0 2023-08-11 14:23:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:23:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:23:45.864: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2891  182e8dc9-144f-4aae-b736-5a719a77578b 18038 0 2023-08-11 14:23:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time, while the watch is closed 08/11/23 14:23:45.864
    STEP: creating a new watch on configmaps from the last resource version observed by the first watch 08/11/23 14:23:45.872
    STEP: deleting the configmap 08/11/23 14:23:45.874
    STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed 08/11/23 14:23:45.881
    Aug 11 14:23:45.881: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2891  182e8dc9-144f-4aae-b736-5a719a77578b 18039 0 2023-08-11 14:23:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:23:45.881: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2891  182e8dc9-144f-4aae-b736-5a719a77578b 18040 0 2023-08-11 14:23:45 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-11 14:23:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 11 14:23:45.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2891" for this suite. 08/11/23 14:23:45.886
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:23:45.894
Aug 11 14:23:45.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:23:45.895
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:45.909
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:45.913
[It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86
STEP: Creating a pod to test emptydir volume type on tmpfs 08/11/23 14:23:45.916
Aug 11 14:23:45.929: INFO: Waiting up to 5m0s for pod "pod-59177d49-3130-494a-b53c-d05805cbff40" in namespace "emptydir-3426" to be "Succeeded or Failed"
Aug 11 14:23:45.934: INFO: Pod "pod-59177d49-3130-494a-b53c-d05805cbff40": Phase="Pending", Reason="", readiness=false. Elapsed: 5.399607ms
Aug 11 14:23:47.939: INFO: Pod "pod-59177d49-3130-494a-b53c-d05805cbff40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010917415s
Aug 11 14:23:49.939: INFO: Pod "pod-59177d49-3130-494a-b53c-d05805cbff40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010086396s
STEP: Saw pod success 08/11/23 14:23:49.939
Aug 11 14:23:49.939: INFO: Pod "pod-59177d49-3130-494a-b53c-d05805cbff40" satisfied condition "Succeeded or Failed"
Aug 11 14:23:49.945: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-59177d49-3130-494a-b53c-d05805cbff40 container test-container: <nil>
STEP: delete the pod 08/11/23 14:23:49.954
Aug 11 14:23:49.968: INFO: Waiting for pod pod-59177d49-3130-494a-b53c-d05805cbff40 to disappear
Aug 11 14:23:49.971: INFO: Pod pod-59177d49-3130-494a-b53c-d05805cbff40 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 14:23:49.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3426" for this suite. 08/11/23 14:23:49.976
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":79,"skipped":1657,"failed":0}
------------------------------
â€¢ [4.089 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:23:45.894
    Aug 11 14:23:45.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:23:45.895
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:45.909
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:45.913
    [It] volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:86
    STEP: Creating a pod to test emptydir volume type on tmpfs 08/11/23 14:23:45.916
    Aug 11 14:23:45.929: INFO: Waiting up to 5m0s for pod "pod-59177d49-3130-494a-b53c-d05805cbff40" in namespace "emptydir-3426" to be "Succeeded or Failed"
    Aug 11 14:23:45.934: INFO: Pod "pod-59177d49-3130-494a-b53c-d05805cbff40": Phase="Pending", Reason="", readiness=false. Elapsed: 5.399607ms
    Aug 11 14:23:47.939: INFO: Pod "pod-59177d49-3130-494a-b53c-d05805cbff40": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010917415s
    Aug 11 14:23:49.939: INFO: Pod "pod-59177d49-3130-494a-b53c-d05805cbff40": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010086396s
    STEP: Saw pod success 08/11/23 14:23:49.939
    Aug 11 14:23:49.939: INFO: Pod "pod-59177d49-3130-494a-b53c-d05805cbff40" satisfied condition "Succeeded or Failed"
    Aug 11 14:23:49.945: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-59177d49-3130-494a-b53c-d05805cbff40 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:23:49.954
    Aug 11 14:23:49.968: INFO: Waiting for pod pod-59177d49-3130-494a-b53c-d05805cbff40 to disappear
    Aug 11 14:23:49.971: INFO: Pod pod-59177d49-3130-494a-b53c-d05805cbff40 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:23:49.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3426" for this suite. 08/11/23 14:23:49.976
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:23:49.983
Aug 11 14:23:49.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:23:49.984
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:50.001
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:50.004
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:23:50.007
Aug 11 14:23:50.017: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8" in namespace "projected-121" to be "Succeeded or Failed"
Aug 11 14:23:50.023: INFO: Pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538303ms
Aug 11 14:23:52.028: INFO: Pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010549675s
Aug 11 14:23:54.028: INFO: Pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010902163s
STEP: Saw pod success 08/11/23 14:23:54.028
Aug 11 14:23:54.028: INFO: Pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8" satisfied condition "Succeeded or Failed"
Aug 11 14:23:54.032: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8 container client-container: <nil>
STEP: delete the pod 08/11/23 14:23:54.04
Aug 11 14:23:54.056: INFO: Waiting for pod downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8 to disappear
Aug 11 14:23:54.059: INFO: Pod downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 14:23:54.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-121" for this suite. 08/11/23 14:23:54.063
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":80,"skipped":1657,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:23:49.983
    Aug 11 14:23:49.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:23:49.984
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:50.001
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:50.004
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:248
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:23:50.007
    Aug 11 14:23:50.017: INFO: Waiting up to 5m0s for pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8" in namespace "projected-121" to be "Succeeded or Failed"
    Aug 11 14:23:50.023: INFO: Pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.538303ms
    Aug 11 14:23:52.028: INFO: Pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010549675s
    Aug 11 14:23:54.028: INFO: Pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010902163s
    STEP: Saw pod success 08/11/23 14:23:54.028
    Aug 11 14:23:54.028: INFO: Pod "downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8" satisfied condition "Succeeded or Failed"
    Aug 11 14:23:54.032: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:23:54.04
    Aug 11 14:23:54.056: INFO: Waiting for pod downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8 to disappear
    Aug 11 14:23:54.059: INFO: Pod downwardapi-volume-c20c5241-3443-4708-921e-e7110d1b86e8 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 14:23:54.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-121" for this suite. 08/11/23 14:23:54.063
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:23:54.072
Aug 11 14:23:54.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename cronjob 08/11/23 14:23:54.073
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:54.089
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:54.093
[It] should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69
STEP: Creating a cronjob 08/11/23 14:23:54.096
STEP: Ensuring more than one job is running at a time 08/11/23 14:23:54.103
STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/11/23 14:25:02.108
STEP: Removing cronjob 08/11/23 14:25:02.112
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 11 14:25:02.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-9441" for this suite. 08/11/23 14:25:02.125
{"msg":"PASSED [sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]","completed":81,"skipped":1669,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.062 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should schedule multiple jobs concurrently [Conformance]
  test/e2e/apps/cronjob.go:69

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:23:54.072
    Aug 11 14:23:54.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename cronjob 08/11/23 14:23:54.073
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:23:54.089
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:23:54.093
    [It] should schedule multiple jobs concurrently [Conformance]
      test/e2e/apps/cronjob.go:69
    STEP: Creating a cronjob 08/11/23 14:23:54.096
    STEP: Ensuring more than one job is running at a time 08/11/23 14:23:54.103
    STEP: Ensuring at least two running jobs exists by listing jobs explicitly 08/11/23 14:25:02.108
    STEP: Removing cronjob 08/11/23 14:25:02.112
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 11 14:25:02.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-9441" for this suite. 08/11/23 14:25:02.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
[BeforeEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:25:02.135
Aug 11 14:25:02.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename limitrange 08/11/23 14:25:02.136
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:02.169
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:02.172
[It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57
STEP: Creating a LimitRange 08/11/23 14:25:02.174
STEP: Setting up watch 08/11/23 14:25:02.175
STEP: Submitting a LimitRange 08/11/23 14:25:02.279
STEP: Verifying LimitRange creation was observed 08/11/23 14:25:02.287
STEP: Fetching the LimitRange to ensure it has proper values 08/11/23 14:25:02.287
Aug 11 14:25:02.290: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 11 14:25:02.290: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with no resource requirements 08/11/23 14:25:02.29
STEP: Ensuring Pod has resource requirements applied from LimitRange 08/11/23 14:25:02.296
Aug 11 14:25:02.302: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
Aug 11 14:25:02.302: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Creating a Pod with partial resource requirements 08/11/23 14:25:02.302
STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/11/23 14:25:02.309
Aug 11 14:25:02.315: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
Aug 11 14:25:02.315: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
STEP: Failing to create a Pod with less than min resources 08/11/23 14:25:02.315
STEP: Failing to create a Pod with more than max resources 08/11/23 14:25:02.317
STEP: Updating a LimitRange 08/11/23 14:25:02.319
STEP: Verifying LimitRange updating is effective 08/11/23 14:25:02.325
STEP: Creating a Pod with less than former min resources 08/11/23 14:25:04.33
STEP: Failing to create a Pod with more than max resources 08/11/23 14:25:04.338
STEP: Deleting a LimitRange 08/11/23 14:25:04.34
STEP: Verifying the LimitRange was deleted 08/11/23 14:25:04.349
Aug 11 14:25:09.354: INFO: limitRange is already deleted
STEP: Creating a Pod with more than former max resources 08/11/23 14:25:09.354
[AfterEach] [sig-scheduling] LimitRange
  test/e2e/framework/framework.go:187
Aug 11 14:25:09.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "limitrange-8171" for this suite. 08/11/23 14:25:09.367
{"msg":"PASSED [sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]","completed":82,"skipped":1677,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.240 seconds]
[sig-scheduling] LimitRange
test/e2e/scheduling/framework.go:40
  should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
  test/e2e/scheduling/limit_range.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:25:02.135
    Aug 11 14:25:02.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename limitrange 08/11/23 14:25:02.136
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:02.169
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:02.172
    [It] should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
      test/e2e/scheduling/limit_range.go:57
    STEP: Creating a LimitRange 08/11/23 14:25:02.174
    STEP: Setting up watch 08/11/23 14:25:02.175
    STEP: Submitting a LimitRange 08/11/23 14:25:02.279
    STEP: Verifying LimitRange creation was observed 08/11/23 14:25:02.287
    STEP: Fetching the LimitRange to ensure it has proper values 08/11/23 14:25:02.287
    Aug 11 14:25:02.290: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 11 14:25:02.290: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with no resource requirements 08/11/23 14:25:02.29
    STEP: Ensuring Pod has resource requirements applied from LimitRange 08/11/23 14:25:02.296
    Aug 11 14:25:02.302: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
    Aug 11 14:25:02.302: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Creating a Pod with partial resource requirements 08/11/23 14:25:02.302
    STEP: Ensuring Pod has merged resource requirements applied from LimitRange 08/11/23 14:25:02.309
    Aug 11 14:25:02.315: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
    Aug 11 14:25:02.315: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
    STEP: Failing to create a Pod with less than min resources 08/11/23 14:25:02.315
    STEP: Failing to create a Pod with more than max resources 08/11/23 14:25:02.317
    STEP: Updating a LimitRange 08/11/23 14:25:02.319
    STEP: Verifying LimitRange updating is effective 08/11/23 14:25:02.325
    STEP: Creating a Pod with less than former min resources 08/11/23 14:25:04.33
    STEP: Failing to create a Pod with more than max resources 08/11/23 14:25:04.338
    STEP: Deleting a LimitRange 08/11/23 14:25:04.34
    STEP: Verifying the LimitRange was deleted 08/11/23 14:25:04.349
    Aug 11 14:25:09.354: INFO: limitRange is already deleted
    STEP: Creating a Pod with more than former max resources 08/11/23 14:25:09.354
    [AfterEach] [sig-scheduling] LimitRange
      test/e2e/framework/framework.go:187
    Aug 11 14:25:09.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "limitrange-8171" for this suite. 08/11/23 14:25:09.367
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:25:09.378
Aug 11 14:25:09.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 14:25:09.379
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:09.401
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:09.404
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394
STEP: creating a Service 08/11/23 14:25:09.41
STEP: watching for the Service to be added 08/11/23 14:25:09.427
Aug 11 14:25:09.428: INFO: Found Service test-service-brc94 in namespace services-9721 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
Aug 11 14:25:09.428: INFO: Service test-service-brc94 created
STEP: Getting /status 08/11/23 14:25:09.428
Aug 11 14:25:09.432: INFO: Service test-service-brc94 has LoadBalancer: {[]}
STEP: patching the ServiceStatus 08/11/23 14:25:09.432
STEP: watching for the Service to be patched 08/11/23 14:25:09.439
Aug 11 14:25:09.440: INFO: observed Service test-service-brc94 in namespace services-9721 with annotations: map[] & LoadBalancer: {[]}
Aug 11 14:25:09.440: INFO: Found Service test-service-brc94 in namespace services-9721 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
Aug 11 14:25:09.440: INFO: Service test-service-brc94 has service status patched
STEP: updating the ServiceStatus 08/11/23 14:25:09.44
Aug 11 14:25:09.451: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Service to be updated 08/11/23 14:25:09.451
Aug 11 14:25:09.452: INFO: Observed Service test-service-brc94 in namespace services-9721 with annotations: map[] & Conditions: {[]}
Aug 11 14:25:09.452: INFO: Observed event: &Service{ObjectMeta:{test-service-brc94  services-9721  d115e24b-550e-40a3-acda-ca8d82e77cc0 18660 0 2023-08-11 14:25:09 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-11 14:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-11 14:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.108.212.72,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.108.212.72],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
Aug 11 14:25:09.453: INFO: Found Service test-service-brc94 in namespace services-9721 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 11 14:25:09.453: INFO: Service test-service-brc94 has service status updated
STEP: patching the service 08/11/23 14:25:09.453
STEP: watching for the Service to be patched 08/11/23 14:25:09.465
Aug 11 14:25:09.467: INFO: observed Service test-service-brc94 in namespace services-9721 with labels: map[test-service-static:true]
Aug 11 14:25:09.467: INFO: observed Service test-service-brc94 in namespace services-9721 with labels: map[test-service-static:true]
Aug 11 14:25:09.467: INFO: observed Service test-service-brc94 in namespace services-9721 with labels: map[test-service-static:true]
Aug 11 14:25:09.467: INFO: Found Service test-service-brc94 in namespace services-9721 with labels: map[test-service:patched test-service-static:true]
Aug 11 14:25:09.467: INFO: Service test-service-brc94 patched
STEP: deleting the service 08/11/23 14:25:09.467
STEP: watching for the Service to be deleted 08/11/23 14:25:09.494
Aug 11 14:25:09.496: INFO: Observed event: ADDED
Aug 11 14:25:09.496: INFO: Observed event: MODIFIED
Aug 11 14:25:09.496: INFO: Observed event: MODIFIED
Aug 11 14:25:09.497: INFO: Observed event: MODIFIED
Aug 11 14:25:09.497: INFO: Found Service test-service-brc94 in namespace services-9721 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
Aug 11 14:25:09.497: INFO: Service test-service-brc94 deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 14:25:09.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9721" for this suite. 08/11/23 14:25:09.502
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should complete a service status lifecycle [Conformance]","completed":83,"skipped":1732,"failed":0}
------------------------------
â€¢ [0.132 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should complete a service status lifecycle [Conformance]
  test/e2e/network/service.go:3394

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:25:09.378
    Aug 11 14:25:09.379: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 14:25:09.379
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:09.401
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:09.404
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should complete a service status lifecycle [Conformance]
      test/e2e/network/service.go:3394
    STEP: creating a Service 08/11/23 14:25:09.41
    STEP: watching for the Service to be added 08/11/23 14:25:09.427
    Aug 11 14:25:09.428: INFO: Found Service test-service-brc94 in namespace services-9721 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
    Aug 11 14:25:09.428: INFO: Service test-service-brc94 created
    STEP: Getting /status 08/11/23 14:25:09.428
    Aug 11 14:25:09.432: INFO: Service test-service-brc94 has LoadBalancer: {[]}
    STEP: patching the ServiceStatus 08/11/23 14:25:09.432
    STEP: watching for the Service to be patched 08/11/23 14:25:09.439
    Aug 11 14:25:09.440: INFO: observed Service test-service-brc94 in namespace services-9721 with annotations: map[] & LoadBalancer: {[]}
    Aug 11 14:25:09.440: INFO: Found Service test-service-brc94 in namespace services-9721 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
    Aug 11 14:25:09.440: INFO: Service test-service-brc94 has service status patched
    STEP: updating the ServiceStatus 08/11/23 14:25:09.44
    Aug 11 14:25:09.451: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Service to be updated 08/11/23 14:25:09.451
    Aug 11 14:25:09.452: INFO: Observed Service test-service-brc94 in namespace services-9721 with annotations: map[] & Conditions: {[]}
    Aug 11 14:25:09.452: INFO: Observed event: &Service{ObjectMeta:{test-service-brc94  services-9721  d115e24b-550e-40a3-acda-ca8d82e77cc0 18660 0 2023-08-11 14:25:09 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-11 14:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-11 14:25:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.108.212.72,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.108.212.72],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
    Aug 11 14:25:09.453: INFO: Found Service test-service-brc94 in namespace services-9721 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 11 14:25:09.453: INFO: Service test-service-brc94 has service status updated
    STEP: patching the service 08/11/23 14:25:09.453
    STEP: watching for the Service to be patched 08/11/23 14:25:09.465
    Aug 11 14:25:09.467: INFO: observed Service test-service-brc94 in namespace services-9721 with labels: map[test-service-static:true]
    Aug 11 14:25:09.467: INFO: observed Service test-service-brc94 in namespace services-9721 with labels: map[test-service-static:true]
    Aug 11 14:25:09.467: INFO: observed Service test-service-brc94 in namespace services-9721 with labels: map[test-service-static:true]
    Aug 11 14:25:09.467: INFO: Found Service test-service-brc94 in namespace services-9721 with labels: map[test-service:patched test-service-static:true]
    Aug 11 14:25:09.467: INFO: Service test-service-brc94 patched
    STEP: deleting the service 08/11/23 14:25:09.467
    STEP: watching for the Service to be deleted 08/11/23 14:25:09.494
    Aug 11 14:25:09.496: INFO: Observed event: ADDED
    Aug 11 14:25:09.496: INFO: Observed event: MODIFIED
    Aug 11 14:25:09.496: INFO: Observed event: MODIFIED
    Aug 11 14:25:09.497: INFO: Observed event: MODIFIED
    Aug 11 14:25:09.497: INFO: Found Service test-service-brc94 in namespace services-9721 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
    Aug 11 14:25:09.497: INFO: Service test-service-brc94 deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 14:25:09.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9721" for this suite. 08/11/23 14:25:09.502
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:25:09.512
Aug 11 14:25:09.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:25:09.513
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:09.535
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:09.537
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87
STEP: Creating projection with secret that has name projected-secret-test-map-93e6d56f-33a2-498f-b8f1-a3d8e363f2c3 08/11/23 14:25:09.54
STEP: Creating a pod to test consume secrets 08/11/23 14:25:09.546
Aug 11 14:25:09.555: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c" in namespace "projected-1298" to be "Succeeded or Failed"
Aug 11 14:25:09.557: INFO: Pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822768ms
Aug 11 14:25:11.562: INFO: Pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00717566s
Aug 11 14:25:13.564: INFO: Pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009768139s
STEP: Saw pod success 08/11/23 14:25:13.564
Aug 11 14:25:13.564: INFO: Pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c" satisfied condition "Succeeded or Failed"
Aug 11 14:25:13.568: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:25:13.58
Aug 11 14:25:13.591: INFO: Waiting for pod pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c to disappear
Aug 11 14:25:13.594: INFO: Pod pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 11 14:25:13.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-1298" for this suite. 08/11/23 14:25:13.599
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":84,"skipped":1734,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:87

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:25:09.512
    Aug 11 14:25:09.512: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:25:09.513
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:09.535
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:09.537
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:87
    STEP: Creating projection with secret that has name projected-secret-test-map-93e6d56f-33a2-498f-b8f1-a3d8e363f2c3 08/11/23 14:25:09.54
    STEP: Creating a pod to test consume secrets 08/11/23 14:25:09.546
    Aug 11 14:25:09.555: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c" in namespace "projected-1298" to be "Succeeded or Failed"
    Aug 11 14:25:09.557: INFO: Pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.822768ms
    Aug 11 14:25:11.562: INFO: Pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00717566s
    Aug 11 14:25:13.564: INFO: Pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009768139s
    STEP: Saw pod success 08/11/23 14:25:13.564
    Aug 11 14:25:13.564: INFO: Pod "pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c" satisfied condition "Succeeded or Failed"
    Aug 11 14:25:13.568: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:25:13.58
    Aug 11 14:25:13.591: INFO: Waiting for pod pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c to disappear
    Aug 11 14:25:13.594: INFO: Pod pod-projected-secrets-df5f3107-bc33-4274-a82a-1d574233a23c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 11 14:25:13.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-1298" for this suite. 08/11/23 14:25:13.599
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch
  watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
[BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:25:13.606
Aug 11 14:25:13.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-watch 08/11/23 14:25:13.607
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:13.624
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:13.627
[It] watch on custom resource definition objects [Conformance]
  test/e2e/apimachinery/crd_watch.go:51
Aug 11 14:25:13.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Creating first CR  08/11/23 14:25:16.175
Aug 11 14:25:16.182: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:16Z]] name:name1 resourceVersion:18763 uid:167fce05-a19d-4ac7-b7a7-baf82f720226] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Creating second CR 08/11/23 14:25:26.184
Aug 11 14:25:26.191: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:26Z]] name:name2 resourceVersion:18825 uid:bae130e5-35e0-4e7a-8b0a-f8826666fe0e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying first CR 08/11/23 14:25:36.195
Aug 11 14:25:36.204: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:36Z]] name:name1 resourceVersion:18886 uid:167fce05-a19d-4ac7-b7a7-baf82f720226] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Modifying second CR 08/11/23 14:25:46.205
Aug 11 14:25:46.211: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:46Z]] name:name2 resourceVersion:18941 uid:bae130e5-35e0-4e7a-8b0a-f8826666fe0e] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting first CR 08/11/23 14:25:56.216
Aug 11 14:25:56.226: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:36Z]] name:name1 resourceVersion:18993 uid:167fce05-a19d-4ac7-b7a7-baf82f720226] num:map[num1:9223372036854775807 num2:1000000]]}
STEP: Deleting second CR 08/11/23 14:26:06.23
Aug 11 14:26:06.239: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:46Z]] name:name2 resourceVersion:19047 uid:bae130e5-35e0-4e7a-8b0a-f8826666fe0e] num:map[num1:9223372036854775807 num2:1000000]]}
[AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:26:16.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-watch-6793" for this suite. 08/11/23 14:26:16.758
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]","completed":85,"skipped":1735,"failed":0}
------------------------------
â€¢ [SLOW TEST] [63.159 seconds]
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  CustomResourceDefinition Watch
  test/e2e/apimachinery/crd_watch.go:44
    watch on custom resource definition objects [Conformance]
    test/e2e/apimachinery/crd_watch.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:25:13.606
    Aug 11 14:25:13.606: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-watch 08/11/23 14:25:13.607
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:25:13.624
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:25:13.627
    [It] watch on custom resource definition objects [Conformance]
      test/e2e/apimachinery/crd_watch.go:51
    Aug 11 14:25:13.630: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Creating first CR  08/11/23 14:25:16.175
    Aug 11 14:25:16.182: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:16Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:16Z]] name:name1 resourceVersion:18763 uid:167fce05-a19d-4ac7-b7a7-baf82f720226] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Creating second CR 08/11/23 14:25:26.184
    Aug 11 14:25:26.191: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:26Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:26Z]] name:name2 resourceVersion:18825 uid:bae130e5-35e0-4e7a-8b0a-f8826666fe0e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying first CR 08/11/23 14:25:36.195
    Aug 11 14:25:36.204: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:36Z]] name:name1 resourceVersion:18886 uid:167fce05-a19d-4ac7-b7a7-baf82f720226] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Modifying second CR 08/11/23 14:25:46.205
    Aug 11 14:25:46.211: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:46Z]] name:name2 resourceVersion:18941 uid:bae130e5-35e0-4e7a-8b0a-f8826666fe0e] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting first CR 08/11/23 14:25:56.216
    Aug 11 14:25:56.226: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:16Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:36Z]] name:name1 resourceVersion:18993 uid:167fce05-a19d-4ac7-b7a7-baf82f720226] num:map[num1:9223372036854775807 num2:1000000]]}
    STEP: Deleting second CR 08/11/23 14:26:06.23
    Aug 11 14:26:06.239: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-11T14:25:26Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-11T14:25:46Z]] name:name2 resourceVersion:19047 uid:bae130e5-35e0-4e7a-8b0a-f8826666fe0e] num:map[num1:9223372036854775807 num2:1000000]]}
    [AfterEach] [sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:26:16.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-watch-6793" for this suite. 08/11/23 14:26:16.758
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] RuntimeClass
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:26:16.765
Aug 11 14:26:16.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:26:16.766
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:16.782
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:16.785
[It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 11 14:26:16.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-9904" for this suite. 08/11/23 14:26:16.799
{"msg":"PASSED [sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]","completed":86,"skipped":1735,"failed":0}
------------------------------
â€¢ [0.042 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:26:16.765
    Aug 11 14:26:16.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:26:16.766
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:16.782
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:16.785
    [It] should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:55
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 11 14:26:16.794: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-9904" for this suite. 08/11/23 14:26:16.799
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits
  should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:26:16.807
Aug 11 14:26:16.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-runtime 08/11/23 14:26:16.808
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:16.822
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:16.825
[It] should run with the expected status [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:51
STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/11/23 14:26:16.837
STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/11/23 14:26:32.911
STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/11/23 14:26:32.915
STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/11/23 14:26:32.921
STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/11/23 14:26:32.921
STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/11/23 14:26:32.946
STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/11/23 14:26:35.966
STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/11/23 14:26:37.978
STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/11/23 14:26:37.985
STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/11/23 14:26:37.985
STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/11/23 14:26:38.009
STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/11/23 14:26:39.02
STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/11/23 14:26:42.036
STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/11/23 14:26:42.042
STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/11/23 14:26:42.042
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 11 14:26:42.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2750" for this suite. 08/11/23 14:26:42.074
{"msg":"PASSED [sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]","completed":87,"skipped":1742,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.274 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    when starting a container that exits
    test/e2e/common/node/runtime.go:44
      should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:26:16.807
    Aug 11 14:26:16.807: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-runtime 08/11/23 14:26:16.808
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:16.822
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:16.825
    [It] should run with the expected status [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:51
    STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' 08/11/23 14:26:16.837
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' 08/11/23 14:26:32.911
    STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition 08/11/23 14:26:32.915
    STEP: Container 'terminate-cmd-rpa': should get the expected 'State' 08/11/23 14:26:32.921
    STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] 08/11/23 14:26:32.921
    STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' 08/11/23 14:26:32.946
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' 08/11/23 14:26:35.966
    STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition 08/11/23 14:26:37.978
    STEP: Container 'terminate-cmd-rpof': should get the expected 'State' 08/11/23 14:26:37.985
    STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] 08/11/23 14:26:37.985
    STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' 08/11/23 14:26:38.009
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' 08/11/23 14:26:39.02
    STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition 08/11/23 14:26:42.036
    STEP: Container 'terminate-cmd-rpn': should get the expected 'State' 08/11/23 14:26:42.042
    STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] 08/11/23 14:26:42.042
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 11 14:26:42.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2750" for this suite. 08/11/23 14:26:42.074
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Downward API
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:26:42.081
Aug 11 14:26:42.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 14:26:42.082
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:42.098
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:42.1
[It] should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89
STEP: Creating a pod to test downward api env vars 08/11/23 14:26:42.103
Aug 11 14:26:42.117: INFO: Waiting up to 5m0s for pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4" in namespace "downward-api-7307" to be "Succeeded or Failed"
Aug 11 14:26:42.123: INFO: Pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963965ms
Aug 11 14:26:44.127: INFO: Pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010751129s
Aug 11 14:26:46.129: INFO: Pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011961714s
STEP: Saw pod success 08/11/23 14:26:46.129
Aug 11 14:26:46.129: INFO: Pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4" satisfied condition "Succeeded or Failed"
Aug 11 14:26:46.132: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4 container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:26:46.153
Aug 11 14:26:46.165: INFO: Waiting for pod downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4 to disappear
Aug 11 14:26:46.168: INFO: Pod downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 11 14:26:46.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-7307" for this suite. 08/11/23 14:26:46.172
{"msg":"PASSED [sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]","completed":88,"skipped":1742,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide host IP as an env var [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:89

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:26:42.081
    Aug 11 14:26:42.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:26:42.082
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:42.098
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:42.1
    [It] should provide host IP as an env var [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:89
    STEP: Creating a pod to test downward api env vars 08/11/23 14:26:42.103
    Aug 11 14:26:42.117: INFO: Waiting up to 5m0s for pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4" in namespace "downward-api-7307" to be "Succeeded or Failed"
    Aug 11 14:26:42.123: INFO: Pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.963965ms
    Aug 11 14:26:44.127: INFO: Pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010751129s
    Aug 11 14:26:46.129: INFO: Pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011961714s
    STEP: Saw pod success 08/11/23 14:26:46.129
    Aug 11 14:26:46.129: INFO: Pod "downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4" satisfied condition "Succeeded or Failed"
    Aug 11 14:26:46.132: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:26:46.153
    Aug 11 14:26:46.165: INFO: Waiting for pod downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4 to disappear
    Aug 11 14:26:46.168: INFO: Pod downward-api-31c9d69f-b9a6-4728-adc6-a57a81a875c4 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 11 14:26:46.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-7307" for this suite. 08/11/23 14:26:46.172
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:26:46.181
Aug 11 14:26:46.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replicaset 08/11/23 14:26:46.181
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:46.198
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:46.201
[It] should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111
Aug 11 14:26:46.204: INFO: Creating ReplicaSet my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155
Aug 11 14:26:46.213: INFO: Pod name my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155: Found 0 pods out of 1
Aug 11 14:26:51.221: INFO: Pod name my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155: Found 1 pods out of 1
Aug 11 14:26:51.221: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155" is running
Aug 11 14:26:51.221: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2" in namespace "replicaset-3438" to be "running"
Aug 11 14:26:51.224: INFO: Pod "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2": Phase="Running", Reason="", readiness=true. Elapsed: 3.742147ms
Aug 11 14:26:51.224: INFO: Pod "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2" satisfied condition "running"
Aug 11 14:26:51.224: INFO: Pod "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:26:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:26:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:26:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:26:46 +0000 UTC Reason: Message:}])
Aug 11 14:26:51.225: INFO: Trying to dial the pod
Aug 11 14:26:56.243: INFO: Controller my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155: Got expected result from replica 1 [my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2]: "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2", 1 of 1 required successes so far
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 11 14:26:56.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3438" for this suite. 08/11/23 14:26:56.248
{"msg":"PASSED [sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]","completed":89,"skipped":1784,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.075 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should serve a basic image on each replica with a public image  [Conformance]
  test/e2e/apps/replica_set.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:26:46.181
    Aug 11 14:26:46.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replicaset 08/11/23 14:26:46.181
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:46.198
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:46.201
    [It] should serve a basic image on each replica with a public image  [Conformance]
      test/e2e/apps/replica_set.go:111
    Aug 11 14:26:46.204: INFO: Creating ReplicaSet my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155
    Aug 11 14:26:46.213: INFO: Pod name my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155: Found 0 pods out of 1
    Aug 11 14:26:51.221: INFO: Pod name my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155: Found 1 pods out of 1
    Aug 11 14:26:51.221: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155" is running
    Aug 11 14:26:51.221: INFO: Waiting up to 5m0s for pod "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2" in namespace "replicaset-3438" to be "running"
    Aug 11 14:26:51.224: INFO: Pod "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2": Phase="Running", Reason="", readiness=true. Elapsed: 3.742147ms
    Aug 11 14:26:51.224: INFO: Pod "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2" satisfied condition "running"
    Aug 11 14:26:51.224: INFO: Pod "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:26:46 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:26:47 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:26:47 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-11 14:26:46 +0000 UTC Reason: Message:}])
    Aug 11 14:26:51.225: INFO: Trying to dial the pod
    Aug 11 14:26:56.243: INFO: Controller my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155: Got expected result from replica 1 [my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2]: "my-hostname-basic-a5c0990a-a0b0-41b7-864f-d72e4ac20155-c2sv2", 1 of 1 required successes so far
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 11 14:26:56.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3438" for this suite. 08/11/23 14:26:56.248
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] Aggregator
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:26:56.256
Aug 11 14:26:56.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename aggregator 08/11/23 14:26:56.257
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:56.273
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:56.276
[BeforeEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:78
Aug 11 14:26:56.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
[It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100
STEP: Registering the sample API server. 08/11/23 14:26:56.279
Aug 11 14:26:56.836: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
Aug 11 14:26:58.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:00.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:02.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:04.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:06.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:08.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:10.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:12.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:14.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:16.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:18.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:27:21.033: INFO: Waited 129.014133ms for the sample-apiserver to be ready to handle requests.
STEP: Read Status for v1alpha1.wardle.example.com 08/11/23 14:27:21.099
STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/11/23 14:27:21.104
STEP: List APIServices 08/11/23 14:27:21.111
Aug 11 14:27:21.118: INFO: Found v1alpha1.wardle.example.com in APIServiceList
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/apimachinery/aggregator.go:68
[AfterEach] [sig-api-machinery] Aggregator
  test/e2e/framework/framework.go:187
Aug 11 14:27:21.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "aggregator-9287" for this suite. 08/11/23 14:27:21.514
{"msg":"PASSED [sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]","completed":90,"skipped":1784,"failed":0}
------------------------------
â€¢ [SLOW TEST] [25.311 seconds]
[sig-api-machinery] Aggregator
test/e2e/apimachinery/framework.go:23
  Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
  test/e2e/apimachinery/aggregator.go:100

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:26:56.256
    Aug 11 14:26:56.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename aggregator 08/11/23 14:26:56.257
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:26:56.273
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:26:56.276
    [BeforeEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:78
    Aug 11 14:26:56.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    [It] Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
      test/e2e/apimachinery/aggregator.go:100
    STEP: Registering the sample API server. 08/11/23 14:26:56.279
    Aug 11 14:26:56.836: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
    Aug 11 14:26:58.891: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:00.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:02.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:04.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:06.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:08.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:10.895: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:12.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:14.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:16.896: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:18.897: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 26, 56, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-5885c99c55\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:27:21.033: INFO: Waited 129.014133ms for the sample-apiserver to be ready to handle requests.
    STEP: Read Status for v1alpha1.wardle.example.com 08/11/23 14:27:21.099
    STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' 08/11/23 14:27:21.104
    STEP: List APIServices 08/11/23 14:27:21.111
    Aug 11 14:27:21.118: INFO: Found v1alpha1.wardle.example.com in APIServiceList
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/apimachinery/aggregator.go:68
    [AfterEach] [sig-api-machinery] Aggregator
      test/e2e/framework/framework.go:187
    Aug 11 14:27:21.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "aggregator-9287" for this suite. 08/11/23 14:27:21.514
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] DNS
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:27:21.569
Aug 11 14:27:21.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename dns 08/11/23 14:27:21.57
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:21.588
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:21.591
[It] should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290
STEP: Creating a test headless service 08/11/23 14:27:21.594
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1507.svc.cluster.local;sleep 1; done
 08/11/23 14:27:21.6
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1507.svc.cluster.local;sleep 1; done
 08/11/23 14:27:21.601
STEP: creating a pod to probe DNS 08/11/23 14:27:21.601
STEP: submitting the pod to kubernetes 08/11/23 14:27:21.601
Aug 11 14:27:21.613: INFO: Waiting up to 15m0s for pod "dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e" in namespace "dns-1507" to be "running"
Aug 11 14:27:21.619: INFO: Pod "dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.290046ms
Aug 11 14:27:23.625: INFO: Pod "dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012301069s
Aug 11 14:27:23.625: INFO: Pod "dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e" satisfied condition "running"
STEP: retrieving the pod 08/11/23 14:27:23.625
STEP: looking for the results for each expected name from probers 08/11/23 14:27:23.629
Aug 11 14:27:23.642: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:23.650: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:23.665: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:23.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:23.678: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:23.685: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:23.693: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-test-service-2.dns-1507.svc.cluster.local]

Aug 11 14:27:28.702: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:28.709: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:28.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:28.735: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:28.748: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

Aug 11 14:27:33.703: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:33.710: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:33.731: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:33.737: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:33.750: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

Aug 11 14:27:38.701: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:38.708: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:38.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:38.735: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:38.748: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

Aug 11 14:27:43.703: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:43.711: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:43.731: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:43.737: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:43.751: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

Aug 11 14:27:48.702: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:48.709: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:48.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:48.735: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
Aug 11 14:27:48.749: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

Aug 11 14:27:53.750: INFO: DNS probes using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e succeeded

STEP: deleting the pod 08/11/23 14:27:53.75
STEP: deleting the test headless service 08/11/23 14:27:53.766
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 11 14:27:53.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1507" for this suite. 08/11/23 14:27:53.792
{"msg":"PASSED [sig-network] DNS should provide DNS for pods for Subdomain [Conformance]","completed":91,"skipped":1789,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.230 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for pods for Subdomain [Conformance]
  test/e2e/network/dns.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:27:21.569
    Aug 11 14:27:21.569: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename dns 08/11/23 14:27:21.57
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:21.588
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:21.591
    [It] should provide DNS for pods for Subdomain [Conformance]
      test/e2e/network/dns.go:290
    STEP: Creating a test headless service 08/11/23 14:27:21.594
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1507.svc.cluster.local;sleep 1; done
     08/11/23 14:27:21.6
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1507.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1507.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1507.svc.cluster.local;sleep 1; done
     08/11/23 14:27:21.601
    STEP: creating a pod to probe DNS 08/11/23 14:27:21.601
    STEP: submitting the pod to kubernetes 08/11/23 14:27:21.601
    Aug 11 14:27:21.613: INFO: Waiting up to 15m0s for pod "dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e" in namespace "dns-1507" to be "running"
    Aug 11 14:27:21.619: INFO: Pod "dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e": Phase="Pending", Reason="", readiness=false. Elapsed: 6.290046ms
    Aug 11 14:27:23.625: INFO: Pod "dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e": Phase="Running", Reason="", readiness=true. Elapsed: 2.012301069s
    Aug 11 14:27:23.625: INFO: Pod "dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 14:27:23.625
    STEP: looking for the results for each expected name from probers 08/11/23 14:27:23.629
    Aug 11 14:27:23.642: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:23.650: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:23.665: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:23.671: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:23.678: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:23.685: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:23.693: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-test-service-2.dns-1507.svc.cluster.local]

    Aug 11 14:27:28.702: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:28.709: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:28.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:28.735: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:28.748: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

    Aug 11 14:27:33.703: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:33.710: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:33.731: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:33.737: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:33.750: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

    Aug 11 14:27:38.701: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:38.708: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:38.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:38.735: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:38.748: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

    Aug 11 14:27:43.703: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:43.711: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:43.731: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:43.737: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:43.751: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

    Aug 11 14:27:48.702: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:48.709: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:48.729: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:48.735: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local from pod dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e: the server could not find the requested resource (get pods dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e)
    Aug 11 14:27:48.749: INFO: Lookups using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1507.svc.cluster.local]

    Aug 11 14:27:53.750: INFO: DNS probes using dns-1507/dns-test-6f6cc657-f898-4943-b923-60ad60ef9c8e succeeded

    STEP: deleting the pod 08/11/23 14:27:53.75
    STEP: deleting the test headless service 08/11/23 14:27:53.766
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 11 14:27:53.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1507" for this suite. 08/11/23 14:27:53.792
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:27:53.799
Aug 11 14:27:53.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:27:53.801
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:53.815
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:53.818
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:27:53.821
Aug 11 14:27:53.831: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5" in namespace "projected-3095" to be "Succeeded or Failed"
Aug 11 14:27:53.838: INFO: Pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.542943ms
Aug 11 14:27:55.843: INFO: Pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012096101s
Aug 11 14:27:57.843: INFO: Pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011441989s
STEP: Saw pod success 08/11/23 14:27:57.843
Aug 11 14:27:57.843: INFO: Pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5" satisfied condition "Succeeded or Failed"
Aug 11 14:27:57.846: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5 container client-container: <nil>
STEP: delete the pod 08/11/23 14:27:57.861
Aug 11 14:27:57.880: INFO: Waiting for pod downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5 to disappear
Aug 11 14:27:57.883: INFO: Pod downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 14:27:57.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3095" for this suite. 08/11/23 14:27:57.887
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]","completed":92,"skipped":1791,"failed":0}
------------------------------
â€¢ [4.094 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:27:53.799
    Aug 11 14:27:53.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:27:53.801
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:53.815
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:53.818
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:52
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:27:53.821
    Aug 11 14:27:53.831: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5" in namespace "projected-3095" to be "Succeeded or Failed"
    Aug 11 14:27:53.838: INFO: Pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.542943ms
    Aug 11 14:27:55.843: INFO: Pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012096101s
    Aug 11 14:27:57.843: INFO: Pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011441989s
    STEP: Saw pod success 08/11/23 14:27:57.843
    Aug 11 14:27:57.843: INFO: Pod "downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5" satisfied condition "Succeeded or Failed"
    Aug 11 14:27:57.846: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:27:57.861
    Aug 11 14:27:57.880: INFO: Waiting for pod downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5 to disappear
    Aug 11 14:27:57.883: INFO: Pod downwardapi-volume-b9c09ecd-4815-4c0d-8bb1-a0f2d15e14b5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 14:27:57.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3095" for this suite. 08/11/23 14:27:57.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:27:57.894
Aug 11 14:27:57.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:27:57.895
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:57.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:57.916
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46
STEP: Creating secret with name secret-test-b15754d0-fc82-4701-a21d-00c43b5365bf 08/11/23 14:27:57.919
STEP: Creating a pod to test consume secrets 08/11/23 14:27:57.924
Aug 11 14:27:57.933: INFO: Waiting up to 5m0s for pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f" in namespace "secrets-1667" to be "Succeeded or Failed"
Aug 11 14:27:57.939: INFO: Pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7866ms
Aug 11 14:27:59.944: INFO: Pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010774672s
Aug 11 14:28:01.943: INFO: Pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009834991s
STEP: Saw pod success 08/11/23 14:28:01.943
Aug 11 14:28:01.943: INFO: Pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f" satisfied condition "Succeeded or Failed"
Aug 11 14:28:01.946: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:28:01.956
Aug 11 14:28:01.967: INFO: Waiting for pod pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f to disappear
Aug 11 14:28:01.970: INFO: Pod pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:28:01.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1667" for this suite. 08/11/23 14:28:01.974
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]","completed":93,"skipped":1808,"failed":0}
------------------------------
â€¢ [4.086 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:27:57.894
    Aug 11 14:27:57.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:27:57.895
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:27:57.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:27:57.916
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:46
    STEP: Creating secret with name secret-test-b15754d0-fc82-4701-a21d-00c43b5365bf 08/11/23 14:27:57.919
    STEP: Creating a pod to test consume secrets 08/11/23 14:27:57.924
    Aug 11 14:27:57.933: INFO: Waiting up to 5m0s for pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f" in namespace "secrets-1667" to be "Succeeded or Failed"
    Aug 11 14:27:57.939: INFO: Pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7866ms
    Aug 11 14:27:59.944: INFO: Pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010774672s
    Aug 11 14:28:01.943: INFO: Pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009834991s
    STEP: Saw pod success 08/11/23 14:28:01.943
    Aug 11 14:28:01.943: INFO: Pod "pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f" satisfied condition "Succeeded or Failed"
    Aug 11 14:28:01.946: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:28:01.956
    Aug 11 14:28:01.967: INFO: Waiting for pod pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f to disappear
    Aug 11 14:28:01.970: INFO: Pod pod-secrets-426c72b2-ed32-47a8-9d34-96ebccc3928f no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:28:01.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1667" for this suite. 08/11/23 14:28:01.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:28:01.981
Aug 11 14:28:01.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename job 08/11/23 14:28:01.982
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:01.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:01.999
[It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194
STEP: Creating Indexed job 08/11/23 14:28:02.001
STEP: Ensuring job reaches completions 08/11/23 14:28:02.008
STEP: Ensuring pods with index for job exist 08/11/23 14:28:12.012
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 11 14:28:12.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-7847" for this suite. 08/11/23 14:28:12.02
{"msg":"PASSED [sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]","completed":94,"skipped":1815,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.047 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
  test/e2e/apps/job.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:28:01.981
    Aug 11 14:28:01.981: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename job 08/11/23 14:28:01.982
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:01.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:01.999
    [It] should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
      test/e2e/apps/job.go:194
    STEP: Creating Indexed job 08/11/23 14:28:02.001
    STEP: Ensuring job reaches completions 08/11/23 14:28:02.008
    STEP: Ensuring pods with index for job exist 08/11/23 14:28:12.012
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 11 14:28:12.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-7847" for this suite. 08/11/23 14:28:12.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:28:12.03
Aug 11 14:28:12.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:28:12.031
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:12.05
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:12.052
[It] should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137
STEP: Creating configMap that has name configmap-test-emptyKey-5ac4f84b-511e-434b-a56a-f606dc8a1c9d 08/11/23 14:28:12.055
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:28:12.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7900" for this suite. 08/11/23 14:28:12.061
{"msg":"PASSED [sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]","completed":95,"skipped":1873,"failed":0}
------------------------------
â€¢ [0.037 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should fail to create ConfigMap with empty key [Conformance]
  test/e2e/common/node/configmap.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:28:12.03
    Aug 11 14:28:12.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:28:12.031
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:12.05
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:12.052
    [It] should fail to create ConfigMap with empty key [Conformance]
      test/e2e/common/node/configmap.go:137
    STEP: Creating configMap that has name configmap-test-emptyKey-5ac4f84b-511e-434b-a56a-f606dc8a1c9d 08/11/23 14:28:12.055
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:28:12.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7900" for this suite. 08/11/23 14:28:12.061
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
[BeforeEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:28:12.069
Aug 11 14:28:12.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/11/23 14:28:12.07
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:12.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:12.089
[It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63
STEP: Setting up the test 08/11/23 14:28:12.092
STEP: Creating hostNetwork=false pod 08/11/23 14:28:12.092
Aug 11 14:28:12.102: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2639" to be "running and ready"
Aug 11 14:28:12.105: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292773ms
Aug 11 14:28:12.105: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:28:14.111: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009519363s
Aug 11 14:28:14.111: INFO: The phase of Pod test-pod is Running (Ready = true)
Aug 11 14:28:14.111: INFO: Pod "test-pod" satisfied condition "running and ready"
STEP: Creating hostNetwork=true pod 08/11/23 14:28:14.115
Aug 11 14:28:14.122: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2639" to be "running and ready"
Aug 11 14:28:14.129: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.765551ms
Aug 11 14:28:14.129: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:28:16.133: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011463123s
Aug 11 14:28:16.133: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
Aug 11 14:28:16.133: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
STEP: Running the test 08/11/23 14:28:16.137
STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/11/23 14:28:16.137
Aug 11 14:28:16.137: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.137: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.137: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 11 14:28:16.208: INFO: Exec stderr: ""
Aug 11 14:28:16.208: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.208: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.208: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 11 14:28:16.280: INFO: Exec stderr: ""
Aug 11 14:28:16.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.280: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.280: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 11 14:28:16.360: INFO: Exec stderr: ""
Aug 11 14:28:16.360: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.361: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.361: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 11 14:28:16.437: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/11/23 14:28:16.437
Aug 11 14:28:16.437: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.437: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.437: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 11 14:28:16.501: INFO: Exec stderr: ""
Aug 11 14:28:16.501: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.502: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.502: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
Aug 11 14:28:16.580: INFO: Exec stderr: ""
STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/11/23 14:28:16.58
Aug 11 14:28:16.580: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.581: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.581: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 11 14:28:16.667: INFO: Exec stderr: ""
Aug 11 14:28:16.667: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.668: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.668: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
Aug 11 14:28:16.744: INFO: Exec stderr: ""
Aug 11 14:28:16.744: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.744: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.744: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 11 14:28:16.822: INFO: Exec stderr: ""
Aug 11 14:28:16.822: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:28:16.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:28:16.823: INFO: ExecWithOptions: Clientset creation
Aug 11 14:28:16.823: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
Aug 11 14:28:16.913: INFO: Exec stderr: ""
[AfterEach] [sig-node] KubeletManagedEtcHosts
  test/e2e/framework/framework.go:187
Aug 11 14:28:16.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "e2e-kubelet-etc-hosts-2639" for this suite. 08/11/23 14:28:16.918
{"msg":"PASSED [sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]","completed":96,"skipped":1913,"failed":0}
------------------------------
â€¢ [4.856 seconds]
[sig-node] KubeletManagedEtcHosts
test/e2e/common/node/framework.go:23
  should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet_etc_hosts.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:28:12.069
    Aug 11 14:28:12.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts 08/11/23 14:28:12.07
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:12.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:12.089
    [It] should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet_etc_hosts.go:63
    STEP: Setting up the test 08/11/23 14:28:12.092
    STEP: Creating hostNetwork=false pod 08/11/23 14:28:12.092
    Aug 11 14:28:12.102: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "e2e-kubelet-etc-hosts-2639" to be "running and ready"
    Aug 11 14:28:12.105: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.292773ms
    Aug 11 14:28:12.105: INFO: The phase of Pod test-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:28:14.111: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009519363s
    Aug 11 14:28:14.111: INFO: The phase of Pod test-pod is Running (Ready = true)
    Aug 11 14:28:14.111: INFO: Pod "test-pod" satisfied condition "running and ready"
    STEP: Creating hostNetwork=true pod 08/11/23 14:28:14.115
    Aug 11 14:28:14.122: INFO: Waiting up to 5m0s for pod "test-host-network-pod" in namespace "e2e-kubelet-etc-hosts-2639" to be "running and ready"
    Aug 11 14:28:14.129: INFO: Pod "test-host-network-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 6.765551ms
    Aug 11 14:28:14.129: INFO: The phase of Pod test-host-network-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:28:16.133: INFO: Pod "test-host-network-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011463123s
    Aug 11 14:28:16.133: INFO: The phase of Pod test-host-network-pod is Running (Ready = true)
    Aug 11 14:28:16.133: INFO: Pod "test-host-network-pod" satisfied condition "running and ready"
    STEP: Running the test 08/11/23 14:28:16.137
    STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false 08/11/23 14:28:16.137
    Aug 11 14:28:16.137: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.137: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.137: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.137: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 11 14:28:16.208: INFO: Exec stderr: ""
    Aug 11 14:28:16.208: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.208: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.208: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.208: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 11 14:28:16.280: INFO: Exec stderr: ""
    Aug 11 14:28:16.280: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.280: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.280: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 11 14:28:16.360: INFO: Exec stderr: ""
    Aug 11 14:28:16.360: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.361: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.361: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 11 14:28:16.437: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount 08/11/23 14:28:16.437
    Aug 11 14:28:16.437: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.437: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.437: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.437: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 11 14:28:16.501: INFO: Exec stderr: ""
    Aug 11 14:28:16.501: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.502: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.502: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
    Aug 11 14:28:16.580: INFO: Exec stderr: ""
    STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true 08/11/23 14:28:16.58
    Aug 11 14:28:16.580: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.581: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.581: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 11 14:28:16.667: INFO: Exec stderr: ""
    Aug 11 14:28:16.667: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.667: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.668: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.668: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
    Aug 11 14:28:16.744: INFO: Exec stderr: ""
    Aug 11 14:28:16.744: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.744: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.744: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 11 14:28:16.822: INFO: Exec stderr: ""
    Aug 11 14:28:16.822: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-2639 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:28:16.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:28:16.823: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:28:16.823: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-2639/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
    Aug 11 14:28:16.913: INFO: Exec stderr: ""
    [AfterEach] [sig-node] KubeletManagedEtcHosts
      test/e2e/framework/framework.go:187
    Aug 11 14:28:16.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "e2e-kubelet-etc-hosts-2639" for this suite. 08/11/23 14:28:16.918
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:28:16.926
Aug 11 14:28:16.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replication-controller 08/11/23 14:28:16.927
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:16.943
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:16.946
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82
Aug 11 14:28:16.949: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/11/23 14:28:16.958
STEP: Checking rc "condition-test" has the desired failure condition set 08/11/23 14:28:16.964
STEP: Scaling down rc "condition-test" to satisfy pod quota 08/11/23 14:28:17.972
Aug 11 14:28:17.982: INFO: Updating replication controller "condition-test"
STEP: Checking rc "condition-test" has no failure condition set 08/11/23 14:28:17.982
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 11 14:28:17.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-6576" for this suite. 08/11/23 14:28:17.994
{"msg":"PASSED [sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]","completed":97,"skipped":1931,"failed":0}
------------------------------
â€¢ [1.074 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should surface a failure condition on a common issue like exceeded quota [Conformance]
  test/e2e/apps/rc.go:82

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:28:16.926
    Aug 11 14:28:16.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replication-controller 08/11/23 14:28:16.927
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:16.943
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:16.946
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should surface a failure condition on a common issue like exceeded quota [Conformance]
      test/e2e/apps/rc.go:82
    Aug 11 14:28:16.949: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
    STEP: Creating rc "condition-test" that asks for more than the allowed pod quota 08/11/23 14:28:16.958
    STEP: Checking rc "condition-test" has the desired failure condition set 08/11/23 14:28:16.964
    STEP: Scaling down rc "condition-test" to satisfy pod quota 08/11/23 14:28:17.972
    Aug 11 14:28:17.982: INFO: Updating replication controller "condition-test"
    STEP: Checking rc "condition-test" has no failure condition set 08/11/23 14:28:17.982
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 11 14:28:17.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-6576" for this suite. 08/11/23 14:28:17.994
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:28:18.001
Aug 11 14:28:18.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:28:18.002
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:18.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:18.02
[It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186
STEP: Creating a pod to test emptydir 0777 on node default medium 08/11/23 14:28:18.023
Aug 11 14:28:18.033: INFO: Waiting up to 5m0s for pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0" in namespace "emptydir-8097" to be "Succeeded or Failed"
Aug 11 14:28:18.040: INFO: Pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.701508ms
Aug 11 14:28:20.045: INFO: Pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011876726s
Aug 11 14:28:22.047: INFO: Pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013912406s
STEP: Saw pod success 08/11/23 14:28:22.047
Aug 11 14:28:22.047: INFO: Pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0" satisfied condition "Succeeded or Failed"
Aug 11 14:28:22.050: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0 container test-container: <nil>
STEP: delete the pod 08/11/23 14:28:22.076
Aug 11 14:28:22.093: INFO: Waiting for pod pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0 to disappear
Aug 11 14:28:22.097: INFO: Pod pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 14:28:22.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8097" for this suite. 08/11/23 14:28:22.102
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":98,"skipped":1931,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:186

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:28:18.001
    Aug 11 14:28:18.001: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:28:18.002
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:18.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:18.02
    [It] should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:186
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/11/23 14:28:18.023
    Aug 11 14:28:18.033: INFO: Waiting up to 5m0s for pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0" in namespace "emptydir-8097" to be "Succeeded or Failed"
    Aug 11 14:28:18.040: INFO: Pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.701508ms
    Aug 11 14:28:20.045: INFO: Pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011876726s
    Aug 11 14:28:22.047: INFO: Pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.013912406s
    STEP: Saw pod success 08/11/23 14:28:22.047
    Aug 11 14:28:22.047: INFO: Pod "pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0" satisfied condition "Succeeded or Failed"
    Aug 11 14:28:22.050: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:28:22.076
    Aug 11 14:28:22.093: INFO: Waiting for pod pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0 to disappear
    Aug 11 14:28:22.097: INFO: Pod pod-fed1cd43-3d21-4c20-8b3d-1e7ac0a8cfb0 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:28:22.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8097" for this suite. 08/11/23 14:28:22.102
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:28:22.111
Aug 11 14:28:22.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename cronjob 08/11/23 14:28:22.111
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:22.13
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:22.133
[It] should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96
STEP: Creating a suspended cronjob 08/11/23 14:28:22.136
STEP: Ensuring no jobs are scheduled 08/11/23 14:28:22.142
STEP: Ensuring no job exists by listing jobs explicitly 08/11/23 14:33:22.149
STEP: Removing cronjob 08/11/23 14:33:22.152
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 11 14:33:22.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5680" for this suite. 08/11/23 14:33:22.164
{"msg":"PASSED [sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]","completed":99,"skipped":1974,"failed":0}
------------------------------
â€¢ [SLOW TEST] [300.059 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule jobs when suspended [Slow] [Conformance]
  test/e2e/apps/cronjob.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:28:22.111
    Aug 11 14:28:22.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename cronjob 08/11/23 14:28:22.111
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:28:22.13
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:28:22.133
    [It] should not schedule jobs when suspended [Slow] [Conformance]
      test/e2e/apps/cronjob.go:96
    STEP: Creating a suspended cronjob 08/11/23 14:28:22.136
    STEP: Ensuring no jobs are scheduled 08/11/23 14:28:22.142
    STEP: Ensuring no job exists by listing jobs explicitly 08/11/23 14:33:22.149
    STEP: Removing cronjob 08/11/23 14:33:22.152
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 11 14:33:22.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5680" for this suite. 08/11/23 14:33:22.164
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:33:22.171
Aug 11 14:33:22.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename deployment 08/11/23 14:33:22.172
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:22.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:22.189
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132
Aug 11 14:33:22.200: INFO: Pod name rollover-pod: Found 0 pods out of 1
Aug 11 14:33:27.205: INFO: Pod name rollover-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 14:33:27.205
Aug 11 14:33:27.205: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
Aug 11 14:33:29.210: INFO: Creating deployment "test-rollover-deployment"
Aug 11 14:33:29.219: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
Aug 11 14:33:31.227: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
Aug 11 14:33:31.235: INFO: Ensure that both replica sets have 1 created replica
Aug 11 14:33:31.242: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
Aug 11 14:33:31.253: INFO: Updating deployment test-rollover-deployment
Aug 11 14:33:31.253: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
Aug 11 14:33:33.263: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
Aug 11 14:33:33.273: INFO: Make sure deployment "test-rollover-deployment" is complete
Aug 11 14:33:33.279: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:33:33.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:33:35.287: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:33:35.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:33:37.287: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:33:37.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:33:39.287: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:33:39.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:33:41.288: INFO: all replica sets need to contain the pod-template-hash label
Aug 11 14:33:41.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 14:33:43.289: INFO: 
Aug 11 14:33:43.289: INFO: Ensure that both old replica sets have no replicas
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 14:33:43.298: INFO: Deployment "test-rollover-deployment":
&Deployment{ObjectMeta:{test-rollover-deployment  deployment-3858  88fd71bf-af0d-4573-b349-1f97dbf359fd 22254 2 2023-08-11 14:33:29 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e0c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 14:33:29 +0000 UTC,LastTransitionTime:2023-08-11 14:33:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-08-11 14:33:42 +0000 UTC,LastTransitionTime:2023-08-11 14:33:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 11 14:33:43.301: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
&ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3858  a13cec08-4607-4f19-8f2a-f86d1a39ad6f 22243 2 2023-08-11 14:33:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 88fd71bf-af0d-4573-b349-1f97dbf359fd 0xc0035e1217 0xc0035e1218}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fd71bf-af0d-4573-b349-1f97dbf359fd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e12c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:33:43.301: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
Aug 11 14:33:43.302: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3858  b4c2bf54-c743-42eb-988e-1f54b9f7bc88 22252 2 2023-08-11 14:33:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 88fd71bf-af0d-4573-b349-1f97dbf359fd 0xc0035e0fc7 0xc0035e0fc8}] [] [{e2e.test Update apps/v1 2023-08-11 14:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fd71bf-af0d-4573-b349-1f97dbf359fd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0035e1088 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:33:43.302: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3858  c8295849-969e-49aa-9cbc-046a61226f71 22159 2 2023-08-11 14:33:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 88fd71bf-af0d-4573-b349-1f97dbf359fd 0xc0035e10f7 0xc0035e10f8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fd71bf-af0d-4573-b349-1f97dbf359fd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e11a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:33:43.305: INFO: Pod "test-rollover-deployment-6d45fd857b-8k2tv" is available:
&Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-8k2tv test-rollover-deployment-6d45fd857b- deployment-3858  c2830fb9-805c-4ed4-b7d4-37eff9b2d8ef 22186 0 2023-08-11 14:33:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a13cec08-4607-4f19-8f2a-f86d1a39ad6f 0xc0035e1827 0xc0035e1828}] [] [{kube-controller-manager Update v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a13cec08-4607-4f19-8f2a-f86d1a39ad6f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:33:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vl69q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vl69q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:33:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:33:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:33:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:33:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.216,StartTime:2023-08-11 14:33:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:33:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://ddd693a2f057c12f2170c00f7df9a18ee40fd4b69030892cd886495941a9fbb6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 11 14:33:43.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3858" for this suite. 08/11/23 14:33:43.309
{"msg":"PASSED [sig-apps] Deployment deployment should support rollover [Conformance]","completed":100,"skipped":1994,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.147 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support rollover [Conformance]
  test/e2e/apps/deployment.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:33:22.171
    Aug 11 14:33:22.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename deployment 08/11/23 14:33:22.172
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:22.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:22.189
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support rollover [Conformance]
      test/e2e/apps/deployment.go:132
    Aug 11 14:33:22.200: INFO: Pod name rollover-pod: Found 0 pods out of 1
    Aug 11 14:33:27.205: INFO: Pod name rollover-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 14:33:27.205
    Aug 11 14:33:27.205: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
    Aug 11 14:33:29.210: INFO: Creating deployment "test-rollover-deployment"
    Aug 11 14:33:29.219: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
    Aug 11 14:33:31.227: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
    Aug 11 14:33:31.235: INFO: Ensure that both replica sets have 1 created replica
    Aug 11 14:33:31.242: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
    Aug 11 14:33:31.253: INFO: Updating deployment test-rollover-deployment
    Aug 11 14:33:31.253: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
    Aug 11 14:33:33.263: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
    Aug 11 14:33:33.273: INFO: Make sure deployment "test-rollover-deployment" is complete
    Aug 11 14:33:33.279: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:33:33.279: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:33:35.287: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:33:35.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:33:37.287: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:33:37.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:33:39.287: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:33:39.287: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:33:41.288: INFO: all replica sets need to contain the pod-template-hash label
    Aug 11 14:33:41.288: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 14, 33, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 14, 33, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-6d45fd857b\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 14:33:43.289: INFO: 
    Aug 11 14:33:43.289: INFO: Ensure that both old replica sets have no replicas
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 14:33:43.298: INFO: Deployment "test-rollover-deployment":
    &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3858  88fd71bf-af0d-4573-b349-1f97dbf359fd 22254 2 2023-08-11 14:33:29 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e0c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 14:33:29 +0000 UTC,LastTransitionTime:2023-08-11 14:33:29 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-6d45fd857b" has successfully progressed.,LastUpdateTime:2023-08-11 14:33:42 +0000 UTC,LastTransitionTime:2023-08-11 14:33:29 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 11 14:33:43.301: INFO: New ReplicaSet "test-rollover-deployment-6d45fd857b" of Deployment "test-rollover-deployment":
    &ReplicaSet{ObjectMeta:{test-rollover-deployment-6d45fd857b  deployment-3858  a13cec08-4607-4f19-8f2a-f86d1a39ad6f 22243 2 2023-08-11 14:33:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 88fd71bf-af0d-4573-b349-1f97dbf359fd 0xc0035e1217 0xc0035e1218}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fd71bf-af0d-4573-b349-1f97dbf359fd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:42 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 6d45fd857b,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e12c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:33:43.301: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
    Aug 11 14:33:43.302: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3858  b4c2bf54-c743-42eb-988e-1f54b9f7bc88 22252 2 2023-08-11 14:33:22 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 88fd71bf-af0d-4573-b349-1f97dbf359fd 0xc0035e0fc7 0xc0035e0fc8}] [] [{e2e.test Update apps/v1 2023-08-11 14:33:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:42 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fd71bf-af0d-4573-b349-1f97dbf359fd\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:42 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0035e1088 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:33:43.302: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-59b9df946d  deployment-3858  c8295849-969e-49aa-9cbc-046a61226f71 22159 2 2023-08-11 14:33:29 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 88fd71bf-af0d-4573-b349-1f97dbf359fd 0xc0035e10f7 0xc0035e10f8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"88fd71bf-af0d-4573-b349-1f97dbf359fd\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 59b9df946d,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:59b9df946d] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035e11a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:33:43.305: INFO: Pod "test-rollover-deployment-6d45fd857b-8k2tv" is available:
    &Pod{ObjectMeta:{test-rollover-deployment-6d45fd857b-8k2tv test-rollover-deployment-6d45fd857b- deployment-3858  c2830fb9-805c-4ed4-b7d4-37eff9b2d8ef 22186 0 2023-08-11 14:33:31 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:6d45fd857b] map[] [{apps/v1 ReplicaSet test-rollover-deployment-6d45fd857b a13cec08-4607-4f19-8f2a-f86d1a39ad6f 0xc0035e1827 0xc0035e1828}] [] [{kube-controller-manager Update v1 2023-08-11 14:33:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a13cec08-4607-4f19-8f2a-f86d1a39ad6f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:33:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vl69q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vl69q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:33:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:33:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:33:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:33:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.216,StartTime:2023-08-11 14:33:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:33:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://ddd693a2f057c12f2170c00f7df9a18ee40fd4b69030892cd886495941a9fbb6,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.216,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 11 14:33:43.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3858" for this suite. 08/11/23 14:33:43.309
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:33:43.32
Aug 11 14:33:43.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename events 08/11/23 14:33:43.321
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:43.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:43.338
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175
STEP: Create set of events 08/11/23 14:33:43.341
Aug 11 14:33:43.349: INFO: created test-event-1
Aug 11 14:33:43.353: INFO: created test-event-2
Aug 11 14:33:43.357: INFO: created test-event-3
STEP: get a list of Events with a label in the current namespace 08/11/23 14:33:43.357
STEP: delete collection of events 08/11/23 14:33:43.36
Aug 11 14:33:43.360: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/11/23 14:33:43.38
Aug 11 14:33:43.380: INFO: requesting list of events to confirm quantity
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Aug 11 14:33:43.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-2070" for this suite. 08/11/23 14:33:43.387
{"msg":"PASSED [sig-instrumentation] Events should delete a collection of events [Conformance]","completed":101,"skipped":2046,"failed":0}
------------------------------
â€¢ [0.073 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/core_events.go:175

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:33:43.32
    Aug 11 14:33:43.320: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename events 08/11/23 14:33:43.321
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:43.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:43.338
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/core_events.go:175
    STEP: Create set of events 08/11/23 14:33:43.341
    Aug 11 14:33:43.349: INFO: created test-event-1
    Aug 11 14:33:43.353: INFO: created test-event-2
    Aug 11 14:33:43.357: INFO: created test-event-3
    STEP: get a list of Events with a label in the current namespace 08/11/23 14:33:43.357
    STEP: delete collection of events 08/11/23 14:33:43.36
    Aug 11 14:33:43.360: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/11/23 14:33:43.38
    Aug 11 14:33:43.380: INFO: requesting list of events to confirm quantity
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Aug 11 14:33:43.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-2070" for this suite. 08/11/23 14:33:43.387
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:33:43.395
Aug 11 14:33:43.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:33:43.396
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:43.409
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:43.412
[It]  should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189
STEP: getting /apis 08/11/23 14:33:43.415
STEP: getting /apis/node.k8s.io 08/11/23 14:33:43.416
STEP: getting /apis/node.k8s.io/v1 08/11/23 14:33:43.417
STEP: creating 08/11/23 14:33:43.418
STEP: watching 08/11/23 14:33:43.434
Aug 11 14:33:43.434: INFO: starting watch
STEP: getting 08/11/23 14:33:43.44
STEP: listing 08/11/23 14:33:43.443
STEP: patching 08/11/23 14:33:43.446
STEP: updating 08/11/23 14:33:43.451
Aug 11 14:33:43.456: INFO: waiting for watch events with expected annotations
STEP: deleting 08/11/23 14:33:43.456
STEP: deleting a collection 08/11/23 14:33:43.467
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 11 14:33:43.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5313" for this suite. 08/11/23 14:33:43.486
{"msg":"PASSED [sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]","completed":102,"skipped":2067,"failed":0}
------------------------------
â€¢ [0.100 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
   should support RuntimeClasses API operations [Conformance]
  test/e2e/common/node/runtimeclass.go:189

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:33:43.395
    Aug 11 14:33:43.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:33:43.396
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:43.409
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:43.412
    [It]  should support RuntimeClasses API operations [Conformance]
      test/e2e/common/node/runtimeclass.go:189
    STEP: getting /apis 08/11/23 14:33:43.415
    STEP: getting /apis/node.k8s.io 08/11/23 14:33:43.416
    STEP: getting /apis/node.k8s.io/v1 08/11/23 14:33:43.417
    STEP: creating 08/11/23 14:33:43.418
    STEP: watching 08/11/23 14:33:43.434
    Aug 11 14:33:43.434: INFO: starting watch
    STEP: getting 08/11/23 14:33:43.44
    STEP: listing 08/11/23 14:33:43.443
    STEP: patching 08/11/23 14:33:43.446
    STEP: updating 08/11/23 14:33:43.451
    Aug 11 14:33:43.456: INFO: waiting for watch events with expected annotations
    STEP: deleting 08/11/23 14:33:43.456
    STEP: deleting a collection 08/11/23 14:33:43.467
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 11 14:33:43.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5313" for this suite. 08/11/23 14:33:43.486
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:33:43.496
Aug 11 14:33:43.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename daemonsets 08/11/23 14:33:43.497
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:43.512
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:43.514
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373
Aug 11 14:33:43.533: INFO: Creating simple daemon set daemon-set
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:33:43.539
Aug 11 14:33:43.544: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:43.544: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:43.544: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:43.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:33:43.550: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:33:44.555: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:44.555: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:44.555: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:44.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:33:44.559: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:33:45.555: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:45.556: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:45.556: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:45.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:33:45.559: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
STEP: Update daemon pods image. 08/11/23 14:33:45.571
STEP: Check that daemon pods images are updated. 08/11/23 14:33:45.584
Aug 11 14:33:45.587: INFO: Wrong image for pod: daemon-set-h9q7m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Aug 11 14:33:45.587: INFO: Wrong image for pod: daemon-set-mg8g2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Aug 11 14:33:45.591: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:45.591: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:45.591: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:46.596: INFO: Wrong image for pod: daemon-set-mg8g2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Aug 11 14:33:46.603: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:46.603: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:46.603: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:47.596: INFO: Wrong image for pod: daemon-set-mg8g2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Aug 11 14:33:47.601: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:47.601: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:47.601: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:48.596: INFO: Wrong image for pod: daemon-set-mg8g2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
Aug 11 14:33:48.596: INFO: Pod daemon-set-mpdkw is not available
Aug 11 14:33:48.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:48.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:48.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:49.599: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:49.599: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:49.599: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:50.596: INFO: Pod daemon-set-l277z is not available
Aug 11 14:33:50.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:50.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:50.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
STEP: Check that daemon pods are still running on every node of the cluster. 08/11/23 14:33:50.6
Aug 11 14:33:50.604: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:50.604: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:50.604: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:50.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 14:33:50.607: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 14:33:51.613: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:51.613: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:51.613: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 14:33:51.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
Aug 11 14:33:51.616: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:33:51.631
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9312, will wait for the garbage collector to delete the pods 08/11/23 14:33:51.632
Aug 11 14:33:51.692: INFO: Deleting DaemonSet.extensions daemon-set took: 7.624728ms
Aug 11 14:33:51.793: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.058354ms
Aug 11 14:33:54.198: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 14:33:54.198: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 14:33:54.202: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22487"},"items":null}

Aug 11 14:33:54.205: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22487"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:33:54.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-9312" for this suite. 08/11/23 14:33:54.223
{"msg":"PASSED [sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]","completed":103,"skipped":2098,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.733 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
  test/e2e/apps/daemon_set.go:373

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:33:43.496
    Aug 11 14:33:43.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename daemonsets 08/11/23 14:33:43.497
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:43.512
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:43.514
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
      test/e2e/apps/daemon_set.go:373
    Aug 11 14:33:43.533: INFO: Creating simple daemon set daemon-set
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 14:33:43.539
    Aug 11 14:33:43.544: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:43.544: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:43.544: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:43.550: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:33:43.550: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:33:44.555: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:44.555: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:44.555: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:44.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:33:44.559: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:33:45.555: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:45.556: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:45.556: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:45.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:33:45.559: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    STEP: Update daemon pods image. 08/11/23 14:33:45.571
    STEP: Check that daemon pods images are updated. 08/11/23 14:33:45.584
    Aug 11 14:33:45.587: INFO: Wrong image for pod: daemon-set-h9q7m. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Aug 11 14:33:45.587: INFO: Wrong image for pod: daemon-set-mg8g2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Aug 11 14:33:45.591: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:45.591: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:45.591: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:46.596: INFO: Wrong image for pod: daemon-set-mg8g2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Aug 11 14:33:46.603: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:46.603: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:46.603: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:47.596: INFO: Wrong image for pod: daemon-set-mg8g2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Aug 11 14:33:47.601: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:47.601: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:47.601: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:48.596: INFO: Wrong image for pod: daemon-set-mg8g2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.40, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-2.
    Aug 11 14:33:48.596: INFO: Pod daemon-set-mpdkw is not available
    Aug 11 14:33:48.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:48.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:48.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:49.599: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:49.599: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:49.599: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:50.596: INFO: Pod daemon-set-l277z is not available
    Aug 11 14:33:50.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:50.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:50.600: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    STEP: Check that daemon pods are still running on every node of the cluster. 08/11/23 14:33:50.6
    Aug 11 14:33:50.604: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:50.604: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:50.604: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:50.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 14:33:50.607: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 14:33:51.613: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:51.613: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:51.613: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 14:33:51.616: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
    Aug 11 14:33:51.616: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 14:33:51.631
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9312, will wait for the garbage collector to delete the pods 08/11/23 14:33:51.632
    Aug 11 14:33:51.692: INFO: Deleting DaemonSet.extensions daemon-set took: 7.624728ms
    Aug 11 14:33:51.793: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.058354ms
    Aug 11 14:33:54.198: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 14:33:54.198: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 14:33:54.202: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"22487"},"items":null}

    Aug 11 14:33:54.205: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"22487"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:33:54.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-9312" for this suite. 08/11/23 14:33:54.223
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:33:54.231
Aug 11 14:33:54.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:33:54.232
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:54.251
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:54.253
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:33:54.256
Aug 11 14:33:54.265: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d" in namespace "projected-3536" to be "Succeeded or Failed"
Aug 11 14:33:54.271: INFO: Pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064189ms
Aug 11 14:33:56.277: INFO: Pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011287097s
Aug 11 14:33:58.277: INFO: Pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01215416s
STEP: Saw pod success 08/11/23 14:33:58.277
Aug 11 14:33:58.278: INFO: Pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d" satisfied condition "Succeeded or Failed"
Aug 11 14:33:58.281: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d container client-container: <nil>
STEP: delete the pod 08/11/23 14:33:58.305
Aug 11 14:33:58.322: INFO: Waiting for pod downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d to disappear
Aug 11 14:33:58.325: INFO: Pod downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 14:33:58.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3536" for this suite. 08/11/23 14:33:58.329
{"msg":"PASSED [sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":104,"skipped":2119,"failed":0}
------------------------------
â€¢ [4.104 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:33:54.231
    Aug 11 14:33:54.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:33:54.232
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:54.251
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:54.253
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:83
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:33:54.256
    Aug 11 14:33:54.265: INFO: Waiting up to 5m0s for pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d" in namespace "projected-3536" to be "Succeeded or Failed"
    Aug 11 14:33:54.271: INFO: Pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d": Phase="Pending", Reason="", readiness=false. Elapsed: 6.064189ms
    Aug 11 14:33:56.277: INFO: Pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011287097s
    Aug 11 14:33:58.277: INFO: Pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01215416s
    STEP: Saw pod success 08/11/23 14:33:58.277
    Aug 11 14:33:58.278: INFO: Pod "downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d" satisfied condition "Succeeded or Failed"
    Aug 11 14:33:58.281: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d container client-container: <nil>
    STEP: delete the pod 08/11/23 14:33:58.305
    Aug 11 14:33:58.322: INFO: Waiting for pod downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d to disappear
    Aug 11 14:33:58.325: INFO: Pod downwardapi-volume-4ae377ba-00cc-4b9e-8185-5d5fba40583d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 14:33:58.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3536" for this suite. 08/11/23 14:33:58.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo
  should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:33:58.339
Aug 11 14:33:58.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:33:58.34
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:58.355
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:58.358
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Update Demo
  test/e2e/kubectl/kubectl.go:324
[It] should create and stop a replication controller  [Conformance]
  test/e2e/kubectl/kubectl.go:337
STEP: creating a replication controller 08/11/23 14:33:58.361
Aug 11 14:33:58.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 create -f -'
Aug 11 14:33:59.158: INFO: stderr: ""
Aug 11 14:33:59.158: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:33:59.158
Aug 11 14:33:59.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:33:59.226: INFO: stderr: ""
Aug 11 14:33:59.226: INFO: stdout: "update-demo-nautilus-pbgw7 update-demo-nautilus-vq5s8 "
Aug 11 14:33:59.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-pbgw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:33:59.283: INFO: stderr: ""
Aug 11 14:33:59.283: INFO: stdout: ""
Aug 11 14:33:59.283: INFO: update-demo-nautilus-pbgw7 is created but not running
Aug 11 14:34:04.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
Aug 11 14:34:04.344: INFO: stderr: ""
Aug 11 14:34:04.344: INFO: stdout: "update-demo-nautilus-pbgw7 update-demo-nautilus-vq5s8 "
Aug 11 14:34:04.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-pbgw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:34:04.404: INFO: stderr: ""
Aug 11 14:34:04.404: INFO: stdout: "true"
Aug 11 14:34:04.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-pbgw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:34:04.460: INFO: stderr: ""
Aug 11 14:34:04.460: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 11 14:34:04.460: INFO: validating pod update-demo-nautilus-pbgw7
Aug 11 14:34:04.475: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:34:04.475: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:34:04.475: INFO: update-demo-nautilus-pbgw7 is verified up and running
Aug 11 14:34:04.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-vq5s8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
Aug 11 14:34:04.531: INFO: stderr: ""
Aug 11 14:34:04.531: INFO: stdout: "true"
Aug 11 14:34:04.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-vq5s8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
Aug 11 14:34:04.591: INFO: stderr: ""
Aug 11 14:34:04.591: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
Aug 11 14:34:04.591: INFO: validating pod update-demo-nautilus-vq5s8
Aug 11 14:34:04.607: INFO: got data: {
  "image": "nautilus.jpg"
}

Aug 11 14:34:04.607: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
Aug 11 14:34:04.607: INFO: update-demo-nautilus-vq5s8 is verified up and running
STEP: using delete to clean up resources 08/11/23 14:34:04.607
Aug 11 14:34:04.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 delete --grace-period=0 --force -f -'
Aug 11 14:34:04.670: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 14:34:04.670: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
Aug 11 14:34:04.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get rc,svc -l name=update-demo --no-headers'
Aug 11 14:34:04.751: INFO: stderr: "No resources found in kubectl-5245 namespace.\n"
Aug 11 14:34:04.751: INFO: stdout: ""
Aug 11 14:34:04.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 11 14:34:04.819: INFO: stderr: ""
Aug 11 14:34:04.819: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:34:04.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5245" for this suite. 08/11/23 14:34:04.824
{"msg":"PASSED [sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]","completed":105,"skipped":2171,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.491 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Update Demo
  test/e2e/kubectl/kubectl.go:322
    should create and stop a replication controller  [Conformance]
    test/e2e/kubectl/kubectl.go:337

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:33:58.339
    Aug 11 14:33:58.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:33:58.34
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:33:58.355
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:33:58.358
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Update Demo
      test/e2e/kubectl/kubectl.go:324
    [It] should create and stop a replication controller  [Conformance]
      test/e2e/kubectl/kubectl.go:337
    STEP: creating a replication controller 08/11/23 14:33:58.361
    Aug 11 14:33:58.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 create -f -'
    Aug 11 14:33:59.158: INFO: stderr: ""
    Aug 11 14:33:59.158: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
    STEP: waiting for all containers in name=update-demo pods to come up. 08/11/23 14:33:59.158
    Aug 11 14:33:59.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:33:59.226: INFO: stderr: ""
    Aug 11 14:33:59.226: INFO: stdout: "update-demo-nautilus-pbgw7 update-demo-nautilus-vq5s8 "
    Aug 11 14:33:59.226: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-pbgw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:33:59.283: INFO: stderr: ""
    Aug 11 14:33:59.283: INFO: stdout: ""
    Aug 11 14:33:59.283: INFO: update-demo-nautilus-pbgw7 is created but not running
    Aug 11 14:34:04.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
    Aug 11 14:34:04.344: INFO: stderr: ""
    Aug 11 14:34:04.344: INFO: stdout: "update-demo-nautilus-pbgw7 update-demo-nautilus-vq5s8 "
    Aug 11 14:34:04.344: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-pbgw7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:34:04.404: INFO: stderr: ""
    Aug 11 14:34:04.404: INFO: stdout: "true"
    Aug 11 14:34:04.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-pbgw7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:34:04.460: INFO: stderr: ""
    Aug 11 14:34:04.460: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 11 14:34:04.460: INFO: validating pod update-demo-nautilus-pbgw7
    Aug 11 14:34:04.475: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:34:04.475: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:34:04.475: INFO: update-demo-nautilus-pbgw7 is verified up and running
    Aug 11 14:34:04.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-vq5s8 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
    Aug 11 14:34:04.531: INFO: stderr: ""
    Aug 11 14:34:04.531: INFO: stdout: "true"
    Aug 11 14:34:04.531: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods update-demo-nautilus-vq5s8 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
    Aug 11 14:34:04.591: INFO: stderr: ""
    Aug 11 14:34:04.591: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.5"
    Aug 11 14:34:04.591: INFO: validating pod update-demo-nautilus-vq5s8
    Aug 11 14:34:04.607: INFO: got data: {
      "image": "nautilus.jpg"
    }

    Aug 11 14:34:04.607: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
    Aug 11 14:34:04.607: INFO: update-demo-nautilus-vq5s8 is verified up and running
    STEP: using delete to clean up resources 08/11/23 14:34:04.607
    Aug 11 14:34:04.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 delete --grace-period=0 --force -f -'
    Aug 11 14:34:04.670: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 14:34:04.670: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
    Aug 11 14:34:04.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get rc,svc -l name=update-demo --no-headers'
    Aug 11 14:34:04.751: INFO: stderr: "No resources found in kubectl-5245 namespace.\n"
    Aug 11 14:34:04.751: INFO: stdout: ""
    Aug 11 14:34:04.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5245 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 11 14:34:04.819: INFO: stderr: ""
    Aug 11 14:34:04.819: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:34:04.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5245" for this suite. 08/11/23 14:34:04.824
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:34:04.832
Aug 11 14:34:04.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:34:04.833
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:34:04.851
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:34:04.854
[It] should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793
STEP: Creating a ResourceQuota with best effort scope 08/11/23 14:34:04.857
STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:34:04.862
STEP: Creating a ResourceQuota with not best effort scope 08/11/23 14:34:06.866
STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:34:06.871
STEP: Creating a best-effort pod 08/11/23 14:34:08.876
STEP: Ensuring resource quota with best effort scope captures the pod usage 08/11/23 14:34:08.89
STEP: Ensuring resource quota with not best effort ignored the pod usage 08/11/23 14:34:10.895
STEP: Deleting the pod 08/11/23 14:34:12.9
STEP: Ensuring resource quota status released the pod usage 08/11/23 14:34:12.917
STEP: Creating a not best-effort pod 08/11/23 14:34:14.921
STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/11/23 14:34:14.933
STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/11/23 14:34:16.937
STEP: Deleting the pod 08/11/23 14:34:18.943
STEP: Ensuring resource quota status released the pod usage 08/11/23 14:34:18.959
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 14:34:20.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3652" for this suite. 08/11/23 14:34:20.969
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]","completed":106,"skipped":2198,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.145 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with best effort scope. [Conformance]
  test/e2e/apimachinery/resource_quota.go:793

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:34:04.832
    Aug 11 14:34:04.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:34:04.833
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:34:04.851
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:34:04.854
    [It] should verify ResourceQuota with best effort scope. [Conformance]
      test/e2e/apimachinery/resource_quota.go:793
    STEP: Creating a ResourceQuota with best effort scope 08/11/23 14:34:04.857
    STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:34:04.862
    STEP: Creating a ResourceQuota with not best effort scope 08/11/23 14:34:06.866
    STEP: Ensuring ResourceQuota status is calculated 08/11/23 14:34:06.871
    STEP: Creating a best-effort pod 08/11/23 14:34:08.876
    STEP: Ensuring resource quota with best effort scope captures the pod usage 08/11/23 14:34:08.89
    STEP: Ensuring resource quota with not best effort ignored the pod usage 08/11/23 14:34:10.895
    STEP: Deleting the pod 08/11/23 14:34:12.9
    STEP: Ensuring resource quota status released the pod usage 08/11/23 14:34:12.917
    STEP: Creating a not best-effort pod 08/11/23 14:34:14.921
    STEP: Ensuring resource quota with not best effort scope captures the pod usage 08/11/23 14:34:14.933
    STEP: Ensuring resource quota with best effort scope ignored the pod usage 08/11/23 14:34:16.937
    STEP: Deleting the pod 08/11/23 14:34:18.943
    STEP: Ensuring resource quota status released the pod usage 08/11/23 14:34:18.959
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 14:34:20.964: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3652" for this suite. 08/11/23 14:34:20.969
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:34:20.98
Aug 11 14:34:20.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename watch 08/11/23 14:34:20.981
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:34:21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:34:21.003
[It] should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142
STEP: creating a new configmap 08/11/23 14:34:21.005
STEP: modifying the configmap once 08/11/23 14:34:21.01
STEP: modifying the configmap a second time 08/11/23 14:34:21.018
STEP: deleting the configmap 08/11/23 14:34:21.026
STEP: creating a watch on configmaps from the resource version returned by the first update 08/11/23 14:34:21.031
STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/11/23 14:34:21.032
Aug 11 14:34:21.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5916  4f2272a8-1eb4-43a5-9d8e-02b7c3ffadad 22785 0 2023-08-11 14:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:34:21.033: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5916  4f2272a8-1eb4-43a5-9d8e-02b7c3ffadad 22786 0 2023-08-11 14:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 11 14:34:21.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-5916" for this suite. 08/11/23 14:34:21.037
{"msg":"PASSED [sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]","completed":107,"skipped":2256,"failed":0}
------------------------------
â€¢ [0.064 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should be able to start watching from a specific resource version [Conformance]
  test/e2e/apimachinery/watch.go:142

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:34:20.98
    Aug 11 14:34:20.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename watch 08/11/23 14:34:20.981
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:34:21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:34:21.003
    [It] should be able to start watching from a specific resource version [Conformance]
      test/e2e/apimachinery/watch.go:142
    STEP: creating a new configmap 08/11/23 14:34:21.005
    STEP: modifying the configmap once 08/11/23 14:34:21.01
    STEP: modifying the configmap a second time 08/11/23 14:34:21.018
    STEP: deleting the configmap 08/11/23 14:34:21.026
    STEP: creating a watch on configmaps from the resource version returned by the first update 08/11/23 14:34:21.031
    STEP: Expecting to observe notifications for all changes to the configmap after the first update 08/11/23 14:34:21.032
    Aug 11 14:34:21.032: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5916  4f2272a8-1eb4-43a5-9d8e-02b7c3ffadad 22785 0 2023-08-11 14:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:34:21.033: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5916  4f2272a8-1eb4-43a5-9d8e-02b7c3ffadad 22786 0 2023-08-11 14:34:20 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:20 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 11 14:34:21.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-5916" for this suite. 08/11/23 14:34:21.037
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:34:21.045
Aug 11 14:34:21.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename watch 08/11/23 14:34:21.046
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:34:21.06
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:34:21.062
[It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257
STEP: creating a watch on configmaps with a certain label 08/11/23 14:34:21.065
STEP: creating a new configmap 08/11/23 14:34:21.066
STEP: modifying the configmap once 08/11/23 14:34:21.071
STEP: changing the label value of the configmap 08/11/23 14:34:21.08
STEP: Expecting to observe a delete notification for the watched object 08/11/23 14:34:21.087
Aug 11 14:34:21.088: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22791 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:34:21.088: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22792 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:34:21.088: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22793 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
STEP: modifying the configmap a second time 08/11/23 14:34:21.088
STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/11/23 14:34:21.096
STEP: changing the label value of the configmap back 08/11/23 14:34:31.096
STEP: modifying the configmap a third time 08/11/23 14:34:31.108
STEP: deleting the configmap 08/11/23 14:34:31.116
STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/11/23 14:34:31.122
Aug 11 14:34:31.123: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22863 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:34:31.123: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22864 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
Aug 11 14:34:31.123: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22865 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 11 14:34:31.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-2227" for this suite. 08/11/23 14:34:31.128
{"msg":"PASSED [sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]","completed":108,"skipped":2262,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.090 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
  test/e2e/apimachinery/watch.go:257

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:34:21.045
    Aug 11 14:34:21.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename watch 08/11/23 14:34:21.046
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:34:21.06
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:34:21.062
    [It] should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
      test/e2e/apimachinery/watch.go:257
    STEP: creating a watch on configmaps with a certain label 08/11/23 14:34:21.065
    STEP: creating a new configmap 08/11/23 14:34:21.066
    STEP: modifying the configmap once 08/11/23 14:34:21.071
    STEP: changing the label value of the configmap 08/11/23 14:34:21.08
    STEP: Expecting to observe a delete notification for the watched object 08/11/23 14:34:21.087
    Aug 11 14:34:21.088: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22791 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:21 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:34:21.088: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22792 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:34:21.088: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22793 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
    STEP: modifying the configmap a second time 08/11/23 14:34:21.088
    STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements 08/11/23 14:34:21.096
    STEP: changing the label value of the configmap back 08/11/23 14:34:31.096
    STEP: modifying the configmap a third time 08/11/23 14:34:31.108
    STEP: deleting the configmap 08/11/23 14:34:31.116
    STEP: Expecting to observe an add notification for the watched object when the label value was restored 08/11/23 14:34:31.122
    Aug 11 14:34:31.123: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22863 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:34:31.123: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22864 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    Aug 11 14:34:31.123: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-2227  4f26b735-0398-4f58-8acc-1c77e7641587 22865 0 2023-08-11 14:34:21 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-11 14:34:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 11 14:34:31.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-2227" for this suite. 08/11/23 14:34:31.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:34:31.136
Aug 11 14:34:31.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-pred 08/11/23 14:34:31.137
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:34:31.154
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:34:31.157
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 11 14:34:31.159: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 11 14:34:31.169: INFO: Waiting for terminating namespaces to be deleted...
Aug 11 14:34:31.172: INFO: 
Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-0lc3 before test
Aug 11 14:34:31.180: INFO: cilium-6s7tr from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.180: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 14:34:31.180: INFO: cilium-operator-5bfff4c47c-92tfw from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.180: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 11 14:34:31.180: INFO: coredns-6c49cf4575-pkkl2 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.180: INFO: 	Container coredns ready: true, restart count 0
Aug 11 14:34:31.180: INFO: csi-gce-pd-node-dsdbs from kube-system started at 2023-08-11 13:55:05 +0000 UTC (2 container statuses recorded)
Aug 11 14:34:31.180: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 14:34:31.180: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 14:34:31.180: INFO: gcp-guest-agent-f5ptd from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.180: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 14:34:31.180: INFO: konnectivity-agent-prvxg from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.180: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 14:34:31.180: INFO: kube-proxy-vbf6p from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.180: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 14:34:31.180: INFO: verification-service-ttpg4 from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.180: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 14:34:31.180: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 14:34:31.180: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:34:31.180: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 14:34:31.180: INFO: 
Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-wzlp before test
Aug 11 14:34:31.188: INFO: cilium-dcbmm from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.188: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 14:34:31.188: INFO: csi-gce-pd-node-tnkjk from kube-system started at 2023-08-11 13:55:09 +0000 UTC (2 container statuses recorded)
Aug 11 14:34:31.188: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 14:34:31.188: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 14:34:31.188: INFO: gcp-guest-agent-7fj57 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.188: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 14:34:31.188: INFO: konnectivity-agent-bwlfc from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.188: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 14:34:31.188: INFO: kube-proxy-9wx2l from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.188: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 14:34:31.188: INFO: verification-service-xmjlz from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.188: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 14:34:31.188: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:01:59 +0000 UTC (1 container statuses recorded)
Aug 11 14:34:31.188: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 11 14:34:31.188: INFO: sonobuoy-e2e-job-2008a9ff359d4340 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 14:34:31.188: INFO: 	Container e2e ready: true, restart count 0
Aug 11 14:34:31.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:34:31.188: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 14:34:31.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 14:34:31.188: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 14:34:31.189
Aug 11 14:34:31.198: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7730" to be "running"
Aug 11 14:34:31.201: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.007424ms
Aug 11 14:34:33.207: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008915274s
Aug 11 14:34:33.207: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 14:34:33.211
STEP: Trying to apply a random label on the found node. 08/11/23 14:34:33.232
STEP: verifying the node has the label kubernetes.io/e2e-ce6d9657-d061-4be8-9fdb-1378feefd18c 95 08/11/23 14:34:33.244
STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/11/23 14:34:33.255
Aug 11 14:34:33.261: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7730" to be "not pending"
Aug 11 14:34:33.267: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.592565ms
Aug 11 14:34:35.271: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009872254s
Aug 11 14:34:35.271: INFO: Pod "pod4" satisfied condition "not pending"
STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.178.2 on the node which pod4 resides and expect not scheduled 08/11/23 14:34:35.271
Aug 11 14:34:35.278: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7730" to be "not pending"
Aug 11 14:34:35.283: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.725268ms
Aug 11 14:34:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008942035s
Aug 11 14:34:39.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009286934s
Aug 11 14:34:41.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010701226s
Aug 11 14:34:43.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009312129s
Aug 11 14:34:45.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009022786s
Aug 11 14:34:47.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00891333s
Aug 11 14:34:49.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009899809s
Aug 11 14:34:51.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010786793s
Aug 11 14:34:53.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010579872s
Aug 11 14:34:55.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010756743s
Aug 11 14:34:57.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009402968s
Aug 11 14:34:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010142418s
Aug 11 14:35:01.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010682102s
Aug 11 14:35:03.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010590536s
Aug 11 14:35:05.292: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.014100144s
Aug 11 14:35:07.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00962684s
Aug 11 14:35:09.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009677518s
Aug 11 14:35:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010421847s
Aug 11 14:35:13.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010783805s
Aug 11 14:35:15.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009460129s
Aug 11 14:35:17.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009493358s
Aug 11 14:35:19.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010927608s
Aug 11 14:35:21.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010228334s
Aug 11 14:35:23.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010276232s
Aug 11 14:35:25.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011083033s
Aug 11 14:35:27.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01006072s
Aug 11 14:35:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010156498s
Aug 11 14:35:31.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009661359s
Aug 11 14:35:33.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009723718s
Aug 11 14:35:35.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010861861s
Aug 11 14:35:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009082821s
Aug 11 14:35:39.286: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008543191s
Aug 11 14:35:41.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010085525s
Aug 11 14:35:43.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010688192s
Aug 11 14:35:45.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010746201s
Aug 11 14:35:47.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00954746s
Aug 11 14:35:49.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009515295s
Aug 11 14:35:51.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008948274s
Aug 11 14:35:53.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009957663s
Aug 11 14:35:55.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009784265s
Aug 11 14:35:57.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009568724s
Aug 11 14:35:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010366406s
Aug 11 14:36:01.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010200487s
Aug 11 14:36:03.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.0110283s
Aug 11 14:36:05.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009189509s
Aug 11 14:36:07.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009743463s
Aug 11 14:36:09.291: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012956871s
Aug 11 14:36:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009935682s
Aug 11 14:36:13.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010829109s
Aug 11 14:36:15.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010243186s
Aug 11 14:36:17.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00920142s
Aug 11 14:36:19.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.01126426s
Aug 11 14:36:21.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010707561s
Aug 11 14:36:23.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00955172s
Aug 11 14:36:25.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011105025s
Aug 11 14:36:27.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009853431s
Aug 11 14:36:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010105316s
Aug 11 14:36:31.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010412904s
Aug 11 14:36:33.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009554944s
Aug 11 14:36:35.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010238223s
Aug 11 14:36:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00901696s
Aug 11 14:36:39.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.010156224s
Aug 11 14:36:41.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.010758919s
Aug 11 14:36:43.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008748192s
Aug 11 14:36:45.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.01102506s
Aug 11 14:36:47.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009385995s
Aug 11 14:36:49.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.010553277s
Aug 11 14:36:51.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010417259s
Aug 11 14:36:53.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010764166s
Aug 11 14:36:55.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01065903s
Aug 11 14:36:57.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.010359349s
Aug 11 14:36:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009814678s
Aug 11 14:37:01.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010361433s
Aug 11 14:37:03.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010763502s
Aug 11 14:37:05.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.010782059s
Aug 11 14:37:07.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009881569s
Aug 11 14:37:09.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010614087s
Aug 11 14:37:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010249022s
Aug 11 14:37:13.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011072715s
Aug 11 14:37:15.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009315916s
Aug 11 14:37:17.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.0098475s
Aug 11 14:37:19.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010817538s
Aug 11 14:37:21.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.010813343s
Aug 11 14:37:23.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.01019836s
Aug 11 14:37:25.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009802145s
Aug 11 14:37:27.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008988556s
Aug 11 14:37:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009859391s
Aug 11 14:37:31.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.010360294s
Aug 11 14:37:33.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010533807s
Aug 11 14:37:35.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.010567732s
Aug 11 14:37:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008762441s
Aug 11 14:37:39.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010351607s
Aug 11 14:37:41.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.010704865s
Aug 11 14:37:43.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.010094273s
Aug 11 14:37:45.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009253775s
Aug 11 14:37:47.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010267241s
Aug 11 14:37:49.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.011019601s
Aug 11 14:37:51.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009779989s
Aug 11 14:37:53.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010306672s
Aug 11 14:37:55.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009361819s
Aug 11 14:37:57.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.010178011s
Aug 11 14:37:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.010111145s
Aug 11 14:38:01.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011678232s
Aug 11 14:38:03.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009408577s
Aug 11 14:38:05.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.010737505s
Aug 11 14:38:07.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008892845s
Aug 11 14:38:09.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009075177s
Aug 11 14:38:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.010366924s
Aug 11 14:38:13.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009058098s
Aug 11 14:38:15.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.010027877s
Aug 11 14:38:17.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008994331s
Aug 11 14:38:19.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009803864s
Aug 11 14:38:21.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009142388s
Aug 11 14:38:23.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010344603s
Aug 11 14:38:25.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.010592127s
Aug 11 14:38:27.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009699546s
Aug 11 14:38:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010590459s
Aug 11 14:38:31.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.010407371s
Aug 11 14:38:33.290: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.012604737s
Aug 11 14:38:35.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.010349703s
Aug 11 14:38:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008956597s
Aug 11 14:38:39.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009702798s
Aug 11 14:38:41.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.011045095s
Aug 11 14:38:43.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009554356s
Aug 11 14:38:45.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010985817s
Aug 11 14:38:47.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008750635s
Aug 11 14:38:49.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010712343s
Aug 11 14:38:51.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009224363s
Aug 11 14:38:53.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009238171s
Aug 11 14:38:55.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.010267879s
Aug 11 14:38:57.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008992646s
Aug 11 14:38:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.010098577s
Aug 11 14:39:01.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010414984s
Aug 11 14:39:03.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.010929037s
Aug 11 14:39:05.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.010395728s
Aug 11 14:39:07.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009218108s
Aug 11 14:39:09.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009628207s
Aug 11 14:39:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.010588544s
Aug 11 14:39:13.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.010183818s
Aug 11 14:39:15.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.010339508s
Aug 11 14:39:17.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009088306s
Aug 11 14:39:19.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.010742655s
Aug 11 14:39:21.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.010390181s
Aug 11 14:39:23.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010707097s
Aug 11 14:39:25.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.010691134s
Aug 11 14:39:27.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009246635s
Aug 11 14:39:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.010038777s
Aug 11 14:39:31.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009843037s
Aug 11 14:39:33.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009620395s
Aug 11 14:39:35.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010241961s
Aug 11 14:39:35.291: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012953515s
STEP: removing the label kubernetes.io/e2e-ce6d9657-d061-4be8-9fdb-1378feefd18c off the node constell-d93e7e1d-worker-d314547c-0lc3 08/11/23 14:39:35.291
STEP: verifying the node doesn't have the label kubernetes.io/e2e-ce6d9657-d061-4be8-9fdb-1378feefd18c 08/11/23 14:39:35.304
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:39:35.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-7730" for this suite. 08/11/23 14:39:35.314
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]","completed":109,"skipped":2285,"failed":0}
------------------------------
â€¢ [SLOW TEST] [304.185 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
  test/e2e/scheduling/predicates.go:699

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:34:31.136
    Aug 11 14:34:31.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-pred 08/11/23 14:34:31.137
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:34:31.154
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:34:31.157
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Aug 11 14:34:31.159: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 11 14:34:31.169: INFO: Waiting for terminating namespaces to be deleted...
    Aug 11 14:34:31.172: INFO: 
    Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-0lc3 before test
    Aug 11 14:34:31.180: INFO: cilium-6s7tr from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.180: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 14:34:31.180: INFO: cilium-operator-5bfff4c47c-92tfw from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.180: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: coredns-6c49cf4575-pkkl2 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.180: INFO: 	Container coredns ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: csi-gce-pd-node-dsdbs from kube-system started at 2023-08-11 13:55:05 +0000 UTC (2 container statuses recorded)
    Aug 11 14:34:31.180: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: gcp-guest-agent-f5ptd from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.180: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: konnectivity-agent-prvxg from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.180: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: kube-proxy-vbf6p from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.180: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: verification-service-ttpg4 from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.180: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 14:34:31.180: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 14:34:31.180: INFO: 
    Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-wzlp before test
    Aug 11 14:34:31.188: INFO: cilium-dcbmm from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.188: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 14:34:31.188: INFO: csi-gce-pd-node-tnkjk from kube-system started at 2023-08-11 13:55:09 +0000 UTC (2 container statuses recorded)
    Aug 11 14:34:31.188: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: gcp-guest-agent-7fj57 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.188: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: konnectivity-agent-bwlfc from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.188: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: kube-proxy-9wx2l from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.188: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: verification-service-xmjlz from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.188: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:01:59 +0000 UTC (1 container statuses recorded)
    Aug 11 14:34:31.188: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: sonobuoy-e2e-job-2008a9ff359d4340 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 14:34:31.188: INFO: 	Container e2e ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 14:34:31.188: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 14:34:31.188: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
      test/e2e/scheduling/predicates.go:699
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 14:34:31.189
    Aug 11 14:34:31.198: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-7730" to be "running"
    Aug 11 14:34:31.201: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.007424ms
    Aug 11 14:34:33.207: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008915274s
    Aug 11 14:34:33.207: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 14:34:33.211
    STEP: Trying to apply a random label on the found node. 08/11/23 14:34:33.232
    STEP: verifying the node has the label kubernetes.io/e2e-ce6d9657-d061-4be8-9fdb-1378feefd18c 95 08/11/23 14:34:33.244
    STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled 08/11/23 14:34:33.255
    Aug 11 14:34:33.261: INFO: Waiting up to 5m0s for pod "pod4" in namespace "sched-pred-7730" to be "not pending"
    Aug 11 14:34:33.267: INFO: Pod "pod4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.592565ms
    Aug 11 14:34:35.271: INFO: Pod "pod4": Phase="Running", Reason="", readiness=true. Elapsed: 2.009872254s
    Aug 11 14:34:35.271: INFO: Pod "pod4" satisfied condition "not pending"
    STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.178.2 on the node which pod4 resides and expect not scheduled 08/11/23 14:34:35.271
    Aug 11 14:34:35.278: INFO: Waiting up to 5m0s for pod "pod5" in namespace "sched-pred-7730" to be "not pending"
    Aug 11 14:34:35.283: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.725268ms
    Aug 11 14:34:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008942035s
    Aug 11 14:34:39.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009286934s
    Aug 11 14:34:41.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.010701226s
    Aug 11 14:34:43.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 8.009312129s
    Aug 11 14:34:45.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 10.009022786s
    Aug 11 14:34:47.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 12.00891333s
    Aug 11 14:34:49.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 14.009899809s
    Aug 11 14:34:51.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 16.010786793s
    Aug 11 14:34:53.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 18.010579872s
    Aug 11 14:34:55.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010756743s
    Aug 11 14:34:57.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 22.009402968s
    Aug 11 14:34:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 24.010142418s
    Aug 11 14:35:01.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010682102s
    Aug 11 14:35:03.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010590536s
    Aug 11 14:35:05.292: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 30.014100144s
    Aug 11 14:35:07.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00962684s
    Aug 11 14:35:09.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009677518s
    Aug 11 14:35:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 36.010421847s
    Aug 11 14:35:13.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010783805s
    Aug 11 14:35:15.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 40.009460129s
    Aug 11 14:35:17.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009493358s
    Aug 11 14:35:19.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 44.010927608s
    Aug 11 14:35:21.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 46.010228334s
    Aug 11 14:35:23.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010276232s
    Aug 11 14:35:25.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 50.011083033s
    Aug 11 14:35:27.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 52.01006072s
    Aug 11 14:35:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010156498s
    Aug 11 14:35:31.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 56.009661359s
    Aug 11 14:35:33.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 58.009723718s
    Aug 11 14:35:35.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.010861861s
    Aug 11 14:35:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009082821s
    Aug 11 14:35:39.286: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.008543191s
    Aug 11 14:35:41.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.010085525s
    Aug 11 14:35:43.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010688192s
    Aug 11 14:35:45.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010746201s
    Aug 11 14:35:47.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.00954746s
    Aug 11 14:35:49.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.009515295s
    Aug 11 14:35:51.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.008948274s
    Aug 11 14:35:53.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009957663s
    Aug 11 14:35:55.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.009784265s
    Aug 11 14:35:57.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.009568724s
    Aug 11 14:35:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010366406s
    Aug 11 14:36:01.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.010200487s
    Aug 11 14:36:03.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.0110283s
    Aug 11 14:36:05.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.009189509s
    Aug 11 14:36:07.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009743463s
    Aug 11 14:36:09.291: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.012956871s
    Aug 11 14:36:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.009935682s
    Aug 11 14:36:13.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010829109s
    Aug 11 14:36:15.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010243186s
    Aug 11 14:36:17.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.00920142s
    Aug 11 14:36:19.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.01126426s
    Aug 11 14:36:21.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010707561s
    Aug 11 14:36:23.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.00955172s
    Aug 11 14:36:25.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.011105025s
    Aug 11 14:36:27.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009853431s
    Aug 11 14:36:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.010105316s
    Aug 11 14:36:31.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010412904s
    Aug 11 14:36:33.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.009554944s
    Aug 11 14:36:35.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.010238223s
    Aug 11 14:36:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m2.00901696s
    Aug 11 14:36:39.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m4.010156224s
    Aug 11 14:36:41.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m6.010758919s
    Aug 11 14:36:43.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m8.008748192s
    Aug 11 14:36:45.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m10.01102506s
    Aug 11 14:36:47.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m12.009385995s
    Aug 11 14:36:49.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m14.010553277s
    Aug 11 14:36:51.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m16.010417259s
    Aug 11 14:36:53.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m18.010764166s
    Aug 11 14:36:55.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m20.01065903s
    Aug 11 14:36:57.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m22.010359349s
    Aug 11 14:36:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m24.009814678s
    Aug 11 14:37:01.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m26.010361433s
    Aug 11 14:37:03.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m28.010763502s
    Aug 11 14:37:05.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m30.010782059s
    Aug 11 14:37:07.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m32.009881569s
    Aug 11 14:37:09.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m34.010614087s
    Aug 11 14:37:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m36.010249022s
    Aug 11 14:37:13.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m38.011072715s
    Aug 11 14:37:15.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m40.009315916s
    Aug 11 14:37:17.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m42.0098475s
    Aug 11 14:37:19.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m44.010817538s
    Aug 11 14:37:21.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m46.010813343s
    Aug 11 14:37:23.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m48.01019836s
    Aug 11 14:37:25.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m50.009802145s
    Aug 11 14:37:27.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m52.008988556s
    Aug 11 14:37:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m54.009859391s
    Aug 11 14:37:31.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m56.010360294s
    Aug 11 14:37:33.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 2m58.010533807s
    Aug 11 14:37:35.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m0.010567732s
    Aug 11 14:37:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m2.008762441s
    Aug 11 14:37:39.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m4.010351607s
    Aug 11 14:37:41.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m6.010704865s
    Aug 11 14:37:43.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m8.010094273s
    Aug 11 14:37:45.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m10.009253775s
    Aug 11 14:37:47.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m12.010267241s
    Aug 11 14:37:49.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m14.011019601s
    Aug 11 14:37:51.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m16.009779989s
    Aug 11 14:37:53.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m18.010306672s
    Aug 11 14:37:55.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m20.009361819s
    Aug 11 14:37:57.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m22.010178011s
    Aug 11 14:37:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m24.010111145s
    Aug 11 14:38:01.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m26.011678232s
    Aug 11 14:38:03.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m28.009408577s
    Aug 11 14:38:05.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m30.010737505s
    Aug 11 14:38:07.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m32.008892845s
    Aug 11 14:38:09.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m34.009075177s
    Aug 11 14:38:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m36.010366924s
    Aug 11 14:38:13.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m38.009058098s
    Aug 11 14:38:15.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m40.010027877s
    Aug 11 14:38:17.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m42.008994331s
    Aug 11 14:38:19.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m44.009803864s
    Aug 11 14:38:21.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m46.009142388s
    Aug 11 14:38:23.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m48.010344603s
    Aug 11 14:38:25.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m50.010592127s
    Aug 11 14:38:27.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m52.009699546s
    Aug 11 14:38:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m54.010590459s
    Aug 11 14:38:31.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m56.010407371s
    Aug 11 14:38:33.290: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 3m58.012604737s
    Aug 11 14:38:35.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m0.010349703s
    Aug 11 14:38:37.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m2.008956597s
    Aug 11 14:38:39.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m4.009702798s
    Aug 11 14:38:41.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m6.011045095s
    Aug 11 14:38:43.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m8.009554356s
    Aug 11 14:38:45.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m10.010985817s
    Aug 11 14:38:47.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m12.008750635s
    Aug 11 14:38:49.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m14.010712343s
    Aug 11 14:38:51.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m16.009224363s
    Aug 11 14:38:53.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m18.009238171s
    Aug 11 14:38:55.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m20.010267879s
    Aug 11 14:38:57.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m22.008992646s
    Aug 11 14:38:59.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m24.010098577s
    Aug 11 14:39:01.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m26.010414984s
    Aug 11 14:39:03.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m28.010929037s
    Aug 11 14:39:05.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m30.010395728s
    Aug 11 14:39:07.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m32.009218108s
    Aug 11 14:39:09.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m34.009628207s
    Aug 11 14:39:11.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m36.010588544s
    Aug 11 14:39:13.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m38.010183818s
    Aug 11 14:39:15.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m40.010339508s
    Aug 11 14:39:17.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m42.009088306s
    Aug 11 14:39:19.289: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m44.010742655s
    Aug 11 14:39:21.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m46.010390181s
    Aug 11 14:39:23.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m48.010707097s
    Aug 11 14:39:25.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m50.010691134s
    Aug 11 14:39:27.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m52.009246635s
    Aug 11 14:39:29.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m54.010038777s
    Aug 11 14:39:31.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m56.009843037s
    Aug 11 14:39:33.287: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 4m58.009620395s
    Aug 11 14:39:35.288: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.010241961s
    Aug 11 14:39:35.291: INFO: Pod "pod5": Phase="Pending", Reason="", readiness=false. Elapsed: 5m0.012953515s
    STEP: removing the label kubernetes.io/e2e-ce6d9657-d061-4be8-9fdb-1378feefd18c off the node constell-d93e7e1d-worker-d314547c-0lc3 08/11/23 14:39:35.291
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-ce6d9657-d061-4be8-9fdb-1378feefd18c 08/11/23 14:39:35.304
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:39:35.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-7730" for this suite. 08/11/23 14:39:35.314
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:39:35.322
Aug 11 14:39:35.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename dns 08/11/23 14:39:35.324
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:35.339
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:35.341
[It] should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411
STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/11/23 14:39:35.344
Aug 11 14:39:35.352: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5265  66ba23d3-7be4-444b-8f1e-49f20ec732c8 24553 0 2023-08-11 14:39:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-11 14:39:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-st85j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-st85j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 14:39:35.353: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5265" to be "running and ready"
Aug 11 14:39:35.356: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.830438ms
Aug 11 14:39:35.356: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:39:37.360: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007337835s
Aug 11 14:39:37.360: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
Aug 11 14:39:37.360: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
STEP: Verifying customized DNS suffix list is configured on pod... 08/11/23 14:39:37.36
Aug 11 14:39:37.360: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5265 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:39:37.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:39:37.361: INFO: ExecWithOptions: Clientset creation
Aug 11 14:39:37.361: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5265/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
STEP: Verifying customized DNS server is configured on pod... 08/11/23 14:39:37.475
Aug 11 14:39:37.476: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5265 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:39:37.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:39:37.476: INFO: ExecWithOptions: Clientset creation
Aug 11 14:39:37.476: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5265/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 14:39:37.556: INFO: Deleting pod test-dns-nameservers...
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 11 14:39:37.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-5265" for this suite. 08/11/23 14:39:37.576
{"msg":"PASSED [sig-network] DNS should support configurable pod DNS nameservers [Conformance]","completed":110,"skipped":2307,"failed":0}
------------------------------
â€¢ [2.264 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should support configurable pod DNS nameservers [Conformance]
  test/e2e/network/dns.go:411

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:39:35.322
    Aug 11 14:39:35.323: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename dns 08/11/23 14:39:35.324
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:35.339
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:35.341
    [It] should support configurable pod DNS nameservers [Conformance]
      test/e2e/network/dns.go:411
    STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... 08/11/23 14:39:35.344
    Aug 11 14:39:35.352: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-5265  66ba23d3-7be4-444b-8f1e-49f20ec732c8 24553 0 2023-08-11 14:39:35 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-11 14:39:35 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-st85j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-st85j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 14:39:35.353: INFO: Waiting up to 5m0s for pod "test-dns-nameservers" in namespace "dns-5265" to be "running and ready"
    Aug 11 14:39:35.356: INFO: Pod "test-dns-nameservers": Phase="Pending", Reason="", readiness=false. Elapsed: 2.830438ms
    Aug 11 14:39:35.356: INFO: The phase of Pod test-dns-nameservers is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:39:37.360: INFO: Pod "test-dns-nameservers": Phase="Running", Reason="", readiness=true. Elapsed: 2.007337835s
    Aug 11 14:39:37.360: INFO: The phase of Pod test-dns-nameservers is Running (Ready = true)
    Aug 11 14:39:37.360: INFO: Pod "test-dns-nameservers" satisfied condition "running and ready"
    STEP: Verifying customized DNS suffix list is configured on pod... 08/11/23 14:39:37.36
    Aug 11 14:39:37.360: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-5265 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:39:37.360: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:39:37.361: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:39:37.361: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5265/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    STEP: Verifying customized DNS server is configured on pod... 08/11/23 14:39:37.475
    Aug 11 14:39:37.476: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-5265 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:39:37.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:39:37.476: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:39:37.476: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-5265/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 14:39:37.556: INFO: Deleting pod test-dns-nameservers...
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 11 14:39:37.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-5265" for this suite. 08/11/23 14:39:37.576
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:39:37.586
Aug 11 14:39:37.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename proxy 08/11/23 14:39:37.587
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:37.605
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:37.607
[It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
  test/e2e/network/proxy.go:286
Aug 11 14:39:37.609: INFO: Creating pod...
Aug 11 14:39:37.616: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-983" to be "running"
Aug 11 14:39:37.619: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.367394ms
Aug 11 14:39:39.624: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007716897s
Aug 11 14:39:39.624: INFO: Pod "agnhost" satisfied condition "running"
Aug 11 14:39:39.624: INFO: Creating service...
Aug 11 14:39:39.646: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/DELETE
Aug 11 14:39:39.663: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 11 14:39:39.663: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/GET
Aug 11 14:39:39.669: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 11 14:39:39.669: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/HEAD
Aug 11 14:39:39.679: INFO: http.Client request:HEAD | StatusCode:200
Aug 11 14:39:39.679: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/OPTIONS
Aug 11 14:39:39.688: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 11 14:39:39.688: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/PATCH
Aug 11 14:39:39.696: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 11 14:39:39.696: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/POST
Aug 11 14:39:39.702: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 11 14:39:39.702: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/PUT
Aug 11 14:39:39.709: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 11 14:39:39.709: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/DELETE
Aug 11 14:39:39.718: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 11 14:39:39.718: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/GET
Aug 11 14:39:39.726: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
Aug 11 14:39:39.726: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/HEAD
Aug 11 14:39:39.734: INFO: http.Client request:HEAD | StatusCode:200
Aug 11 14:39:39.734: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/OPTIONS
Aug 11 14:39:39.746: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 11 14:39:39.746: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/PATCH
Aug 11 14:39:39.752: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 11 14:39:39.752: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/POST
Aug 11 14:39:39.759: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 11 14:39:39.759: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/PUT
Aug 11 14:39:39.769: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Aug 11 14:39:39.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-983" for this suite. 08/11/23 14:39:39.774
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]","completed":111,"skipped":2308,"failed":0}
------------------------------
â€¢ [2.194 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
    test/e2e/network/proxy.go:286

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:39:37.586
    Aug 11 14:39:37.587: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename proxy 08/11/23 14:39:37.587
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:37.605
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:37.607
    [It] A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
      test/e2e/network/proxy.go:286
    Aug 11 14:39:37.609: INFO: Creating pod...
    Aug 11 14:39:37.616: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-983" to be "running"
    Aug 11 14:39:37.619: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 2.367394ms
    Aug 11 14:39:39.624: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.007716897s
    Aug 11 14:39:39.624: INFO: Pod "agnhost" satisfied condition "running"
    Aug 11 14:39:39.624: INFO: Creating service...
    Aug 11 14:39:39.646: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/DELETE
    Aug 11 14:39:39.663: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 11 14:39:39.663: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/GET
    Aug 11 14:39:39.669: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 11 14:39:39.669: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/HEAD
    Aug 11 14:39:39.679: INFO: http.Client request:HEAD | StatusCode:200
    Aug 11 14:39:39.679: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/OPTIONS
    Aug 11 14:39:39.688: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 11 14:39:39.688: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/PATCH
    Aug 11 14:39:39.696: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 11 14:39:39.696: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/POST
    Aug 11 14:39:39.702: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 11 14:39:39.702: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/pods/agnhost/proxy/some/path/with/PUT
    Aug 11 14:39:39.709: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 11 14:39:39.709: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/DELETE
    Aug 11 14:39:39.718: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 11 14:39:39.718: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/GET
    Aug 11 14:39:39.726: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
    Aug 11 14:39:39.726: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/HEAD
    Aug 11 14:39:39.734: INFO: http.Client request:HEAD | StatusCode:200
    Aug 11 14:39:39.734: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/OPTIONS
    Aug 11 14:39:39.746: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 11 14:39:39.746: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/PATCH
    Aug 11 14:39:39.752: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 11 14:39:39.752: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/POST
    Aug 11 14:39:39.759: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 11 14:39:39.759: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-983/services/test-service/proxy/some/path/with/PUT
    Aug 11 14:39:39.769: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Aug 11 14:39:39.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-983" for this suite. 08/11/23 14:39:39.774
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:39:39.782
Aug 11 14:39:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:39:39.783
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:39.797
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:39.8
[It] should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933
STEP: Creating a ResourceQuota 08/11/23 14:39:39.803
STEP: Getting a ResourceQuota 08/11/23 14:39:39.808
STEP: Listing all ResourceQuotas with LabelSelector 08/11/23 14:39:39.812
STEP: Patching the ResourceQuota 08/11/23 14:39:39.815
STEP: Deleting a Collection of ResourceQuotas 08/11/23 14:39:39.822
STEP: Verifying the deleted ResourceQuota 08/11/23 14:39:39.831
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 14:39:39.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1646" for this suite. 08/11/23 14:39:39.837
{"msg":"PASSED [sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]","completed":112,"skipped":2328,"failed":0}
------------------------------
â€¢ [0.062 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should manage the lifecycle of a ResourceQuota [Conformance]
  test/e2e/apimachinery/resource_quota.go:933

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:39:39.782
    Aug 11 14:39:39.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:39:39.783
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:39.797
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:39.8
    [It] should manage the lifecycle of a ResourceQuota [Conformance]
      test/e2e/apimachinery/resource_quota.go:933
    STEP: Creating a ResourceQuota 08/11/23 14:39:39.803
    STEP: Getting a ResourceQuota 08/11/23 14:39:39.808
    STEP: Listing all ResourceQuotas with LabelSelector 08/11/23 14:39:39.812
    STEP: Patching the ResourceQuota 08/11/23 14:39:39.815
    STEP: Deleting a Collection of ResourceQuotas 08/11/23 14:39:39.822
    STEP: Verifying the deleted ResourceQuota 08/11/23 14:39:39.831
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 14:39:39.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1646" for this suite. 08/11/23 14:39:39.837
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:39:39.844
Aug 11 14:39:39.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:39:39.845
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:39.861
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:39.864
[It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185
Aug 11 14:39:39.873: INFO: Waiting up to 2m0s for pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25" in namespace "var-expansion-5018" to be "container 0 failed with reason CreateContainerConfigError"
Aug 11 14:39:39.876: INFO: Pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673613ms
Aug 11 14:39:41.881: INFO: Pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007459419s
Aug 11 14:39:41.881: INFO: Pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 11 14:39:41.881: INFO: Deleting pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25" in namespace "var-expansion-5018"
Aug 11 14:39:41.889: INFO: Wait up to 5m0s for pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 11 14:39:43.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5018" for this suite. 08/11/23 14:39:43.901
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]","completed":113,"skipped":2329,"failed":0}
------------------------------
â€¢ [4.064 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
  test/e2e/common/node/expansion.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:39:39.844
    Aug 11 14:39:39.844: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:39:39.845
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:39.861
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:39.864
    [It] should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
      test/e2e/common/node/expansion.go:185
    Aug 11 14:39:39.873: INFO: Waiting up to 2m0s for pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25" in namespace "var-expansion-5018" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 11 14:39:39.876: INFO: Pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.673613ms
    Aug 11 14:39:41.881: INFO: Pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007459419s
    Aug 11 14:39:41.881: INFO: Pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 11 14:39:41.881: INFO: Deleting pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25" in namespace "var-expansion-5018"
    Aug 11 14:39:41.889: INFO: Wait up to 5m0s for pod "var-expansion-3e1b4c75-4dc7-4e06-a148-2fbb2e9c9c25" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 11 14:39:43.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5018" for this suite. 08/11/23 14:39:43.901
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] Pods
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:39:43.909
Aug 11 14:39:43.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 14:39:43.91
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:43.925
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:43.928
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203
STEP: creating pod 08/11/23 14:39:43.931
Aug 11 14:39:43.940: INFO: Waiting up to 5m0s for pod "pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c" in namespace "pods-3669" to be "running and ready"
Aug 11 14:39:43.945: INFO: Pod "pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2122ms
Aug 11 14:39:43.945: INFO: The phase of Pod pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:39:45.949: INFO: Pod "pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00879727s
Aug 11 14:39:45.949: INFO: The phase of Pod pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c is Running (Ready = true)
Aug 11 14:39:45.949: INFO: Pod "pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c" satisfied condition "running and ready"
Aug 11 14:39:45.956: INFO: Pod pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c has hostIP: 192.168.178.3
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 14:39:45.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3669" for this suite. 08/11/23 14:39:45.96
{"msg":"PASSED [sig-node] Pods should get a host IP [NodeConformance] [Conformance]","completed":114,"skipped":2331,"failed":0}
------------------------------
â€¢ [2.059 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should get a host IP [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:203

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:39:43.909
    Aug 11 14:39:43.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 14:39:43.91
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:43.925
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:43.928
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should get a host IP [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:203
    STEP: creating pod 08/11/23 14:39:43.931
    Aug 11 14:39:43.940: INFO: Waiting up to 5m0s for pod "pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c" in namespace "pods-3669" to be "running and ready"
    Aug 11 14:39:43.945: INFO: Pod "pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.2122ms
    Aug 11 14:39:43.945: INFO: The phase of Pod pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:39:45.949: INFO: Pod "pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c": Phase="Running", Reason="", readiness=true. Elapsed: 2.00879727s
    Aug 11 14:39:45.949: INFO: The phase of Pod pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c is Running (Ready = true)
    Aug 11 14:39:45.949: INFO: Pod "pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c" satisfied condition "running and ready"
    Aug 11 14:39:45.956: INFO: Pod pod-hostip-36097384-e6f5-4b94-b6a0-0b38b7daf97c has hostIP: 192.168.178.3
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 14:39:45.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3669" for this suite. 08/11/23 14:39:45.96
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance]
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:39:45.969
Aug 11 14:39:45.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename ephemeral-containers-test 08/11/23 14:39:45.97
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:45.99
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:45.993
[BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/common/node/ephemeral_containers.go:38
[It] will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45
STEP: creating a target pod 08/11/23 14:39:45.997
Aug 11 14:39:46.005: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4934" to be "running and ready"
Aug 11 14:39:46.011: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.622845ms
Aug 11 14:39:46.011: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:39:48.016: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010614296s
Aug 11 14:39:48.016: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
Aug 11 14:39:48.016: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
STEP: adding an ephemeral container 08/11/23 14:39:48.018
Aug 11 14:39:48.030: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4934" to be "container debugger running"
Aug 11 14:39:48.033: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.685213ms
Aug 11 14:39:50.036: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006561092s
Aug 11 14:39:50.036: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
STEP: checking pod container endpoints 08/11/23 14:39:50.037
Aug 11 14:39:50.037: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4934 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:39:50.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:39:50.037: INFO: ExecWithOptions: Clientset creation
Aug 11 14:39:50.037: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4934/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
Aug 11 14:39:50.110: INFO: Exec stderr: ""
[AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 11 14:39:50.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ephemeral-containers-test-4934" for this suite. 08/11/23 14:39:50.135
{"msg":"PASSED [sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]","completed":115,"skipped":2334,"failed":0}
------------------------------
â€¢ [4.173 seconds]
[sig-node] Ephemeral Containers [NodeConformance]
test/e2e/common/node/framework.go:23
  will start an ephemeral container in an existing pod [Conformance]
  test/e2e/common/node/ephemeral_containers.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:39:45.969
    Aug 11 14:39:45.969: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename ephemeral-containers-test 08/11/23 14:39:45.97
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:45.99
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:45.993
    [BeforeEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/common/node/ephemeral_containers.go:38
    [It] will start an ephemeral container in an existing pod [Conformance]
      test/e2e/common/node/ephemeral_containers.go:45
    STEP: creating a target pod 08/11/23 14:39:45.997
    Aug 11 14:39:46.005: INFO: Waiting up to 5m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4934" to be "running and ready"
    Aug 11 14:39:46.011: INFO: Pod "ephemeral-containers-target-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.622845ms
    Aug 11 14:39:46.011: INFO: The phase of Pod ephemeral-containers-target-pod is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:39:48.016: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010614296s
    Aug 11 14:39:48.016: INFO: The phase of Pod ephemeral-containers-target-pod is Running (Ready = true)
    Aug 11 14:39:48.016: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "running and ready"
    STEP: adding an ephemeral container 08/11/23 14:39:48.018
    Aug 11 14:39:48.030: INFO: Waiting up to 1m0s for pod "ephemeral-containers-target-pod" in namespace "ephemeral-containers-test-4934" to be "container debugger running"
    Aug 11 14:39:48.033: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.685213ms
    Aug 11 14:39:50.036: INFO: Pod "ephemeral-containers-target-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.006561092s
    Aug 11 14:39:50.036: INFO: Pod "ephemeral-containers-target-pod" satisfied condition "container debugger running"
    STEP: checking pod container endpoints 08/11/23 14:39:50.037
    Aug 11 14:39:50.037: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-4934 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:39:50.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:39:50.037: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:39:50.037: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-4934/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
    Aug 11 14:39:50.110: INFO: Exec stderr: ""
    [AfterEach] [sig-node] Ephemeral Containers [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 11 14:39:50.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ephemeral-containers-test-4934" for this suite. 08/11/23 14:39:50.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:39:50.145
Aug 11 14:39:50.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:39:50.145
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:50.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:50.162
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161
STEP: Creating the pod 08/11/23 14:39:50.165
Aug 11 14:39:50.177: INFO: Waiting up to 5m0s for pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398" in namespace "projected-7134" to be "running and ready"
Aug 11 14:39:50.186: INFO: Pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398": Phase="Pending", Reason="", readiness=false. Elapsed: 8.396121ms
Aug 11 14:39:50.186: INFO: The phase of Pod annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:39:52.191: INFO: Pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398": Phase="Running", Reason="", readiness=true. Elapsed: 2.013505597s
Aug 11 14:39:52.191: INFO: The phase of Pod annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398 is Running (Ready = true)
Aug 11 14:39:52.191: INFO: Pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398" satisfied condition "running and ready"
Aug 11 14:39:52.715: INFO: Successfully updated pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 14:39:56.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-7134" for this suite. 08/11/23 14:39:56.776
{"msg":"PASSED [sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]","completed":116,"skipped":2364,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.638 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:39:50.145
    Aug 11 14:39:50.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:39:50.145
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:50.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:50.162
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:161
    STEP: Creating the pod 08/11/23 14:39:50.165
    Aug 11 14:39:50.177: INFO: Waiting up to 5m0s for pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398" in namespace "projected-7134" to be "running and ready"
    Aug 11 14:39:50.186: INFO: Pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398": Phase="Pending", Reason="", readiness=false. Elapsed: 8.396121ms
    Aug 11 14:39:50.186: INFO: The phase of Pod annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:39:52.191: INFO: Pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398": Phase="Running", Reason="", readiness=true. Elapsed: 2.013505597s
    Aug 11 14:39:52.191: INFO: The phase of Pod annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398 is Running (Ready = true)
    Aug 11 14:39:52.191: INFO: Pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398" satisfied condition "running and ready"
    Aug 11 14:39:52.715: INFO: Successfully updated pod "annotationupdatec89de4e2-323d-4979-9d2c-be3e40909398"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 14:39:56.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-7134" for this suite. 08/11/23 14:39:56.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] EndpointSlice
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:39:56.783
Aug 11 14:39:56.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename endpointslice 08/11/23 14:39:56.785
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:56.802
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:56.804
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352
STEP: getting /apis 08/11/23 14:39:56.807
STEP: getting /apis/discovery.k8s.io 08/11/23 14:39:56.808
STEP: getting /apis/discovery.k8s.iov1 08/11/23 14:39:56.809
STEP: creating 08/11/23 14:39:56.81
STEP: getting 08/11/23 14:39:56.827
STEP: listing 08/11/23 14:39:56.83
STEP: watching 08/11/23 14:39:56.833
Aug 11 14:39:56.833: INFO: starting watch
STEP: cluster-wide listing 08/11/23 14:39:56.834
STEP: cluster-wide watching 08/11/23 14:39:56.837
Aug 11 14:39:56.837: INFO: starting watch
STEP: patching 08/11/23 14:39:56.838
STEP: updating 08/11/23 14:39:56.845
Aug 11 14:39:56.852: INFO: waiting for watch events with expected annotations
Aug 11 14:39:56.852: INFO: saw patched and updated annotations
STEP: deleting 08/11/23 14:39:56.853
STEP: deleting a collection 08/11/23 14:39:56.863
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Aug 11 14:39:56.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6894" for this suite. 08/11/23 14:39:56.883
{"msg":"PASSED [sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]","completed":117,"skipped":2370,"failed":0}
------------------------------
â€¢ [0.106 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should support creating EndpointSlice API operations [Conformance]
  test/e2e/network/endpointslice.go:352

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:39:56.783
    Aug 11 14:39:56.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename endpointslice 08/11/23 14:39:56.785
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:56.802
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:56.804
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should support creating EndpointSlice API operations [Conformance]
      test/e2e/network/endpointslice.go:352
    STEP: getting /apis 08/11/23 14:39:56.807
    STEP: getting /apis/discovery.k8s.io 08/11/23 14:39:56.808
    STEP: getting /apis/discovery.k8s.iov1 08/11/23 14:39:56.809
    STEP: creating 08/11/23 14:39:56.81
    STEP: getting 08/11/23 14:39:56.827
    STEP: listing 08/11/23 14:39:56.83
    STEP: watching 08/11/23 14:39:56.833
    Aug 11 14:39:56.833: INFO: starting watch
    STEP: cluster-wide listing 08/11/23 14:39:56.834
    STEP: cluster-wide watching 08/11/23 14:39:56.837
    Aug 11 14:39:56.837: INFO: starting watch
    STEP: patching 08/11/23 14:39:56.838
    STEP: updating 08/11/23 14:39:56.845
    Aug 11 14:39:56.852: INFO: waiting for watch events with expected annotations
    Aug 11 14:39:56.852: INFO: saw patched and updated annotations
    STEP: deleting 08/11/23 14:39:56.853
    STEP: deleting a collection 08/11/23 14:39:56.863
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Aug 11 14:39:56.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6894" for this suite. 08/11/23 14:39:56.883
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:39:56.891
Aug 11 14:39:56.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:39:56.891
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:56.908
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:56.911
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78
STEP: Creating secret with name secret-test-map-be73e7a4-c454-492f-91c9-d56d6b166c53 08/11/23 14:39:56.914
STEP: Creating a pod to test consume secrets 08/11/23 14:39:56.921
Aug 11 14:39:56.931: INFO: Waiting up to 5m0s for pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95" in namespace "secrets-4458" to be "Succeeded or Failed"
Aug 11 14:39:56.935: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95": Phase="Pending", Reason="", readiness=false. Elapsed: 3.696725ms
Aug 11 14:39:58.940: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95": Phase="Running", Reason="", readiness=true. Elapsed: 2.008617503s
Aug 11 14:40:00.939: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95": Phase="Running", Reason="", readiness=false. Elapsed: 4.008169257s
Aug 11 14:40:02.940: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009278228s
STEP: Saw pod success 08/11/23 14:40:02.94
Aug 11 14:40:02.940: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95" satisfied condition "Succeeded or Failed"
Aug 11 14:40:02.944: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:40:02.967
Aug 11 14:40:02.979: INFO: Waiting for pod pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95 to disappear
Aug 11 14:40:02.982: INFO: Pod pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:40:02.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4458" for this suite. 08/11/23 14:40:02.986
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":118,"skipped":2384,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.103 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:78

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:39:56.891
    Aug 11 14:39:56.891: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:39:56.891
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:39:56.908
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:39:56.911
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:78
    STEP: Creating secret with name secret-test-map-be73e7a4-c454-492f-91c9-d56d6b166c53 08/11/23 14:39:56.914
    STEP: Creating a pod to test consume secrets 08/11/23 14:39:56.921
    Aug 11 14:39:56.931: INFO: Waiting up to 5m0s for pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95" in namespace "secrets-4458" to be "Succeeded or Failed"
    Aug 11 14:39:56.935: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95": Phase="Pending", Reason="", readiness=false. Elapsed: 3.696725ms
    Aug 11 14:39:58.940: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95": Phase="Running", Reason="", readiness=true. Elapsed: 2.008617503s
    Aug 11 14:40:00.939: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95": Phase="Running", Reason="", readiness=false. Elapsed: 4.008169257s
    Aug 11 14:40:02.940: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.009278228s
    STEP: Saw pod success 08/11/23 14:40:02.94
    Aug 11 14:40:02.940: INFO: Pod "pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95" satisfied condition "Succeeded or Failed"
    Aug 11 14:40:02.944: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:40:02.967
    Aug 11 14:40:02.979: INFO: Waiting for pod pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95 to disappear
    Aug 11 14:40:02.982: INFO: Pod pod-secrets-1909cf15-f0d0-4b37-baa1-9724ad835e95 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:40:02.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4458" for this suite. 08/11/23 14:40:02.986
  << End Captured GinkgoWriter Output
------------------------------
[sig-auth] ServiceAccounts
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:40:02.994
Aug 11 14:40:02.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:40:02.995
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:03.011
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:03.014
[It] should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75
Aug 11 14:40:03.030: INFO: Waiting up to 5m0s for pod "pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b" in namespace "svcaccounts-9312" to be "running"
Aug 11 14:40:03.036: INFO: Pod "pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.887773ms
Aug 11 14:40:05.040: INFO: Pod "pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009859184s
Aug 11 14:40:05.040: INFO: Pod "pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b" satisfied condition "running"
STEP: reading a file in the container 08/11/23 14:40:05.04
Aug 11 14:40:05.040: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9312 pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
STEP: reading a file in the container 08/11/23 14:40:05.174
Aug 11 14:40:05.174: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9312 pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
STEP: reading a file in the container 08/11/23 14:40:05.314
Aug 11 14:40:05.314: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9312 pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
Aug 11 14:40:05.460: INFO: Got root ca configmap in namespace "svcaccounts-9312"
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 11 14:40:05.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-9312" for this suite. 08/11/23 14:40:05.467
{"msg":"PASSED [sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]","completed":119,"skipped":2384,"failed":0}
------------------------------
â€¢ [2.480 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should mount an API token into pods  [Conformance]
  test/e2e/auth/service_accounts.go:75

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:40:02.994
    Aug 11 14:40:02.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:40:02.995
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:03.011
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:03.014
    [It] should mount an API token into pods  [Conformance]
      test/e2e/auth/service_accounts.go:75
    Aug 11 14:40:03.030: INFO: Waiting up to 5m0s for pod "pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b" in namespace "svcaccounts-9312" to be "running"
    Aug 11 14:40:03.036: INFO: Pod "pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b": Phase="Pending", Reason="", readiness=false. Elapsed: 5.887773ms
    Aug 11 14:40:05.040: INFO: Pod "pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b": Phase="Running", Reason="", readiness=true. Elapsed: 2.009859184s
    Aug 11 14:40:05.040: INFO: Pod "pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b" satisfied condition "running"
    STEP: reading a file in the container 08/11/23 14:40:05.04
    Aug 11 14:40:05.040: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9312 pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
    STEP: reading a file in the container 08/11/23 14:40:05.174
    Aug 11 14:40:05.174: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9312 pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
    STEP: reading a file in the container 08/11/23 14:40:05.314
    Aug 11 14:40:05.314: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9312 pod-service-account-cd4c1e05-72dc-4456-85c0-67d572bd556b -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
    Aug 11 14:40:05.460: INFO: Got root ca configmap in namespace "svcaccounts-9312"
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 11 14:40:05.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-9312" for this suite. 08/11/23 14:40:05.467
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:40:05.474
Aug 11 14:40:05.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:40:05.475
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:05.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:05.492
[It] works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152
Aug 11 14:40:05.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 14:40:09.679
Aug 11 14:40:09.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 --namespace=crd-publish-openapi-3577 create -f -'
Aug 11 14:40:10.273: INFO: stderr: ""
Aug 11 14:40:10.274: INFO: stdout: "e2e-test-crd-publish-openapi-8524-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 11 14:40:10.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 --namespace=crd-publish-openapi-3577 delete e2e-test-crd-publish-openapi-8524-crds test-cr'
Aug 11 14:40:10.337: INFO: stderr: ""
Aug 11 14:40:10.337: INFO: stdout: "e2e-test-crd-publish-openapi-8524-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
Aug 11 14:40:10.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 --namespace=crd-publish-openapi-3577 apply -f -'
Aug 11 14:40:10.936: INFO: stderr: ""
Aug 11 14:40:10.936: INFO: stdout: "e2e-test-crd-publish-openapi-8524-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
Aug 11 14:40:10.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 --namespace=crd-publish-openapi-3577 delete e2e-test-crd-publish-openapi-8524-crds test-cr'
Aug 11 14:40:11.004: INFO: stderr: ""
Aug 11 14:40:11.004: INFO: stdout: "e2e-test-crd-publish-openapi-8524-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR without validation schema 08/11/23 14:40:11.004
Aug 11 14:40:11.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 explain e2e-test-crd-publish-openapi-8524-crds'
Aug 11 14:40:11.595: INFO: stderr: ""
Aug 11 14:40:11.595: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8524-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:40:15.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-3577" for this suite. 08/11/23 14:40:15.349
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]","completed":120,"skipped":2394,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.884 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD without validation schema [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:40:05.474
    Aug 11 14:40:05.474: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:40:05.475
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:05.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:05.492
    [It] works for CRD without validation schema [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:152
    Aug 11 14:40:05.494: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 14:40:09.679
    Aug 11 14:40:09.680: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 --namespace=crd-publish-openapi-3577 create -f -'
    Aug 11 14:40:10.273: INFO: stderr: ""
    Aug 11 14:40:10.274: INFO: stdout: "e2e-test-crd-publish-openapi-8524-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 11 14:40:10.274: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 --namespace=crd-publish-openapi-3577 delete e2e-test-crd-publish-openapi-8524-crds test-cr'
    Aug 11 14:40:10.337: INFO: stderr: ""
    Aug 11 14:40:10.337: INFO: stdout: "e2e-test-crd-publish-openapi-8524-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    Aug 11 14:40:10.337: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 --namespace=crd-publish-openapi-3577 apply -f -'
    Aug 11 14:40:10.936: INFO: stderr: ""
    Aug 11 14:40:10.936: INFO: stdout: "e2e-test-crd-publish-openapi-8524-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
    Aug 11 14:40:10.936: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 --namespace=crd-publish-openapi-3577 delete e2e-test-crd-publish-openapi-8524-crds test-cr'
    Aug 11 14:40:11.004: INFO: stderr: ""
    Aug 11 14:40:11.004: INFO: stdout: "e2e-test-crd-publish-openapi-8524-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR without validation schema 08/11/23 14:40:11.004
    Aug 11 14:40:11.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-3577 explain e2e-test-crd-publish-openapi-8524-crds'
    Aug 11 14:40:11.595: INFO: stderr: ""
    Aug 11 14:40:11.595: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-8524-crd\nVERSION:  crd-publish-openapi-test-empty.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:40:15.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-3577" for this suite. 08/11/23 14:40:15.349
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:40:15.359
Aug 11 14:40:15.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 14:40:15.36
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:15.375
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:15.378
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268
STEP: creating service nodeport-test with type=NodePort in namespace services-4960 08/11/23 14:40:15.382
STEP: creating replication controller nodeport-test in namespace services-4960 08/11/23 14:40:15.404
I0811 14:40:15.415300      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4960, replica count: 2
I0811 14:40:18.467936      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:40:18.467: INFO: Creating new exec pod
Aug 11 14:40:18.478: INFO: Waiting up to 5m0s for pod "execpodcfwbv" in namespace "services-4960" to be "running"
Aug 11 14:40:18.481: INFO: Pod "execpodcfwbv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867721ms
Aug 11 14:40:20.485: INFO: Pod "execpodcfwbv": Phase="Running", Reason="", readiness=true. Elapsed: 2.007872803s
Aug 11 14:40:20.485: INFO: Pod "execpodcfwbv" satisfied condition "running"
Aug 11 14:40:21.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4960 exec execpodcfwbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
Aug 11 14:40:21.634: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
Aug 11 14:40:21.634: INFO: stdout: "nodeport-test-q6tt6"
Aug 11 14:40:21.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4960 exec execpodcfwbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.185.218 80'
Aug 11 14:40:21.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.185.218 80\nConnection to 10.102.185.218 80 port [tcp/http] succeeded!\n"
Aug 11 14:40:21.768: INFO: stdout: "nodeport-test-jqnlr"
Aug 11 14:40:21.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4960 exec execpodcfwbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.2 32689'
Aug 11 14:40:21.906: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.2 32689\nConnection to 192.168.178.2 32689 port [tcp/*] succeeded!\n"
Aug 11 14:40:21.906: INFO: stdout: "nodeport-test-jqnlr"
Aug 11 14:40:21.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4960 exec execpodcfwbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.3 32689'
Aug 11 14:40:22.038: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.3 32689\nConnection to 192.168.178.3 32689 port [tcp/*] succeeded!\n"
Aug 11 14:40:22.038: INFO: stdout: "nodeport-test-q6tt6"
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 14:40:22.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4960" for this suite. 08/11/23 14:40:22.043
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to create a functioning NodePort service [Conformance]","completed":121,"skipped":2399,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.690 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to create a functioning NodePort service [Conformance]
  test/e2e/network/service.go:1268

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:40:15.359
    Aug 11 14:40:15.359: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 14:40:15.36
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:15.375
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:15.378
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to create a functioning NodePort service [Conformance]
      test/e2e/network/service.go:1268
    STEP: creating service nodeport-test with type=NodePort in namespace services-4960 08/11/23 14:40:15.382
    STEP: creating replication controller nodeport-test in namespace services-4960 08/11/23 14:40:15.404
    I0811 14:40:15.415300      22 runners.go:193] Created replication controller with name: nodeport-test, namespace: services-4960, replica count: 2
    I0811 14:40:18.467936      22 runners.go:193] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:40:18.467: INFO: Creating new exec pod
    Aug 11 14:40:18.478: INFO: Waiting up to 5m0s for pod "execpodcfwbv" in namespace "services-4960" to be "running"
    Aug 11 14:40:18.481: INFO: Pod "execpodcfwbv": Phase="Pending", Reason="", readiness=false. Elapsed: 3.867721ms
    Aug 11 14:40:20.485: INFO: Pod "execpodcfwbv": Phase="Running", Reason="", readiness=true. Elapsed: 2.007872803s
    Aug 11 14:40:20.485: INFO: Pod "execpodcfwbv" satisfied condition "running"
    Aug 11 14:40:21.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4960 exec execpodcfwbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
    Aug 11 14:40:21.634: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
    Aug 11 14:40:21.634: INFO: stdout: "nodeport-test-q6tt6"
    Aug 11 14:40:21.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4960 exec execpodcfwbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.185.218 80'
    Aug 11 14:40:21.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.185.218 80\nConnection to 10.102.185.218 80 port [tcp/http] succeeded!\n"
    Aug 11 14:40:21.768: INFO: stdout: "nodeport-test-jqnlr"
    Aug 11 14:40:21.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4960 exec execpodcfwbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.2 32689'
    Aug 11 14:40:21.906: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.2 32689\nConnection to 192.168.178.2 32689 port [tcp/*] succeeded!\n"
    Aug 11 14:40:21.906: INFO: stdout: "nodeport-test-jqnlr"
    Aug 11 14:40:21.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4960 exec execpodcfwbv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.3 32689'
    Aug 11 14:40:22.038: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.3 32689\nConnection to 192.168.178.3 32689 port [tcp/*] succeeded!\n"
    Aug 11 14:40:22.038: INFO: stdout: "nodeport-test-q6tt6"
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 14:40:22.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4960" for this suite. 08/11/23 14:40:22.043
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:40:22.05
Aug 11 14:40:22.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:40:22.051
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:22.066
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:22.069
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422
STEP: Creating configMap with name configmap-test-volume-ec9c6753-f5ff-46c6-a2fe-ef88649fb1f7 08/11/23 14:40:22.071
STEP: Creating a pod to test consume configMaps 08/11/23 14:40:22.076
Aug 11 14:40:22.087: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1" in namespace "configmap-2089" to be "Succeeded or Failed"
Aug 11 14:40:22.090: INFO: Pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.346444ms
Aug 11 14:40:24.096: INFO: Pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008838652s
Aug 11 14:40:26.095: INFO: Pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008138567s
STEP: Saw pod success 08/11/23 14:40:26.095
Aug 11 14:40:26.095: INFO: Pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1" satisfied condition "Succeeded or Failed"
Aug 11 14:40:26.098: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1 container configmap-volume-test: <nil>
STEP: delete the pod 08/11/23 14:40:26.107
Aug 11 14:40:26.123: INFO: Waiting for pod pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1 to disappear
Aug 11 14:40:26.126: INFO: Pod pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:40:26.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-2089" for this suite. 08/11/23 14:40:26.131
{"msg":"PASSED [sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":122,"skipped":2410,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:422

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:40:22.05
    Aug 11 14:40:22.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:40:22.051
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:22.066
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:22.069
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:422
    STEP: Creating configMap with name configmap-test-volume-ec9c6753-f5ff-46c6-a2fe-ef88649fb1f7 08/11/23 14:40:22.071
    STEP: Creating a pod to test consume configMaps 08/11/23 14:40:22.076
    Aug 11 14:40:22.087: INFO: Waiting up to 5m0s for pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1" in namespace "configmap-2089" to be "Succeeded or Failed"
    Aug 11 14:40:22.090: INFO: Pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.346444ms
    Aug 11 14:40:24.096: INFO: Pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008838652s
    Aug 11 14:40:26.095: INFO: Pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008138567s
    STEP: Saw pod success 08/11/23 14:40:26.095
    Aug 11 14:40:26.095: INFO: Pod "pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1" satisfied condition "Succeeded or Failed"
    Aug 11 14:40:26.098: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1 container configmap-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:40:26.107
    Aug 11 14:40:26.123: INFO: Waiting for pod pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1 to disappear
    Aug 11 14:40:26.126: INFO: Pod pod-configmaps-2c2e8ab1-21ea-43d1-82b3-3eb6f397b2b1 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:40:26.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-2089" for this suite. 08/11/23 14:40:26.131
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:40:26.138
Aug 11 14:40:26.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:40:26.139
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:26.162
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:26.165
[It] pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226
STEP: Creating Pod 08/11/23 14:40:26.168
Aug 11 14:40:26.178: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6" in namespace "emptydir-8" to be "running"
Aug 11 14:40:26.183: INFO: Pod "pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.211113ms
Aug 11 14:40:28.188: INFO: Pod "pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6": Phase="Running", Reason="", readiness=false. Elapsed: 2.010490713s
Aug 11 14:40:28.188: INFO: Pod "pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6" satisfied condition "running"
STEP: Reading file content from the nginx-container 08/11/23 14:40:28.188
Aug 11 14:40:28.188: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8 PodName:pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:40:28.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:40:28.189: INFO: ExecWithOptions: Clientset creation
Aug 11 14:40:28.189: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8/pods/pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
Aug 11 14:40:28.263: INFO: Exec stderr: ""
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 14:40:28.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-8" for this suite. 08/11/23 14:40:28.269
{"msg":"PASSED [sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]","completed":123,"skipped":2419,"failed":0}
------------------------------
â€¢ [2.138 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  pod should support shared volumes between containers [Conformance]
  test/e2e/common/storage/empty_dir.go:226

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:40:26.138
    Aug 11 14:40:26.138: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:40:26.139
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:26.162
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:26.165
    [It] pod should support shared volumes between containers [Conformance]
      test/e2e/common/storage/empty_dir.go:226
    STEP: Creating Pod 08/11/23 14:40:26.168
    Aug 11 14:40:26.178: INFO: Waiting up to 5m0s for pod "pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6" in namespace "emptydir-8" to be "running"
    Aug 11 14:40:26.183: INFO: Pod "pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.211113ms
    Aug 11 14:40:28.188: INFO: Pod "pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6": Phase="Running", Reason="", readiness=false. Elapsed: 2.010490713s
    Aug 11 14:40:28.188: INFO: Pod "pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6" satisfied condition "running"
    STEP: Reading file content from the nginx-container 08/11/23 14:40:28.188
    Aug 11 14:40:28.188: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8 PodName:pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:40:28.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:40:28.189: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:40:28.189: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8/pods/pod-sharedvolume-f030da68-4ff6-46b3-8684-3eb6b01695d6/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
    Aug 11 14:40:28.263: INFO: Exec stderr: ""
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:40:28.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-8" for this suite. 08/11/23 14:40:28.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:40:28.277
Aug 11 14:40:28.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:40:28.278
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:28.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:28.297
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:40:28.308
Aug 11 14:40:28.317: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5378" to be "running and ready"
Aug 11 14:40:28.322: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.529472ms
Aug 11 14:40:28.322: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:40:30.327: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010782792s
Aug 11 14:40:30.327: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 11 14:40:30.327: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:97
STEP: create the pod with lifecycle hook 08/11/23 14:40:30.331
Aug 11 14:40:30.337: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5378" to be "running and ready"
Aug 11 14:40:30.344: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.826412ms
Aug 11 14:40:30.344: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:40:32.348: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010982757s
Aug 11 14:40:32.348: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
Aug 11 14:40:32.348: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/11/23 14:40:32.351
STEP: delete the pod with lifecycle hook 08/11/23 14:40:32.36
Aug 11 14:40:32.371: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 11 14:40:32.374: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 11 14:40:34.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 11 14:40:34.378: INFO: Pod pod-with-poststart-exec-hook still exists
Aug 11 14:40:36.375: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
Aug 11 14:40:36.379: INFO: Pod pod-with-poststart-exec-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Aug 11 14:40:36.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-5378" for this suite. 08/11/23 14:40:36.384
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]","completed":124,"skipped":2427,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.115 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:97

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:40:28.277
    Aug 11 14:40:28.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:40:28.278
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:28.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:28.297
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:40:28.308
    Aug 11 14:40:28.317: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-5378" to be "running and ready"
    Aug 11 14:40:28.322: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.529472ms
    Aug 11 14:40:28.322: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:40:30.327: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.010782792s
    Aug 11 14:40:30.327: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 11 14:40:30.327: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:97
    STEP: create the pod with lifecycle hook 08/11/23 14:40:30.331
    Aug 11 14:40:30.337: INFO: Waiting up to 5m0s for pod "pod-with-poststart-exec-hook" in namespace "container-lifecycle-hook-5378" to be "running and ready"
    Aug 11 14:40:30.344: INFO: Pod "pod-with-poststart-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 6.826412ms
    Aug 11 14:40:30.344: INFO: The phase of Pod pod-with-poststart-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:40:32.348: INFO: Pod "pod-with-poststart-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.010982757s
    Aug 11 14:40:32.348: INFO: The phase of Pod pod-with-poststart-exec-hook is Running (Ready = true)
    Aug 11 14:40:32.348: INFO: Pod "pod-with-poststart-exec-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/11/23 14:40:32.351
    STEP: delete the pod with lifecycle hook 08/11/23 14:40:32.36
    Aug 11 14:40:32.371: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 11 14:40:32.374: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 11 14:40:34.374: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 11 14:40:34.378: INFO: Pod pod-with-poststart-exec-hook still exists
    Aug 11 14:40:36.375: INFO: Waiting for pod pod-with-poststart-exec-hook to disappear
    Aug 11 14:40:36.379: INFO: Pod pod-with-poststart-exec-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Aug 11 14:40:36.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-5378" for this suite. 08/11/23 14:40:36.384
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial]
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:40:36.392
Aug 11 14:40:36.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename taint-single-pod 08/11/23 14:40:36.393
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:36.408
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:36.411
[BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/node/taints.go:166
Aug 11 14:40:36.413: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 14:41:36.452: INFO: Waiting for terminating namespaces to be deleted...
[It] removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289
Aug 11 14:41:36.455: INFO: Starting informer...
STEP: Starting pod... 08/11/23 14:41:36.455
Aug 11 14:41:36.670: INFO: Pod is running on constell-d93e7e1d-worker-d314547c-0lc3. Tainting Node
STEP: Trying to apply a taint on the Node 08/11/23 14:41:36.67
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:41:36.682
STEP: Waiting short time to make sure Pod is queued for deletion 08/11/23 14:41:36.684
Aug 11 14:41:36.685: INFO: Pod wasn't evicted. Proceeding
Aug 11 14:41:36.685: INFO: Removing taint from Node
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:41:36.696
STEP: Waiting some time to make sure that toleration time passed. 08/11/23 14:41:36.705
Aug 11 14:42:51.706: INFO: Pod wasn't evicted. Test successful
[AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:42:51.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-single-pod-3279" for this suite. 08/11/23 14:42:51.711
{"msg":"PASSED [sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]","completed":125,"skipped":2432,"failed":0}
------------------------------
â€¢ [SLOW TEST] [135.326 seconds]
[sig-node] NoExecuteTaintManager Single Pod [Serial]
test/e2e/node/framework.go:23
  removing taint cancels eviction [Disruptive] [Conformance]
  test/e2e/node/taints.go:289

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:40:36.392
    Aug 11 14:40:36.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename taint-single-pod 08/11/23 14:40:36.393
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:40:36.408
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:40:36.411
    [BeforeEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/node/taints.go:166
    Aug 11 14:40:36.413: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 14:41:36.452: INFO: Waiting for terminating namespaces to be deleted...
    [It] removing taint cancels eviction [Disruptive] [Conformance]
      test/e2e/node/taints.go:289
    Aug 11 14:41:36.455: INFO: Starting informer...
    STEP: Starting pod... 08/11/23 14:41:36.455
    Aug 11 14:41:36.670: INFO: Pod is running on constell-d93e7e1d-worker-d314547c-0lc3. Tainting Node
    STEP: Trying to apply a taint on the Node 08/11/23 14:41:36.67
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:41:36.682
    STEP: Waiting short time to make sure Pod is queued for deletion 08/11/23 14:41:36.684
    Aug 11 14:41:36.685: INFO: Pod wasn't evicted. Proceeding
    Aug 11 14:41:36.685: INFO: Removing taint from Node
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 14:41:36.696
    STEP: Waiting some time to make sure that toleration time passed. 08/11/23 14:41:36.705
    Aug 11 14:42:51.706: INFO: Pod wasn't evicted. Test successful
    [AfterEach] [sig-node] NoExecuteTaintManager Single Pod [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:42:51.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-single-pod-3279" for this suite. 08/11/23 14:42:51.711
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
[BeforeEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:42:51.721
Aug 11 14:42:51.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename watch 08/11/23 14:42:51.721
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:42:51.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:42:51.743
[It] should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334
STEP: getting a starting resourceVersion 08/11/23 14:42:51.746
STEP: starting a background goroutine to produce watch events 08/11/23 14:42:51.75
STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/11/23 14:42:51.75
[AfterEach] [sig-api-machinery] Watchers
  test/e2e/framework/framework.go:187
Aug 11 14:42:54.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "watch-1616" for this suite. 08/11/23 14:42:54.577
{"msg":"PASSED [sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]","completed":126,"skipped":2486,"failed":0}
------------------------------
â€¢ [2.910 seconds]
[sig-api-machinery] Watchers
test/e2e/apimachinery/framework.go:23
  should receive events on concurrent watches in same order [Conformance]
  test/e2e/apimachinery/watch.go:334

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:42:51.721
    Aug 11 14:42:51.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename watch 08/11/23 14:42:51.721
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:42:51.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:42:51.743
    [It] should receive events on concurrent watches in same order [Conformance]
      test/e2e/apimachinery/watch.go:334
    STEP: getting a starting resourceVersion 08/11/23 14:42:51.746
    STEP: starting a background goroutine to produce watch events 08/11/23 14:42:51.75
    STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order 08/11/23 14:42:51.75
    [AfterEach] [sig-api-machinery] Watchers
      test/e2e/framework/framework.go:187
    Aug 11 14:42:54.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "watch-1616" for this suite. 08/11/23 14:42:54.577
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:42:54.632
Aug 11 14:42:54.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:42:54.633
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:42:54.647
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:42:54.649
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:42:54.652
Aug 11 14:42:54.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04" in namespace "projected-6409" to be "Succeeded or Failed"
Aug 11 14:42:54.663: INFO: Pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.976603ms
Aug 11 14:42:56.669: INFO: Pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008575714s
Aug 11 14:42:58.669: INFO: Pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008610172s
STEP: Saw pod success 08/11/23 14:42:58.669
Aug 11 14:42:58.669: INFO: Pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04" satisfied condition "Succeeded or Failed"
Aug 11 14:42:58.673: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04 container client-container: <nil>
STEP: delete the pod 08/11/23 14:42:58.692
Aug 11 14:42:58.710: INFO: Waiting for pod downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04 to disappear
Aug 11 14:42:58.713: INFO: Pod downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 14:42:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6409" for this suite. 08/11/23 14:42:58.718
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]","completed":127,"skipped":2515,"failed":0}
------------------------------
â€¢ [4.092 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:42:54.632
    Aug 11 14:42:54.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:42:54.633
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:42:54.647
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:42:54.649
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:206
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:42:54.652
    Aug 11 14:42:54.660: INFO: Waiting up to 5m0s for pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04" in namespace "projected-6409" to be "Succeeded or Failed"
    Aug 11 14:42:54.663: INFO: Pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.976603ms
    Aug 11 14:42:56.669: INFO: Pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008575714s
    Aug 11 14:42:58.669: INFO: Pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008610172s
    STEP: Saw pod success 08/11/23 14:42:58.669
    Aug 11 14:42:58.669: INFO: Pod "downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04" satisfied condition "Succeeded or Failed"
    Aug 11 14:42:58.673: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:42:58.692
    Aug 11 14:42:58.710: INFO: Waiting for pod downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04 to disappear
    Aug 11 14:42:58.713: INFO: Pod downwardapi-volume-0998fee2-8b0c-413d-8d3e-25e00299ce04 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 14:42:58.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6409" for this suite. 08/11/23 14:42:58.718
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath
  runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:42:58.725
Aug 11 14:42:58.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:42:58.726
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:42:58.743
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:42:58.747
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 11 14:42:58.765: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 14:43:58.807: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:43:58.81
Aug 11 14:43:58.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-preemption-path 08/11/23 14:43:58.811
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:58.828
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:58.831
[BeforeEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:496
STEP: Finding an available node 08/11/23 14:43:58.835
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 14:43:58.835
Aug 11 14:43:58.843: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3403" to be "running"
Aug 11 14:43:58.846: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305253ms
Aug 11 14:44:00.851: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008502391s
Aug 11 14:44:00.851: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 14:44:00.854
Aug 11 14:44:00.866: INFO: found a healthy node: constell-d93e7e1d-worker-d314547c-0lc3
[It] runs ReplicaSets to verify preemption running path [Conformance]
  test/e2e/scheduling/preemption.go:543
Aug 11 14:44:14.952: INFO: pods created so far: [1 1 1]
Aug 11 14:44:14.952: INFO: length of pods created so far: 3
Aug 11 14:44:16.965: INFO: pods created so far: [2 2 1]
[AfterEach] PreemptionExecutionPath
  test/e2e/framework/framework.go:187
Aug 11 14:44:23.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-3403" for this suite. 08/11/23 14:44:23.973
[AfterEach] PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:470
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:44:24.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-6438" for this suite. 08/11/23 14:44:24.019
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]","completed":128,"skipped":2522,"failed":0}
------------------------------
â€¢ [SLOW TEST] [85.339 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PreemptionExecutionPath
  test/e2e/scheduling/preemption.go:458
    runs ReplicaSets to verify preemption running path [Conformance]
    test/e2e/scheduling/preemption.go:543

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:42:58.725
    Aug 11 14:42:58.725: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:42:58.726
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:42:58.743
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:42:58.747
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Aug 11 14:42:58.765: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 14:43:58.807: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:43:58.81
    Aug 11 14:43:58.811: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-preemption-path 08/11/23 14:43:58.811
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:43:58.828
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:43:58.831
    [BeforeEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:496
    STEP: Finding an available node 08/11/23 14:43:58.835
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 14:43:58.835
    Aug 11 14:43:58.843: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-preemption-path-3403" to be "running"
    Aug 11 14:43:58.846: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 3.305253ms
    Aug 11 14:44:00.851: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.008502391s
    Aug 11 14:44:00.851: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 14:44:00.854
    Aug 11 14:44:00.866: INFO: found a healthy node: constell-d93e7e1d-worker-d314547c-0lc3
    [It] runs ReplicaSets to verify preemption running path [Conformance]
      test/e2e/scheduling/preemption.go:543
    Aug 11 14:44:14.952: INFO: pods created so far: [1 1 1]
    Aug 11 14:44:14.952: INFO: length of pods created so far: 3
    Aug 11 14:44:16.965: INFO: pods created so far: [2 2 1]
    [AfterEach] PreemptionExecutionPath
      test/e2e/framework/framework.go:187
    Aug 11 14:44:23.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-3403" for this suite. 08/11/23 14:44:23.973
    [AfterEach] PreemptionExecutionPath
      test/e2e/scheduling/preemption.go:470
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:44:24.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-6438" for this suite. 08/11/23 14:44:24.019
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:44:24.065
Aug 11 14:44:24.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:44:24.066
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:24.082
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:24.085
[It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43
STEP: Creating a pod to test env composition 08/11/23 14:44:24.087
Aug 11 14:44:24.096: INFO: Waiting up to 5m0s for pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68" in namespace "var-expansion-5570" to be "Succeeded or Failed"
Aug 11 14:44:24.101: INFO: Pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68": Phase="Pending", Reason="", readiness=false. Elapsed: 5.714538ms
Aug 11 14:44:26.106: INFO: Pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010752212s
Aug 11 14:44:28.107: INFO: Pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011899543s
STEP: Saw pod success 08/11/23 14:44:28.107
Aug 11 14:44:28.108: INFO: Pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68" satisfied condition "Succeeded or Failed"
Aug 11 14:44:28.112: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod var-expansion-8d57c331-8718-46dc-9343-c935d3615c68 container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:44:28.122
Aug 11 14:44:28.137: INFO: Waiting for pod var-expansion-8d57c331-8718-46dc-9343-c935d3615c68 to disappear
Aug 11 14:44:28.140: INFO: Pod var-expansion-8d57c331-8718-46dc-9343-c935d3615c68 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 11 14:44:28.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-5570" for this suite. 08/11/23 14:44:28.144
{"msg":"PASSED [sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]","completed":129,"skipped":2537,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow composing env vars into new env vars [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:44:24.065
    Aug 11 14:44:24.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:44:24.066
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:24.082
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:24.085
    [It] should allow composing env vars into new env vars [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:43
    STEP: Creating a pod to test env composition 08/11/23 14:44:24.087
    Aug 11 14:44:24.096: INFO: Waiting up to 5m0s for pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68" in namespace "var-expansion-5570" to be "Succeeded or Failed"
    Aug 11 14:44:24.101: INFO: Pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68": Phase="Pending", Reason="", readiness=false. Elapsed: 5.714538ms
    Aug 11 14:44:26.106: INFO: Pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010752212s
    Aug 11 14:44:28.107: INFO: Pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011899543s
    STEP: Saw pod success 08/11/23 14:44:28.107
    Aug 11 14:44:28.108: INFO: Pod "var-expansion-8d57c331-8718-46dc-9343-c935d3615c68" satisfied condition "Succeeded or Failed"
    Aug 11 14:44:28.112: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod var-expansion-8d57c331-8718-46dc-9343-c935d3615c68 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:44:28.122
    Aug 11 14:44:28.137: INFO: Waiting for pod var-expansion-8d57c331-8718-46dc-9343-c935d3615c68 to disappear
    Aug 11 14:44:28.140: INFO: Pod var-expansion-8d57c331-8718-46dc-9343-c935d3615c68 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 11 14:44:28.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-5570" for this suite. 08/11/23 14:44:28.144
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run
  should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:44:28.153
Aug 11 14:44:28.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:44:28.154
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:28.172
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:28.176
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if kubectl can dry-run update Pods [Conformance]
  test/e2e/kubectl/kubectl.go:960
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/11/23 14:44:28.179
Aug 11 14:44:28.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3570 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 11 14:44:28.242: INFO: stderr: ""
Aug 11 14:44:28.242: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: replace the image in the pod with server-side dry-run 08/11/23 14:44:28.242
Aug 11 14:44:28.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3570 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
Aug 11 14:44:28.852: INFO: stderr: ""
Aug 11 14:44:28.852: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/11/23 14:44:28.852
Aug 11 14:44:28.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3570 delete pods e2e-test-httpd-pod'
Aug 11 14:44:30.461: INFO: stderr: ""
Aug 11 14:44:30.461: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:44:30.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3570" for this suite. 08/11/23 14:44:30.466
{"msg":"PASSED [sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]","completed":130,"skipped":2551,"failed":0}
------------------------------
â€¢ [2.319 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl server-side dry-run
  test/e2e/kubectl/kubectl.go:954
    should check if kubectl can dry-run update Pods [Conformance]
    test/e2e/kubectl/kubectl.go:960

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:44:28.153
    Aug 11 14:44:28.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:44:28.154
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:28.172
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:28.176
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if kubectl can dry-run update Pods [Conformance]
      test/e2e/kubectl/kubectl.go:960
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/11/23 14:44:28.179
    Aug 11 14:44:28.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3570 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 11 14:44:28.242: INFO: stderr: ""
    Aug 11 14:44:28.242: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: replace the image in the pod with server-side dry-run 08/11/23 14:44:28.242
    Aug 11 14:44:28.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3570 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-2"}]}} --dry-run=server'
    Aug 11 14:44:28.852: INFO: stderr: ""
    Aug 11 14:44:28.852: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/11/23 14:44:28.852
    Aug 11 14:44:28.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3570 delete pods e2e-test-httpd-pod'
    Aug 11 14:44:30.461: INFO: stderr: ""
    Aug 11 14:44:30.461: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:44:30.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3570" for this suite. 08/11/23 14:44:30.466
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Secrets
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:44:30.473
Aug 11 14:44:30.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:44:30.474
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:30.489
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:30.491
[It] should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45
STEP: Creating secret with name secret-test-1c94d52c-1ea5-4350-af90-8a85a3f537d1 08/11/23 14:44:30.493
STEP: Creating a pod to test consume secrets 08/11/23 14:44:30.499
Aug 11 14:44:30.507: INFO: Waiting up to 5m0s for pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17" in namespace "secrets-4251" to be "Succeeded or Failed"
Aug 11 14:44:30.509: INFO: Pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.589531ms
Aug 11 14:44:32.514: INFO: Pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00747525s
Aug 11 14:44:34.514: INFO: Pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007427745s
STEP: Saw pod success 08/11/23 14:44:34.514
Aug 11 14:44:34.514: INFO: Pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17" satisfied condition "Succeeded or Failed"
Aug 11 14:44:34.517: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17 container secret-env-test: <nil>
STEP: delete the pod 08/11/23 14:44:34.53
Aug 11 14:44:34.543: INFO: Waiting for pod pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17 to disappear
Aug 11 14:44:34.546: INFO: Pod pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17 no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:44:34.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4251" for this suite. 08/11/23 14:44:34.552
{"msg":"PASSED [sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]","completed":131,"skipped":2556,"failed":0}
------------------------------
â€¢ [4.086 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable from pods in env vars [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:44:30.473
    Aug 11 14:44:30.473: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:44:30.474
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:30.489
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:30.491
    [It] should be consumable from pods in env vars [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:45
    STEP: Creating secret with name secret-test-1c94d52c-1ea5-4350-af90-8a85a3f537d1 08/11/23 14:44:30.493
    STEP: Creating a pod to test consume secrets 08/11/23 14:44:30.499
    Aug 11 14:44:30.507: INFO: Waiting up to 5m0s for pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17" in namespace "secrets-4251" to be "Succeeded or Failed"
    Aug 11 14:44:30.509: INFO: Pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.589531ms
    Aug 11 14:44:32.514: INFO: Pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00747525s
    Aug 11 14:44:34.514: INFO: Pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007427745s
    STEP: Saw pod success 08/11/23 14:44:34.514
    Aug 11 14:44:34.514: INFO: Pod "pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17" satisfied condition "Succeeded or Failed"
    Aug 11 14:44:34.517: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17 container secret-env-test: <nil>
    STEP: delete the pod 08/11/23 14:44:34.53
    Aug 11 14:44:34.543: INFO: Waiting for pod pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17 to disappear
    Aug 11 14:44:34.546: INFO: Pod pod-secrets-c2b379cc-34d8-4a40-be2d-b976bd6d9f17 no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:44:34.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4251" for this suite. 08/11/23 14:44:34.552
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:44:34.562
Aug 11 14:44:34.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 14:44:34.563
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:34.577
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:34.579
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:44:34.582
Aug 11 14:44:34.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb" in namespace "downward-api-3440" to be "Succeeded or Failed"
Aug 11 14:44:34.599: INFO: Pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04806ms
Aug 11 14:44:36.604: INFO: Pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01294756s
Aug 11 14:44:38.604: INFO: Pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012489702s
STEP: Saw pod success 08/11/23 14:44:38.604
Aug 11 14:44:38.604: INFO: Pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb" satisfied condition "Succeeded or Failed"
Aug 11 14:44:38.607: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb container client-container: <nil>
STEP: delete the pod 08/11/23 14:44:38.616
Aug 11 14:44:38.628: INFO: Waiting for pod downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb to disappear
Aug 11 14:44:38.630: INFO: Pod downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 14:44:38.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-3440" for this suite. 08/11/23 14:44:38.634
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]","completed":132,"skipped":2588,"failed":0}
------------------------------
â€¢ [4.077 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:44:34.562
    Aug 11 14:44:34.562: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:44:34.563
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:34.577
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:34.579
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:206
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:44:34.582
    Aug 11 14:44:34.591: INFO: Waiting up to 5m0s for pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb" in namespace "downward-api-3440" to be "Succeeded or Failed"
    Aug 11 14:44:34.599: INFO: Pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb": Phase="Pending", Reason="", readiness=false. Elapsed: 8.04806ms
    Aug 11 14:44:36.604: INFO: Pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01294756s
    Aug 11 14:44:38.604: INFO: Pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.012489702s
    STEP: Saw pod success 08/11/23 14:44:38.604
    Aug 11 14:44:38.604: INFO: Pod "downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb" satisfied condition "Succeeded or Failed"
    Aug 11 14:44:38.607: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb container client-container: <nil>
    STEP: delete the pod 08/11/23 14:44:38.616
    Aug 11 14:44:38.628: INFO: Waiting for pod downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb to disappear
    Aug 11 14:44:38.630: INFO: Pod downwardapi-volume-6a1517d9-4a8c-479a-bccc-64d6676861fb no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 14:44:38.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-3440" for this suite. 08/11/23 14:44:38.634
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:44:38.64
Aug 11 14:44:38.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 14:44:38.641
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:38.654
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:38.656
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:44:38.658
Aug 11 14:44:38.666: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32" in namespace "downward-api-8207" to be "Succeeded or Failed"
Aug 11 14:44:38.672: INFO: Pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32": Phase="Pending", Reason="", readiness=false. Elapsed: 5.706388ms
Aug 11 14:44:40.676: INFO: Pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009532913s
Aug 11 14:44:42.676: INFO: Pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009966014s
STEP: Saw pod success 08/11/23 14:44:42.676
Aug 11 14:44:42.676: INFO: Pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32" satisfied condition "Succeeded or Failed"
Aug 11 14:44:42.679: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32 container client-container: <nil>
STEP: delete the pod 08/11/23 14:44:42.688
Aug 11 14:44:42.702: INFO: Waiting for pod downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32 to disappear
Aug 11 14:44:42.704: INFO: Pod downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 14:44:42.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8207" for this suite. 08/11/23 14:44:42.709
{"msg":"PASSED [sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]","completed":133,"skipped":2594,"failed":0}
------------------------------
â€¢ [4.076 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:83

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:44:38.64
    Aug 11 14:44:38.640: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:44:38.641
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:38.654
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:38.656
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:83
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:44:38.658
    Aug 11 14:44:38.666: INFO: Waiting up to 5m0s for pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32" in namespace "downward-api-8207" to be "Succeeded or Failed"
    Aug 11 14:44:38.672: INFO: Pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32": Phase="Pending", Reason="", readiness=false. Elapsed: 5.706388ms
    Aug 11 14:44:40.676: INFO: Pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009532913s
    Aug 11 14:44:42.676: INFO: Pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009966014s
    STEP: Saw pod success 08/11/23 14:44:42.676
    Aug 11 14:44:42.676: INFO: Pod "downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32" satisfied condition "Succeeded or Failed"
    Aug 11 14:44:42.679: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:44:42.688
    Aug 11 14:44:42.702: INFO: Waiting for pod downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32 to disappear
    Aug 11 14:44:42.704: INFO: Pod downwardapi-volume-b25881f8-c51f-4624-9448-a9c584c3bb32 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 14:44:42.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8207" for this suite. 08/11/23 14:44:42.709
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints
  verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:44:42.718
Aug 11 14:44:42.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:44:42.719
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:42.739
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:42.741
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 11 14:44:42.759: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 14:45:42.809: INFO: Waiting for terminating namespaces to be deleted...
[BeforeEach] PriorityClass endpoints
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:45:42.812
Aug 11 14:45:42.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-preemption-path 08/11/23 14:45:42.813
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:45:42.829
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:45:42.833
[BeforeEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:690
[It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
  test/e2e/scheduling/preemption.go:733
Aug 11 14:45:42.852: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
Aug 11 14:45:42.856: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
[AfterEach] PriorityClass endpoints
  test/e2e/framework/framework.go:187
Aug 11 14:45:42.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-path-1867" for this suite. 08/11/23 14:45:42.88
[AfterEach] PriorityClass endpoints
  test/e2e/scheduling/preemption.go:706
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:45:42.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-230" for this suite. 08/11/23 14:45:42.903
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]","completed":134,"skipped":2635,"failed":0}
------------------------------
â€¢ [SLOW TEST] [60.231 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  PriorityClass endpoints
  test/e2e/scheduling/preemption.go:683
    verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
    test/e2e/scheduling/preemption.go:733

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:44:42.718
    Aug 11 14:44:42.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-preemption 08/11/23 14:44:42.719
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:44:42.739
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:44:42.741
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Aug 11 14:44:42.759: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 14:45:42.809: INFO: Waiting for terminating namespaces to be deleted...
    [BeforeEach] PriorityClass endpoints
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:45:42.812
    Aug 11 14:45:42.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-preemption-path 08/11/23 14:45:42.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:45:42.829
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:45:42.833
    [BeforeEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:690
    [It] verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
      test/e2e/scheduling/preemption.go:733
    Aug 11 14:45:42.852: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
    Aug 11 14:45:42.856: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
    [AfterEach] PriorityClass endpoints
      test/e2e/framework/framework.go:187
    Aug 11 14:45:42.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-path-1867" for this suite. 08/11/23 14:45:42.88
    [AfterEach] PriorityClass endpoints
      test/e2e/scheduling/preemption.go:706
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:45:42.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-230" for this suite. 08/11/23 14:45:42.903
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:45:42.951
Aug 11 14:45:42.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:45:42.952
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:45:42.966
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:45:42.968
[It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65
STEP: Counting existing ResourceQuota 08/11/23 14:45:42.972
STEP: Creating a ResourceQuota 08/11/23 14:45:47.977
STEP: Ensuring resource quota status is calculated 08/11/23 14:45:47.984
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 14:45:49.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-9251" for this suite. 08/11/23 14:45:49.994
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]","completed":135,"skipped":2646,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.051 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
  test/e2e/apimachinery/resource_quota.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:45:42.951
    Aug 11 14:45:42.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:45:42.952
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:45:42.966
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:45:42.968
    [It] should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
      test/e2e/apimachinery/resource_quota.go:65
    STEP: Counting existing ResourceQuota 08/11/23 14:45:42.972
    STEP: Creating a ResourceQuota 08/11/23 14:45:47.977
    STEP: Ensuring resource quota status is calculated 08/11/23 14:45:47.984
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 14:45:49.989: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-9251" for this suite. 08/11/23 14:45:49.994
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:45:50.002
Aug 11 14:45:50.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:45:50.003
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:45:50.019
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:45:50.021
[It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224
STEP: creating the pod with failed condition 08/11/23 14:45:50.024
Aug 11 14:45:50.032: INFO: Waiting up to 2m0s for pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" in namespace "var-expansion-2416" to be "running"
Aug 11 14:45:50.037: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.034236ms
Aug 11 14:45:52.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009230154s
Aug 11 14:45:54.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009460498s
Aug 11 14:45:56.046: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014025586s
Aug 11 14:45:58.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01073529s
Aug 11 14:46:00.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00956282s
Aug 11 14:46:02.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009542837s
Aug 11 14:46:04.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010309197s
Aug 11 14:46:06.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009606522s
Aug 11 14:46:08.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009921108s
Aug 11 14:46:10.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010830293s
Aug 11 14:46:12.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010428037s
Aug 11 14:46:14.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009293959s
Aug 11 14:46:16.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010438611s
Aug 11 14:46:18.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010119677s
Aug 11 14:46:20.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010951679s
Aug 11 14:46:22.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00978313s
Aug 11 14:46:24.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009913701s
Aug 11 14:46:26.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00997384s
Aug 11 14:46:28.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010048119s
Aug 11 14:46:30.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010963134s
Aug 11 14:46:32.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009404454s
Aug 11 14:46:34.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008666856s
Aug 11 14:46:36.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009633562s
Aug 11 14:46:38.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010046583s
Aug 11 14:46:40.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00944467s
Aug 11 14:46:42.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00986349s
Aug 11 14:46:44.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010704493s
Aug 11 14:46:46.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010530554s
Aug 11 14:46:48.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010605753s
Aug 11 14:46:50.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01090685s
Aug 11 14:46:52.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009109791s
Aug 11 14:46:54.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010424638s
Aug 11 14:46:56.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009987792s
Aug 11 14:46:58.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010064852s
Aug 11 14:47:00.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010194784s
Aug 11 14:47:02.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010080806s
Aug 11 14:47:04.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010078343s
Aug 11 14:47:06.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011198963s
Aug 11 14:47:08.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009881119s
Aug 11 14:47:10.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01069644s
Aug 11 14:47:12.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010065057s
Aug 11 14:47:14.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010722044s
Aug 11 14:47:16.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00973644s
Aug 11 14:47:18.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009576552s
Aug 11 14:47:20.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01023742s
Aug 11 14:47:22.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009591337s
Aug 11 14:47:24.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010550813s
Aug 11 14:47:26.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010708735s
Aug 11 14:47:28.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010542386s
Aug 11 14:47:30.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010374288s
Aug 11 14:47:32.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010434937s
Aug 11 14:47:34.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010548386s
Aug 11 14:47:36.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010356307s
Aug 11 14:47:38.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009568399s
Aug 11 14:47:40.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009830305s
Aug 11 14:47:42.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009879454s
Aug 11 14:47:44.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009380934s
Aug 11 14:47:46.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010063812s
Aug 11 14:47:48.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010229544s
Aug 11 14:47:50.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009328242s
Aug 11 14:47:50.044: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012426588s
STEP: updating the pod 08/11/23 14:47:50.044
Aug 11 14:47:50.557: INFO: Successfully updated pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9"
STEP: waiting for pod running 08/11/23 14:47:50.557
Aug 11 14:47:50.557: INFO: Waiting up to 2m0s for pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" in namespace "var-expansion-2416" to be "running"
Aug 11 14:47:50.562: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.061497ms
Aug 11 14:47:52.567: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010156091s
Aug 11 14:47:52.567: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" satisfied condition "running"
STEP: deleting the pod gracefully 08/11/23 14:47:52.567
Aug 11 14:47:52.567: INFO: Deleting pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" in namespace "var-expansion-2416"
Aug 11 14:47:52.575: INFO: Wait up to 5m0s for pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 11 14:48:24.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-2416" for this suite. 08/11/23 14:48:24.588
{"msg":"PASSED [sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]","completed":136,"skipped":2649,"failed":0}
------------------------------
â€¢ [SLOW TEST] [154.593 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
  test/e2e/common/node/expansion.go:224

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:45:50.002
    Aug 11 14:45:50.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:45:50.003
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:45:50.019
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:45:50.021
    [It] should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
      test/e2e/common/node/expansion.go:224
    STEP: creating the pod with failed condition 08/11/23 14:45:50.024
    Aug 11 14:45:50.032: INFO: Waiting up to 2m0s for pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" in namespace "var-expansion-2416" to be "running"
    Aug 11 14:45:50.037: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.034236ms
    Aug 11 14:45:52.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009230154s
    Aug 11 14:45:54.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.009460498s
    Aug 11 14:45:56.046: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.014025586s
    Aug 11 14:45:58.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 8.01073529s
    Aug 11 14:46:00.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 10.00956282s
    Aug 11 14:46:02.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 12.009542837s
    Aug 11 14:46:04.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 14.010309197s
    Aug 11 14:46:06.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 16.009606522s
    Aug 11 14:46:08.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 18.009921108s
    Aug 11 14:46:10.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 20.010830293s
    Aug 11 14:46:12.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 22.010428037s
    Aug 11 14:46:14.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 24.009293959s
    Aug 11 14:46:16.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 26.010438611s
    Aug 11 14:46:18.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 28.010119677s
    Aug 11 14:46:20.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 30.010951679s
    Aug 11 14:46:22.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 32.00978313s
    Aug 11 14:46:24.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 34.009913701s
    Aug 11 14:46:26.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 36.00997384s
    Aug 11 14:46:28.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 38.010048119s
    Aug 11 14:46:30.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 40.010963134s
    Aug 11 14:46:32.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 42.009404454s
    Aug 11 14:46:34.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 44.008666856s
    Aug 11 14:46:36.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 46.009633562s
    Aug 11 14:46:38.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 48.010046583s
    Aug 11 14:46:40.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 50.00944467s
    Aug 11 14:46:42.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 52.00986349s
    Aug 11 14:46:44.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 54.010704493s
    Aug 11 14:46:46.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 56.010530554s
    Aug 11 14:46:48.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 58.010605753s
    Aug 11 14:46:50.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m0.01090685s
    Aug 11 14:46:52.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m2.009109791s
    Aug 11 14:46:54.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m4.010424638s
    Aug 11 14:46:56.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m6.009987792s
    Aug 11 14:46:58.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m8.010064852s
    Aug 11 14:47:00.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m10.010194784s
    Aug 11 14:47:02.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m12.010080806s
    Aug 11 14:47:04.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m14.010078343s
    Aug 11 14:47:06.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m16.011198963s
    Aug 11 14:47:08.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m18.009881119s
    Aug 11 14:47:10.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m20.01069644s
    Aug 11 14:47:12.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m22.010065057s
    Aug 11 14:47:14.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m24.010722044s
    Aug 11 14:47:16.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m26.00973644s
    Aug 11 14:47:18.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m28.009576552s
    Aug 11 14:47:20.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m30.01023742s
    Aug 11 14:47:22.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m32.009591337s
    Aug 11 14:47:24.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m34.010550813s
    Aug 11 14:47:26.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m36.010708735s
    Aug 11 14:47:28.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m38.010542386s
    Aug 11 14:47:30.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m40.010374288s
    Aug 11 14:47:32.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m42.010434937s
    Aug 11 14:47:34.043: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m44.010548386s
    Aug 11 14:47:36.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m46.010356307s
    Aug 11 14:47:38.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m48.009568399s
    Aug 11 14:47:40.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m50.009830305s
    Aug 11 14:47:42.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m52.009879454s
    Aug 11 14:47:44.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m54.009380934s
    Aug 11 14:47:46.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m56.010063812s
    Aug 11 14:47:48.042: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 1m58.010229544s
    Aug 11 14:47:50.041: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.009328242s
    Aug 11 14:47:50.044: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2m0.012426588s
    STEP: updating the pod 08/11/23 14:47:50.044
    Aug 11 14:47:50.557: INFO: Successfully updated pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9"
    STEP: waiting for pod running 08/11/23 14:47:50.557
    Aug 11 14:47:50.557: INFO: Waiting up to 2m0s for pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" in namespace "var-expansion-2416" to be "running"
    Aug 11 14:47:50.562: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Pending", Reason="", readiness=false. Elapsed: 5.061497ms
    Aug 11 14:47:52.567: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010156091s
    Aug 11 14:47:52.567: INFO: Pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" satisfied condition "running"
    STEP: deleting the pod gracefully 08/11/23 14:47:52.567
    Aug 11 14:47:52.567: INFO: Deleting pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" in namespace "var-expansion-2416"
    Aug 11 14:47:52.575: INFO: Wait up to 5m0s for pod "var-expansion-f8583a76-49d5-4acd-89c3-6a5b10b675d9" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 11 14:48:24.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-2416" for this suite. 08/11/23 14:48:24.588
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:48:24.596
Aug 11 14:48:24.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename deployment 08/11/23 14:48:24.597
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:24.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:24.614
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113
Aug 11 14:48:24.616: INFO: Creating deployment "test-recreate-deployment"
Aug 11 14:48:24.621: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
Aug 11 14:48:24.631: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
Aug 11 14:48:26.640: INFO: Waiting deployment "test-recreate-deployment" to complete
Aug 11 14:48:26.644: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
Aug 11 14:48:26.655: INFO: Updating deployment test-recreate-deployment
Aug 11 14:48:26.655: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 14:48:26.734: INFO: Deployment "test-recreate-deployment":
&Deployment{ObjectMeta:{test-recreate-deployment  deployment-5704  f47c1a1a-3508-4b9f-864f-1a607bcf1c3c 28716 2 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401b298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 14:48:26 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-08-11 14:48:26 +0000 UTC,LastTransitionTime:2023-08-11 14:48:24 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

Aug 11 14:48:26.737: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
&ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-5704  4b0da933-8138-4d2a-b9ce-6d5ae7047fc8 28715 1 2023-08-11 14:48:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f47c1a1a-3508-4b9f-864f-1a607bcf1c3c 0xc00401b760 0xc00401b761}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f47c1a1a-3508-4b9f-864f-1a607bcf1c3c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401b7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:48:26.737: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
Aug 11 14:48:26.737: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-5704  737132d9-1fe2-41af-9f9d-7b502d78d94e 28701 2 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f47c1a1a-3508-4b9f-864f-1a607bcf1c3c 0xc00401b647 0xc00401b648}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f47c1a1a-3508-4b9f-864f-1a607bcf1c3c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401b6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:48:26.743: INFO: Pod "test-recreate-deployment-9d58999df-vp7w4" is not available:
&Pod{ObjectMeta:{test-recreate-deployment-9d58999df-vp7w4 test-recreate-deployment-9d58999df- deployment-5704  fbc85c86-5076-4cad-bf71-2158c15a7820 28714 0 2023-08-11 14:48:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 4b0da933-8138-4d2a-b9ce-6d5ae7047fc8 0xc00401bc70 0xc00401bc71}] [] [{kube-controller-manager Update v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b0da933-8138-4d2a-b9ce-6d5ae7047fc8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qt9fh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qt9fh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:,StartTime:2023-08-11 14:48:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 11 14:48:26.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-5704" for this suite. 08/11/23 14:48:26.747
{"msg":"PASSED [sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]","completed":137,"skipped":2675,"failed":0}
------------------------------
â€¢ [2.158 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RecreateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:113

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:48:24.596
    Aug 11 14:48:24.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename deployment 08/11/23 14:48:24.597
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:24.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:24.614
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RecreateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:113
    Aug 11 14:48:24.616: INFO: Creating deployment "test-recreate-deployment"
    Aug 11 14:48:24.621: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
    Aug 11 14:48:24.631: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
    Aug 11 14:48:26.640: INFO: Waiting deployment "test-recreate-deployment" to complete
    Aug 11 14:48:26.644: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
    Aug 11 14:48:26.655: INFO: Updating deployment test-recreate-deployment
    Aug 11 14:48:26.655: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 14:48:26.734: INFO: Deployment "test-recreate-deployment":
    &Deployment{ObjectMeta:{test-recreate-deployment  deployment-5704  f47c1a1a-3508-4b9f-864f-1a607bcf1c3c 28716 2 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401b298 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 14:48:26 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-9d58999df" is progressing.,LastUpdateTime:2023-08-11 14:48:26 +0000 UTC,LastTransitionTime:2023-08-11 14:48:24 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

    Aug 11 14:48:26.737: INFO: New ReplicaSet "test-recreate-deployment-9d58999df" of Deployment "test-recreate-deployment":
    &ReplicaSet{ObjectMeta:{test-recreate-deployment-9d58999df  deployment-5704  4b0da933-8138-4d2a-b9ce-6d5ae7047fc8 28715 1 2023-08-11 14:48:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment f47c1a1a-3508-4b9f-864f-1a607bcf1c3c 0xc00401b760 0xc00401b761}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f47c1a1a-3508-4b9f-864f-1a607bcf1c3c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 9d58999df,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401b7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:48:26.737: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
    Aug 11 14:48:26.737: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-7d8b6f647f  deployment-5704  737132d9-1fe2-41af-9f9d-7b502d78d94e 28701 2 2023-08-11 14:48:24 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment f47c1a1a-3508-4b9f-864f-1a607bcf1c3c 0xc00401b647 0xc00401b648}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f47c1a1a-3508-4b9f-864f-1a607bcf1c3c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 7d8b6f647f,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:7d8b6f647f] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00401b6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:48:26.743: INFO: Pod "test-recreate-deployment-9d58999df-vp7w4" is not available:
    &Pod{ObjectMeta:{test-recreate-deployment-9d58999df-vp7w4 test-recreate-deployment-9d58999df- deployment-5704  fbc85c86-5076-4cad-bf71-2158c15a7820 28714 0 2023-08-11 14:48:26 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:9d58999df] map[] [{apps/v1 ReplicaSet test-recreate-deployment-9d58999df 4b0da933-8138-4d2a-b9ce-6d5ae7047fc8 0xc00401bc70 0xc00401bc71}] [] [{kube-controller-manager Update v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4b0da933-8138-4d2a-b9ce-6d5ae7047fc8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:48:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qt9fh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qt9fh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:48:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:,StartTime:2023-08-11 14:48:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 11 14:48:26.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-5704" for this suite. 08/11/23 14:48:26.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:48:26.755
Aug 11 14:48:26.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:48:26.756
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:26.77
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:26.774
[It] should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150
STEP: Discovering how many secrets are in namespace by default 08/11/23 14:48:26.776
STEP: Counting existing ResourceQuota 08/11/23 14:48:31.781
STEP: Creating a ResourceQuota 08/11/23 14:48:36.786
STEP: Ensuring resource quota status is calculated 08/11/23 14:48:36.791
STEP: Creating a Secret 08/11/23 14:48:38.797
STEP: Ensuring resource quota status captures secret creation 08/11/23 14:48:38.809
STEP: Deleting a secret 08/11/23 14:48:40.813
STEP: Ensuring resource quota status released usage 08/11/23 14:48:40.819
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 14:48:42.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1103" for this suite. 08/11/23 14:48:42.83
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]","completed":138,"skipped":2686,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.085 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a secret. [Conformance]
  test/e2e/apimachinery/resource_quota.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:48:26.755
    Aug 11 14:48:26.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:48:26.756
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:26.77
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:26.774
    [It] should create a ResourceQuota and capture the life of a secret. [Conformance]
      test/e2e/apimachinery/resource_quota.go:150
    STEP: Discovering how many secrets are in namespace by default 08/11/23 14:48:26.776
    STEP: Counting existing ResourceQuota 08/11/23 14:48:31.781
    STEP: Creating a ResourceQuota 08/11/23 14:48:36.786
    STEP: Ensuring resource quota status is calculated 08/11/23 14:48:36.791
    STEP: Creating a Secret 08/11/23 14:48:38.797
    STEP: Ensuring resource quota status captures secret creation 08/11/23 14:48:38.809
    STEP: Deleting a secret 08/11/23 14:48:40.813
    STEP: Ensuring resource quota status released usage 08/11/23 14:48:40.819
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 14:48:42.825: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1103" for this suite. 08/11/23 14:48:42.83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:48:42.843
Aug 11 14:48:42.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename namespaces 08/11/23 14:48:42.844
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:42.863
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:42.866
[It] should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298
STEP: Read namespace status 08/11/23 14:48:42.87
Aug 11 14:48:42.874: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
STEP: Patch namespace status 08/11/23 14:48:42.874
Aug 11 14:48:42.881: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
STEP: Update namespace status 08/11/23 14:48:42.881
Aug 11 14:48:42.892: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Aug 11 14:48:42.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-2740" for this suite. 08/11/23 14:48:42.896
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]","completed":139,"skipped":2717,"failed":0}
------------------------------
â€¢ [0.061 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should apply changes to a namespace status [Conformance]
  test/e2e/apimachinery/namespace.go:298

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:48:42.843
    Aug 11 14:48:42.843: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename namespaces 08/11/23 14:48:42.844
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:42.863
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:42.866
    [It] should apply changes to a namespace status [Conformance]
      test/e2e/apimachinery/namespace.go:298
    STEP: Read namespace status 08/11/23 14:48:42.87
    Aug 11 14:48:42.874: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
    STEP: Patch namespace status 08/11/23 14:48:42.874
    Aug 11 14:48:42.881: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
    STEP: Update namespace status 08/11/23 14:48:42.881
    Aug 11 14:48:42.892: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 14:48:42.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-2740" for this suite. 08/11/23 14:48:42.896
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:48:42.905
Aug 11 14:48:42.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 14:48:42.906
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:42.921
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:42.925
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:48:42.928
Aug 11 14:48:42.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814" in namespace "downward-api-576" to be "Succeeded or Failed"
Aug 11 14:48:42.941: INFO: Pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574704ms
Aug 11 14:48:44.947: INFO: Pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01100931s
Aug 11 14:48:46.947: INFO: Pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011086649s
STEP: Saw pod success 08/11/23 14:48:46.947
Aug 11 14:48:46.947: INFO: Pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814" satisfied condition "Succeeded or Failed"
Aug 11 14:48:46.951: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814 container client-container: <nil>
STEP: delete the pod 08/11/23 14:48:46.974
Aug 11 14:48:46.991: INFO: Waiting for pod downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814 to disappear
Aug 11 14:48:46.996: INFO: Pod downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 14:48:46.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-576" for this suite. 08/11/23 14:48:47
{"msg":"PASSED [sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]","completed":140,"skipped":2736,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide podname only [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:48:42.905
    Aug 11 14:48:42.905: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:48:42.906
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:42.921
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:42.925
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide podname only [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:52
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:48:42.928
    Aug 11 14:48:42.936: INFO: Waiting up to 5m0s for pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814" in namespace "downward-api-576" to be "Succeeded or Failed"
    Aug 11 14:48:42.941: INFO: Pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814": Phase="Pending", Reason="", readiness=false. Elapsed: 5.574704ms
    Aug 11 14:48:44.947: INFO: Pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01100931s
    Aug 11 14:48:46.947: INFO: Pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011086649s
    STEP: Saw pod success 08/11/23 14:48:46.947
    Aug 11 14:48:46.947: INFO: Pod "downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814" satisfied condition "Succeeded or Failed"
    Aug 11 14:48:46.951: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:48:46.974
    Aug 11 14:48:46.991: INFO: Waiting for pod downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814 to disappear
    Aug 11 14:48:46.996: INFO: Pod downwardapi-volume-30db99d9-7a4a-414d-bb08-648c8347b814 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 14:48:46.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-576" for this suite. 08/11/23 14:48:47
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:48:47.007
Aug 11 14:48:47.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:48:47.008
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:47.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:47.024
[It] should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198
STEP: fetching the /apis discovery document 08/11/23 14:48:47.027
STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/11/23 14:48:47.028
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/11/23 14:48:47.028
STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/11/23 14:48:47.028
STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/11/23 14:48:47.029
STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/11/23 14:48:47.029
STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/11/23 14:48:47.03
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:48:47.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-6456" for this suite. 08/11/23 14:48:47.034
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]","completed":141,"skipped":2750,"failed":0}
------------------------------
â€¢ [0.032 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include custom resource definition resources in discovery documents [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:198

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:48:47.007
    Aug 11 14:48:47.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:48:47.008
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:47.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:47.024
    [It] should include custom resource definition resources in discovery documents [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:198
    STEP: fetching the /apis discovery document 08/11/23 14:48:47.027
    STEP: finding the apiextensions.k8s.io API group in the /apis discovery document 08/11/23 14:48:47.028
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document 08/11/23 14:48:47.028
    STEP: fetching the /apis/apiextensions.k8s.io discovery document 08/11/23 14:48:47.028
    STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document 08/11/23 14:48:47.029
    STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document 08/11/23 14:48:47.029
    STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document 08/11/23 14:48:47.03
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:48:47.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-6456" for this suite. 08/11/23 14:48:47.034
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:48:47.039
Aug 11 14:48:47.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pod-network-test 08/11/23 14:48:47.04
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:47.055
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:47.058
[It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:93
STEP: Performing setup for networking test in namespace pod-network-test-8899 08/11/23 14:48:47.06
STEP: creating a selector 08/11/23 14:48:47.061
STEP: Creating the service pods in kubernetes 08/11/23 14:48:47.061
Aug 11 14:48:47.061: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 11 14:48:47.083: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8899" to be "running and ready"
Aug 11 14:48:47.090: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122981ms
Aug 11 14:48:47.090: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:48:49.094: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010318178s
Aug 11 14:48:49.094: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:48:51.098: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014146844s
Aug 11 14:48:51.098: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:48:53.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011238599s
Aug 11 14:48:53.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:48:55.096: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012512345s
Aug 11 14:48:55.096: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:48:57.094: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010435848s
Aug 11 14:48:57.094: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:48:59.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011599091s
Aug 11 14:48:59.095: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 11 14:48:59.095: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 11 14:48:59.098: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8899" to be "running and ready"
Aug 11 14:48:59.102: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.497819ms
Aug 11 14:48:59.102: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 11 14:48:59.102: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/11/23 14:48:59.105
Aug 11 14:48:59.112: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8899" to be "running"
Aug 11 14:48:59.117: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.449919ms
Aug 11 14:49:01.123: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01101584s
Aug 11 14:49:01.123: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 11 14:49:01.126: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 11 14:49:01.126: INFO: Breadth first check of 10.10.0.221 on host 192.168.178.2...
Aug 11 14:49:01.130: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.21:9080/dial?request=hostname&protocol=udp&host=10.10.0.221&port=8081&tries=1'] Namespace:pod-network-test-8899 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:49:01.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:49:01.130: INFO: ExecWithOptions: Clientset creation
Aug 11 14:49:01.130: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8899/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.21%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.0.221%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 11 14:49:01.209: INFO: Waiting for responses: map[]
Aug 11 14:49:01.209: INFO: reached 10.10.0.221 after 0/1 tries
Aug 11 14:49:01.209: INFO: Breadth first check of 10.10.1.129 on host 192.168.178.3...
Aug 11 14:49:01.213: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.21:9080/dial?request=hostname&protocol=udp&host=10.10.1.129&port=8081&tries=1'] Namespace:pod-network-test-8899 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:49:01.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:49:01.213: INFO: ExecWithOptions: Clientset creation
Aug 11 14:49:01.214: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8899/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.21%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.1.129%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
Aug 11 14:49:01.287: INFO: Waiting for responses: map[]
Aug 11 14:49:01.287: INFO: reached 10.10.1.129 after 0/1 tries
Aug 11 14:49:01.287: INFO: Going to retry 0 out of 2 pods....
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Aug 11 14:49:01.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-8899" for this suite. 08/11/23 14:49:01.292
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]","completed":142,"skipped":2757,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.259 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for intra-pod communication: udp [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:93

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:48:47.039
    Aug 11 14:48:47.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pod-network-test 08/11/23 14:48:47.04
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:48:47.055
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:48:47.058
    [It] should function for intra-pod communication: udp [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:93
    STEP: Performing setup for networking test in namespace pod-network-test-8899 08/11/23 14:48:47.06
    STEP: creating a selector 08/11/23 14:48:47.061
    STEP: Creating the service pods in kubernetes 08/11/23 14:48:47.061
    Aug 11 14:48:47.061: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 11 14:48:47.083: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-8899" to be "running and ready"
    Aug 11 14:48:47.090: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.122981ms
    Aug 11 14:48:47.090: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:48:49.094: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.010318178s
    Aug 11 14:48:49.094: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:48:51.098: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.014146844s
    Aug 11 14:48:51.098: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:48:53.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.011238599s
    Aug 11 14:48:53.095: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:48:55.096: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.012512345s
    Aug 11 14:48:55.096: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:48:57.094: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.010435848s
    Aug 11 14:48:57.094: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:48:59.095: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.011599091s
    Aug 11 14:48:59.095: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 11 14:48:59.095: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 11 14:48:59.098: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-8899" to be "running and ready"
    Aug 11 14:48:59.102: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.497819ms
    Aug 11 14:48:59.102: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 11 14:48:59.102: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/11/23 14:48:59.105
    Aug 11 14:48:59.112: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-8899" to be "running"
    Aug 11 14:48:59.117: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.449919ms
    Aug 11 14:49:01.123: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.01101584s
    Aug 11 14:49:01.123: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 11 14:49:01.126: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 11 14:49:01.126: INFO: Breadth first check of 10.10.0.221 on host 192.168.178.2...
    Aug 11 14:49:01.130: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.21:9080/dial?request=hostname&protocol=udp&host=10.10.0.221&port=8081&tries=1'] Namespace:pod-network-test-8899 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:49:01.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:49:01.130: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:49:01.130: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8899/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.21%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.0.221%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 11 14:49:01.209: INFO: Waiting for responses: map[]
    Aug 11 14:49:01.209: INFO: reached 10.10.0.221 after 0/1 tries
    Aug 11 14:49:01.209: INFO: Breadth first check of 10.10.1.129 on host 192.168.178.3...
    Aug 11 14:49:01.213: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.10.0.21:9080/dial?request=hostname&protocol=udp&host=10.10.1.129&port=8081&tries=1'] Namespace:pod-network-test-8899 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:49:01.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:49:01.213: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:49:01.214: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8899/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.10.0.21%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.10.1.129%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
    Aug 11 14:49:01.287: INFO: Waiting for responses: map[]
    Aug 11 14:49:01.287: INFO: reached 10.10.1.129 after 0/1 tries
    Aug 11 14:49:01.287: INFO: Going to retry 0 out of 2 pods....
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Aug 11 14:49:01.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-8899" for this suite. 08/11/23 14:49:01.292
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1
  A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:49:01.3
Aug 11 14:49:01.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename proxy 08/11/23 14:49:01.301
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:01.316
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:01.319
[It] A set of valid responses are returned for both pod and service Proxy [Conformance]
  test/e2e/network/proxy.go:380
Aug 11 14:49:01.321: INFO: Creating pod...
Aug 11 14:49:01.330: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4890" to be "running"
Aug 11 14:49:01.336: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.643266ms
Aug 11 14:49:03.341: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.01069953s
Aug 11 14:49:03.341: INFO: Pod "agnhost" satisfied condition "running"
Aug 11 14:49:03.341: INFO: Creating service...
Aug 11 14:49:03.356: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=DELETE
Aug 11 14:49:03.371: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 11 14:49:03.371: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=OPTIONS
Aug 11 14:49:03.377: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 11 14:49:03.377: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=PATCH
Aug 11 14:49:03.384: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 11 14:49:03.384: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=POST
Aug 11 14:49:03.390: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 11 14:49:03.390: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=PUT
Aug 11 14:49:03.397: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 11 14:49:03.397: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=DELETE
Aug 11 14:49:03.405: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
Aug 11 14:49:03.405: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=OPTIONS
Aug 11 14:49:03.418: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
Aug 11 14:49:03.418: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=PATCH
Aug 11 14:49:03.426: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
Aug 11 14:49:03.426: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=POST
Aug 11 14:49:03.434: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
Aug 11 14:49:03.434: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=PUT
Aug 11 14:49:03.442: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
Aug 11 14:49:03.442: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=GET
Aug 11 14:49:03.446: INFO: http.Client request:GET StatusCode:301
Aug 11 14:49:03.446: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=GET
Aug 11 14:49:03.450: INFO: http.Client request:GET StatusCode:301
Aug 11 14:49:03.450: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=HEAD
Aug 11 14:49:03.453: INFO: http.Client request:HEAD StatusCode:301
Aug 11 14:49:03.453: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=HEAD
Aug 11 14:49:03.458: INFO: http.Client request:HEAD StatusCode:301
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Aug 11 14:49:03.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-4890" for this suite. 08/11/23 14:49:03.462
{"msg":"PASSED [sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]","completed":143,"skipped":2771,"failed":0}
------------------------------
â€¢ [2.169 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    A set of valid responses are returned for both pod and service Proxy [Conformance]
    test/e2e/network/proxy.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:49:01.3
    Aug 11 14:49:01.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename proxy 08/11/23 14:49:01.301
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:01.316
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:01.319
    [It] A set of valid responses are returned for both pod and service Proxy [Conformance]
      test/e2e/network/proxy.go:380
    Aug 11 14:49:01.321: INFO: Creating pod...
    Aug 11 14:49:01.330: INFO: Waiting up to 5m0s for pod "agnhost" in namespace "proxy-4890" to be "running"
    Aug 11 14:49:01.336: INFO: Pod "agnhost": Phase="Pending", Reason="", readiness=false. Elapsed: 5.643266ms
    Aug 11 14:49:03.341: INFO: Pod "agnhost": Phase="Running", Reason="", readiness=true. Elapsed: 2.01069953s
    Aug 11 14:49:03.341: INFO: Pod "agnhost" satisfied condition "running"
    Aug 11 14:49:03.341: INFO: Creating service...
    Aug 11 14:49:03.356: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=DELETE
    Aug 11 14:49:03.371: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 11 14:49:03.371: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=OPTIONS
    Aug 11 14:49:03.377: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 11 14:49:03.377: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=PATCH
    Aug 11 14:49:03.384: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 11 14:49:03.384: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=POST
    Aug 11 14:49:03.390: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 11 14:49:03.390: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=PUT
    Aug 11 14:49:03.397: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 11 14:49:03.397: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=DELETE
    Aug 11 14:49:03.405: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
    Aug 11 14:49:03.405: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=OPTIONS
    Aug 11 14:49:03.418: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
    Aug 11 14:49:03.418: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=PATCH
    Aug 11 14:49:03.426: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
    Aug 11 14:49:03.426: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=POST
    Aug 11 14:49:03.434: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
    Aug 11 14:49:03.434: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=PUT
    Aug 11 14:49:03.442: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
    Aug 11 14:49:03.442: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=GET
    Aug 11 14:49:03.446: INFO: http.Client request:GET StatusCode:301
    Aug 11 14:49:03.446: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=GET
    Aug 11 14:49:03.450: INFO: http.Client request:GET StatusCode:301
    Aug 11 14:49:03.450: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/pods/agnhost/proxy?method=HEAD
    Aug 11 14:49:03.453: INFO: http.Client request:HEAD StatusCode:301
    Aug 11 14:49:03.453: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-4890/services/e2e-proxy-test-service/proxy?method=HEAD
    Aug 11 14:49:03.458: INFO: http.Client request:HEAD StatusCode:301
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Aug 11 14:49:03.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-4890" for this suite. 08/11/23 14:49:03.462
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:49:03.472
Aug 11 14:49:03.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename job 08/11/23 14:49:03.472
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:03.487
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:03.489
[It] should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464
STEP: Creating a job 08/11/23 14:49:03.492
STEP: Ensure pods equal to paralellism count is attached to the job 08/11/23 14:49:03.497
STEP: patching /status 08/11/23 14:49:05.502
STEP: updating /status 08/11/23 14:49:05.512
STEP: get /status 08/11/23 14:49:05.542
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 11 14:49:05.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-2884" for this suite. 08/11/23 14:49:05.55
{"msg":"PASSED [sig-apps] Job should apply changes to a job status [Conformance]","completed":144,"skipped":2819,"failed":0}
------------------------------
â€¢ [2.086 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should apply changes to a job status [Conformance]
  test/e2e/apps/job.go:464

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:49:03.472
    Aug 11 14:49:03.472: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename job 08/11/23 14:49:03.472
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:03.487
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:03.489
    [It] should apply changes to a job status [Conformance]
      test/e2e/apps/job.go:464
    STEP: Creating a job 08/11/23 14:49:03.492
    STEP: Ensure pods equal to paralellism count is attached to the job 08/11/23 14:49:03.497
    STEP: patching /status 08/11/23 14:49:05.502
    STEP: updating /status 08/11/23 14:49:05.512
    STEP: get /status 08/11/23 14:49:05.542
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 11 14:49:05.546: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-2884" for this suite. 08/11/23 14:49:05.55
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Discovery
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:49:05.558
Aug 11 14:49:05.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename discovery 08/11/23 14:49:05.559
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:05.575
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:05.578
[BeforeEach] [sig-api-machinery] Discovery
  test/e2e/apimachinery/discovery.go:43
STEP: Setting up server cert 08/11/23 14:49:05.582
[It] should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122
Aug 11 14:49:05.995: INFO: Checking APIGroup: apiregistration.k8s.io
Aug 11 14:49:05.996: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
Aug 11 14:49:05.996: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
Aug 11 14:49:05.996: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
Aug 11 14:49:05.996: INFO: Checking APIGroup: apps
Aug 11 14:49:05.997: INFO: PreferredVersion.GroupVersion: apps/v1
Aug 11 14:49:05.997: INFO: Versions found [{apps/v1 v1}]
Aug 11 14:49:05.997: INFO: apps/v1 matches apps/v1
Aug 11 14:49:05.997: INFO: Checking APIGroup: events.k8s.io
Aug 11 14:49:05.998: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
Aug 11 14:49:05.998: INFO: Versions found [{events.k8s.io/v1 v1}]
Aug 11 14:49:05.998: INFO: events.k8s.io/v1 matches events.k8s.io/v1
Aug 11 14:49:05.998: INFO: Checking APIGroup: authentication.k8s.io
Aug 11 14:49:05.999: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
Aug 11 14:49:05.999: INFO: Versions found [{authentication.k8s.io/v1 v1}]
Aug 11 14:49:05.999: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
Aug 11 14:49:05.999: INFO: Checking APIGroup: authorization.k8s.io
Aug 11 14:49:06.000: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
Aug 11 14:49:06.000: INFO: Versions found [{authorization.k8s.io/v1 v1}]
Aug 11 14:49:06.000: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
Aug 11 14:49:06.000: INFO: Checking APIGroup: autoscaling
Aug 11 14:49:06.000: INFO: PreferredVersion.GroupVersion: autoscaling/v2
Aug 11 14:49:06.000: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
Aug 11 14:49:06.001: INFO: autoscaling/v2 matches autoscaling/v2
Aug 11 14:49:06.001: INFO: Checking APIGroup: batch
Aug 11 14:49:06.001: INFO: PreferredVersion.GroupVersion: batch/v1
Aug 11 14:49:06.001: INFO: Versions found [{batch/v1 v1}]
Aug 11 14:49:06.001: INFO: batch/v1 matches batch/v1
Aug 11 14:49:06.001: INFO: Checking APIGroup: certificates.k8s.io
Aug 11 14:49:06.002: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
Aug 11 14:49:06.002: INFO: Versions found [{certificates.k8s.io/v1 v1}]
Aug 11 14:49:06.002: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
Aug 11 14:49:06.002: INFO: Checking APIGroup: networking.k8s.io
Aug 11 14:49:06.003: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
Aug 11 14:49:06.003: INFO: Versions found [{networking.k8s.io/v1 v1}]
Aug 11 14:49:06.003: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
Aug 11 14:49:06.003: INFO: Checking APIGroup: policy
Aug 11 14:49:06.003: INFO: PreferredVersion.GroupVersion: policy/v1
Aug 11 14:49:06.003: INFO: Versions found [{policy/v1 v1}]
Aug 11 14:49:06.003: INFO: policy/v1 matches policy/v1
Aug 11 14:49:06.003: INFO: Checking APIGroup: rbac.authorization.k8s.io
Aug 11 14:49:06.004: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
Aug 11 14:49:06.004: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
Aug 11 14:49:06.004: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
Aug 11 14:49:06.004: INFO: Checking APIGroup: storage.k8s.io
Aug 11 14:49:06.005: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
Aug 11 14:49:06.005: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
Aug 11 14:49:06.005: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
Aug 11 14:49:06.005: INFO: Checking APIGroup: admissionregistration.k8s.io
Aug 11 14:49:06.005: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
Aug 11 14:49:06.006: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
Aug 11 14:49:06.006: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
Aug 11 14:49:06.006: INFO: Checking APIGroup: apiextensions.k8s.io
Aug 11 14:49:06.006: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
Aug 11 14:49:06.006: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
Aug 11 14:49:06.006: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
Aug 11 14:49:06.006: INFO: Checking APIGroup: scheduling.k8s.io
Aug 11 14:49:06.007: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
Aug 11 14:49:06.007: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
Aug 11 14:49:06.007: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
Aug 11 14:49:06.007: INFO: Checking APIGroup: coordination.k8s.io
Aug 11 14:49:06.008: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
Aug 11 14:49:06.008: INFO: Versions found [{coordination.k8s.io/v1 v1}]
Aug 11 14:49:06.008: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
Aug 11 14:49:06.008: INFO: Checking APIGroup: node.k8s.io
Aug 11 14:49:06.009: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
Aug 11 14:49:06.009: INFO: Versions found [{node.k8s.io/v1 v1}]
Aug 11 14:49:06.009: INFO: node.k8s.io/v1 matches node.k8s.io/v1
Aug 11 14:49:06.009: INFO: Checking APIGroup: discovery.k8s.io
Aug 11 14:49:06.010: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
Aug 11 14:49:06.010: INFO: Versions found [{discovery.k8s.io/v1 v1}]
Aug 11 14:49:06.010: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
Aug 11 14:49:06.010: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
Aug 11 14:49:06.011: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
Aug 11 14:49:06.011: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
Aug 11 14:49:06.011: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
Aug 11 14:49:06.011: INFO: Checking APIGroup: acme.cert-manager.io
Aug 11 14:49:06.012: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
Aug 11 14:49:06.012: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
Aug 11 14:49:06.012: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
Aug 11 14:49:06.012: INFO: Checking APIGroup: cert-manager.io
Aug 11 14:49:06.013: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
Aug 11 14:49:06.013: INFO: Versions found [{cert-manager.io/v1 v1}]
Aug 11 14:49:06.013: INFO: cert-manager.io/v1 matches cert-manager.io/v1
Aug 11 14:49:06.013: INFO: Checking APIGroup: update.edgeless.systems
Aug 11 14:49:06.013: INFO: PreferredVersion.GroupVersion: update.edgeless.systems/v1alpha1
Aug 11 14:49:06.013: INFO: Versions found [{update.edgeless.systems/v1alpha1 v1alpha1}]
Aug 11 14:49:06.013: INFO: update.edgeless.systems/v1alpha1 matches update.edgeless.systems/v1alpha1
Aug 11 14:49:06.014: INFO: Checking APIGroup: nodemaintenance.medik8s.io
Aug 11 14:49:06.015: INFO: PreferredVersion.GroupVersion: nodemaintenance.medik8s.io/v1beta1
Aug 11 14:49:06.015: INFO: Versions found [{nodemaintenance.medik8s.io/v1beta1 v1beta1}]
Aug 11 14:49:06.015: INFO: nodemaintenance.medik8s.io/v1beta1 matches nodemaintenance.medik8s.io/v1beta1
Aug 11 14:49:06.015: INFO: Checking APIGroup: cilium.io
Aug 11 14:49:06.016: INFO: PreferredVersion.GroupVersion: cilium.io/v2
Aug 11 14:49:06.016: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
Aug 11 14:49:06.016: INFO: cilium.io/v2 matches cilium.io/v2
[AfterEach] [sig-api-machinery] Discovery
  test/e2e/framework/framework.go:187
Aug 11 14:49:06.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "discovery-1128" for this suite. 08/11/23 14:49:06.02
{"msg":"PASSED [sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]","completed":145,"skipped":2826,"failed":0}
------------------------------
â€¢ [0.468 seconds]
[sig-api-machinery] Discovery
test/e2e/apimachinery/framework.go:23
  should validate PreferredVersion for each APIGroup [Conformance]
  test/e2e/apimachinery/discovery.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:49:05.558
    Aug 11 14:49:05.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename discovery 08/11/23 14:49:05.559
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:05.575
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:05.578
    [BeforeEach] [sig-api-machinery] Discovery
      test/e2e/apimachinery/discovery.go:43
    STEP: Setting up server cert 08/11/23 14:49:05.582
    [It] should validate PreferredVersion for each APIGroup [Conformance]
      test/e2e/apimachinery/discovery.go:122
    Aug 11 14:49:05.995: INFO: Checking APIGroup: apiregistration.k8s.io
    Aug 11 14:49:05.996: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
    Aug 11 14:49:05.996: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
    Aug 11 14:49:05.996: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
    Aug 11 14:49:05.996: INFO: Checking APIGroup: apps
    Aug 11 14:49:05.997: INFO: PreferredVersion.GroupVersion: apps/v1
    Aug 11 14:49:05.997: INFO: Versions found [{apps/v1 v1}]
    Aug 11 14:49:05.997: INFO: apps/v1 matches apps/v1
    Aug 11 14:49:05.997: INFO: Checking APIGroup: events.k8s.io
    Aug 11 14:49:05.998: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
    Aug 11 14:49:05.998: INFO: Versions found [{events.k8s.io/v1 v1}]
    Aug 11 14:49:05.998: INFO: events.k8s.io/v1 matches events.k8s.io/v1
    Aug 11 14:49:05.998: INFO: Checking APIGroup: authentication.k8s.io
    Aug 11 14:49:05.999: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
    Aug 11 14:49:05.999: INFO: Versions found [{authentication.k8s.io/v1 v1}]
    Aug 11 14:49:05.999: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
    Aug 11 14:49:05.999: INFO: Checking APIGroup: authorization.k8s.io
    Aug 11 14:49:06.000: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
    Aug 11 14:49:06.000: INFO: Versions found [{authorization.k8s.io/v1 v1}]
    Aug 11 14:49:06.000: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
    Aug 11 14:49:06.000: INFO: Checking APIGroup: autoscaling
    Aug 11 14:49:06.000: INFO: PreferredVersion.GroupVersion: autoscaling/v2
    Aug 11 14:49:06.000: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1} {autoscaling/v2beta2 v2beta2}]
    Aug 11 14:49:06.001: INFO: autoscaling/v2 matches autoscaling/v2
    Aug 11 14:49:06.001: INFO: Checking APIGroup: batch
    Aug 11 14:49:06.001: INFO: PreferredVersion.GroupVersion: batch/v1
    Aug 11 14:49:06.001: INFO: Versions found [{batch/v1 v1}]
    Aug 11 14:49:06.001: INFO: batch/v1 matches batch/v1
    Aug 11 14:49:06.001: INFO: Checking APIGroup: certificates.k8s.io
    Aug 11 14:49:06.002: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
    Aug 11 14:49:06.002: INFO: Versions found [{certificates.k8s.io/v1 v1}]
    Aug 11 14:49:06.002: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
    Aug 11 14:49:06.002: INFO: Checking APIGroup: networking.k8s.io
    Aug 11 14:49:06.003: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
    Aug 11 14:49:06.003: INFO: Versions found [{networking.k8s.io/v1 v1}]
    Aug 11 14:49:06.003: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
    Aug 11 14:49:06.003: INFO: Checking APIGroup: policy
    Aug 11 14:49:06.003: INFO: PreferredVersion.GroupVersion: policy/v1
    Aug 11 14:49:06.003: INFO: Versions found [{policy/v1 v1}]
    Aug 11 14:49:06.003: INFO: policy/v1 matches policy/v1
    Aug 11 14:49:06.003: INFO: Checking APIGroup: rbac.authorization.k8s.io
    Aug 11 14:49:06.004: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
    Aug 11 14:49:06.004: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
    Aug 11 14:49:06.004: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
    Aug 11 14:49:06.004: INFO: Checking APIGroup: storage.k8s.io
    Aug 11 14:49:06.005: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
    Aug 11 14:49:06.005: INFO: Versions found [{storage.k8s.io/v1 v1} {storage.k8s.io/v1beta1 v1beta1}]
    Aug 11 14:49:06.005: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
    Aug 11 14:49:06.005: INFO: Checking APIGroup: admissionregistration.k8s.io
    Aug 11 14:49:06.005: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
    Aug 11 14:49:06.006: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
    Aug 11 14:49:06.006: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
    Aug 11 14:49:06.006: INFO: Checking APIGroup: apiextensions.k8s.io
    Aug 11 14:49:06.006: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
    Aug 11 14:49:06.006: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
    Aug 11 14:49:06.006: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
    Aug 11 14:49:06.006: INFO: Checking APIGroup: scheduling.k8s.io
    Aug 11 14:49:06.007: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
    Aug 11 14:49:06.007: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
    Aug 11 14:49:06.007: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
    Aug 11 14:49:06.007: INFO: Checking APIGroup: coordination.k8s.io
    Aug 11 14:49:06.008: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
    Aug 11 14:49:06.008: INFO: Versions found [{coordination.k8s.io/v1 v1}]
    Aug 11 14:49:06.008: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
    Aug 11 14:49:06.008: INFO: Checking APIGroup: node.k8s.io
    Aug 11 14:49:06.009: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
    Aug 11 14:49:06.009: INFO: Versions found [{node.k8s.io/v1 v1}]
    Aug 11 14:49:06.009: INFO: node.k8s.io/v1 matches node.k8s.io/v1
    Aug 11 14:49:06.009: INFO: Checking APIGroup: discovery.k8s.io
    Aug 11 14:49:06.010: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
    Aug 11 14:49:06.010: INFO: Versions found [{discovery.k8s.io/v1 v1}]
    Aug 11 14:49:06.010: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
    Aug 11 14:49:06.010: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
    Aug 11 14:49:06.011: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta2
    Aug 11 14:49:06.011: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta2 v1beta2} {flowcontrol.apiserver.k8s.io/v1beta1 v1beta1}]
    Aug 11 14:49:06.011: INFO: flowcontrol.apiserver.k8s.io/v1beta2 matches flowcontrol.apiserver.k8s.io/v1beta2
    Aug 11 14:49:06.011: INFO: Checking APIGroup: acme.cert-manager.io
    Aug 11 14:49:06.012: INFO: PreferredVersion.GroupVersion: acme.cert-manager.io/v1
    Aug 11 14:49:06.012: INFO: Versions found [{acme.cert-manager.io/v1 v1}]
    Aug 11 14:49:06.012: INFO: acme.cert-manager.io/v1 matches acme.cert-manager.io/v1
    Aug 11 14:49:06.012: INFO: Checking APIGroup: cert-manager.io
    Aug 11 14:49:06.013: INFO: PreferredVersion.GroupVersion: cert-manager.io/v1
    Aug 11 14:49:06.013: INFO: Versions found [{cert-manager.io/v1 v1}]
    Aug 11 14:49:06.013: INFO: cert-manager.io/v1 matches cert-manager.io/v1
    Aug 11 14:49:06.013: INFO: Checking APIGroup: update.edgeless.systems
    Aug 11 14:49:06.013: INFO: PreferredVersion.GroupVersion: update.edgeless.systems/v1alpha1
    Aug 11 14:49:06.013: INFO: Versions found [{update.edgeless.systems/v1alpha1 v1alpha1}]
    Aug 11 14:49:06.013: INFO: update.edgeless.systems/v1alpha1 matches update.edgeless.systems/v1alpha1
    Aug 11 14:49:06.014: INFO: Checking APIGroup: nodemaintenance.medik8s.io
    Aug 11 14:49:06.015: INFO: PreferredVersion.GroupVersion: nodemaintenance.medik8s.io/v1beta1
    Aug 11 14:49:06.015: INFO: Versions found [{nodemaintenance.medik8s.io/v1beta1 v1beta1}]
    Aug 11 14:49:06.015: INFO: nodemaintenance.medik8s.io/v1beta1 matches nodemaintenance.medik8s.io/v1beta1
    Aug 11 14:49:06.015: INFO: Checking APIGroup: cilium.io
    Aug 11 14:49:06.016: INFO: PreferredVersion.GroupVersion: cilium.io/v2
    Aug 11 14:49:06.016: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
    Aug 11 14:49:06.016: INFO: cilium.io/v2 matches cilium.io/v2
    [AfterEach] [sig-api-machinery] Discovery
      test/e2e/framework/framework.go:187
    Aug 11 14:49:06.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "discovery-1128" for this suite. 08/11/23 14:49:06.02
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:49:06.027
Aug 11 14:49:06.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:49:06.028
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:06.043
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:06.045
[It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382
STEP: Counting existing ResourceQuota 08/11/23 14:49:06.048
STEP: Creating a ResourceQuota 08/11/23 14:49:11.052
STEP: Ensuring resource quota status is calculated 08/11/23 14:49:11.058
STEP: Creating a ReplicationController 08/11/23 14:49:13.064
STEP: Ensuring resource quota status captures replication controller creation 08/11/23 14:49:13.078
STEP: Deleting a ReplicationController 08/11/23 14:49:15.083
STEP: Ensuring resource quota status released usage 08/11/23 14:49:15.09
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 14:49:17.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1595" for this suite. 08/11/23 14:49:17.098
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]","completed":146,"skipped":2851,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.078 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replication controller. [Conformance]
  test/e2e/apimachinery/resource_quota.go:382

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:49:06.027
    Aug 11 14:49:06.027: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:49:06.028
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:06.043
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:06.045
    [It] should create a ResourceQuota and capture the life of a replication controller. [Conformance]
      test/e2e/apimachinery/resource_quota.go:382
    STEP: Counting existing ResourceQuota 08/11/23 14:49:06.048
    STEP: Creating a ResourceQuota 08/11/23 14:49:11.052
    STEP: Ensuring resource quota status is calculated 08/11/23 14:49:11.058
    STEP: Creating a ReplicationController 08/11/23 14:49:13.064
    STEP: Ensuring resource quota status captures replication controller creation 08/11/23 14:49:13.078
    STEP: Deleting a ReplicationController 08/11/23 14:49:15.083
    STEP: Ensuring resource quota status released usage 08/11/23 14:49:15.09
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 14:49:17.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1595" for this suite. 08/11/23 14:49:17.098
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:49:17.107
Aug 11 14:49:17.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename gc 08/11/23 14:49:17.108
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:17.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:17.127
[It] should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370
STEP: create the rc 08/11/23 14:49:17.134
STEP: delete the rc 08/11/23 14:49:22.148
STEP: wait for the rc to be deleted 08/11/23 14:49:22.155
STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/11/23 14:49:27.161
STEP: Gathering metrics 08/11/23 14:49:57.173
Aug 11 14:49:57.211: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
Aug 11 14:49:57.215: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.360954ms
Aug 11 14:49:57.215: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
Aug 11 14:49:57.215: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
Aug 11 14:49:57.274: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 11 14:49:57.274: INFO: Deleting pod "simpletest.rc-22zmm" in namespace "gc-385"
Aug 11 14:49:57.291: INFO: Deleting pod "simpletest.rc-2cdp4" in namespace "gc-385"
Aug 11 14:49:57.306: INFO: Deleting pod "simpletest.rc-2gsch" in namespace "gc-385"
Aug 11 14:49:57.326: INFO: Deleting pod "simpletest.rc-2pfs5" in namespace "gc-385"
Aug 11 14:49:57.348: INFO: Deleting pod "simpletest.rc-44mkr" in namespace "gc-385"
Aug 11 14:49:57.366: INFO: Deleting pod "simpletest.rc-4ddnc" in namespace "gc-385"
Aug 11 14:49:57.387: INFO: Deleting pod "simpletest.rc-4g9df" in namespace "gc-385"
Aug 11 14:49:57.406: INFO: Deleting pod "simpletest.rc-4pdhb" in namespace "gc-385"
Aug 11 14:49:57.420: INFO: Deleting pod "simpletest.rc-4t92k" in namespace "gc-385"
Aug 11 14:49:57.434: INFO: Deleting pod "simpletest.rc-5f7pj" in namespace "gc-385"
Aug 11 14:49:57.457: INFO: Deleting pod "simpletest.rc-66tv4" in namespace "gc-385"
Aug 11 14:49:57.473: INFO: Deleting pod "simpletest.rc-6br4x" in namespace "gc-385"
Aug 11 14:49:57.488: INFO: Deleting pod "simpletest.rc-6d9ks" in namespace "gc-385"
Aug 11 14:49:57.499: INFO: Deleting pod "simpletest.rc-6dqrf" in namespace "gc-385"
Aug 11 14:49:57.523: INFO: Deleting pod "simpletest.rc-6lpck" in namespace "gc-385"
Aug 11 14:49:57.541: INFO: Deleting pod "simpletest.rc-6tq49" in namespace "gc-385"
Aug 11 14:49:57.558: INFO: Deleting pod "simpletest.rc-72kkw" in namespace "gc-385"
Aug 11 14:49:57.586: INFO: Deleting pod "simpletest.rc-7849f" in namespace "gc-385"
Aug 11 14:49:57.622: INFO: Deleting pod "simpletest.rc-7dlmt" in namespace "gc-385"
Aug 11 14:49:57.639: INFO: Deleting pod "simpletest.rc-7prrn" in namespace "gc-385"
Aug 11 14:49:57.657: INFO: Deleting pod "simpletest.rc-8dl7g" in namespace "gc-385"
Aug 11 14:49:57.673: INFO: Deleting pod "simpletest.rc-8r8wg" in namespace "gc-385"
Aug 11 14:49:57.686: INFO: Deleting pod "simpletest.rc-8wzsm" in namespace "gc-385"
Aug 11 14:49:57.700: INFO: Deleting pod "simpletest.rc-9g6nn" in namespace "gc-385"
Aug 11 14:49:57.712: INFO: Deleting pod "simpletest.rc-9qk4c" in namespace "gc-385"
Aug 11 14:49:57.726: INFO: Deleting pod "simpletest.rc-9r4pn" in namespace "gc-385"
Aug 11 14:49:57.739: INFO: Deleting pod "simpletest.rc-9sd6p" in namespace "gc-385"
Aug 11 14:49:57.750: INFO: Deleting pod "simpletest.rc-bp4g7" in namespace "gc-385"
Aug 11 14:49:57.761: INFO: Deleting pod "simpletest.rc-bv69t" in namespace "gc-385"
Aug 11 14:49:57.776: INFO: Deleting pod "simpletest.rc-bwhm8" in namespace "gc-385"
Aug 11 14:49:57.790: INFO: Deleting pod "simpletest.rc-cb5cp" in namespace "gc-385"
Aug 11 14:49:57.804: INFO: Deleting pod "simpletest.rc-cwggb" in namespace "gc-385"
Aug 11 14:49:57.816: INFO: Deleting pod "simpletest.rc-djr9x" in namespace "gc-385"
Aug 11 14:49:57.834: INFO: Deleting pod "simpletest.rc-dt4s5" in namespace "gc-385"
Aug 11 14:49:57.843: INFO: Deleting pod "simpletest.rc-dw7t9" in namespace "gc-385"
Aug 11 14:49:57.855: INFO: Deleting pod "simpletest.rc-f547k" in namespace "gc-385"
Aug 11 14:49:57.869: INFO: Deleting pod "simpletest.rc-fg5wv" in namespace "gc-385"
Aug 11 14:49:57.883: INFO: Deleting pod "simpletest.rc-g9lxm" in namespace "gc-385"
Aug 11 14:49:57.898: INFO: Deleting pod "simpletest.rc-gh65m" in namespace "gc-385"
Aug 11 14:49:57.912: INFO: Deleting pod "simpletest.rc-gh7ns" in namespace "gc-385"
Aug 11 14:49:57.928: INFO: Deleting pod "simpletest.rc-gzpkv" in namespace "gc-385"
Aug 11 14:49:57.942: INFO: Deleting pod "simpletest.rc-gzs22" in namespace "gc-385"
Aug 11 14:49:57.953: INFO: Deleting pod "simpletest.rc-h44m9" in namespace "gc-385"
Aug 11 14:49:57.964: INFO: Deleting pod "simpletest.rc-h99sr" in namespace "gc-385"
Aug 11 14:49:57.975: INFO: Deleting pod "simpletest.rc-hhftl" in namespace "gc-385"
Aug 11 14:49:57.986: INFO: Deleting pod "simpletest.rc-jcn6s" in namespace "gc-385"
Aug 11 14:49:57.996: INFO: Deleting pod "simpletest.rc-jhshc" in namespace "gc-385"
Aug 11 14:49:58.007: INFO: Deleting pod "simpletest.rc-jmfng" in namespace "gc-385"
Aug 11 14:49:58.018: INFO: Deleting pod "simpletest.rc-k22kj" in namespace "gc-385"
Aug 11 14:49:58.028: INFO: Deleting pod "simpletest.rc-k9tgj" in namespace "gc-385"
Aug 11 14:49:58.042: INFO: Deleting pod "simpletest.rc-krffc" in namespace "gc-385"
Aug 11 14:49:58.056: INFO: Deleting pod "simpletest.rc-kskdz" in namespace "gc-385"
Aug 11 14:49:58.070: INFO: Deleting pod "simpletest.rc-kvnqf" in namespace "gc-385"
Aug 11 14:49:58.091: INFO: Deleting pod "simpletest.rc-l7bd5" in namespace "gc-385"
Aug 11 14:49:58.102: INFO: Deleting pod "simpletest.rc-ld4rn" in namespace "gc-385"
Aug 11 14:49:58.116: INFO: Deleting pod "simpletest.rc-ldbkr" in namespace "gc-385"
Aug 11 14:49:58.137: INFO: Deleting pod "simpletest.rc-lh5gp" in namespace "gc-385"
Aug 11 14:49:58.151: INFO: Deleting pod "simpletest.rc-ll8gk" in namespace "gc-385"
Aug 11 14:49:58.165: INFO: Deleting pod "simpletest.rc-lmb7w" in namespace "gc-385"
Aug 11 14:49:58.177: INFO: Deleting pod "simpletest.rc-lqc6d" in namespace "gc-385"
Aug 11 14:49:58.190: INFO: Deleting pod "simpletest.rc-lv5jn" in namespace "gc-385"
Aug 11 14:49:58.204: INFO: Deleting pod "simpletest.rc-lxqrn" in namespace "gc-385"
Aug 11 14:49:58.218: INFO: Deleting pod "simpletest.rc-mgdwm" in namespace "gc-385"
Aug 11 14:49:58.230: INFO: Deleting pod "simpletest.rc-mhqns" in namespace "gc-385"
Aug 11 14:49:58.246: INFO: Deleting pod "simpletest.rc-mm82w" in namespace "gc-385"
Aug 11 14:49:58.261: INFO: Deleting pod "simpletest.rc-ng8n8" in namespace "gc-385"
Aug 11 14:49:58.273: INFO: Deleting pod "simpletest.rc-npj9t" in namespace "gc-385"
Aug 11 14:49:58.284: INFO: Deleting pod "simpletest.rc-nrrbd" in namespace "gc-385"
Aug 11 14:49:58.296: INFO: Deleting pod "simpletest.rc-ntdkb" in namespace "gc-385"
Aug 11 14:49:58.333: INFO: Deleting pod "simpletest.rc-pdcf2" in namespace "gc-385"
Aug 11 14:49:58.372: INFO: Deleting pod "simpletest.rc-ptz6t" in namespace "gc-385"
Aug 11 14:49:58.428: INFO: Deleting pod "simpletest.rc-pvvlh" in namespace "gc-385"
Aug 11 14:49:58.475: INFO: Deleting pod "simpletest.rc-qm62q" in namespace "gc-385"
Aug 11 14:49:58.526: INFO: Deleting pod "simpletest.rc-r2j2l" in namespace "gc-385"
Aug 11 14:49:58.576: INFO: Deleting pod "simpletest.rc-r56g6" in namespace "gc-385"
Aug 11 14:49:58.628: INFO: Deleting pod "simpletest.rc-rwxnc" in namespace "gc-385"
Aug 11 14:49:58.677: INFO: Deleting pod "simpletest.rc-rzcwh" in namespace "gc-385"
Aug 11 14:49:58.722: INFO: Deleting pod "simpletest.rc-sbfl4" in namespace "gc-385"
Aug 11 14:49:58.776: INFO: Deleting pod "simpletest.rc-sh2qq" in namespace "gc-385"
Aug 11 14:49:58.826: INFO: Deleting pod "simpletest.rc-sj94v" in namespace "gc-385"
Aug 11 14:49:58.873: INFO: Deleting pod "simpletest.rc-sk45x" in namespace "gc-385"
Aug 11 14:49:58.938: INFO: Deleting pod "simpletest.rc-spjnd" in namespace "gc-385"
Aug 11 14:49:58.983: INFO: Deleting pod "simpletest.rc-srnh7" in namespace "gc-385"
Aug 11 14:49:59.027: INFO: Deleting pod "simpletest.rc-swhvr" in namespace "gc-385"
Aug 11 14:49:59.074: INFO: Deleting pod "simpletest.rc-swnml" in namespace "gc-385"
Aug 11 14:49:59.130: INFO: Deleting pod "simpletest.rc-tl2kb" in namespace "gc-385"
Aug 11 14:49:59.174: INFO: Deleting pod "simpletest.rc-tvsjs" in namespace "gc-385"
Aug 11 14:49:59.228: INFO: Deleting pod "simpletest.rc-vj5rk" in namespace "gc-385"
Aug 11 14:49:59.274: INFO: Deleting pod "simpletest.rc-vsqrb" in namespace "gc-385"
Aug 11 14:49:59.332: INFO: Deleting pod "simpletest.rc-vv4n6" in namespace "gc-385"
Aug 11 14:49:59.374: INFO: Deleting pod "simpletest.rc-vxv5k" in namespace "gc-385"
Aug 11 14:49:59.429: INFO: Deleting pod "simpletest.rc-wcbbh" in namespace "gc-385"
Aug 11 14:49:59.475: INFO: Deleting pod "simpletest.rc-wqgn8" in namespace "gc-385"
Aug 11 14:49:59.537: INFO: Deleting pod "simpletest.rc-wx4lb" in namespace "gc-385"
Aug 11 14:49:59.573: INFO: Deleting pod "simpletest.rc-xrv2g" in namespace "gc-385"
Aug 11 14:49:59.628: INFO: Deleting pod "simpletest.rc-xtrnv" in namespace "gc-385"
Aug 11 14:49:59.684: INFO: Deleting pod "simpletest.rc-z48g8" in namespace "gc-385"
Aug 11 14:49:59.731: INFO: Deleting pod "simpletest.rc-z7lkz" in namespace "gc-385"
Aug 11 14:49:59.775: INFO: Deleting pod "simpletest.rc-z8844" in namespace "gc-385"
Aug 11 14:49:59.828: INFO: Deleting pod "simpletest.rc-zmzf8" in namespace "gc-385"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 11 14:49:59.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-385" for this suite. 08/11/23 14:49:59.918
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]","completed":147,"skipped":2857,"failed":0}
------------------------------
â€¢ [SLOW TEST] [42.863 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan pods created by rc if delete options say so [Conformance]
  test/e2e/apimachinery/garbage_collector.go:370

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:49:17.107
    Aug 11 14:49:17.107: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename gc 08/11/23 14:49:17.108
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:17.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:17.127
    [It] should orphan pods created by rc if delete options say so [Conformance]
      test/e2e/apimachinery/garbage_collector.go:370
    STEP: create the rc 08/11/23 14:49:17.134
    STEP: delete the rc 08/11/23 14:49:22.148
    STEP: wait for the rc to be deleted 08/11/23 14:49:22.155
    STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods 08/11/23 14:49:27.161
    STEP: Gathering metrics 08/11/23 14:49:57.173
    Aug 11 14:49:57.211: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
    Aug 11 14:49:57.215: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.360954ms
    Aug 11 14:49:57.215: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
    Aug 11 14:49:57.215: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
    Aug 11 14:49:57.274: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 11 14:49:57.274: INFO: Deleting pod "simpletest.rc-22zmm" in namespace "gc-385"
    Aug 11 14:49:57.291: INFO: Deleting pod "simpletest.rc-2cdp4" in namespace "gc-385"
    Aug 11 14:49:57.306: INFO: Deleting pod "simpletest.rc-2gsch" in namespace "gc-385"
    Aug 11 14:49:57.326: INFO: Deleting pod "simpletest.rc-2pfs5" in namespace "gc-385"
    Aug 11 14:49:57.348: INFO: Deleting pod "simpletest.rc-44mkr" in namespace "gc-385"
    Aug 11 14:49:57.366: INFO: Deleting pod "simpletest.rc-4ddnc" in namespace "gc-385"
    Aug 11 14:49:57.387: INFO: Deleting pod "simpletest.rc-4g9df" in namespace "gc-385"
    Aug 11 14:49:57.406: INFO: Deleting pod "simpletest.rc-4pdhb" in namespace "gc-385"
    Aug 11 14:49:57.420: INFO: Deleting pod "simpletest.rc-4t92k" in namespace "gc-385"
    Aug 11 14:49:57.434: INFO: Deleting pod "simpletest.rc-5f7pj" in namespace "gc-385"
    Aug 11 14:49:57.457: INFO: Deleting pod "simpletest.rc-66tv4" in namespace "gc-385"
    Aug 11 14:49:57.473: INFO: Deleting pod "simpletest.rc-6br4x" in namespace "gc-385"
    Aug 11 14:49:57.488: INFO: Deleting pod "simpletest.rc-6d9ks" in namespace "gc-385"
    Aug 11 14:49:57.499: INFO: Deleting pod "simpletest.rc-6dqrf" in namespace "gc-385"
    Aug 11 14:49:57.523: INFO: Deleting pod "simpletest.rc-6lpck" in namespace "gc-385"
    Aug 11 14:49:57.541: INFO: Deleting pod "simpletest.rc-6tq49" in namespace "gc-385"
    Aug 11 14:49:57.558: INFO: Deleting pod "simpletest.rc-72kkw" in namespace "gc-385"
    Aug 11 14:49:57.586: INFO: Deleting pod "simpletest.rc-7849f" in namespace "gc-385"
    Aug 11 14:49:57.622: INFO: Deleting pod "simpletest.rc-7dlmt" in namespace "gc-385"
    Aug 11 14:49:57.639: INFO: Deleting pod "simpletest.rc-7prrn" in namespace "gc-385"
    Aug 11 14:49:57.657: INFO: Deleting pod "simpletest.rc-8dl7g" in namespace "gc-385"
    Aug 11 14:49:57.673: INFO: Deleting pod "simpletest.rc-8r8wg" in namespace "gc-385"
    Aug 11 14:49:57.686: INFO: Deleting pod "simpletest.rc-8wzsm" in namespace "gc-385"
    Aug 11 14:49:57.700: INFO: Deleting pod "simpletest.rc-9g6nn" in namespace "gc-385"
    Aug 11 14:49:57.712: INFO: Deleting pod "simpletest.rc-9qk4c" in namespace "gc-385"
    Aug 11 14:49:57.726: INFO: Deleting pod "simpletest.rc-9r4pn" in namespace "gc-385"
    Aug 11 14:49:57.739: INFO: Deleting pod "simpletest.rc-9sd6p" in namespace "gc-385"
    Aug 11 14:49:57.750: INFO: Deleting pod "simpletest.rc-bp4g7" in namespace "gc-385"
    Aug 11 14:49:57.761: INFO: Deleting pod "simpletest.rc-bv69t" in namespace "gc-385"
    Aug 11 14:49:57.776: INFO: Deleting pod "simpletest.rc-bwhm8" in namespace "gc-385"
    Aug 11 14:49:57.790: INFO: Deleting pod "simpletest.rc-cb5cp" in namespace "gc-385"
    Aug 11 14:49:57.804: INFO: Deleting pod "simpletest.rc-cwggb" in namespace "gc-385"
    Aug 11 14:49:57.816: INFO: Deleting pod "simpletest.rc-djr9x" in namespace "gc-385"
    Aug 11 14:49:57.834: INFO: Deleting pod "simpletest.rc-dt4s5" in namespace "gc-385"
    Aug 11 14:49:57.843: INFO: Deleting pod "simpletest.rc-dw7t9" in namespace "gc-385"
    Aug 11 14:49:57.855: INFO: Deleting pod "simpletest.rc-f547k" in namespace "gc-385"
    Aug 11 14:49:57.869: INFO: Deleting pod "simpletest.rc-fg5wv" in namespace "gc-385"
    Aug 11 14:49:57.883: INFO: Deleting pod "simpletest.rc-g9lxm" in namespace "gc-385"
    Aug 11 14:49:57.898: INFO: Deleting pod "simpletest.rc-gh65m" in namespace "gc-385"
    Aug 11 14:49:57.912: INFO: Deleting pod "simpletest.rc-gh7ns" in namespace "gc-385"
    Aug 11 14:49:57.928: INFO: Deleting pod "simpletest.rc-gzpkv" in namespace "gc-385"
    Aug 11 14:49:57.942: INFO: Deleting pod "simpletest.rc-gzs22" in namespace "gc-385"
    Aug 11 14:49:57.953: INFO: Deleting pod "simpletest.rc-h44m9" in namespace "gc-385"
    Aug 11 14:49:57.964: INFO: Deleting pod "simpletest.rc-h99sr" in namespace "gc-385"
    Aug 11 14:49:57.975: INFO: Deleting pod "simpletest.rc-hhftl" in namespace "gc-385"
    Aug 11 14:49:57.986: INFO: Deleting pod "simpletest.rc-jcn6s" in namespace "gc-385"
    Aug 11 14:49:57.996: INFO: Deleting pod "simpletest.rc-jhshc" in namespace "gc-385"
    Aug 11 14:49:58.007: INFO: Deleting pod "simpletest.rc-jmfng" in namespace "gc-385"
    Aug 11 14:49:58.018: INFO: Deleting pod "simpletest.rc-k22kj" in namespace "gc-385"
    Aug 11 14:49:58.028: INFO: Deleting pod "simpletest.rc-k9tgj" in namespace "gc-385"
    Aug 11 14:49:58.042: INFO: Deleting pod "simpletest.rc-krffc" in namespace "gc-385"
    Aug 11 14:49:58.056: INFO: Deleting pod "simpletest.rc-kskdz" in namespace "gc-385"
    Aug 11 14:49:58.070: INFO: Deleting pod "simpletest.rc-kvnqf" in namespace "gc-385"
    Aug 11 14:49:58.091: INFO: Deleting pod "simpletest.rc-l7bd5" in namespace "gc-385"
    Aug 11 14:49:58.102: INFO: Deleting pod "simpletest.rc-ld4rn" in namespace "gc-385"
    Aug 11 14:49:58.116: INFO: Deleting pod "simpletest.rc-ldbkr" in namespace "gc-385"
    Aug 11 14:49:58.137: INFO: Deleting pod "simpletest.rc-lh5gp" in namespace "gc-385"
    Aug 11 14:49:58.151: INFO: Deleting pod "simpletest.rc-ll8gk" in namespace "gc-385"
    Aug 11 14:49:58.165: INFO: Deleting pod "simpletest.rc-lmb7w" in namespace "gc-385"
    Aug 11 14:49:58.177: INFO: Deleting pod "simpletest.rc-lqc6d" in namespace "gc-385"
    Aug 11 14:49:58.190: INFO: Deleting pod "simpletest.rc-lv5jn" in namespace "gc-385"
    Aug 11 14:49:58.204: INFO: Deleting pod "simpletest.rc-lxqrn" in namespace "gc-385"
    Aug 11 14:49:58.218: INFO: Deleting pod "simpletest.rc-mgdwm" in namespace "gc-385"
    Aug 11 14:49:58.230: INFO: Deleting pod "simpletest.rc-mhqns" in namespace "gc-385"
    Aug 11 14:49:58.246: INFO: Deleting pod "simpletest.rc-mm82w" in namespace "gc-385"
    Aug 11 14:49:58.261: INFO: Deleting pod "simpletest.rc-ng8n8" in namespace "gc-385"
    Aug 11 14:49:58.273: INFO: Deleting pod "simpletest.rc-npj9t" in namespace "gc-385"
    Aug 11 14:49:58.284: INFO: Deleting pod "simpletest.rc-nrrbd" in namespace "gc-385"
    Aug 11 14:49:58.296: INFO: Deleting pod "simpletest.rc-ntdkb" in namespace "gc-385"
    Aug 11 14:49:58.333: INFO: Deleting pod "simpletest.rc-pdcf2" in namespace "gc-385"
    Aug 11 14:49:58.372: INFO: Deleting pod "simpletest.rc-ptz6t" in namespace "gc-385"
    Aug 11 14:49:58.428: INFO: Deleting pod "simpletest.rc-pvvlh" in namespace "gc-385"
    Aug 11 14:49:58.475: INFO: Deleting pod "simpletest.rc-qm62q" in namespace "gc-385"
    Aug 11 14:49:58.526: INFO: Deleting pod "simpletest.rc-r2j2l" in namespace "gc-385"
    Aug 11 14:49:58.576: INFO: Deleting pod "simpletest.rc-r56g6" in namespace "gc-385"
    Aug 11 14:49:58.628: INFO: Deleting pod "simpletest.rc-rwxnc" in namespace "gc-385"
    Aug 11 14:49:58.677: INFO: Deleting pod "simpletest.rc-rzcwh" in namespace "gc-385"
    Aug 11 14:49:58.722: INFO: Deleting pod "simpletest.rc-sbfl4" in namespace "gc-385"
    Aug 11 14:49:58.776: INFO: Deleting pod "simpletest.rc-sh2qq" in namespace "gc-385"
    Aug 11 14:49:58.826: INFO: Deleting pod "simpletest.rc-sj94v" in namespace "gc-385"
    Aug 11 14:49:58.873: INFO: Deleting pod "simpletest.rc-sk45x" in namespace "gc-385"
    Aug 11 14:49:58.938: INFO: Deleting pod "simpletest.rc-spjnd" in namespace "gc-385"
    Aug 11 14:49:58.983: INFO: Deleting pod "simpletest.rc-srnh7" in namespace "gc-385"
    Aug 11 14:49:59.027: INFO: Deleting pod "simpletest.rc-swhvr" in namespace "gc-385"
    Aug 11 14:49:59.074: INFO: Deleting pod "simpletest.rc-swnml" in namespace "gc-385"
    Aug 11 14:49:59.130: INFO: Deleting pod "simpletest.rc-tl2kb" in namespace "gc-385"
    Aug 11 14:49:59.174: INFO: Deleting pod "simpletest.rc-tvsjs" in namespace "gc-385"
    Aug 11 14:49:59.228: INFO: Deleting pod "simpletest.rc-vj5rk" in namespace "gc-385"
    Aug 11 14:49:59.274: INFO: Deleting pod "simpletest.rc-vsqrb" in namespace "gc-385"
    Aug 11 14:49:59.332: INFO: Deleting pod "simpletest.rc-vv4n6" in namespace "gc-385"
    Aug 11 14:49:59.374: INFO: Deleting pod "simpletest.rc-vxv5k" in namespace "gc-385"
    Aug 11 14:49:59.429: INFO: Deleting pod "simpletest.rc-wcbbh" in namespace "gc-385"
    Aug 11 14:49:59.475: INFO: Deleting pod "simpletest.rc-wqgn8" in namespace "gc-385"
    Aug 11 14:49:59.537: INFO: Deleting pod "simpletest.rc-wx4lb" in namespace "gc-385"
    Aug 11 14:49:59.573: INFO: Deleting pod "simpletest.rc-xrv2g" in namespace "gc-385"
    Aug 11 14:49:59.628: INFO: Deleting pod "simpletest.rc-xtrnv" in namespace "gc-385"
    Aug 11 14:49:59.684: INFO: Deleting pod "simpletest.rc-z48g8" in namespace "gc-385"
    Aug 11 14:49:59.731: INFO: Deleting pod "simpletest.rc-z7lkz" in namespace "gc-385"
    Aug 11 14:49:59.775: INFO: Deleting pod "simpletest.rc-z8844" in namespace "gc-385"
    Aug 11 14:49:59.828: INFO: Deleting pod "simpletest.rc-zmzf8" in namespace "gc-385"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 11 14:49:59.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-385" for this suite. 08/11/23 14:49:59.918
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:49:59.97
Aug 11 14:49:59.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:49:59.971
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:59.988
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:59.99
[It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146
STEP: Creating a pod to test emptydir 0777 on tmpfs 08/11/23 14:49:59.992
Aug 11 14:50:00.002: INFO: Waiting up to 5m0s for pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f" in namespace "emptydir-1468" to be "Succeeded or Failed"
Aug 11 14:50:00.004: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587681ms
Aug 11 14:50:02.010: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007912793s
Aug 11 14:50:04.010: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007927861s
Aug 11 14:50:06.012: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009833317s
Aug 11 14:50:08.009: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007671337s
Aug 11 14:50:10.009: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.007361572s
STEP: Saw pod success 08/11/23 14:50:10.009
Aug 11 14:50:10.009: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f" satisfied condition "Succeeded or Failed"
Aug 11 14:50:10.014: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f container test-container: <nil>
STEP: delete the pod 08/11/23 14:50:10.024
Aug 11 14:50:10.039: INFO: Waiting for pod pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f to disappear
Aug 11 14:50:10.042: INFO: Pod pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 14:50:10.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1468" for this suite. 08/11/23 14:50:10.047
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":148,"skipped":2858,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:146

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:49:59.97
    Aug 11 14:49:59.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:49:59.971
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:49:59.988
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:49:59.99
    [It] should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:146
    STEP: Creating a pod to test emptydir 0777 on tmpfs 08/11/23 14:49:59.992
    Aug 11 14:50:00.002: INFO: Waiting up to 5m0s for pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f" in namespace "emptydir-1468" to be "Succeeded or Failed"
    Aug 11 14:50:00.004: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.587681ms
    Aug 11 14:50:02.010: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007912793s
    Aug 11 14:50:04.010: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 4.007927861s
    Aug 11 14:50:06.012: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.009833317s
    Aug 11 14:50:08.009: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Pending", Reason="", readiness=false. Elapsed: 8.007671337s
    Aug 11 14:50:10.009: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 10.007361572s
    STEP: Saw pod success 08/11/23 14:50:10.009
    Aug 11 14:50:10.009: INFO: Pod "pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f" satisfied condition "Succeeded or Failed"
    Aug 11 14:50:10.014: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f container test-container: <nil>
    STEP: delete the pod 08/11/23 14:50:10.024
    Aug 11 14:50:10.039: INFO: Waiting for pod pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f to disappear
    Aug 11 14:50:10.042: INFO: Pod pod-07ee1b55-8b21-400b-9ea3-7e7b3ebf453f no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:50:10.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1468" for this suite. 08/11/23 14:50:10.047
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:10.055
Aug 11 14:50:10.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:50:10.056
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:10.073
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:10.075
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67
STEP: Creating secret with name secret-test-76a0567a-96a5-4ee7-bf59-90517bc13f24 08/11/23 14:50:10.077
STEP: Creating a pod to test consume secrets 08/11/23 14:50:10.082
Aug 11 14:50:10.091: INFO: Waiting up to 5m0s for pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894" in namespace "secrets-4790" to be "Succeeded or Failed"
Aug 11 14:50:10.095: INFO: Pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118879ms
Aug 11 14:50:12.100: INFO: Pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009495344s
Aug 11 14:50:14.101: INFO: Pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009905792s
STEP: Saw pod success 08/11/23 14:50:14.101
Aug 11 14:50:14.101: INFO: Pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894" satisfied condition "Succeeded or Failed"
Aug 11 14:50:14.104: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:50:14.113
Aug 11 14:50:14.127: INFO: Waiting for pod pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894 to disappear
Aug 11 14:50:14.131: INFO: Pod pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:50:14.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-4790" for this suite. 08/11/23 14:50:14.135
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":149,"skipped":2867,"failed":0}
------------------------------
â€¢ [4.086 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:10.055
    Aug 11 14:50:10.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:50:10.056
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:10.073
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:10.075
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:67
    STEP: Creating secret with name secret-test-76a0567a-96a5-4ee7-bf59-90517bc13f24 08/11/23 14:50:10.077
    STEP: Creating a pod to test consume secrets 08/11/23 14:50:10.082
    Aug 11 14:50:10.091: INFO: Waiting up to 5m0s for pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894" in namespace "secrets-4790" to be "Succeeded or Failed"
    Aug 11 14:50:10.095: INFO: Pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894": Phase="Pending", Reason="", readiness=false. Elapsed: 4.118879ms
    Aug 11 14:50:12.100: INFO: Pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009495344s
    Aug 11 14:50:14.101: INFO: Pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009905792s
    STEP: Saw pod success 08/11/23 14:50:14.101
    Aug 11 14:50:14.101: INFO: Pod "pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894" satisfied condition "Succeeded or Failed"
    Aug 11 14:50:14.104: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:50:14.113
    Aug 11 14:50:14.127: INFO: Waiting for pod pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894 to disappear
    Aug 11 14:50:14.131: INFO: Pod pod-secrets-ea0b754b-bd61-4e33-9ef8-d646ad9b9894 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:50:14.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-4790" for this suite. 08/11/23 14:50:14.135
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:14.142
Aug 11 14:50:14.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename containers 08/11/23 14:50:14.143
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:14.16
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:14.163
[It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58
STEP: Creating a pod to test override arguments 08/11/23 14:50:14.165
Aug 11 14:50:14.173: INFO: Waiting up to 5m0s for pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4" in namespace "containers-8740" to be "Succeeded or Failed"
Aug 11 14:50:14.178: INFO: Pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.059057ms
Aug 11 14:50:16.183: INFO: Pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010235105s
Aug 11 14:50:18.182: INFO: Pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009182608s
STEP: Saw pod success 08/11/23 14:50:18.182
Aug 11 14:50:18.182: INFO: Pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4" satisfied condition "Succeeded or Failed"
Aug 11 14:50:18.185: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:50:18.193
Aug 11 14:50:18.207: INFO: Waiting for pod client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4 to disappear
Aug 11 14:50:18.210: INFO: Pod client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Aug 11 14:50:18.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-8740" for this suite. 08/11/23 14:50:18.214
{"msg":"PASSED [sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]","completed":150,"skipped":2873,"failed":0}
------------------------------
â€¢ [4.079 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:14.142
    Aug 11 14:50:14.142: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename containers 08/11/23 14:50:14.143
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:14.16
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:14.163
    [It] should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:58
    STEP: Creating a pod to test override arguments 08/11/23 14:50:14.165
    Aug 11 14:50:14.173: INFO: Waiting up to 5m0s for pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4" in namespace "containers-8740" to be "Succeeded or Failed"
    Aug 11 14:50:14.178: INFO: Pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4": Phase="Pending", Reason="", readiness=false. Elapsed: 5.059057ms
    Aug 11 14:50:16.183: INFO: Pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010235105s
    Aug 11 14:50:18.182: INFO: Pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009182608s
    STEP: Saw pod success 08/11/23 14:50:18.182
    Aug 11 14:50:18.182: INFO: Pod "client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4" satisfied condition "Succeeded or Failed"
    Aug 11 14:50:18.185: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:50:18.193
    Aug 11 14:50:18.207: INFO: Waiting for pod client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4 to disappear
    Aug 11 14:50:18.210: INFO: Pod client-containers-66a0fd5a-5efc-488d-be83-7ac79bcbf1f4 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Aug 11 14:50:18.210: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-8740" for this suite. 08/11/23 14:50:18.214
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:18.221
Aug 11 14:50:18.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:50:18.222
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:18.241
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:18.244
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129
STEP: Creating the pod 08/11/23 14:50:18.247
Aug 11 14:50:18.255: INFO: Waiting up to 5m0s for pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9" in namespace "projected-453" to be "running and ready"
Aug 11 14:50:18.262: INFO: Pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051598ms
Aug 11 14:50:18.262: INFO: The phase of Pod labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:50:20.267: INFO: Pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010984859s
Aug 11 14:50:20.267: INFO: The phase of Pod labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9 is Running (Ready = true)
Aug 11 14:50:20.267: INFO: Pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9" satisfied condition "running and ready"
Aug 11 14:50:20.804: INFO: Successfully updated pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9"
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 14:50:24.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-453" for this suite. 08/11/23 14:50:24.837
{"msg":"PASSED [sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]","completed":151,"skipped":2888,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.622 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:18.221
    Aug 11 14:50:18.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:50:18.222
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:18.241
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:18.244
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:129
    STEP: Creating the pod 08/11/23 14:50:18.247
    Aug 11 14:50:18.255: INFO: Waiting up to 5m0s for pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9" in namespace "projected-453" to be "running and ready"
    Aug 11 14:50:18.262: INFO: Pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.051598ms
    Aug 11 14:50:18.262: INFO: The phase of Pod labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:50:20.267: INFO: Pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9": Phase="Running", Reason="", readiness=true. Elapsed: 2.010984859s
    Aug 11 14:50:20.267: INFO: The phase of Pod labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9 is Running (Ready = true)
    Aug 11 14:50:20.267: INFO: Pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9" satisfied condition "running and ready"
    Aug 11 14:50:20.804: INFO: Successfully updated pod "labelsupdatefcdd5d0c-d0b4-40bf-91f5-9d99693bcfa9"
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 14:50:24.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-453" for this suite. 08/11/23 14:50:24.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:24.845
Aug 11 14:50:24.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:50:24.846
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:24.865
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:24.868
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:50:24.882
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:50:25.179
STEP: Deploying the webhook pod 08/11/23 14:50:25.188
STEP: Wait for the deployment to be ready 08/11/23 14:50:25.198
Aug 11 14:50:25.207: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:50:27.216
STEP: Verifying the service has paired with the endpoint 08/11/23 14:50:27.231
Aug 11 14:50:28.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412
STEP: Creating a validating webhook configuration 08/11/23 14:50:28.234
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:50:28.259
STEP: Updating a validating webhook configuration's rules to not include the create operation 08/11/23 14:50:28.277
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:50:28.289
STEP: Patching a validating webhook configuration's rules to include the create operation 08/11/23 14:50:28.3
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:50:28.309
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:50:28.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-2861" for this suite. 08/11/23 14:50:28.325
STEP: Destroying namespace "webhook-2861-markers" for this suite. 08/11/23 14:50:28.332
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]","completed":152,"skipped":2902,"failed":0}
------------------------------
â€¢ [3.548 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a validating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:412

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:24.845
    Aug 11 14:50:24.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:50:24.846
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:24.865
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:24.868
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:50:24.882
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:50:25.179
    STEP: Deploying the webhook pod 08/11/23 14:50:25.188
    STEP: Wait for the deployment to be ready 08/11/23 14:50:25.198
    Aug 11 14:50:25.207: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:50:27.216
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:50:27.231
    Aug 11 14:50:28.231: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a validating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:412
    STEP: Creating a validating webhook configuration 08/11/23 14:50:28.234
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:50:28.259
    STEP: Updating a validating webhook configuration's rules to not include the create operation 08/11/23 14:50:28.277
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:50:28.289
    STEP: Patching a validating webhook configuration's rules to include the create operation 08/11/23 14:50:28.3
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 14:50:28.309
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:50:28.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-2861" for this suite. 08/11/23 14:50:28.325
    STEP: Destroying namespace "webhook-2861-markers" for this suite. 08/11/23 14:50:28.332
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:28.394
Aug 11 14:50:28.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:50:28.396
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:28.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:28.418
[It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73
STEP: Creating configMap with name projected-configmap-test-volume-3d070837-6440-4a42-875f-48521a0fa7b0 08/11/23 14:50:28.42
STEP: Creating a pod to test consume configMaps 08/11/23 14:50:28.425
Aug 11 14:50:28.438: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8" in namespace "projected-5535" to be "Succeeded or Failed"
Aug 11 14:50:28.444: INFO: Pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.583754ms
Aug 11 14:50:30.448: INFO: Pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010263336s
Aug 11 14:50:32.449: INFO: Pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01051068s
STEP: Saw pod success 08/11/23 14:50:32.449
Aug 11 14:50:32.449: INFO: Pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8" satisfied condition "Succeeded or Failed"
Aug 11 14:50:32.452: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:50:32.461
Aug 11 14:50:32.477: INFO: Waiting for pod pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8 to disappear
Aug 11 14:50:32.479: INFO: Pod pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 11 14:50:32.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5535" for this suite. 08/11/23 14:50:32.483
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]","completed":153,"skipped":2922,"failed":0}
------------------------------
â€¢ [4.095 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:73

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:28.394
    Aug 11 14:50:28.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:50:28.396
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:28.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:28.418
    [It] should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:73
    STEP: Creating configMap with name projected-configmap-test-volume-3d070837-6440-4a42-875f-48521a0fa7b0 08/11/23 14:50:28.42
    STEP: Creating a pod to test consume configMaps 08/11/23 14:50:28.425
    Aug 11 14:50:28.438: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8" in namespace "projected-5535" to be "Succeeded or Failed"
    Aug 11 14:50:28.444: INFO: Pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.583754ms
    Aug 11 14:50:30.448: INFO: Pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010263336s
    Aug 11 14:50:32.449: INFO: Pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01051068s
    STEP: Saw pod success 08/11/23 14:50:32.449
    Aug 11 14:50:32.449: INFO: Pod "pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8" satisfied condition "Succeeded or Failed"
    Aug 11 14:50:32.452: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:50:32.461
    Aug 11 14:50:32.477: INFO: Waiting for pod pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8 to disappear
    Aug 11 14:50:32.479: INFO: Pod pod-projected-configmaps-c3bfd0dd-bf48-446c-8b04-34474f107bd8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 11 14:50:32.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5535" for this suite. 08/11/23 14:50:32.483
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:32.49
Aug 11 14:50:32.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:50:32.491
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:32.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:32.509
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94
STEP: creating secret secrets-8076/secret-test-1bab5f77-b745-4c25-a4b3-6cc1fc148132 08/11/23 14:50:32.512
STEP: Creating a pod to test consume secrets 08/11/23 14:50:32.521
Aug 11 14:50:32.528: INFO: Waiting up to 5m0s for pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c" in namespace "secrets-8076" to be "Succeeded or Failed"
Aug 11 14:50:32.531: INFO: Pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.683683ms
Aug 11 14:50:34.536: INFO: Pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007865191s
Aug 11 14:50:36.541: INFO: Pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01272819s
STEP: Saw pod success 08/11/23 14:50:36.541
Aug 11 14:50:36.541: INFO: Pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c" satisfied condition "Succeeded or Failed"
Aug 11 14:50:36.555: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c container env-test: <nil>
STEP: delete the pod 08/11/23 14:50:36.567
Aug 11 14:50:36.580: INFO: Waiting for pod pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c to disappear
Aug 11 14:50:36.583: INFO: Pod pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c no longer exists
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:50:36.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8076" for this suite. 08/11/23 14:50:36.587
{"msg":"PASSED [sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]","completed":154,"skipped":2939,"failed":0}
------------------------------
â€¢ [4.103 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/secrets.go:94

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:32.49
    Aug 11 14:50:32.490: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:50:32.491
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:32.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:32.509
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/secrets.go:94
    STEP: creating secret secrets-8076/secret-test-1bab5f77-b745-4c25-a4b3-6cc1fc148132 08/11/23 14:50:32.512
    STEP: Creating a pod to test consume secrets 08/11/23 14:50:32.521
    Aug 11 14:50:32.528: INFO: Waiting up to 5m0s for pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c" in namespace "secrets-8076" to be "Succeeded or Failed"
    Aug 11 14:50:32.531: INFO: Pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.683683ms
    Aug 11 14:50:34.536: INFO: Pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007865191s
    Aug 11 14:50:36.541: INFO: Pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01272819s
    STEP: Saw pod success 08/11/23 14:50:36.541
    Aug 11 14:50:36.541: INFO: Pod "pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c" satisfied condition "Succeeded or Failed"
    Aug 11 14:50:36.555: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c container env-test: <nil>
    STEP: delete the pod 08/11/23 14:50:36.567
    Aug 11 14:50:36.580: INFO: Waiting for pod pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c to disappear
    Aug 11 14:50:36.583: INFO: Pod pod-configmaps-c73b6258-a3db-4118-bb47-1f57ce17752c no longer exists
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:50:36.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8076" for this suite. 08/11/23 14:50:36.587
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:36.595
Aug 11 14:50:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:50:36.596
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:36.612
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:36.614
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:50:36.632
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:50:36.785
STEP: Deploying the webhook pod 08/11/23 14:50:36.791
STEP: Wait for the deployment to be ready 08/11/23 14:50:36.804
Aug 11 14:50:36.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:50:38.821
STEP: Verifying the service has paired with the endpoint 08/11/23 14:50:38.837
Aug 11 14:50:39.837: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116
STEP: fetching the /apis discovery document 08/11/23 14:50:39.841
STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/11/23 14:50:39.843
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/11/23 14:50:39.843
STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/11/23 14:50:39.843
STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/11/23 14:50:39.844
STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/11/23 14:50:39.844
STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/11/23 14:50:39.845
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:50:39.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3974" for this suite. 08/11/23 14:50:39.849
STEP: Destroying namespace "webhook-3974-markers" for this suite. 08/11/23 14:50:39.857
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]","completed":155,"skipped":2993,"failed":0}
------------------------------
â€¢ [3.317 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should include webhook resources in discovery documents [Conformance]
  test/e2e/apimachinery/webhook.go:116

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:36.595
    Aug 11 14:50:36.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:50:36.596
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:36.612
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:36.614
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:50:36.632
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:50:36.785
    STEP: Deploying the webhook pod 08/11/23 14:50:36.791
    STEP: Wait for the deployment to be ready 08/11/23 14:50:36.804
    Aug 11 14:50:36.812: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:50:38.821
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:50:38.837
    Aug 11 14:50:39.837: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should include webhook resources in discovery documents [Conformance]
      test/e2e/apimachinery/webhook.go:116
    STEP: fetching the /apis discovery document 08/11/23 14:50:39.841
    STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document 08/11/23 14:50:39.843
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document 08/11/23 14:50:39.843
    STEP: fetching the /apis/admissionregistration.k8s.io discovery document 08/11/23 14:50:39.843
    STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document 08/11/23 14:50:39.844
    STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document 08/11/23 14:50:39.844
    STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document 08/11/23 14:50:39.845
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:50:39.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3974" for this suite. 08/11/23 14:50:39.849
    STEP: Destroying namespace "webhook-3974-markers" for this suite. 08/11/23 14:50:39.857
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] EndpointSliceMirroring
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:39.912
Aug 11 14:50:39.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename endpointslicemirroring 08/11/23 14:50:39.913
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:39.932
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:39.935
[BeforeEach] [sig-network] EndpointSliceMirroring
  test/e2e/network/endpointslicemirroring.go:41
[It] should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53
STEP: mirroring a new custom Endpoint 08/11/23 14:50:39.954
Aug 11 14:50:39.963: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
STEP: mirroring an update to a custom Endpoint 08/11/23 14:50:41.967
Aug 11 14:50:41.982: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
STEP: mirroring deletion of a custom Endpoint 08/11/23 14:50:43.987
[AfterEach] [sig-network] EndpointSliceMirroring
  test/e2e/framework/framework.go:187
Aug 11 14:50:43.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslicemirroring-5836" for this suite. 08/11/23 14:50:44.003
{"msg":"PASSED [sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]","completed":156,"skipped":2994,"failed":0}
------------------------------
â€¢ [4.098 seconds]
[sig-network] EndpointSliceMirroring
test/e2e/network/common/framework.go:23
  should mirror a custom Endpoints resource through create update and delete [Conformance]
  test/e2e/network/endpointslicemirroring.go:53

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:39.912
    Aug 11 14:50:39.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename endpointslicemirroring 08/11/23 14:50:39.913
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:39.932
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:39.935
    [BeforeEach] [sig-network] EndpointSliceMirroring
      test/e2e/network/endpointslicemirroring.go:41
    [It] should mirror a custom Endpoints resource through create update and delete [Conformance]
      test/e2e/network/endpointslicemirroring.go:53
    STEP: mirroring a new custom Endpoint 08/11/23 14:50:39.954
    Aug 11 14:50:39.963: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
    STEP: mirroring an update to a custom Endpoint 08/11/23 14:50:41.967
    Aug 11 14:50:41.982: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
    STEP: mirroring deletion of a custom Endpoint 08/11/23 14:50:43.987
    [AfterEach] [sig-network] EndpointSliceMirroring
      test/e2e/framework/framework.go:187
    Aug 11 14:50:43.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslicemirroring-5836" for this suite. 08/11/23 14:50:44.003
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] ConfigMap
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:44.011
Aug 11 14:50:44.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:50:44.012
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:44.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:44.033
[It] should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168
STEP: creating a ConfigMap 08/11/23 14:50:44.035
STEP: fetching the ConfigMap 08/11/23 14:50:44.042
STEP: patching the ConfigMap 08/11/23 14:50:44.046
STEP: listing all ConfigMaps in all namespaces with a label selector 08/11/23 14:50:44.052
STEP: deleting the ConfigMap by collection with a label selector 08/11/23 14:50:44.055
STEP: listing all ConfigMaps in test namespace 08/11/23 14:50:44.064
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:50:44.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9888" for this suite. 08/11/23 14:50:44.071
{"msg":"PASSED [sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]","completed":157,"skipped":3001,"failed":0}
------------------------------
â€¢ [0.067 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should run through a ConfigMap lifecycle [Conformance]
  test/e2e/common/node/configmap.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:44.011
    Aug 11 14:50:44.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:50:44.012
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:44.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:44.033
    [It] should run through a ConfigMap lifecycle [Conformance]
      test/e2e/common/node/configmap.go:168
    STEP: creating a ConfigMap 08/11/23 14:50:44.035
    STEP: fetching the ConfigMap 08/11/23 14:50:44.042
    STEP: patching the ConfigMap 08/11/23 14:50:44.046
    STEP: listing all ConfigMaps in all namespaces with a label selector 08/11/23 14:50:44.052
    STEP: deleting the ConfigMap by collection with a label selector 08/11/23 14:50:44.055
    STEP: listing all ConfigMaps in test namespace 08/11/23 14:50:44.064
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:50:44.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9888" for this suite. 08/11/23 14:50:44.071
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Security Context When creating a container with runAsUser
  should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:44.079
Aug 11 14:50:44.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename security-context-test 08/11/23 14:50:44.08
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:44.094
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:44.096
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:346
Aug 11 14:50:44.107: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f" in namespace "security-context-test-8386" to be "Succeeded or Failed"
Aug 11 14:50:44.111: INFO: Pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.734016ms
Aug 11 14:50:46.117: INFO: Pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009637997s
Aug 11 14:50:48.115: INFO: Pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008034633s
Aug 11 14:50:48.115: INFO: Pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 11 14:50:48.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-8386" for this suite. 08/11/23 14:50:48.12
{"msg":"PASSED [sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]","completed":158,"skipped":3002,"failed":0}
------------------------------
â€¢ [4.048 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a container with runAsUser
  test/e2e/common/node/security_context.go:308
    should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:44.079
    Aug 11 14:50:44.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename security-context-test 08/11/23 14:50:44.08
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:44.094
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:44.096
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:346
    Aug 11 14:50:44.107: INFO: Waiting up to 5m0s for pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f" in namespace "security-context-test-8386" to be "Succeeded or Failed"
    Aug 11 14:50:44.111: INFO: Pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.734016ms
    Aug 11 14:50:46.117: INFO: Pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009637997s
    Aug 11 14:50:48.115: INFO: Pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008034633s
    Aug 11 14:50:48.115: INFO: Pod "busybox-user-65534-e61e4c16-1eb5-4d8c-a963-61f661f8247f" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 11 14:50:48.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-8386" for this suite. 08/11/23 14:50:48.12
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:48.127
Aug 11 14:50:48.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename deployment 08/11/23 14:50:48.129
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:48.145
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:48.147
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150
Aug 11 14:50:48.150: INFO: Creating simple deployment test-new-deployment
Aug 11 14:50:48.167: INFO: deployment "test-new-deployment" doesn't have the required revision set
STEP: getting scale subresource 08/11/23 14:50:50.182
STEP: updating a scale subresource 08/11/23 14:50:50.185
STEP: verifying the deployment Spec.Replicas was modified 08/11/23 14:50:50.191
STEP: Patch a scale subresource 08/11/23 14:50:50.194
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 14:50:50.233: INFO: Deployment "test-new-deployment":
&Deployment{ObjectMeta:{test-new-deployment  deployment-1158  b2f82749-e6ad-4971-bbb7-483997d8fea9 32115 3 2023-08-11 14:50:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-11 14:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003352d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-08-11 14:50:49 +0000 UTC,LastTransitionTime:2023-08-11 14:50:48 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 14:50:50 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 11 14:50:50.243: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
&ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1158  424b61ad-6efe-43e8-95d1-7a16185881a3 32119 3 2023-08-11 14:50:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b2f82749-e6ad-4971-bbb7-483997d8fea9 0xc003ffeb07 0xc003ffeb08}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2f82749-e6ad-4971-bbb7-483997d8fea9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ffeb98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 14:50:50.251: INFO: Pod "test-new-deployment-845c8977d9-7ps4s" is available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-7ps4s test-new-deployment-845c8977d9- deployment-1158  fafb9336-1b37-4169-9a86-7c04d48bf537 32098 0 2023-08-11 14:50:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 424b61ad-6efe-43e8-95d1-7a16185881a3 0xc003ffef97 0xc003ffef98}] [] [{kube-controller-manager Update v1 2023-08-11 14:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"424b61ad-6efe-43e8-95d1-7a16185881a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:50:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9n2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9n2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.223,StartTime:2023-08-11 14:50:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:50:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cef1dc2e400fd8f4848cf96c27626a7e119d8dab1268538b2d3dc611e593df85,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 14:50:50.251: INFO: Pod "test-new-deployment-845c8977d9-dlvw6" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-dlvw6 test-new-deployment-845c8977d9- deployment-1158  2dcc96e3-39de-4095-ba70-b5e60ddc08e3 32125 0 2023-08-11 14:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 424b61ad-6efe-43e8-95d1-7a16185881a3 0xc003fff170 0xc003fff171}] [] [{kube-controller-manager Update v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"424b61ad-6efe-43e8-95d1-7a16185881a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrqjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrqjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 14:50:50.251: INFO: Pod "test-new-deployment-845c8977d9-f76wh" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-f76wh test-new-deployment-845c8977d9- deployment-1158  6d658e7c-1c7b-46dc-a12e-6cd4a2c68112 32126 0 2023-08-11 14:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 424b61ad-6efe-43e8-95d1-7a16185881a3 0xc003fff2c0 0xc003fff2c1}] [] [{kube-controller-manager Update v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"424b61ad-6efe-43e8-95d1-7a16185881a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j96ld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j96ld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 14:50:50.251: INFO: Pod "test-new-deployment-845c8977d9-qhkzm" is not available:
&Pod{ObjectMeta:{test-new-deployment-845c8977d9-qhkzm test-new-deployment-845c8977d9- deployment-1158  04375479-d7de-4b16-9594-35fab1fc5067 32118 0 2023-08-11 14:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 424b61ad-6efe-43e8-95d1-7a16185881a3 0xc003fff410 0xc003fff411}] [] [{kube-controller-manager Update v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"424b61ad-6efe-43e8-95d1-7a16185881a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-klbzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-klbzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 14:50:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 11 14:50:50.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-1158" for this suite. 08/11/23 14:50:50.262
{"msg":"PASSED [sig-apps] Deployment Deployment should have a working scale subresource [Conformance]","completed":159,"skipped":3007,"failed":0}
------------------------------
â€¢ [2.146 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  Deployment should have a working scale subresource [Conformance]
  test/e2e/apps/deployment.go:150

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:48.127
    Aug 11 14:50:48.128: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename deployment 08/11/23 14:50:48.129
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:48.145
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:48.147
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] Deployment should have a working scale subresource [Conformance]
      test/e2e/apps/deployment.go:150
    Aug 11 14:50:48.150: INFO: Creating simple deployment test-new-deployment
    Aug 11 14:50:48.167: INFO: deployment "test-new-deployment" doesn't have the required revision set
    STEP: getting scale subresource 08/11/23 14:50:50.182
    STEP: updating a scale subresource 08/11/23 14:50:50.185
    STEP: verifying the deployment Spec.Replicas was modified 08/11/23 14:50:50.191
    STEP: Patch a scale subresource 08/11/23 14:50:50.194
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 14:50:50.233: INFO: Deployment "test-new-deployment":
    &Deployment{ObjectMeta:{test-new-deployment  deployment-1158  b2f82749-e6ad-4971-bbb7-483997d8fea9 32115 3 2023-08-11 14:50:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-11 14:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003352d18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-845c8977d9" has successfully progressed.,LastUpdateTime:2023-08-11 14:50:49 +0000 UTC,LastTransitionTime:2023-08-11 14:50:48 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 14:50:50 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 11 14:50:50.243: INFO: New ReplicaSet "test-new-deployment-845c8977d9" of Deployment "test-new-deployment":
    &ReplicaSet{ObjectMeta:{test-new-deployment-845c8977d9  deployment-1158  424b61ad-6efe-43e8-95d1-7a16185881a3 32119 3 2023-08-11 14:50:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment b2f82749-e6ad-4971-bbb7-483997d8fea9 0xc003ffeb07 0xc003ffeb08}] [] [{kube-controller-manager Update apps/v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b2f82749-e6ad-4971-bbb7-483997d8fea9\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003ffeb98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 14:50:50.251: INFO: Pod "test-new-deployment-845c8977d9-7ps4s" is available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-7ps4s test-new-deployment-845c8977d9- deployment-1158  fafb9336-1b37-4169-9a86-7c04d48bf537 32098 0 2023-08-11 14:50:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 424b61ad-6efe-43e8-95d1-7a16185881a3 0xc003ffef97 0xc003ffef98}] [] [{kube-controller-manager Update v1 2023-08-11 14:50:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"424b61ad-6efe-43e8-95d1-7a16185881a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:50:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.223\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9n2d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9n2d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:49 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.223,StartTime:2023-08-11 14:50:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 14:50:48 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://cef1dc2e400fd8f4848cf96c27626a7e119d8dab1268538b2d3dc611e593df85,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.223,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 14:50:50.251: INFO: Pod "test-new-deployment-845c8977d9-dlvw6" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-dlvw6 test-new-deployment-845c8977d9- deployment-1158  2dcc96e3-39de-4095-ba70-b5e60ddc08e3 32125 0 2023-08-11 14:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 424b61ad-6efe-43e8-95d1-7a16185881a3 0xc003fff170 0xc003fff171}] [] [{kube-controller-manager Update v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"424b61ad-6efe-43e8-95d1-7a16185881a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrqjn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrqjn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 14:50:50.251: INFO: Pod "test-new-deployment-845c8977d9-f76wh" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-f76wh test-new-deployment-845c8977d9- deployment-1158  6d658e7c-1c7b-46dc-a12e-6cd4a2c68112 32126 0 2023-08-11 14:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 424b61ad-6efe-43e8-95d1-7a16185881a3 0xc003fff2c0 0xc003fff2c1}] [] [{kube-controller-manager Update v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"424b61ad-6efe-43e8-95d1-7a16185881a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j96ld,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j96ld,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 14:50:50.251: INFO: Pod "test-new-deployment-845c8977d9-qhkzm" is not available:
    &Pod{ObjectMeta:{test-new-deployment-845c8977d9-qhkzm test-new-deployment-845c8977d9- deployment-1158  04375479-d7de-4b16-9594-35fab1fc5067 32118 0 2023-08-11 14:50:50 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet test-new-deployment-845c8977d9 424b61ad-6efe-43e8-95d1-7a16185881a3 0xc003fff410 0xc003fff411}] [] [{kube-controller-manager Update v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"424b61ad-6efe-43e8-95d1-7a16185881a3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 14:50:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-klbzq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-klbzq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 14:50:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 14:50:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 11 14:50:50.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-1158" for this suite. 08/11/23 14:50:50.262
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:50.274
Aug 11 14:50:50.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:50:50.275
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:50.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:50.294
[It] should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91
STEP: Creating a pod to test substitution in container's args 08/11/23 14:50:50.297
Aug 11 14:50:50.307: INFO: Waiting up to 5m0s for pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219" in namespace "var-expansion-1791" to be "Succeeded or Failed"
Aug 11 14:50:50.312: INFO: Pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219": Phase="Pending", Reason="", readiness=false. Elapsed: 5.282065ms
Aug 11 14:50:52.317: INFO: Pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010002468s
Aug 11 14:50:54.317: INFO: Pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01017021s
STEP: Saw pod success 08/11/23 14:50:54.317
Aug 11 14:50:54.317: INFO: Pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219" satisfied condition "Succeeded or Failed"
Aug 11 14:50:54.320: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219 container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:50:54.328
Aug 11 14:50:54.342: INFO: Waiting for pod var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219 to disappear
Aug 11 14:50:54.345: INFO: Pod var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 11 14:50:54.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-1791" for this suite. 08/11/23 14:50:54.35
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]","completed":160,"skipped":3008,"failed":0}
------------------------------
â€¢ [4.082 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's args [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:50.274
    Aug 11 14:50:50.274: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:50:50.275
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:50.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:50.294
    [It] should allow substituting values in a container's args [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:91
    STEP: Creating a pod to test substitution in container's args 08/11/23 14:50:50.297
    Aug 11 14:50:50.307: INFO: Waiting up to 5m0s for pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219" in namespace "var-expansion-1791" to be "Succeeded or Failed"
    Aug 11 14:50:50.312: INFO: Pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219": Phase="Pending", Reason="", readiness=false. Elapsed: 5.282065ms
    Aug 11 14:50:52.317: INFO: Pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010002468s
    Aug 11 14:50:54.317: INFO: Pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.01017021s
    STEP: Saw pod success 08/11/23 14:50:54.317
    Aug 11 14:50:54.317: INFO: Pod "var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219" satisfied condition "Succeeded or Failed"
    Aug 11 14:50:54.320: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:50:54.328
    Aug 11 14:50:54.342: INFO: Waiting for pod var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219 to disappear
    Aug 11 14:50:54.345: INFO: Pod var-expansion-00dc0a4f-7beb-41a5-9735-434c52690219 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 11 14:50:54.345: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-1791" for this suite. 08/11/23 14:50:54.35
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:54.357
Aug 11 14:50:54.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:50:54.358
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:54.378
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:54.381
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:50:54.398
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:50:54.603
STEP: Deploying the webhook pod 08/11/23 14:50:54.611
STEP: Wait for the deployment to be ready 08/11/23 14:50:54.631
Aug 11 14:50:54.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:50:56.654
STEP: Verifying the service has paired with the endpoint 08/11/23 14:50:56.671
Aug 11 14:50:57.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307
STEP: Registering the crd webhook via the AdmissionRegistration API 08/11/23 14:50:57.676
STEP: Creating a custom resource definition that should be denied by the webhook 08/11/23 14:50:57.702
Aug 11 14:50:57.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:50:57.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4321" for this suite. 08/11/23 14:50:57.729
STEP: Destroying namespace "webhook-4321-markers" for this suite. 08/11/23 14:50:57.736
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]","completed":161,"skipped":3017,"failed":0}
------------------------------
â€¢ [3.434 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should deny crd creation [Conformance]
  test/e2e/apimachinery/webhook.go:307

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:54.357
    Aug 11 14:50:54.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:50:54.358
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:54.378
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:54.381
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:50:54.398
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:50:54.603
    STEP: Deploying the webhook pod 08/11/23 14:50:54.611
    STEP: Wait for the deployment to be ready 08/11/23 14:50:54.631
    Aug 11 14:50:54.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:50:56.654
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:50:56.671
    Aug 11 14:50:57.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should deny crd creation [Conformance]
      test/e2e/apimachinery/webhook.go:307
    STEP: Registering the crd webhook via the AdmissionRegistration API 08/11/23 14:50:57.676
    STEP: Creating a custom resource definition that should be denied by the webhook 08/11/23 14:50:57.702
    Aug 11 14:50:57.702: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:50:57.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4321" for this suite. 08/11/23 14:50:57.729
    STEP: Destroying namespace "webhook-4321-markers" for this suite. 08/11/23 14:50:57.736
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:50:57.792
Aug 11 14:50:57.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:50:57.793
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:57.811
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:57.813
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77
STEP: Creating projection with secret that has name projected-secret-test-map-ebd5f022-dfc0-414b-80fd-bad8995b1b5e 08/11/23 14:50:57.816
STEP: Creating a pod to test consume secrets 08/11/23 14:50:57.822
Aug 11 14:50:57.830: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523" in namespace "projected-5230" to be "Succeeded or Failed"
Aug 11 14:50:57.835: INFO: Pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523": Phase="Pending", Reason="", readiness=false. Elapsed: 5.677007ms
Aug 11 14:50:59.840: INFO: Pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010428342s
Aug 11 14:51:01.840: INFO: Pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010334106s
STEP: Saw pod success 08/11/23 14:51:01.84
Aug 11 14:51:01.840: INFO: Pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523" satisfied condition "Succeeded or Failed"
Aug 11 14:51:01.843: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523 container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:51:01.857
Aug 11 14:51:01.868: INFO: Waiting for pod pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523 to disappear
Aug 11 14:51:01.870: INFO: Pod pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523 no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 11 14:51:01.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5230" for this suite. 08/11/23 14:51:01.874
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":162,"skipped":3021,"failed":0}
------------------------------
â€¢ [4.092 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:50:57.792
    Aug 11 14:50:57.792: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:50:57.793
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:50:57.811
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:50:57.813
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:77
    STEP: Creating projection with secret that has name projected-secret-test-map-ebd5f022-dfc0-414b-80fd-bad8995b1b5e 08/11/23 14:50:57.816
    STEP: Creating a pod to test consume secrets 08/11/23 14:50:57.822
    Aug 11 14:50:57.830: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523" in namespace "projected-5230" to be "Succeeded or Failed"
    Aug 11 14:50:57.835: INFO: Pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523": Phase="Pending", Reason="", readiness=false. Elapsed: 5.677007ms
    Aug 11 14:50:59.840: INFO: Pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010428342s
    Aug 11 14:51:01.840: INFO: Pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010334106s
    STEP: Saw pod success 08/11/23 14:51:01.84
    Aug 11 14:51:01.840: INFO: Pod "pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523" satisfied condition "Succeeded or Failed"
    Aug 11 14:51:01.843: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523 container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:51:01.857
    Aug 11 14:51:01.868: INFO: Waiting for pod pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523 to disappear
    Aug 11 14:51:01.870: INFO: Pod pod-projected-secrets-4039d1ab-cf53-4992-9960-9c278eb75523 no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 11 14:51:01.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5230" for this suite. 08/11/23 14:51:01.874
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:01.885
Aug 11 14:51:01.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:51:01.886
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:01.902
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:01.904
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124
STEP: Creating secret with name secret-test-accb5c09-079f-4526-b42b-d15c55510a21 08/11/23 14:51:01.906
STEP: Creating a pod to test consume secrets 08/11/23 14:51:01.911
Aug 11 14:51:01.919: INFO: Waiting up to 5m0s for pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164" in namespace "secrets-5055" to be "Succeeded or Failed"
Aug 11 14:51:01.923: INFO: Pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164": Phase="Pending", Reason="", readiness=false. Elapsed: 4.868382ms
Aug 11 14:51:03.928: INFO: Pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009477462s
Aug 11 14:51:05.928: INFO: Pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009922231s
STEP: Saw pod success 08/11/23 14:51:05.929
Aug 11 14:51:05.929: INFO: Pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164" satisfied condition "Succeeded or Failed"
Aug 11 14:51:05.932: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:51:05.94
Aug 11 14:51:05.955: INFO: Waiting for pod pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164 to disappear
Aug 11 14:51:05.958: INFO: Pod pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:51:05.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-5055" for this suite. 08/11/23 14:51:05.962
{"msg":"PASSED [sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":163,"skipped":3031,"failed":0}
------------------------------
â€¢ [4.083 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:01.885
    Aug 11 14:51:01.885: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:51:01.886
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:01.902
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:01.904
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:124
    STEP: Creating secret with name secret-test-accb5c09-079f-4526-b42b-d15c55510a21 08/11/23 14:51:01.906
    STEP: Creating a pod to test consume secrets 08/11/23 14:51:01.911
    Aug 11 14:51:01.919: INFO: Waiting up to 5m0s for pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164" in namespace "secrets-5055" to be "Succeeded or Failed"
    Aug 11 14:51:01.923: INFO: Pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164": Phase="Pending", Reason="", readiness=false. Elapsed: 4.868382ms
    Aug 11 14:51:03.928: INFO: Pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009477462s
    Aug 11 14:51:05.928: INFO: Pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009922231s
    STEP: Saw pod success 08/11/23 14:51:05.929
    Aug 11 14:51:05.929: INFO: Pod "pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164" satisfied condition "Succeeded or Failed"
    Aug 11 14:51:05.932: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:51:05.94
    Aug 11 14:51:05.955: INFO: Waiting for pod pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164 to disappear
    Aug 11 14:51:05.958: INFO: Pod pod-secrets-66d29351-07de-489b-ba37-21e31e5c2164 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:51:05.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-5055" for this suite. 08/11/23 14:51:05.962
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:05.971
Aug 11 14:51:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:51:05.972
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:05.987
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:05.989
[It] works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193
Aug 11 14:51:05.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 14:51:09.616
Aug 11 14:51:09.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 --namespace=crd-publish-openapi-4830 create -f -'
Aug 11 14:51:10.215: INFO: stderr: ""
Aug 11 14:51:10.215: INFO: stdout: "e2e-test-crd-publish-openapi-7136-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 11 14:51:10.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 --namespace=crd-publish-openapi-4830 delete e2e-test-crd-publish-openapi-7136-crds test-cr'
Aug 11 14:51:10.293: INFO: stderr: ""
Aug 11 14:51:10.293: INFO: stdout: "e2e-test-crd-publish-openapi-7136-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
Aug 11 14:51:10.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 --namespace=crd-publish-openapi-4830 apply -f -'
Aug 11 14:51:10.880: INFO: stderr: ""
Aug 11 14:51:10.880: INFO: stdout: "e2e-test-crd-publish-openapi-7136-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
Aug 11 14:51:10.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 --namespace=crd-publish-openapi-4830 delete e2e-test-crd-publish-openapi-7136-crds test-cr'
Aug 11 14:51:10.945: INFO: stderr: ""
Aug 11 14:51:10.945: INFO: stdout: "e2e-test-crd-publish-openapi-7136-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/11/23 14:51:10.945
Aug 11 14:51:10.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 explain e2e-test-crd-publish-openapi-7136-crds'
Aug 11 14:51:11.505: INFO: stderr: ""
Aug 11 14:51:11.505: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7136-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:51:14.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4830" for this suite. 08/11/23 14:51:14.125
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]","completed":164,"skipped":3068,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.161 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields at the schema root [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:05.971
    Aug 11 14:51:05.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 14:51:05.972
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:05.987
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:05.989
    [It] works for CRD preserving unknown fields at the schema root [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:193
    Aug 11 14:51:05.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 14:51:09.616
    Aug 11 14:51:09.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 --namespace=crd-publish-openapi-4830 create -f -'
    Aug 11 14:51:10.215: INFO: stderr: ""
    Aug 11 14:51:10.215: INFO: stdout: "e2e-test-crd-publish-openapi-7136-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 11 14:51:10.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 --namespace=crd-publish-openapi-4830 delete e2e-test-crd-publish-openapi-7136-crds test-cr'
    Aug 11 14:51:10.293: INFO: stderr: ""
    Aug 11 14:51:10.293: INFO: stdout: "e2e-test-crd-publish-openapi-7136-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    Aug 11 14:51:10.293: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 --namespace=crd-publish-openapi-4830 apply -f -'
    Aug 11 14:51:10.880: INFO: stderr: ""
    Aug 11 14:51:10.880: INFO: stdout: "e2e-test-crd-publish-openapi-7136-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
    Aug 11 14:51:10.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 --namespace=crd-publish-openapi-4830 delete e2e-test-crd-publish-openapi-7136-crds test-cr'
    Aug 11 14:51:10.945: INFO: stderr: ""
    Aug 11 14:51:10.945: INFO: stdout: "e2e-test-crd-publish-openapi-7136-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/11/23 14:51:10.945
    Aug 11 14:51:10.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-4830 explain e2e-test-crd-publish-openapi-7136-crds'
    Aug 11 14:51:11.505: INFO: stderr: ""
    Aug 11 14:51:11.505: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-7136-crd\nVERSION:  crd-publish-openapi-test-unknown-at-root.example.com/v1\n\nDESCRIPTION:\n     <empty>\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:51:14.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4830" for this suite. 08/11/23 14:51:14.125
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
[BeforeEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:14.133
Aug 11 14:51:14.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename podtemplate 08/11/23 14:51:14.134
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:14.149
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:14.152
[It] should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122
STEP: Create set of pod templates 08/11/23 14:51:14.154
Aug 11 14:51:14.160: INFO: created test-podtemplate-1
Aug 11 14:51:14.167: INFO: created test-podtemplate-2
Aug 11 14:51:14.172: INFO: created test-podtemplate-3
STEP: get a list of pod templates with a label in the current namespace 08/11/23 14:51:14.172
STEP: delete collection of pod templates 08/11/23 14:51:14.176
Aug 11 14:51:14.176: INFO: requesting DeleteCollection of pod templates
STEP: check that the list of pod templates matches the requested quantity 08/11/23 14:51:14.192
Aug 11 14:51:14.192: INFO: requesting list of pod templates to confirm quantity
[AfterEach] [sig-node] PodTemplates
  test/e2e/framework/framework.go:187
Aug 11 14:51:14.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "podtemplate-3136" for this suite. 08/11/23 14:51:14.199
{"msg":"PASSED [sig-node] PodTemplates should delete a collection of pod templates [Conformance]","completed":165,"skipped":3090,"failed":0}
------------------------------
â€¢ [0.072 seconds]
[sig-node] PodTemplates
test/e2e/common/node/framework.go:23
  should delete a collection of pod templates [Conformance]
  test/e2e/common/node/podtemplates.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:14.133
    Aug 11 14:51:14.133: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename podtemplate 08/11/23 14:51:14.134
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:14.149
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:14.152
    [It] should delete a collection of pod templates [Conformance]
      test/e2e/common/node/podtemplates.go:122
    STEP: Create set of pod templates 08/11/23 14:51:14.154
    Aug 11 14:51:14.160: INFO: created test-podtemplate-1
    Aug 11 14:51:14.167: INFO: created test-podtemplate-2
    Aug 11 14:51:14.172: INFO: created test-podtemplate-3
    STEP: get a list of pod templates with a label in the current namespace 08/11/23 14:51:14.172
    STEP: delete collection of pod templates 08/11/23 14:51:14.176
    Aug 11 14:51:14.176: INFO: requesting DeleteCollection of pod templates
    STEP: check that the list of pod templates matches the requested quantity 08/11/23 14:51:14.192
    Aug 11 14:51:14.192: INFO: requesting list of pod templates to confirm quantity
    [AfterEach] [sig-node] PodTemplates
      test/e2e/framework/framework.go:187
    Aug 11 14:51:14.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "podtemplate-3136" for this suite. 08/11/23 14:51:14.199
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:14.206
Aug 11 14:51:14.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 14:51:14.207
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:14.221
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:14.223
[It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216
STEP: Creating a pod to test downward api env vars 08/11/23 14:51:14.225
Aug 11 14:51:14.233: INFO: Waiting up to 5m0s for pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5" in namespace "downward-api-5416" to be "Succeeded or Failed"
Aug 11 14:51:14.237: INFO: Pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.614953ms
Aug 11 14:51:16.241: INFO: Pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00809703s
Aug 11 14:51:18.243: INFO: Pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009419338s
STEP: Saw pod success 08/11/23 14:51:18.243
Aug 11 14:51:18.243: INFO: Pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5" satisfied condition "Succeeded or Failed"
Aug 11 14:51:18.246: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-728fc523-604c-4766-b385-b0a5169ad2c5 container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:51:18.27
Aug 11 14:51:18.283: INFO: Waiting for pod downward-api-728fc523-604c-4766-b385-b0a5169ad2c5 to disappear
Aug 11 14:51:18.286: INFO: Pod downward-api-728fc523-604c-4766-b385-b0a5169ad2c5 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 11 14:51:18.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5416" for this suite. 08/11/23 14:51:18.29
{"msg":"PASSED [sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]","completed":166,"skipped":3112,"failed":0}
------------------------------
â€¢ [4.090 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:14.206
    Aug 11 14:51:14.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:51:14.207
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:14.221
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:14.223
    [It] should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:216
    STEP: Creating a pod to test downward api env vars 08/11/23 14:51:14.225
    Aug 11 14:51:14.233: INFO: Waiting up to 5m0s for pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5" in namespace "downward-api-5416" to be "Succeeded or Failed"
    Aug 11 14:51:14.237: INFO: Pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 3.614953ms
    Aug 11 14:51:16.241: INFO: Pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00809703s
    Aug 11 14:51:18.243: INFO: Pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009419338s
    STEP: Saw pod success 08/11/23 14:51:18.243
    Aug 11 14:51:18.243: INFO: Pod "downward-api-728fc523-604c-4766-b385-b0a5169ad2c5" satisfied condition "Succeeded or Failed"
    Aug 11 14:51:18.246: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-728fc523-604c-4766-b385-b0a5169ad2c5 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:51:18.27
    Aug 11 14:51:18.283: INFO: Waiting for pod downward-api-728fc523-604c-4766-b385-b0a5169ad2c5 to disappear
    Aug 11 14:51:18.286: INFO: Pod downward-api-728fc523-604c-4766-b385-b0a5169ad2c5 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 11 14:51:18.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5416" for this suite. 08/11/23 14:51:18.29
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:18.297
Aug 11 14:51:18.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:51:18.298
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:18.315
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:18.318
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:51:18.322
Aug 11 14:51:18.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5" in namespace "projected-8829" to be "Succeeded or Failed"
Aug 11 14:51:18.340: INFO: Pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.589042ms
Aug 11 14:51:20.346: INFO: Pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010625407s
Aug 11 14:51:22.345: INFO: Pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009444137s
STEP: Saw pod success 08/11/23 14:51:22.345
Aug 11 14:51:22.345: INFO: Pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5" satisfied condition "Succeeded or Failed"
Aug 11 14:51:22.349: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5 container client-container: <nil>
STEP: delete the pod 08/11/23 14:51:22.358
Aug 11 14:51:22.370: INFO: Waiting for pod downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5 to disappear
Aug 11 14:51:22.372: INFO: Pod downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 14:51:22.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8829" for this suite. 08/11/23 14:51:22.377
{"msg":"PASSED [sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":167,"skipped":3121,"failed":0}
------------------------------
â€¢ [4.086 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:18.297
    Aug 11 14:51:18.297: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:51:18.298
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:18.315
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:18.318
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:67
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:51:18.322
    Aug 11 14:51:18.335: INFO: Waiting up to 5m0s for pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5" in namespace "projected-8829" to be "Succeeded or Failed"
    Aug 11 14:51:18.340: INFO: Pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5": Phase="Pending", Reason="", readiness=false. Elapsed: 4.589042ms
    Aug 11 14:51:20.346: INFO: Pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010625407s
    Aug 11 14:51:22.345: INFO: Pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009444137s
    STEP: Saw pod success 08/11/23 14:51:22.345
    Aug 11 14:51:22.345: INFO: Pod "downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5" satisfied condition "Succeeded or Failed"
    Aug 11 14:51:22.349: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:51:22.358
    Aug 11 14:51:22.370: INFO: Waiting for pod downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5 to disappear
    Aug 11 14:51:22.372: INFO: Pod downwardapi-volume-bebe0355-444f-4ede-97c6-279a1f7bd3f5 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 14:51:22.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8829" for this suite. 08/11/23 14:51:22.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:22.385
Aug 11 14:51:22.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sysctl 08/11/23 14:51:22.386
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:22.4
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:22.402
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77
STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/11/23 14:51:22.405
STEP: Watching for error events or started pod 08/11/23 14:51:22.414
STEP: Waiting for pod completion 08/11/23 14:51:24.418
Aug 11 14:51:24.418: INFO: Waiting up to 3m0s for pod "sysctl-81a55a98-5b63-4942-bb11-187c111d812b" in namespace "sysctl-2993" to be "completed"
Aug 11 14:51:24.421: INFO: Pod "sysctl-81a55a98-5b63-4942-bb11-187c111d812b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765506ms
Aug 11 14:51:26.425: INFO: Pod "sysctl-81a55a98-5b63-4942-bb11-187c111d812b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00712788s
Aug 11 14:51:26.425: INFO: Pod "sysctl-81a55a98-5b63-4942-bb11-187c111d812b" satisfied condition "completed"
STEP: Checking that the pod succeeded 08/11/23 14:51:26.428
STEP: Getting logs from the pod 08/11/23 14:51:26.429
STEP: Checking that the sysctl is actually updated 08/11/23 14:51:26.437
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 11 14:51:26.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-2993" for this suite. 08/11/23 14:51:26.442
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":168,"skipped":3127,"failed":0}
------------------------------
â€¢ [4.063 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:77

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:22.385
    Aug 11 14:51:22.385: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sysctl 08/11/23 14:51:22.386
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:22.4
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:22.402
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:77
    STEP: Creating a pod with the kernel.shm_rmid_forced sysctl 08/11/23 14:51:22.405
    STEP: Watching for error events or started pod 08/11/23 14:51:22.414
    STEP: Waiting for pod completion 08/11/23 14:51:24.418
    Aug 11 14:51:24.418: INFO: Waiting up to 3m0s for pod "sysctl-81a55a98-5b63-4942-bb11-187c111d812b" in namespace "sysctl-2993" to be "completed"
    Aug 11 14:51:24.421: INFO: Pod "sysctl-81a55a98-5b63-4942-bb11-187c111d812b": Phase="Pending", Reason="", readiness=false. Elapsed: 2.765506ms
    Aug 11 14:51:26.425: INFO: Pod "sysctl-81a55a98-5b63-4942-bb11-187c111d812b": Phase="Succeeded", Reason="", readiness=false. Elapsed: 2.00712788s
    Aug 11 14:51:26.425: INFO: Pod "sysctl-81a55a98-5b63-4942-bb11-187c111d812b" satisfied condition "completed"
    STEP: Checking that the pod succeeded 08/11/23 14:51:26.428
    STEP: Getting logs from the pod 08/11/23 14:51:26.429
    STEP: Checking that the sysctl is actually updated 08/11/23 14:51:26.437
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 11 14:51:26.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-2993" for this suite. 08/11/23 14:51:26.442
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:26.449
Aug 11 14:51:26.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replicaset 08/11/23 14:51:26.45
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:26.466
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:26.468
[It] should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176
STEP: Create a Replicaset 08/11/23 14:51:26.475
STEP: Verify that the required pods have come up. 08/11/23 14:51:26.481
Aug 11 14:51:26.488: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 14:51:26.488
Aug 11 14:51:26.488: INFO: Waiting up to 5m0s for pod "test-rs-zb66m" in namespace "replicaset-6474" to be "running"
Aug 11 14:51:26.496: INFO: Pod "test-rs-zb66m": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604036ms
Aug 11 14:51:28.500: INFO: Pod "test-rs-zb66m": Phase="Running", Reason="", readiness=true. Elapsed: 2.012246807s
Aug 11 14:51:28.500: INFO: Pod "test-rs-zb66m" satisfied condition "running"
STEP: Getting /status 08/11/23 14:51:28.5
Aug 11 14:51:28.504: INFO: Replicaset test-rs has Conditions: []
STEP: updating the Replicaset Status 08/11/23 14:51:28.504
Aug 11 14:51:28.514: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the ReplicaSet status to be updated 08/11/23 14:51:28.514
Aug 11 14:51:28.516: INFO: Observed &ReplicaSet event: ADDED
Aug 11 14:51:28.516: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 14:51:28.516: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 14:51:28.516: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 14:51:28.516: INFO: Found replicaset test-rs in namespace replicaset-6474 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
Aug 11 14:51:28.516: INFO: Replicaset test-rs has an updated status
STEP: patching the Replicaset Status 08/11/23 14:51:28.516
Aug 11 14:51:28.516: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 11 14:51:28.523: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Replicaset status to be patched 08/11/23 14:51:28.523
Aug 11 14:51:28.525: INFO: Observed &ReplicaSet event: ADDED
Aug 11 14:51:28.525: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 14:51:28.525: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 14:51:28.526: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 14:51:28.526: INFO: Observed replicaset test-rs in namespace replicaset-6474 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 14:51:28.526: INFO: Observed &ReplicaSet event: MODIFIED
Aug 11 14:51:28.526: INFO: Found replicaset test-rs in namespace replicaset-6474 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
Aug 11 14:51:28.526: INFO: Replicaset test-rs has a patched status
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 11 14:51:28.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6474" for this suite. 08/11/23 14:51:28.53
{"msg":"PASSED [sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]","completed":169,"skipped":3155,"failed":0}
------------------------------
â€¢ [2.087 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should validate Replicaset Status endpoints [Conformance]
  test/e2e/apps/replica_set.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:26.449
    Aug 11 14:51:26.449: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replicaset 08/11/23 14:51:26.45
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:26.466
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:26.468
    [It] should validate Replicaset Status endpoints [Conformance]
      test/e2e/apps/replica_set.go:176
    STEP: Create a Replicaset 08/11/23 14:51:26.475
    STEP: Verify that the required pods have come up. 08/11/23 14:51:26.481
    Aug 11 14:51:26.488: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 14:51:26.488
    Aug 11 14:51:26.488: INFO: Waiting up to 5m0s for pod "test-rs-zb66m" in namespace "replicaset-6474" to be "running"
    Aug 11 14:51:26.496: INFO: Pod "test-rs-zb66m": Phase="Pending", Reason="", readiness=false. Elapsed: 7.604036ms
    Aug 11 14:51:28.500: INFO: Pod "test-rs-zb66m": Phase="Running", Reason="", readiness=true. Elapsed: 2.012246807s
    Aug 11 14:51:28.500: INFO: Pod "test-rs-zb66m" satisfied condition "running"
    STEP: Getting /status 08/11/23 14:51:28.5
    Aug 11 14:51:28.504: INFO: Replicaset test-rs has Conditions: []
    STEP: updating the Replicaset Status 08/11/23 14:51:28.504
    Aug 11 14:51:28.514: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the ReplicaSet status to be updated 08/11/23 14:51:28.514
    Aug 11 14:51:28.516: INFO: Observed &ReplicaSet event: ADDED
    Aug 11 14:51:28.516: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 14:51:28.516: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 14:51:28.516: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 14:51:28.516: INFO: Found replicaset test-rs in namespace replicaset-6474 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
    Aug 11 14:51:28.516: INFO: Replicaset test-rs has an updated status
    STEP: patching the Replicaset Status 08/11/23 14:51:28.516
    Aug 11 14:51:28.516: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 11 14:51:28.523: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Replicaset status to be patched 08/11/23 14:51:28.523
    Aug 11 14:51:28.525: INFO: Observed &ReplicaSet event: ADDED
    Aug 11 14:51:28.525: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 14:51:28.525: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 14:51:28.526: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 14:51:28.526: INFO: Observed replicaset test-rs in namespace replicaset-6474 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 14:51:28.526: INFO: Observed &ReplicaSet event: MODIFIED
    Aug 11 14:51:28.526: INFO: Found replicaset test-rs in namespace replicaset-6474 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
    Aug 11 14:51:28.526: INFO: Replicaset test-rs has a patched status
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 11 14:51:28.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6474" for this suite. 08/11/23 14:51:28.53
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Job
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:28.537
Aug 11 14:51:28.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename job 08/11/23 14:51:28.539
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:28.554
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:28.556
[It] should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531
STEP: Creating a suspended job 08/11/23 14:51:28.562
STEP: Patching the Job 08/11/23 14:51:28.568
STEP: Watching for Job to be patched 08/11/23 14:51:28.583
Aug 11 14:51:28.585: INFO: Event ADDED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 11 14:51:28.585: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking:]
Aug 11 14:51:28.585: INFO: Event MODIFIED found for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking:]
STEP: Updating the job 08/11/23 14:51:28.585
STEP: Watching for Job to be updated 08/11/23 14:51:28.595
Aug 11 14:51:28.596: INFO: Event MODIFIED found for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 14:51:28.596: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
STEP: Listing all Jobs with LabelSelector 08/11/23 14:51:28.596
Aug 11 14:51:28.601: INFO: Job: e2e-48gp7 as labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7]
STEP: Waiting for job to complete 08/11/23 14:51:28.601
STEP: Delete a job collection with a labelselector 08/11/23 14:51:38.606
STEP: Watching for Job to be deleted 08/11/23 14:51:38.615
Aug 11 14:51:38.617: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 14:51:38.617: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 14:51:38.617: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 14:51:38.618: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 14:51:38.618: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
Aug 11 14:51:38.618: INFO: Event DELETED found for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
STEP: Relist jobs to confirm deletion 08/11/23 14:51:38.618
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 11 14:51:38.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-4713" for this suite. 08/11/23 14:51:38.631
{"msg":"PASSED [sig-apps] Job should manage the lifecycle of a job [Conformance]","completed":170,"skipped":3164,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.109 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should manage the lifecycle of a job [Conformance]
  test/e2e/apps/job.go:531

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:28.537
    Aug 11 14:51:28.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename job 08/11/23 14:51:28.539
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:28.554
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:28.556
    [It] should manage the lifecycle of a job [Conformance]
      test/e2e/apps/job.go:531
    STEP: Creating a suspended job 08/11/23 14:51:28.562
    STEP: Patching the Job 08/11/23 14:51:28.568
    STEP: Watching for Job to be patched 08/11/23 14:51:28.583
    Aug 11 14:51:28.585: INFO: Event ADDED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 11 14:51:28.585: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking:]
    Aug 11 14:51:28.585: INFO: Event MODIFIED found for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking:]
    STEP: Updating the job 08/11/23 14:51:28.585
    STEP: Watching for Job to be updated 08/11/23 14:51:28.595
    Aug 11 14:51:28.596: INFO: Event MODIFIED found for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 14:51:28.596: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
    STEP: Listing all Jobs with LabelSelector 08/11/23 14:51:28.596
    Aug 11 14:51:28.601: INFO: Job: e2e-48gp7 as labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7]
    STEP: Waiting for job to complete 08/11/23 14:51:28.601
    STEP: Delete a job collection with a labelselector 08/11/23 14:51:38.606
    STEP: Watching for Job to be deleted 08/11/23 14:51:38.615
    Aug 11 14:51:38.617: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 14:51:38.617: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 14:51:38.617: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 14:51:38.618: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 14:51:38.618: INFO: Event MODIFIED observed for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    Aug 11 14:51:38.618: INFO: Event DELETED found for Job e2e-48gp7 in namespace job-4713 with labels: map[e2e-48gp7:patched e2e-job-label:e2e-48gp7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
    STEP: Relist jobs to confirm deletion 08/11/23 14:51:38.618
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 11 14:51:38.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-4713" for this suite. 08/11/23 14:51:38.631
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:38.65
Aug 11 14:51:38.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:51:38.651
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:38.676
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:38.678
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:51:38.695
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:51:38.856
STEP: Deploying the webhook pod 08/11/23 14:51:38.865
STEP: Wait for the deployment to be ready 08/11/23 14:51:38.878
Aug 11 14:51:38.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:51:40.895
STEP: Verifying the service has paired with the endpoint 08/11/23 14:51:40.91
Aug 11 14:51:41.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322
Aug 11 14:51:41.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9987-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 14:51:42.424
STEP: Creating a custom resource while v1 is storage version 08/11/23 14:51:42.451
STEP: Patching Custom Resource Definition to set v2 as storage 08/11/23 14:51:44.506
STEP: Patching the custom resource while v2 is storage version 08/11/23 14:51:44.517
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:51:45.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-9685" for this suite. 08/11/23 14:51:45.095
STEP: Destroying namespace "webhook-9685-markers" for this suite. 08/11/23 14:51:45.101
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]","completed":171,"skipped":3219,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.502 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource with different stored version [Conformance]
  test/e2e/apimachinery/webhook.go:322

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:38.65
    Aug 11 14:51:38.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:51:38.651
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:38.676
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:38.678
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:51:38.695
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:51:38.856
    STEP: Deploying the webhook pod 08/11/23 14:51:38.865
    STEP: Wait for the deployment to be ready 08/11/23 14:51:38.878
    Aug 11 14:51:38.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:51:40.895
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:51:40.91
    Aug 11 14:51:41.910: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource with different stored version [Conformance]
      test/e2e/apimachinery/webhook.go:322
    Aug 11 14:51:41.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9987-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 14:51:42.424
    STEP: Creating a custom resource while v1 is storage version 08/11/23 14:51:42.451
    STEP: Patching Custom Resource Definition to set v2 as storage 08/11/23 14:51:44.506
    STEP: Patching the custom resource while v2 is storage version 08/11/23 14:51:44.517
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:51:45.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-9685" for this suite. 08/11/23 14:51:45.095
    STEP: Destroying namespace "webhook-9685-markers" for this suite. 08/11/23 14:51:45.101
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:45.152
Aug 11 14:51:45.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:51:45.154
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:45.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:45.177
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:51:45.202
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:51:45.605
STEP: Deploying the webhook pod 08/11/23 14:51:45.613
STEP: Wait for the deployment to be ready 08/11/23 14:51:45.634
Aug 11 14:51:45.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:51:47.655
STEP: Verifying the service has paired with the endpoint 08/11/23 14:51:47.671
Aug 11 14:51:48.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655
STEP: Listing all of the created validation webhooks 08/11/23 14:51:48.734
STEP: Creating a configMap that should be mutated 08/11/23 14:51:48.754
STEP: Deleting the collection of validation webhooks 08/11/23 14:51:48.807
STEP: Creating a configMap that should not be mutated 08/11/23 14:51:48.851
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:51:48.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8031" for this suite. 08/11/23 14:51:48.866
STEP: Destroying namespace "webhook-8031-markers" for this suite. 08/11/23 14:51:48.874
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]","completed":172,"skipped":3223,"failed":0}
------------------------------
â€¢ [3.771 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing mutating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:655

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:45.152
    Aug 11 14:51:45.153: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:51:45.154
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:45.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:45.177
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:51:45.202
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:51:45.605
    STEP: Deploying the webhook pod 08/11/23 14:51:45.613
    STEP: Wait for the deployment to be ready 08/11/23 14:51:45.634
    Aug 11 14:51:45.644: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:51:47.655
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:51:47.671
    Aug 11 14:51:48.671: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing mutating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:655
    STEP: Listing all of the created validation webhooks 08/11/23 14:51:48.734
    STEP: Creating a configMap that should be mutated 08/11/23 14:51:48.754
    STEP: Deleting the collection of validation webhooks 08/11/23 14:51:48.807
    STEP: Creating a configMap that should not be mutated 08/11/23 14:51:48.851
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:51:48.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8031" for this suite. 08/11/23 14:51:48.866
    STEP: Destroying namespace "webhook-8031-markers" for this suite. 08/11/23 14:51:48.874
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:48.926
Aug 11 14:51:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:51:48.927
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:48.948
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:48.951
[It] custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269
Aug 11 14:51:48.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:51:52.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9278" for this suite. 08/11/23 14:51:52.068
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]","completed":173,"skipped":3235,"failed":0}
------------------------------
â€¢ [3.150 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  custom resource defaulting for requests and from storage works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:269

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:48.926
    Aug 11 14:51:48.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:51:48.927
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:48.948
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:48.951
    [It] custom resource defaulting for requests and from storage works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:269
    Aug 11 14:51:48.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:51:52.064: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9278" for this suite. 08/11/23 14:51:52.068
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:52.076
Aug 11 14:51:52.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename var-expansion 08/11/23 14:51:52.077
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:52.093
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:52.097
[It] should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111
STEP: Creating a pod to test substitution in volume subpath 08/11/23 14:51:52.1
Aug 11 14:51:52.108: INFO: Waiting up to 5m0s for pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8" in namespace "var-expansion-314" to be "Succeeded or Failed"
Aug 11 14:51:52.114: INFO: Pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072519ms
Aug 11 14:51:54.118: INFO: Pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010498452s
Aug 11 14:51:56.119: INFO: Pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010792889s
STEP: Saw pod success 08/11/23 14:51:56.119
Aug 11 14:51:56.119: INFO: Pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8" satisfied condition "Succeeded or Failed"
Aug 11 14:51:56.122: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8 container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:51:56.132
Aug 11 14:51:56.146: INFO: Waiting for pod var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8 to disappear
Aug 11 14:51:56.148: INFO: Pod var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8 no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 11 14:51:56.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-314" for this suite. 08/11/23 14:51:56.152
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]","completed":174,"skipped":3238,"failed":0}
------------------------------
â€¢ [4.083 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a volume subpath [Conformance]
  test/e2e/common/node/expansion.go:111

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:52.076
    Aug 11 14:51:52.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename var-expansion 08/11/23 14:51:52.077
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:52.093
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:52.097
    [It] should allow substituting values in a volume subpath [Conformance]
      test/e2e/common/node/expansion.go:111
    STEP: Creating a pod to test substitution in volume subpath 08/11/23 14:51:52.1
    Aug 11 14:51:52.108: INFO: Waiting up to 5m0s for pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8" in namespace "var-expansion-314" to be "Succeeded or Failed"
    Aug 11 14:51:52.114: INFO: Pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.072519ms
    Aug 11 14:51:54.118: INFO: Pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010498452s
    Aug 11 14:51:56.119: INFO: Pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010792889s
    STEP: Saw pod success 08/11/23 14:51:56.119
    Aug 11 14:51:56.119: INFO: Pod "var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8" satisfied condition "Succeeded or Failed"
    Aug 11 14:51:56.122: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:51:56.132
    Aug 11 14:51:56.146: INFO: Waiting for pod var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8 to disappear
    Aug 11 14:51:56.148: INFO: Pod var-expansion-a71b11ec-48ef-4792-96c4-52208fbc1df8 no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 11 14:51:56.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-314" for this suite. 08/11/23 14:51:56.152
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:56.161
Aug 11 14:51:56.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 14:51:56.162
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:56.177
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:56.179
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 14:51:56.193
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:51:56.327
STEP: Deploying the webhook pod 08/11/23 14:51:56.333
STEP: Wait for the deployment to be ready 08/11/23 14:51:56.347
Aug 11 14:51:56.360: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 14:51:58.37
STEP: Verifying the service has paired with the endpoint 08/11/23 14:51:58.385
Aug 11 14:51:59.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507
STEP: Creating a mutating webhook configuration 08/11/23 14:51:59.391
STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/11/23 14:51:59.415
STEP: Creating a configMap that should not be mutated 08/11/23 14:51:59.422
STEP: Patching a mutating webhook configuration's rules to include the create operation 08/11/23 14:51:59.433
STEP: Creating a configMap that should be mutated 08/11/23 14:51:59.441
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:51:59.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1282" for this suite. 08/11/23 14:51:59.476
STEP: Destroying namespace "webhook-1282-markers" for this suite. 08/11/23 14:51:59.483
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]","completed":175,"skipped":3286,"failed":0}
------------------------------
â€¢ [3.379 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  patching/updating a mutating webhook should work [Conformance]
  test/e2e/apimachinery/webhook.go:507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:56.161
    Aug 11 14:51:56.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 14:51:56.162
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:56.177
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:56.179
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 14:51:56.193
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 14:51:56.327
    STEP: Deploying the webhook pod 08/11/23 14:51:56.333
    STEP: Wait for the deployment to be ready 08/11/23 14:51:56.347
    Aug 11 14:51:56.360: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 14:51:58.37
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:51:58.385
    Aug 11 14:51:59.386: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] patching/updating a mutating webhook should work [Conformance]
      test/e2e/apimachinery/webhook.go:507
    STEP: Creating a mutating webhook configuration 08/11/23 14:51:59.391
    STEP: Updating a mutating webhook configuration's rules to not include the create operation 08/11/23 14:51:59.415
    STEP: Creating a configMap that should not be mutated 08/11/23 14:51:59.422
    STEP: Patching a mutating webhook configuration's rules to include the create operation 08/11/23 14:51:59.433
    STEP: Creating a configMap that should be mutated 08/11/23 14:51:59.441
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:51:59.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1282" for this suite. 08/11/23 14:51:59.476
    STEP: Destroying namespace "webhook-1282-markers" for this suite. 08/11/23 14:51:59.483
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:51:59.544
Aug 11 14:51:59.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:51:59.545
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:59.561
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:59.564
[It] should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220
STEP: Counting existing ResourceQuota 08/11/23 14:51:59.567
STEP: Creating a ResourceQuota 08/11/23 14:52:04.571
STEP: Ensuring resource quota status is calculated 08/11/23 14:52:04.578
STEP: Creating a Pod that fits quota 08/11/23 14:52:06.583
STEP: Ensuring ResourceQuota status captures the pod usage 08/11/23 14:52:06.597
STEP: Not allowing a pod to be created that exceeds remaining quota 08/11/23 14:52:08.602
STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/11/23 14:52:08.605
STEP: Ensuring a pod cannot update its resource requirements 08/11/23 14:52:08.607
STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/11/23 14:52:08.61
STEP: Deleting the pod 08/11/23 14:52:10.615
STEP: Ensuring resource quota status released the pod usage 08/11/23 14:52:10.632
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 14:52:12.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-7744" for this suite. 08/11/23 14:52:12.642
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]","completed":176,"skipped":3310,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.106 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a pod. [Conformance]
  test/e2e/apimachinery/resource_quota.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:51:59.544
    Aug 11 14:51:59.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:51:59.545
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:51:59.561
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:51:59.564
    [It] should create a ResourceQuota and capture the life of a pod. [Conformance]
      test/e2e/apimachinery/resource_quota.go:220
    STEP: Counting existing ResourceQuota 08/11/23 14:51:59.567
    STEP: Creating a ResourceQuota 08/11/23 14:52:04.571
    STEP: Ensuring resource quota status is calculated 08/11/23 14:52:04.578
    STEP: Creating a Pod that fits quota 08/11/23 14:52:06.583
    STEP: Ensuring ResourceQuota status captures the pod usage 08/11/23 14:52:06.597
    STEP: Not allowing a pod to be created that exceeds remaining quota 08/11/23 14:52:08.602
    STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) 08/11/23 14:52:08.605
    STEP: Ensuring a pod cannot update its resource requirements 08/11/23 14:52:08.607
    STEP: Ensuring attempts to update pod resource requirements did not change quota usage 08/11/23 14:52:08.61
    STEP: Deleting the pod 08/11/23 14:52:10.615
    STEP: Ensuring resource quota status released the pod usage 08/11/23 14:52:10.632
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 14:52:12.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-7744" for this suite. 08/11/23 14:52:12.642
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:52:12.651
Aug 11 14:52:12.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 14:52:12.652
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:12.667
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:12.67
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481
STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9912 08/11/23 14:52:12.672
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/11/23 14:52:12.688
STEP: creating service externalsvc in namespace services-9912 08/11/23 14:52:12.688
STEP: creating replication controller externalsvc in namespace services-9912 08/11/23 14:52:12.708
I0811 14:52:12.716254      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9912, replica count: 2
I0811 14:52:15.767546      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the ClusterIP service to type=ExternalName 08/11/23 14:52:15.771
Aug 11 14:52:15.791: INFO: Creating new exec pod
Aug 11 14:52:15.800: INFO: Waiting up to 5m0s for pod "execpodfq9w9" in namespace "services-9912" to be "running"
Aug 11 14:52:15.808: INFO: Pod "execpodfq9w9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.39881ms
Aug 11 14:52:17.813: INFO: Pod "execpodfq9w9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012745293s
Aug 11 14:52:17.813: INFO: Pod "execpodfq9w9" satisfied condition "running"
Aug 11 14:52:17.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-9912 exec execpodfq9w9 -- /bin/sh -x -c nslookup clusterip-service.services-9912.svc.cluster.local'
Aug 11 14:52:17.962: INFO: stderr: "+ nslookup clusterip-service.services-9912.svc.cluster.local\n"
Aug 11 14:52:17.962: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9912.svc.cluster.local\tcanonical name = externalsvc.services-9912.svc.cluster.local.\nName:\texternalsvc.services-9912.svc.cluster.local\nAddress: 10.106.183.28\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-9912, will wait for the garbage collector to delete the pods 08/11/23 14:52:17.962
Aug 11 14:52:18.024: INFO: Deleting ReplicationController externalsvc took: 7.345248ms
Aug 11 14:52:18.125: INFO: Terminating ReplicationController externalsvc pods took: 101.111706ms
Aug 11 14:52:20.048: INFO: Cleaning up the ClusterIP to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 14:52:20.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-9912" for this suite. 08/11/23 14:52:20.064
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]","completed":177,"skipped":3343,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.419 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ClusterIP to ExternalName [Conformance]
  test/e2e/network/service.go:1481

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:52:12.651
    Aug 11 14:52:12.651: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 14:52:12.652
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:12.667
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:12.67
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ClusterIP to ExternalName [Conformance]
      test/e2e/network/service.go:1481
    STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9912 08/11/23 14:52:12.672
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/11/23 14:52:12.688
    STEP: creating service externalsvc in namespace services-9912 08/11/23 14:52:12.688
    STEP: creating replication controller externalsvc in namespace services-9912 08/11/23 14:52:12.708
    I0811 14:52:12.716254      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-9912, replica count: 2
    I0811 14:52:15.767546      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the ClusterIP service to type=ExternalName 08/11/23 14:52:15.771
    Aug 11 14:52:15.791: INFO: Creating new exec pod
    Aug 11 14:52:15.800: INFO: Waiting up to 5m0s for pod "execpodfq9w9" in namespace "services-9912" to be "running"
    Aug 11 14:52:15.808: INFO: Pod "execpodfq9w9": Phase="Pending", Reason="", readiness=false. Elapsed: 7.39881ms
    Aug 11 14:52:17.813: INFO: Pod "execpodfq9w9": Phase="Running", Reason="", readiness=true. Elapsed: 2.012745293s
    Aug 11 14:52:17.813: INFO: Pod "execpodfq9w9" satisfied condition "running"
    Aug 11 14:52:17.813: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-9912 exec execpodfq9w9 -- /bin/sh -x -c nslookup clusterip-service.services-9912.svc.cluster.local'
    Aug 11 14:52:17.962: INFO: stderr: "+ nslookup clusterip-service.services-9912.svc.cluster.local\n"
    Aug 11 14:52:17.962: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9912.svc.cluster.local\tcanonical name = externalsvc.services-9912.svc.cluster.local.\nName:\texternalsvc.services-9912.svc.cluster.local\nAddress: 10.106.183.28\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-9912, will wait for the garbage collector to delete the pods 08/11/23 14:52:17.962
    Aug 11 14:52:18.024: INFO: Deleting ReplicationController externalsvc took: 7.345248ms
    Aug 11 14:52:18.125: INFO: Terminating ReplicationController externalsvc pods took: 101.111706ms
    Aug 11 14:52:20.048: INFO: Cleaning up the ClusterIP to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 14:52:20.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-9912" for this suite. 08/11/23 14:52:20.064
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:52:20.071
Aug 11 14:52:20.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 14:52:20.072
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:20.086
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:20.088
[It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438
STEP: Counting existing ResourceQuota 08/11/23 14:52:20.091
STEP: Creating a ResourceQuota 08/11/23 14:52:25.095
STEP: Ensuring resource quota status is calculated 08/11/23 14:52:25.1
STEP: Creating a ReplicaSet 08/11/23 14:52:27.105
STEP: Ensuring resource quota status captures replicaset creation 08/11/23 14:52:27.117
STEP: Deleting a ReplicaSet 08/11/23 14:52:29.122
STEP: Ensuring resource quota status released usage 08/11/23 14:52:29.128
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 14:52:31.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-3861" for this suite. 08/11/23 14:52:31.139
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]","completed":178,"skipped":3347,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.075 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a replica set. [Conformance]
  test/e2e/apimachinery/resource_quota.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:52:20.071
    Aug 11 14:52:20.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 14:52:20.072
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:20.086
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:20.088
    [It] should create a ResourceQuota and capture the life of a replica set. [Conformance]
      test/e2e/apimachinery/resource_quota.go:438
    STEP: Counting existing ResourceQuota 08/11/23 14:52:20.091
    STEP: Creating a ResourceQuota 08/11/23 14:52:25.095
    STEP: Ensuring resource quota status is calculated 08/11/23 14:52:25.1
    STEP: Creating a ReplicaSet 08/11/23 14:52:27.105
    STEP: Ensuring resource quota status captures replicaset creation 08/11/23 14:52:27.117
    STEP: Deleting a ReplicaSet 08/11/23 14:52:29.122
    STEP: Ensuring resource quota status released usage 08/11/23 14:52:29.128
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 14:52:31.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-3861" for this suite. 08/11/23 14:52:31.139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl logs
  should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:52:31.147
Aug 11 14:52:31.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:52:31.148
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:31.165
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:31.167
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1570
STEP: creating an pod 08/11/23 14:52:31.17
Aug 11 14:52:31.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
Aug 11 14:52:31.241: INFO: stderr: ""
Aug 11 14:52:31.241: INFO: stdout: "pod/logs-generator created\n"
[It] should be able to retrieve and filter logs  [Conformance]
  test/e2e/kubectl/kubectl.go:1590
STEP: Waiting for log generator to start. 08/11/23 14:52:31.241
Aug 11 14:52:31.241: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
Aug 11 14:52:31.241: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2444" to be "running and ready, or succeeded"
Aug 11 14:52:31.249: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.250266ms
Aug 11 14:52:31.249: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'constell-d93e7e1d-worker-d314547c-0lc3' to be 'Running' but was 'Pending'
Aug 11 14:52:33.252: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.011112854s
Aug 11 14:52:33.253: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
Aug 11 14:52:33.253: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
STEP: checking for a matching strings 08/11/23 14:52:33.253
Aug 11 14:52:33.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator'
Aug 11 14:52:33.333: INFO: stderr: ""
Aug 11 14:52:33.333: INFO: stdout: "I0811 14:52:31.894402       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/z6dx 479\nI0811 14:52:32.094585       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/wt6 507\nI0811 14:52:32.295145       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/rph 370\nI0811 14:52:32.495503       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/8597 528\nI0811 14:52:32.694902       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/h42 463\nI0811 14:52:32.895246       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/b8x 575\nI0811 14:52:33.094515       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/cmkt 308\n"
STEP: limiting log lines 08/11/23 14:52:33.333
Aug 11 14:52:33.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --tail=1'
Aug 11 14:52:33.402: INFO: stderr: ""
Aug 11 14:52:33.402: INFO: stdout: "I0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\n"
Aug 11 14:52:33.402: INFO: got output "I0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\n"
STEP: limiting log bytes 08/11/23 14:52:33.402
Aug 11 14:52:33.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --limit-bytes=1'
Aug 11 14:52:33.464: INFO: stderr: ""
Aug 11 14:52:33.464: INFO: stdout: "I"
Aug 11 14:52:33.464: INFO: got output "I"
STEP: exposing timestamps 08/11/23 14:52:33.464
Aug 11 14:52:33.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --tail=1 --timestamps'
Aug 11 14:52:33.532: INFO: stderr: ""
Aug 11 14:52:33.532: INFO: stdout: "2023-08-11T14:52:33.294984493Z I0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\n"
Aug 11 14:52:33.532: INFO: got output "2023-08-11T14:52:33.294984493Z I0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\n"
STEP: restricting to a time range 08/11/23 14:52:33.532
Aug 11 14:52:36.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --since=1s'
Aug 11 14:52:36.095: INFO: stderr: ""
Aug 11 14:52:36.095: INFO: stdout: "I0811 14:52:35.094595       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/bzl 518\nI0811 14:52:35.294965       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/qsp 361\nI0811 14:52:35.495352       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/696 555\nI0811 14:52:35.694632       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/xlnn 439\nI0811 14:52:35.894897       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jzg 386\n"
Aug 11 14:52:36.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --since=24h'
Aug 11 14:52:36.159: INFO: stderr: ""
Aug 11 14:52:36.159: INFO: stdout: "I0811 14:52:31.894402       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/z6dx 479\nI0811 14:52:32.094585       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/wt6 507\nI0811 14:52:32.295145       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/rph 370\nI0811 14:52:32.495503       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/8597 528\nI0811 14:52:32.694902       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/h42 463\nI0811 14:52:32.895246       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/b8x 575\nI0811 14:52:33.094515       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/cmkt 308\nI0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\nI0811 14:52:33.495207       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/t866 458\nI0811 14:52:33.694503       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/c97g 506\nI0811 14:52:33.894781       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/dxkf 246\nI0811 14:52:34.095138       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/s96c 291\nI0811 14:52:34.295501       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/k2fs 569\nI0811 14:52:34.494867       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/2dg 209\nI0811 14:52:34.695228       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/fhhg 545\nI0811 14:52:34.895464       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4vd 521\nI0811 14:52:35.094595       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/bzl 518\nI0811 14:52:35.294965       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/qsp 361\nI0811 14:52:35.495352       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/696 555\nI0811 14:52:35.694632       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/xlnn 439\nI0811 14:52:35.894897       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jzg 386\n"
[AfterEach] Kubectl logs
  test/e2e/kubectl/kubectl.go:1575
Aug 11 14:52:36.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 delete pod logs-generator'
Aug 11 14:52:37.015: INFO: stderr: ""
Aug 11 14:52:37.015: INFO: stdout: "pod \"logs-generator\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:52:37.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2444" for this suite. 08/11/23 14:52:37.019
{"msg":"PASSED [sig-cli] Kubectl client Kubectl logs should be able to retrieve and filter logs  [Conformance]","completed":179,"skipped":3360,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.879 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl logs
  test/e2e/kubectl/kubectl.go:1567
    should be able to retrieve and filter logs  [Conformance]
    test/e2e/kubectl/kubectl.go:1590

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:52:31.147
    Aug 11 14:52:31.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:52:31.148
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:31.165
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:31.167
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1570
    STEP: creating an pod 08/11/23 14:52:31.17
    Aug 11 14:52:31.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.40 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
    Aug 11 14:52:31.241: INFO: stderr: ""
    Aug 11 14:52:31.241: INFO: stdout: "pod/logs-generator created\n"
    [It] should be able to retrieve and filter logs  [Conformance]
      test/e2e/kubectl/kubectl.go:1590
    STEP: Waiting for log generator to start. 08/11/23 14:52:31.241
    Aug 11 14:52:31.241: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
    Aug 11 14:52:31.241: INFO: Waiting up to 5m0s for pod "logs-generator" in namespace "kubectl-2444" to be "running and ready, or succeeded"
    Aug 11 14:52:31.249: INFO: Pod "logs-generator": Phase="Pending", Reason="", readiness=false. Elapsed: 7.250266ms
    Aug 11 14:52:31.249: INFO: Error evaluating pod condition running and ready, or succeeded: want pod 'logs-generator' on 'constell-d93e7e1d-worker-d314547c-0lc3' to be 'Running' but was 'Pending'
    Aug 11 14:52:33.252: INFO: Pod "logs-generator": Phase="Running", Reason="", readiness=true. Elapsed: 2.011112854s
    Aug 11 14:52:33.253: INFO: Pod "logs-generator" satisfied condition "running and ready, or succeeded"
    Aug 11 14:52:33.253: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
    STEP: checking for a matching strings 08/11/23 14:52:33.253
    Aug 11 14:52:33.253: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator'
    Aug 11 14:52:33.333: INFO: stderr: ""
    Aug 11 14:52:33.333: INFO: stdout: "I0811 14:52:31.894402       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/z6dx 479\nI0811 14:52:32.094585       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/wt6 507\nI0811 14:52:32.295145       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/rph 370\nI0811 14:52:32.495503       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/8597 528\nI0811 14:52:32.694902       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/h42 463\nI0811 14:52:32.895246       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/b8x 575\nI0811 14:52:33.094515       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/cmkt 308\n"
    STEP: limiting log lines 08/11/23 14:52:33.333
    Aug 11 14:52:33.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --tail=1'
    Aug 11 14:52:33.402: INFO: stderr: ""
    Aug 11 14:52:33.402: INFO: stdout: "I0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\n"
    Aug 11 14:52:33.402: INFO: got output "I0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\n"
    STEP: limiting log bytes 08/11/23 14:52:33.402
    Aug 11 14:52:33.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --limit-bytes=1'
    Aug 11 14:52:33.464: INFO: stderr: ""
    Aug 11 14:52:33.464: INFO: stdout: "I"
    Aug 11 14:52:33.464: INFO: got output "I"
    STEP: exposing timestamps 08/11/23 14:52:33.464
    Aug 11 14:52:33.465: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --tail=1 --timestamps'
    Aug 11 14:52:33.532: INFO: stderr: ""
    Aug 11 14:52:33.532: INFO: stdout: "2023-08-11T14:52:33.294984493Z I0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\n"
    Aug 11 14:52:33.532: INFO: got output "2023-08-11T14:52:33.294984493Z I0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\n"
    STEP: restricting to a time range 08/11/23 14:52:33.532
    Aug 11 14:52:36.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --since=1s'
    Aug 11 14:52:36.095: INFO: stderr: ""
    Aug 11 14:52:36.095: INFO: stdout: "I0811 14:52:35.094595       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/bzl 518\nI0811 14:52:35.294965       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/qsp 361\nI0811 14:52:35.495352       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/696 555\nI0811 14:52:35.694632       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/xlnn 439\nI0811 14:52:35.894897       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jzg 386\n"
    Aug 11 14:52:36.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 logs logs-generator logs-generator --since=24h'
    Aug 11 14:52:36.159: INFO: stderr: ""
    Aug 11 14:52:36.159: INFO: stdout: "I0811 14:52:31.894402       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/z6dx 479\nI0811 14:52:32.094585       1 logs_generator.go:76] 1 GET /api/v1/namespaces/default/pods/wt6 507\nI0811 14:52:32.295145       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/rph 370\nI0811 14:52:32.495503       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/8597 528\nI0811 14:52:32.694902       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/kube-system/pods/h42 463\nI0811 14:52:32.895246       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/b8x 575\nI0811 14:52:33.094515       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/cmkt 308\nI0811 14:52:33.294860       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/kube-system/pods/2w5 542\nI0811 14:52:33.495207       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/default/pods/t866 458\nI0811 14:52:33.694503       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/c97g 506\nI0811 14:52:33.894781       1 logs_generator.go:76] 10 GET /api/v1/namespaces/default/pods/dxkf 246\nI0811 14:52:34.095138       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/s96c 291\nI0811 14:52:34.295501       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/ns/pods/k2fs 569\nI0811 14:52:34.494867       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/2dg 209\nI0811 14:52:34.695228       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/fhhg 545\nI0811 14:52:34.895464       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/4vd 521\nI0811 14:52:35.094595       1 logs_generator.go:76] 16 POST /api/v1/namespaces/kube-system/pods/bzl 518\nI0811 14:52:35.294965       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/qsp 361\nI0811 14:52:35.495352       1 logs_generator.go:76] 18 POST /api/v1/namespaces/default/pods/696 555\nI0811 14:52:35.694632       1 logs_generator.go:76] 19 GET /api/v1/namespaces/ns/pods/xlnn 439\nI0811 14:52:35.894897       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/jzg 386\n"
    [AfterEach] Kubectl logs
      test/e2e/kubectl/kubectl.go:1575
    Aug 11 14:52:36.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2444 delete pod logs-generator'
    Aug 11 14:52:37.015: INFO: stderr: ""
    Aug 11 14:52:37.015: INFO: stdout: "pod \"logs-generator\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:52:37.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2444" for this suite. 08/11/23 14:52:37.019
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-storage] Projected secret
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:52:37.026
Aug 11 14:52:37.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:52:37.027
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:37.039
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:37.041
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214
STEP: Creating secret with name s-test-opt-del-59c62787-6754-4b92-ae09-dd03d55a88bc 08/11/23 14:52:37.049
STEP: Creating secret with name s-test-opt-upd-0fc940d9-cf2b-4fbe-999b-65186d21ea57 08/11/23 14:52:37.054
STEP: Creating the pod 08/11/23 14:52:37.059
Aug 11 14:52:37.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1" in namespace "projected-8380" to be "running and ready"
Aug 11 14:52:37.070: INFO: Pod "pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607291ms
Aug 11 14:52:37.070: INFO: The phase of Pod pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:52:39.075: INFO: Pod "pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007522783s
Aug 11 14:52:39.075: INFO: The phase of Pod pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1 is Running (Ready = true)
Aug 11 14:52:39.075: INFO: Pod "pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1" satisfied condition "running and ready"
STEP: Deleting secret s-test-opt-del-59c62787-6754-4b92-ae09-dd03d55a88bc 08/11/23 14:52:39.11
STEP: Updating secret s-test-opt-upd-0fc940d9-cf2b-4fbe-999b-65186d21ea57 08/11/23 14:52:39.117
STEP: Creating secret with name s-test-opt-create-94085c27-d93d-48a5-8a0a-5f41998b2e66 08/11/23 14:52:39.122
STEP: waiting to observe update in volume 08/11/23 14:52:39.126
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 11 14:54:07.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8380" for this suite. 08/11/23 14:54:07.685
{"msg":"PASSED [sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":180,"skipped":3363,"failed":0}
------------------------------
â€¢ [SLOW TEST] [90.665 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:214

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:52:37.026
    Aug 11 14:52:37.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:52:37.027
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:52:37.039
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:52:37.041
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:214
    STEP: Creating secret with name s-test-opt-del-59c62787-6754-4b92-ae09-dd03d55a88bc 08/11/23 14:52:37.049
    STEP: Creating secret with name s-test-opt-upd-0fc940d9-cf2b-4fbe-999b-65186d21ea57 08/11/23 14:52:37.054
    STEP: Creating the pod 08/11/23 14:52:37.059
    Aug 11 14:52:37.067: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1" in namespace "projected-8380" to be "running and ready"
    Aug 11 14:52:37.070: INFO: Pod "pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.607291ms
    Aug 11 14:52:37.070: INFO: The phase of Pod pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:52:39.075: INFO: Pod "pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007522783s
    Aug 11 14:52:39.075: INFO: The phase of Pod pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1 is Running (Ready = true)
    Aug 11 14:52:39.075: INFO: Pod "pod-projected-secrets-c7183b10-096e-4dd3-bc4c-c0c6a73c47e1" satisfied condition "running and ready"
    STEP: Deleting secret s-test-opt-del-59c62787-6754-4b92-ae09-dd03d55a88bc 08/11/23 14:52:39.11
    STEP: Updating secret s-test-opt-upd-0fc940d9-cf2b-4fbe-999b-65186d21ea57 08/11/23 14:52:39.117
    STEP: Creating secret with name s-test-opt-create-94085c27-d93d-48a5-8a0a-5f41998b2e66 08/11/23 14:52:39.122
    STEP: waiting to observe update in volume 08/11/23 14:52:39.126
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 11 14:54:07.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8380" for this suite. 08/11/23 14:54:07.685
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:54:07.692
Aug 11 14:54:07.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:54:07.693
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:07.708
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:07.71
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173
STEP: Creating configMap with name cm-test-opt-del-bb9deaea-0974-4299-b90d-c16024dae039 08/11/23 14:54:07.717
STEP: Creating configMap with name cm-test-opt-upd-1b0d6a9e-bd62-4365-b54d-17a6b1fea97b 08/11/23 14:54:07.722
STEP: Creating the pod 08/11/23 14:54:07.726
Aug 11 14:54:07.735: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25" in namespace "projected-8008" to be "running and ready"
Aug 11 14:54:07.741: INFO: Pod "pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25": Phase="Pending", Reason="", readiness=false. Elapsed: 5.560763ms
Aug 11 14:54:07.741: INFO: The phase of Pod pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:54:09.746: INFO: Pod "pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25": Phase="Running", Reason="", readiness=true. Elapsed: 2.010737361s
Aug 11 14:54:09.746: INFO: The phase of Pod pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25 is Running (Ready = true)
Aug 11 14:54:09.746: INFO: Pod "pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-bb9deaea-0974-4299-b90d-c16024dae039 08/11/23 14:54:09.774
STEP: Updating configmap cm-test-opt-upd-1b0d6a9e-bd62-4365-b54d-17a6b1fea97b 08/11/23 14:54:09.78
STEP: Creating configMap with name cm-test-opt-create-524bd8cd-b0bf-42dd-8674-c7f57333914f 08/11/23 14:54:09.785
STEP: waiting to observe update in volume 08/11/23 14:54:09.789
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 11 14:54:11.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-8008" for this suite. 08/11/23 14:54:11.828
{"msg":"PASSED [sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":181,"skipped":3377,"failed":0}
------------------------------
â€¢ [4.142 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:173

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:54:07.692
    Aug 11 14:54:07.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:54:07.693
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:07.708
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:07.71
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:173
    STEP: Creating configMap with name cm-test-opt-del-bb9deaea-0974-4299-b90d-c16024dae039 08/11/23 14:54:07.717
    STEP: Creating configMap with name cm-test-opt-upd-1b0d6a9e-bd62-4365-b54d-17a6b1fea97b 08/11/23 14:54:07.722
    STEP: Creating the pod 08/11/23 14:54:07.726
    Aug 11 14:54:07.735: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25" in namespace "projected-8008" to be "running and ready"
    Aug 11 14:54:07.741: INFO: Pod "pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25": Phase="Pending", Reason="", readiness=false. Elapsed: 5.560763ms
    Aug 11 14:54:07.741: INFO: The phase of Pod pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:54:09.746: INFO: Pod "pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25": Phase="Running", Reason="", readiness=true. Elapsed: 2.010737361s
    Aug 11 14:54:09.746: INFO: The phase of Pod pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25 is Running (Ready = true)
    Aug 11 14:54:09.746: INFO: Pod "pod-projected-configmaps-0045735a-4c79-4c56-9332-95e3faf8cd25" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-bb9deaea-0974-4299-b90d-c16024dae039 08/11/23 14:54:09.774
    STEP: Updating configmap cm-test-opt-upd-1b0d6a9e-bd62-4365-b54d-17a6b1fea97b 08/11/23 14:54:09.78
    STEP: Creating configMap with name cm-test-opt-create-524bd8cd-b0bf-42dd-8674-c7f57333914f 08/11/23 14:54:09.785
    STEP: waiting to observe update in volume 08/11/23 14:54:09.789
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 11 14:54:11.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-8008" for this suite. 08/11/23 14:54:11.828
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:54:11.836
Aug 11 14:54:11.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:54:11.837
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:11.852
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:11.854
[It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66
STEP: Creating projection with secret that has name projected-secret-test-f5d5fa0f-e4a9-4f10-92a2-42372db7e64f 08/11/23 14:54:11.856
STEP: Creating a pod to test consume secrets 08/11/23 14:54:11.862
Aug 11 14:54:11.873: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e" in namespace "projected-5376" to be "Succeeded or Failed"
Aug 11 14:54:11.876: INFO: Pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20772ms
Aug 11 14:54:13.880: INFO: Pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007856742s
Aug 11 14:54:15.881: INFO: Pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00858738s
STEP: Saw pod success 08/11/23 14:54:15.881
Aug 11 14:54:15.881: INFO: Pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e" satisfied condition "Succeeded or Failed"
Aug 11 14:54:15.884: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:54:15.906
Aug 11 14:54:15.919: INFO: Waiting for pod pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e to disappear
Aug 11 14:54:15.922: INFO: Pod pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 11 14:54:15.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5376" for this suite. 08/11/23 14:54:15.926
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]","completed":182,"skipped":3399,"failed":0}
------------------------------
â€¢ [4.097 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:66

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:54:11.836
    Aug 11 14:54:11.836: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:54:11.837
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:11.852
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:11.854
    [It] should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:66
    STEP: Creating projection with secret that has name projected-secret-test-f5d5fa0f-e4a9-4f10-92a2-42372db7e64f 08/11/23 14:54:11.856
    STEP: Creating a pod to test consume secrets 08/11/23 14:54:11.862
    Aug 11 14:54:11.873: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e" in namespace "projected-5376" to be "Succeeded or Failed"
    Aug 11 14:54:11.876: INFO: Pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20772ms
    Aug 11 14:54:13.880: INFO: Pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007856742s
    Aug 11 14:54:15.881: INFO: Pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00858738s
    STEP: Saw pod success 08/11/23 14:54:15.881
    Aug 11 14:54:15.881: INFO: Pod "pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e" satisfied condition "Succeeded or Failed"
    Aug 11 14:54:15.884: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-wzlp pod pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:54:15.906
    Aug 11 14:54:15.919: INFO: Waiting for pod pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e to disappear
    Aug 11 14:54:15.922: INFO: Pod pod-projected-secrets-5feca750-09e3-4c96-b15a-c7e20630525e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 11 14:54:15.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5376" for this suite. 08/11/23 14:54:15.926
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:54:15.935
Aug 11 14:54:15.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename cronjob 08/11/23 14:54:15.936
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:15.949
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:15.951
[It] should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160
STEP: Creating a ReplaceConcurrent cronjob 08/11/23 14:54:15.954
STEP: Ensuring a job is scheduled 08/11/23 14:54:15.959
STEP: Ensuring exactly one is scheduled 08/11/23 14:55:01.964
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/11/23 14:55:01.967
STEP: Ensuring the job is replaced with a new one 08/11/23 14:55:01.969
STEP: Removing cronjob 08/11/23 14:56:01.975
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 11 14:56:01.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-7489" for this suite. 08/11/23 14:56:01.986
{"msg":"PASSED [sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]","completed":183,"skipped":3450,"failed":0}
------------------------------
â€¢ [SLOW TEST] [106.060 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should replace jobs when ReplaceConcurrent [Conformance]
  test/e2e/apps/cronjob.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:54:15.935
    Aug 11 14:54:15.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename cronjob 08/11/23 14:54:15.936
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:54:15.949
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:54:15.951
    [It] should replace jobs when ReplaceConcurrent [Conformance]
      test/e2e/apps/cronjob.go:160
    STEP: Creating a ReplaceConcurrent cronjob 08/11/23 14:54:15.954
    STEP: Ensuring a job is scheduled 08/11/23 14:54:15.959
    STEP: Ensuring exactly one is scheduled 08/11/23 14:55:01.964
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/11/23 14:55:01.967
    STEP: Ensuring the job is replaced with a new one 08/11/23 14:55:01.969
    STEP: Removing cronjob 08/11/23 14:56:01.975
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 11 14:56:01.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-7489" for this suite. 08/11/23 14:56:01.986
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:56:01.996
Aug 11 14:56:01.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename subpath 08/11/23 14:56:01.997
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:02.017
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:02.019
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 14:56:02.021
[It] should support subpaths with configmap pod [Conformance]
  test/e2e/storage/subpath.go:70
STEP: Creating pod pod-subpath-test-configmap-cz5x 08/11/23 14:56:02.031
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:56:02.031
Aug 11 14:56:02.039: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cz5x" in namespace "subpath-6973" to be "Succeeded or Failed"
Aug 11 14:56:02.043: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Pending", Reason="", readiness=false. Elapsed: 3.991554ms
Aug 11 14:56:04.048: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 2.008988977s
Aug 11 14:56:06.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 4.009310343s
Aug 11 14:56:08.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 6.00932523s
Aug 11 14:56:10.048: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 8.008373688s
Aug 11 14:56:12.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 10.009438607s
Aug 11 14:56:14.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 12.009255068s
Aug 11 14:56:16.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 14.00942648s
Aug 11 14:56:18.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 16.009566511s
Aug 11 14:56:20.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 18.009412663s
Aug 11 14:56:22.048: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 20.009145061s
Aug 11 14:56:24.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=false. Elapsed: 22.009729517s
Aug 11 14:56:26.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009868849s
STEP: Saw pod success 08/11/23 14:56:26.049
Aug 11 14:56:26.049: INFO: Pod "pod-subpath-test-configmap-cz5x" satisfied condition "Succeeded or Failed"
Aug 11 14:56:26.052: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-configmap-cz5x container test-container-subpath-configmap-cz5x: <nil>
STEP: delete the pod 08/11/23 14:56:26.07
Aug 11 14:56:26.085: INFO: Waiting for pod pod-subpath-test-configmap-cz5x to disappear
Aug 11 14:56:26.088: INFO: Pod pod-subpath-test-configmap-cz5x no longer exists
STEP: Deleting pod pod-subpath-test-configmap-cz5x 08/11/23 14:56:26.088
Aug 11 14:56:26.088: INFO: Deleting pod "pod-subpath-test-configmap-cz5x" in namespace "subpath-6973"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 11 14:56:26.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-6973" for this suite. 08/11/23 14:56:26.095
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]","completed":184,"skipped":3479,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.106 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod [Conformance]
    test/e2e/storage/subpath.go:70

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:56:01.996
    Aug 11 14:56:01.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename subpath 08/11/23 14:56:01.997
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:02.017
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:02.019
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 14:56:02.021
    [It] should support subpaths with configmap pod [Conformance]
      test/e2e/storage/subpath.go:70
    STEP: Creating pod pod-subpath-test-configmap-cz5x 08/11/23 14:56:02.031
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:56:02.031
    Aug 11 14:56:02.039: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-cz5x" in namespace "subpath-6973" to be "Succeeded or Failed"
    Aug 11 14:56:02.043: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Pending", Reason="", readiness=false. Elapsed: 3.991554ms
    Aug 11 14:56:04.048: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 2.008988977s
    Aug 11 14:56:06.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 4.009310343s
    Aug 11 14:56:08.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 6.00932523s
    Aug 11 14:56:10.048: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 8.008373688s
    Aug 11 14:56:12.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 10.009438607s
    Aug 11 14:56:14.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 12.009255068s
    Aug 11 14:56:16.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 14.00942648s
    Aug 11 14:56:18.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 16.009566511s
    Aug 11 14:56:20.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 18.009412663s
    Aug 11 14:56:22.048: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=true. Elapsed: 20.009145061s
    Aug 11 14:56:24.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Running", Reason="", readiness=false. Elapsed: 22.009729517s
    Aug 11 14:56:26.049: INFO: Pod "pod-subpath-test-configmap-cz5x": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.009868849s
    STEP: Saw pod success 08/11/23 14:56:26.049
    Aug 11 14:56:26.049: INFO: Pod "pod-subpath-test-configmap-cz5x" satisfied condition "Succeeded or Failed"
    Aug 11 14:56:26.052: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-configmap-cz5x container test-container-subpath-configmap-cz5x: <nil>
    STEP: delete the pod 08/11/23 14:56:26.07
    Aug 11 14:56:26.085: INFO: Waiting for pod pod-subpath-test-configmap-cz5x to disappear
    Aug 11 14:56:26.088: INFO: Pod pod-subpath-test-configmap-cz5x no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-cz5x 08/11/23 14:56:26.088
    Aug 11 14:56:26.088: INFO: Deleting pod "pod-subpath-test-configmap-cz5x" in namespace "subpath-6973"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 11 14:56:26.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-6973" for this suite. 08/11/23 14:56:26.095
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:56:26.104
Aug 11 14:56:26.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-runtime 08/11/23 14:56:26.105
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:26.121
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:26.123
[It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:215
STEP: create the container 08/11/23 14:56:26.126
STEP: wait for the container to reach Failed 08/11/23 14:56:26.135
STEP: get the container status 08/11/23 14:56:29.155
STEP: the container should be terminated 08/11/23 14:56:29.158
STEP: the termination message should be set 08/11/23 14:56:29.158
Aug 11 14:56:29.159: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/11/23 14:56:29.159
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 11 14:56:29.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-16" for this suite. 08/11/23 14:56:29.179
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":185,"skipped":3508,"failed":0}
------------------------------
â€¢ [3.082 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:56:26.104
    Aug 11 14:56:26.104: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-runtime 08/11/23 14:56:26.105
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:26.121
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:26.123
    [It] should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:215
    STEP: create the container 08/11/23 14:56:26.126
    STEP: wait for the container to reach Failed 08/11/23 14:56:26.135
    STEP: get the container status 08/11/23 14:56:29.155
    STEP: the container should be terminated 08/11/23 14:56:29.158
    STEP: the termination message should be set 08/11/23 14:56:29.158
    Aug 11 14:56:29.159: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/11/23 14:56:29.159
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 11 14:56:29.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-16" for this suite. 08/11/23 14:56:29.179
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:56:29.187
Aug 11 14:56:29.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:56:29.188
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.205
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.208
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:56:29.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-3563" for this suite. 08/11/23 14:56:29.251
{"msg":"PASSED [sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]","completed":186,"skipped":3526,"failed":0}
------------------------------
â€¢ [0.070 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/configmap_volume.go:503

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:56:29.187
    Aug 11 14:56:29.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:56:29.188
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.205
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.208
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/configmap_volume.go:503
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:56:29.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-3563" for this suite. 08/11/23 14:56:29.251
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:56:29.258
Aug 11 14:56:29.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:56:29.259
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.274
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.277
[It] should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646
STEP: creating a ServiceAccount 08/11/23 14:56:29.279
STEP: watching for the ServiceAccount to be added 08/11/23 14:56:29.287
STEP: patching the ServiceAccount 08/11/23 14:56:29.289
STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/11/23 14:56:29.295
STEP: deleting the ServiceAccount 08/11/23 14:56:29.298
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 11 14:56:29.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-592" for this suite. 08/11/23 14:56:29.315
{"msg":"PASSED [sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]","completed":187,"skipped":3533,"failed":0}
------------------------------
â€¢ [0.062 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should run through the lifecycle of a ServiceAccount [Conformance]
  test/e2e/auth/service_accounts.go:646

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:56:29.258
    Aug 11 14:56:29.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 14:56:29.259
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.274
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.277
    [It] should run through the lifecycle of a ServiceAccount [Conformance]
      test/e2e/auth/service_accounts.go:646
    STEP: creating a ServiceAccount 08/11/23 14:56:29.279
    STEP: watching for the ServiceAccount to be added 08/11/23 14:56:29.287
    STEP: patching the ServiceAccount 08/11/23 14:56:29.289
    STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) 08/11/23 14:56:29.295
    STEP: deleting the ServiceAccount 08/11/23 14:56:29.298
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 11 14:56:29.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-592" for this suite. 08/11/23 14:56:29.315
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Lease
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[BeforeEach] [sig-node] Lease
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:56:29.32
Aug 11 14:56:29.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename lease-test 08/11/23 14:56:29.322
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.336
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.338
[It] lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72
[AfterEach] [sig-node] Lease
  test/e2e/framework/framework.go:187
Aug 11 14:56:29.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "lease-test-6391" for this suite. 08/11/23 14:56:29.393
{"msg":"PASSED [sig-node] Lease lease API should be available [Conformance]","completed":188,"skipped":3536,"failed":0}
------------------------------
â€¢ [0.078 seconds]
[sig-node] Lease
test/e2e/common/node/framework.go:23
  lease API should be available [Conformance]
  test/e2e/common/node/lease.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Lease
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:56:29.32
    Aug 11 14:56:29.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename lease-test 08/11/23 14:56:29.322
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.336
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.338
    [It] lease API should be available [Conformance]
      test/e2e/common/node/lease.go:72
    [AfterEach] [sig-node] Lease
      test/e2e/framework/framework.go:187
    Aug 11 14:56:29.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "lease-test-6391" for this suite. 08/11/23 14:56:29.393
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-network] EndpointSlice
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:56:29.398
Aug 11 14:56:29.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename endpointslice 08/11/23 14:56:29.399
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.415
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.418
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204
STEP: referencing a single matching pod 08/11/23 14:56:34.511
STEP: referencing matching pods with named port 08/11/23 14:56:39.521
STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/11/23 14:56:44.53
STEP: recreating EndpointSlices after they've been deleted 08/11/23 14:56:49.54
Aug 11 14:56:49.564: INFO: EndpointSlice for Service endpointslice-6322/example-named-port not found
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Aug 11 14:56:59.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6322" for this suite. 08/11/23 14:56:59.582
{"msg":"PASSED [sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]","completed":189,"skipped":3538,"failed":0}
------------------------------
â€¢ [SLOW TEST] [30.190 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
  test/e2e/network/endpointslice.go:204

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:56:29.398
    Aug 11 14:56:29.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename endpointslice 08/11/23 14:56:29.399
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:29.415
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:29.418
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
      test/e2e/network/endpointslice.go:204
    STEP: referencing a single matching pod 08/11/23 14:56:34.511
    STEP: referencing matching pods with named port 08/11/23 14:56:39.521
    STEP: creating empty Endpoints and EndpointSlices for no matching Pods 08/11/23 14:56:44.53
    STEP: recreating EndpointSlices after they've been deleted 08/11/23 14:56:49.54
    Aug 11 14:56:49.564: INFO: EndpointSlice for Service endpointslice-6322/example-named-port not found
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Aug 11 14:56:59.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6322" for this suite. 08/11/23 14:56:59.582
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:56:59.591
Aug 11 14:56:59.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:56:59.592
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:59.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:59.608
[It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176
STEP: Creating a pod to test emptydir 0666 on node default medium 08/11/23 14:56:59.61
Aug 11 14:56:59.619: INFO: Waiting up to 5m0s for pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005" in namespace "emptydir-7176" to be "Succeeded or Failed"
Aug 11 14:56:59.623: INFO: Pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005": Phase="Pending", Reason="", readiness=false. Elapsed: 4.584812ms
Aug 11 14:57:01.627: INFO: Pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008269023s
Aug 11 14:57:03.628: INFO: Pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009413365s
STEP: Saw pod success 08/11/23 14:57:03.628
Aug 11 14:57:03.628: INFO: Pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005" satisfied condition "Succeeded or Failed"
Aug 11 14:57:03.631: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-51338615-4ce0-4ea0-ae71-b2bda210b005 container test-container: <nil>
STEP: delete the pod 08/11/23 14:57:03.64
Aug 11 14:57:03.650: INFO: Waiting for pod pod-51338615-4ce0-4ea0-ae71-b2bda210b005 to disappear
Aug 11 14:57:03.653: INFO: Pod pod-51338615-4ce0-4ea0-ae71-b2bda210b005 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 14:57:03.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-7176" for this suite. 08/11/23 14:57:03.657
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":190,"skipped":3583,"failed":0}
------------------------------
â€¢ [4.073 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:176

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:56:59.591
    Aug 11 14:56:59.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:56:59.592
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:56:59.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:56:59.608
    [It] should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:176
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/11/23 14:56:59.61
    Aug 11 14:56:59.619: INFO: Waiting up to 5m0s for pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005" in namespace "emptydir-7176" to be "Succeeded or Failed"
    Aug 11 14:56:59.623: INFO: Pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005": Phase="Pending", Reason="", readiness=false. Elapsed: 4.584812ms
    Aug 11 14:57:01.627: INFO: Pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008269023s
    Aug 11 14:57:03.628: INFO: Pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009413365s
    STEP: Saw pod success 08/11/23 14:57:03.628
    Aug 11 14:57:03.628: INFO: Pod "pod-51338615-4ce0-4ea0-ae71-b2bda210b005" satisfied condition "Succeeded or Failed"
    Aug 11 14:57:03.631: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-51338615-4ce0-4ea0-ae71-b2bda210b005 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:57:03.64
    Aug 11 14:57:03.650: INFO: Waiting for pod pod-51338615-4ce0-4ea0-ae71-b2bda210b005 to disappear
    Aug 11 14:57:03.653: INFO: Pod pod-51338615-4ce0-4ea0-ae71-b2bda210b005 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:57:03.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-7176" for this suite. 08/11/23 14:57:03.657
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:03.664
Aug 11 14:57:03.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:57:03.665
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:03.679
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:03.682
[It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206
STEP: Creating a pod to test emptydir 0666 on node default medium 08/11/23 14:57:03.684
Aug 11 14:57:03.693: INFO: Waiting up to 5m0s for pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9" in namespace "emptydir-470" to be "Succeeded or Failed"
Aug 11 14:57:03.700: INFO: Pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.323167ms
Aug 11 14:57:05.705: INFO: Pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011371681s
Aug 11 14:57:07.705: INFO: Pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011201293s
STEP: Saw pod success 08/11/23 14:57:07.705
Aug 11 14:57:07.705: INFO: Pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9" satisfied condition "Succeeded or Failed"
Aug 11 14:57:07.709: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9 container test-container: <nil>
STEP: delete the pod 08/11/23 14:57:07.719
Aug 11 14:57:07.730: INFO: Waiting for pod pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9 to disappear
Aug 11 14:57:07.732: INFO: Pod pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 14:57:07.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-470" for this suite. 08/11/23 14:57:07.74
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":191,"skipped":3590,"failed":0}
------------------------------
â€¢ [4.082 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:206

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:03.664
    Aug 11 14:57:03.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:57:03.665
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:03.679
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:03.682
    [It] should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:206
    STEP: Creating a pod to test emptydir 0666 on node default medium 08/11/23 14:57:03.684
    Aug 11 14:57:03.693: INFO: Waiting up to 5m0s for pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9" in namespace "emptydir-470" to be "Succeeded or Failed"
    Aug 11 14:57:03.700: INFO: Pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9": Phase="Pending", Reason="", readiness=false. Elapsed: 6.323167ms
    Aug 11 14:57:05.705: INFO: Pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011371681s
    Aug 11 14:57:07.705: INFO: Pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011201293s
    STEP: Saw pod success 08/11/23 14:57:07.705
    Aug 11 14:57:07.705: INFO: Pod "pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9" satisfied condition "Succeeded or Failed"
    Aug 11 14:57:07.709: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9 container test-container: <nil>
    STEP: delete the pod 08/11/23 14:57:07.719
    Aug 11 14:57:07.730: INFO: Waiting for pod pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9 to disappear
    Aug 11 14:57:07.732: INFO: Pod pod-57a47e97-7e4c-4db0-adca-566fe6a5b6d9 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 14:57:07.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-470" for this suite. 08/11/23 14:57:07.74
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1
  should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
[BeforeEach] version v1
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:07.747
Aug 11 14:57:07.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename proxy 08/11/23 14:57:07.748
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:07.763
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:07.766
[It] should proxy through a service and a pod  [Conformance]
  test/e2e/network/proxy.go:101
STEP: starting an echo server on multiple ports 08/11/23 14:57:07.785
STEP: creating replication controller proxy-service-xr5ct in namespace proxy-1872 08/11/23 14:57:07.785
I0811 14:57:07.796501      22 runners.go:193] Created replication controller with name: proxy-service-xr5ct, namespace: proxy-1872, replica count: 1
I0811 14:57:08.846972      22 runners.go:193] proxy-service-xr5ct Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
I0811 14:57:09.848084      22 runners.go:193] proxy-service-xr5ct Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 14:57:09.851: INFO: setup took 2.082502295s, starting test cases
STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/11/23 14:57:09.851
Aug 11 14:57:09.866: INFO: (0) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 14.591163ms)
Aug 11 14:57:09.866: INFO: (0) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 14.448479ms)
Aug 11 14:57:09.866: INFO: (0) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 14.449499ms)
Aug 11 14:57:09.867: INFO: (0) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 15.154121ms)
Aug 11 14:57:09.867: INFO: (0) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 15.587815ms)
Aug 11 14:57:09.868: INFO: (0) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 16.7209ms)
Aug 11 14:57:09.869: INFO: (0) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 16.962467ms)
Aug 11 14:57:09.869: INFO: (0) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 17.297477ms)
Aug 11 14:57:09.871: INFO: (0) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 19.578938ms)
Aug 11 14:57:09.871: INFO: (0) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 19.446054ms)
Aug 11 14:57:09.871: INFO: (0) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 19.438473ms)
Aug 11 14:57:09.872: INFO: (0) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 20.540478ms)
Aug 11 14:57:09.876: INFO: (0) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 24.353926ms)
Aug 11 14:57:09.876: INFO: (0) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 24.017476ms)
Aug 11 14:57:09.877: INFO: (0) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 25.156761ms)
Aug 11 14:57:09.877: INFO: (0) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 25.093469ms)
Aug 11 14:57:09.886: INFO: (1) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.046812ms)
Aug 11 14:57:09.889: INFO: (1) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.453306ms)
Aug 11 14:57:09.889: INFO: (1) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.168547ms)
Aug 11 14:57:09.889: INFO: (1) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 12.177849ms)
Aug 11 14:57:09.890: INFO: (1) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 12.372285ms)
Aug 11 14:57:09.890: INFO: (1) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.586401ms)
Aug 11 14:57:09.890: INFO: (1) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 12.984664ms)
Aug 11 14:57:09.890: INFO: (1) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.990104ms)
Aug 11 14:57:09.891: INFO: (1) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 13.366636ms)
Aug 11 14:57:09.891: INFO: (1) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.643095ms)
Aug 11 14:57:09.891: INFO: (1) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 13.406927ms)
Aug 11 14:57:09.892: INFO: (1) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 14.121109ms)
Aug 11 14:57:09.892: INFO: (1) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 14.44779ms)
Aug 11 14:57:09.893: INFO: (1) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 15.270925ms)
Aug 11 14:57:09.893: INFO: (1) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.173183ms)
Aug 11 14:57:09.894: INFO: (1) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 16.738351ms)
Aug 11 14:57:09.905: INFO: (2) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.854008ms)
Aug 11 14:57:09.905: INFO: (2) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.905379ms)
Aug 11 14:57:09.905: INFO: (2) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 10.825517ms)
Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 11.128956ms)
Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.394505ms)
Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.421175ms)
Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.614552ms)
Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 12.047955ms)
Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.87259ms)
Aug 11 14:57:09.907: INFO: (2) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 12.594971ms)
Aug 11 14:57:09.907: INFO: (2) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 12.836339ms)
Aug 11 14:57:09.908: INFO: (2) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 13.162319ms)
Aug 11 14:57:09.910: INFO: (2) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 15.567644ms)
Aug 11 14:57:09.910: INFO: (2) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 15.699018ms)
Aug 11 14:57:09.910: INFO: (2) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 15.677638ms)
Aug 11 14:57:09.917: INFO: (2) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 22.325235ms)
Aug 11 14:57:09.925: INFO: (3) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 7.811093ms)
Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.335932ms)
Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.6342ms)
Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 11.325012ms)
Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.469066ms)
Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.365184ms)
Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.173997ms)
Aug 11 14:57:09.929: INFO: (3) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.517959ms)
Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 12.365035ms)
Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 12.443807ms)
Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.53677ms)
Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 13.050256ms)
Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.17594ms)
Aug 11 14:57:09.931: INFO: (3) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.239152ms)
Aug 11 14:57:09.932: INFO: (3) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.49133ms)
Aug 11 14:57:09.934: INFO: (3) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.306068ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.846928ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.87569ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 12.021034ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 11.916651ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.043665ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.134917ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 12.191019ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 12.479988ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 12.344254ms)
Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.564431ms)
Aug 11 14:57:09.947: INFO: (4) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 13.460978ms)
Aug 11 14:57:09.948: INFO: (4) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.945334ms)
Aug 11 14:57:09.950: INFO: (4) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 16.022299ms)
Aug 11 14:57:09.950: INFO: (4) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 16.184723ms)
Aug 11 14:57:09.951: INFO: (4) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 16.7104ms)
Aug 11 14:57:09.951: INFO: (4) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.889015ms)
Aug 11 14:57:09.960: INFO: (5) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.162405ms)
Aug 11 14:57:09.963: INFO: (5) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.045215ms)
Aug 11 14:57:09.964: INFO: (5) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 13.144909ms)
Aug 11 14:57:09.964: INFO: (5) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 13.298104ms)
Aug 11 14:57:09.964: INFO: (5) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 13.514691ms)
Aug 11 14:57:09.966: INFO: (5) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.969326ms)
Aug 11 14:57:09.966: INFO: (5) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 15.453741ms)
Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 15.819812ms)
Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 15.775851ms)
Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 16.06529ms)
Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 16.038319ms)
Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 16.371849ms)
Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 16.70694ms)
Aug 11 14:57:09.968: INFO: (5) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.705979ms)
Aug 11 14:57:09.972: INFO: (5) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 20.624002ms)
Aug 11 14:57:09.972: INFO: (5) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 21.20818ms)
Aug 11 14:57:09.983: INFO: (6) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.889728ms)
Aug 11 14:57:09.983: INFO: (6) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.475456ms)
Aug 11 14:57:09.985: INFO: (6) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 12.216809ms)
Aug 11 14:57:09.985: INFO: (6) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 12.456588ms)
Aug 11 14:57:09.985: INFO: (6) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 12.486879ms)
Aug 11 14:57:09.986: INFO: (6) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 13.517121ms)
Aug 11 14:57:09.986: INFO: (6) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 14.073809ms)
Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 13.939854ms)
Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 14.042207ms)
Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 14.559563ms)
Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.309246ms)
Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.478321ms)
Aug 11 14:57:09.988: INFO: (6) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 15.226784ms)
Aug 11 14:57:09.989: INFO: (6) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.478583ms)
Aug 11 14:57:09.989: INFO: (6) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 16.976178ms)
Aug 11 14:57:09.990: INFO: (6) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 17.34503ms)
Aug 11 14:57:10.001: INFO: (7) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.467345ms)
Aug 11 14:57:10.001: INFO: (7) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.234379ms)
Aug 11 14:57:10.002: INFO: (7) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.618232ms)
Aug 11 14:57:10.003: INFO: (7) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.660793ms)
Aug 11 14:57:10.003: INFO: (7) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.272392ms)
Aug 11 14:57:10.003: INFO: (7) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 12.473349ms)
Aug 11 14:57:10.004: INFO: (7) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 13.391547ms)
Aug 11 14:57:10.004: INFO: (7) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 12.957523ms)
Aug 11 14:57:10.004: INFO: (7) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 13.138759ms)
Aug 11 14:57:10.004: INFO: (7) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.913391ms)
Aug 11 14:57:10.005: INFO: (7) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.853621ms)
Aug 11 14:57:10.005: INFO: (7) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 14.395288ms)
Aug 11 14:57:10.005: INFO: (7) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.805601ms)
Aug 11 14:57:10.005: INFO: (7) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 14.723638ms)
Aug 11 14:57:10.006: INFO: (7) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.617405ms)
Aug 11 14:57:10.008: INFO: (7) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 16.898256ms)
Aug 11 14:57:10.017: INFO: (8) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 9.344651ms)
Aug 11 14:57:10.017: INFO: (8) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.388142ms)
Aug 11 14:57:10.017: INFO: (8) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.458455ms)
Aug 11 14:57:10.017: INFO: (8) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 9.673201ms)
Aug 11 14:57:10.018: INFO: (8) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 9.855437ms)
Aug 11 14:57:10.018: INFO: (8) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 10.210388ms)
Aug 11 14:57:10.019: INFO: (8) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 10.461576ms)
Aug 11 14:57:10.019: INFO: (8) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.479496ms)
Aug 11 14:57:10.019: INFO: (8) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.621171ms)
Aug 11 14:57:10.020: INFO: (8) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 12.456208ms)
Aug 11 14:57:10.020: INFO: (8) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.435977ms)
Aug 11 14:57:10.021: INFO: (8) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 13.142959ms)
Aug 11 14:57:10.021: INFO: (8) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.070616ms)
Aug 11 14:57:10.022: INFO: (8) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.529092ms)
Aug 11 14:57:10.022: INFO: (8) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 14.320705ms)
Aug 11 14:57:10.022: INFO: (8) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 14.239263ms)
Aug 11 14:57:10.032: INFO: (9) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 8.68157ms)
Aug 11 14:57:10.032: INFO: (9) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 8.752762ms)
Aug 11 14:57:10.032: INFO: (9) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 9.442994ms)
Aug 11 14:57:10.032: INFO: (9) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.272239ms)
Aug 11 14:57:10.033: INFO: (9) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 10.719094ms)
Aug 11 14:57:10.034: INFO: (9) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.167687ms)
Aug 11 14:57:10.034: INFO: (9) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.154938ms)
Aug 11 14:57:10.034: INFO: (9) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.400914ms)
Aug 11 14:57:10.035: INFO: (9) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.88662ms)
Aug 11 14:57:10.035: INFO: (9) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.931251ms)
Aug 11 14:57:10.035: INFO: (9) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.051655ms)
Aug 11 14:57:10.037: INFO: (9) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.13703ms)
Aug 11 14:57:10.037: INFO: (9) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 13.990155ms)
Aug 11 14:57:10.038: INFO: (9) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.705648ms)
Aug 11 14:57:10.039: INFO: (9) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.207135ms)
Aug 11 14:57:10.040: INFO: (9) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 17.0414ms)
Aug 11 14:57:10.048: INFO: (10) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 7.71083ms)
Aug 11 14:57:10.049: INFO: (10) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 8.808214ms)
Aug 11 14:57:10.049: INFO: (10) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.278358ms)
Aug 11 14:57:10.049: INFO: (10) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 9.148985ms)
Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 9.232718ms)
Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.416193ms)
Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.343621ms)
Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 9.762254ms)
Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.124265ms)
Aug 11 14:57:10.051: INFO: (10) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 10.377133ms)
Aug 11 14:57:10.055: INFO: (10) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 15.031628ms)
Aug 11 14:57:10.055: INFO: (10) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 15.340918ms)
Aug 11 14:57:10.055: INFO: (10) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 15.238734ms)
Aug 11 14:57:10.056: INFO: (10) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 15.579364ms)
Aug 11 14:57:10.056: INFO: (10) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.079961ms)
Aug 11 14:57:10.056: INFO: (10) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 16.167943ms)
Aug 11 14:57:10.064: INFO: (11) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 7.032719ms)
Aug 11 14:57:10.066: INFO: (11) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.789684ms)
Aug 11 14:57:10.067: INFO: (11) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.031972ms)
Aug 11 14:57:10.067: INFO: (11) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.94559ms)
Aug 11 14:57:10.067: INFO: (11) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.487447ms)
Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 10.734204ms)
Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.205379ms)
Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.199888ms)
Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.480937ms)
Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.56262ms)
Aug 11 14:57:10.070: INFO: (11) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 13.218471ms)
Aug 11 14:57:10.071: INFO: (11) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.954495ms)
Aug 11 14:57:10.072: INFO: (11) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.975866ms)
Aug 11 14:57:10.073: INFO: (11) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 15.838333ms)
Aug 11 14:57:10.073: INFO: (11) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.118621ms)
Aug 11 14:57:10.074: INFO: (11) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 16.844944ms)
Aug 11 14:57:10.088: INFO: (12) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 14.218192ms)
Aug 11 14:57:10.089: INFO: (12) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 14.546423ms)
Aug 11 14:57:10.089: INFO: (12) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 14.816131ms)
Aug 11 14:57:10.089: INFO: (12) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 14.906764ms)
Aug 11 14:57:10.089: INFO: (12) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 15.178633ms)
Aug 11 14:57:10.090: INFO: (12) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 16.146283ms)
Aug 11 14:57:10.090: INFO: (12) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 16.236785ms)
Aug 11 14:57:10.090: INFO: (12) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 16.182384ms)
Aug 11 14:57:10.091: INFO: (12) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 16.474853ms)
Aug 11 14:57:10.091: INFO: (12) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 16.593466ms)
Aug 11 14:57:10.091: INFO: (12) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 17.638548ms)
Aug 11 14:57:10.094: INFO: (12) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 19.657072ms)
Aug 11 14:57:10.094: INFO: (12) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 20.589771ms)
Aug 11 14:57:10.095: INFO: (12) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 21.396036ms)
Aug 11 14:57:10.097: INFO: (12) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 22.8052ms)
Aug 11 14:57:10.097: INFO: (12) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 23.216152ms)
Aug 11 14:57:10.105: INFO: (13) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 7.106121ms)
Aug 11 14:57:10.105: INFO: (13) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 8.012499ms)
Aug 11 14:57:10.107: INFO: (13) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 9.144744ms)
Aug 11 14:57:10.108: INFO: (13) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.409684ms)
Aug 11 14:57:10.108: INFO: (13) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.26084ms)
Aug 11 14:57:10.109: INFO: (13) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.054884ms)
Aug 11 14:57:10.109: INFO: (13) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 11.055184ms)
Aug 11 14:57:10.109: INFO: (13) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.313933ms)
Aug 11 14:57:10.110: INFO: (13) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.020494ms)
Aug 11 14:57:10.110: INFO: (13) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.131268ms)
Aug 11 14:57:10.111: INFO: (13) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.278353ms)
Aug 11 14:57:10.111: INFO: (13) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 13.507911ms)
Aug 11 14:57:10.112: INFO: (13) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.055028ms)
Aug 11 14:57:10.112: INFO: (13) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 14.519512ms)
Aug 11 14:57:10.113: INFO: (13) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 15.483142ms)
Aug 11 14:57:10.113: INFO: (13) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 15.73001ms)
Aug 11 14:57:10.123: INFO: (14) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 9.33367ms)
Aug 11 14:57:10.123: INFO: (14) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 9.882398ms)
Aug 11 14:57:10.123: INFO: (14) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.906588ms)
Aug 11 14:57:10.124: INFO: (14) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.022531ms)
Aug 11 14:57:10.124: INFO: (14) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.28737ms)
Aug 11 14:57:10.124: INFO: (14) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.675342ms)
Aug 11 14:57:10.124: INFO: (14) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.043774ms)
Aug 11 14:57:10.125: INFO: (14) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 11.140997ms)
Aug 11 14:57:10.126: INFO: (14) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 12.278292ms)
Aug 11 14:57:10.126: INFO: (14) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.099557ms)
Aug 11 14:57:10.126: INFO: (14) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.1903ms)
Aug 11 14:57:10.126: INFO: (14) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 12.598052ms)
Aug 11 14:57:10.127: INFO: (14) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.411688ms)
Aug 11 14:57:10.127: INFO: (14) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 13.371376ms)
Aug 11 14:57:10.128: INFO: (14) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.603984ms)
Aug 11 14:57:10.132: INFO: (14) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 18.200307ms)
Aug 11 14:57:10.139: INFO: (15) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 7.382ms)
Aug 11 14:57:10.141: INFO: (15) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.422843ms)
Aug 11 14:57:10.142: INFO: (15) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 10.016632ms)
Aug 11 14:57:10.142: INFO: (15) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.057143ms)
Aug 11 14:57:10.142: INFO: (15) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 10.427825ms)
Aug 11 14:57:10.142: INFO: (15) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.443445ms)
Aug 11 14:57:10.143: INFO: (15) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.280521ms)
Aug 11 14:57:10.143: INFO: (15) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.187228ms)
Aug 11 14:57:10.143: INFO: (15) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.544939ms)
Aug 11 14:57:10.144: INFO: (15) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.424107ms)
Aug 11 14:57:10.144: INFO: (15) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 12.379225ms)
Aug 11 14:57:10.145: INFO: (15) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 13.085477ms)
Aug 11 14:57:10.146: INFO: (15) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.637594ms)
Aug 11 14:57:10.146: INFO: (15) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 13.757908ms)
Aug 11 14:57:10.146: INFO: (15) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.670936ms)
Aug 11 14:57:10.147: INFO: (15) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.617385ms)
Aug 11 14:57:10.155: INFO: (16) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 8.317149ms)
Aug 11 14:57:10.157: INFO: (16) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 9.938699ms)
Aug 11 14:57:10.157: INFO: (16) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 10.381793ms)
Aug 11 14:57:10.158: INFO: (16) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.498968ms)
Aug 11 14:57:10.159: INFO: (16) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 11.89488ms)
Aug 11 14:57:10.160: INFO: (16) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 13.239252ms)
Aug 11 14:57:10.160: INFO: (16) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 13.603293ms)
Aug 11 14:57:10.160: INFO: (16) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 13.596293ms)
Aug 11 14:57:10.160: INFO: (16) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 13.620514ms)
Aug 11 14:57:10.161: INFO: (16) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 13.971284ms)
Aug 11 14:57:10.161: INFO: (16) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.681227ms)
Aug 11 14:57:10.162: INFO: (16) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.724468ms)
Aug 11 14:57:10.162: INFO: (16) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 15.641537ms)
Aug 11 14:57:10.163: INFO: (16) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 16.135922ms)
Aug 11 14:57:10.163: INFO: (16) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 16.367049ms)
Aug 11 14:57:10.163: INFO: (16) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 16.491763ms)
Aug 11 14:57:10.173: INFO: (17) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 9.563208ms)
Aug 11 14:57:10.173: INFO: (17) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 9.419974ms)
Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.28962ms)
Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 10.052923ms)
Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.148606ms)
Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.069984ms)
Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 10.351423ms)
Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.443045ms)
Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 10.91801ms)
Aug 11 14:57:10.175: INFO: (17) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.978801ms)
Aug 11 14:57:10.175: INFO: (17) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.053394ms)
Aug 11 14:57:10.176: INFO: (17) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.284903ms)
Aug 11 14:57:10.176: INFO: (17) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 12.974373ms)
Aug 11 14:57:10.177: INFO: (17) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.429919ms)
Aug 11 14:57:10.177: INFO: (17) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 13.461599ms)
Aug 11 14:57:10.177: INFO: (17) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 13.580032ms)
Aug 11 14:57:10.187: INFO: (18) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.543617ms)
Aug 11 14:57:10.187: INFO: (18) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.712322ms)
Aug 11 14:57:10.187: INFO: (18) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.935829ms)
Aug 11 14:57:10.187: INFO: (18) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 9.94985ms)
Aug 11 14:57:10.188: INFO: (18) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.354593ms)
Aug 11 14:57:10.188: INFO: (18) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 10.242859ms)
Aug 11 14:57:10.188: INFO: (18) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.657211ms)
Aug 11 14:57:10.188: INFO: (18) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 10.701843ms)
Aug 11 14:57:10.189: INFO: (18) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.675433ms)
Aug 11 14:57:10.189: INFO: (18) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.751836ms)
Aug 11 14:57:10.190: INFO: (18) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.217201ms)
Aug 11 14:57:10.190: INFO: (18) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 12.418316ms)
Aug 11 14:57:10.191: INFO: (18) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 13.690486ms)
Aug 11 14:57:10.191: INFO: (18) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.059417ms)
Aug 11 14:57:10.191: INFO: (18) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 14.166371ms)
Aug 11 14:57:10.192: INFO: (18) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.802071ms)
Aug 11 14:57:10.201: INFO: (19) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 8.843225ms)
Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.198426ms)
Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 9.63665ms)
Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.758924ms)
Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 10.115185ms)
Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.078534ms)
Aug 11 14:57:10.203: INFO: (19) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 10.825337ms)
Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 11.643002ms)
Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 11.698595ms)
Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.88277ms)
Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.694334ms)
Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 11.88239ms)
Aug 11 14:57:10.205: INFO: (19) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.612592ms)
Aug 11 14:57:10.206: INFO: (19) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.469529ms)
Aug 11 14:57:10.206: INFO: (19) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 13.700706ms)
Aug 11 14:57:10.207: INFO: (19) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.568973ms)
STEP: deleting ReplicationController proxy-service-xr5ct in namespace proxy-1872, will wait for the garbage collector to delete the pods 08/11/23 14:57:10.207
Aug 11 14:57:10.271: INFO: Deleting ReplicationController proxy-service-xr5ct took: 10.437965ms
Aug 11 14:57:10.373: INFO: Terminating ReplicationController proxy-service-xr5ct pods took: 101.156316ms
[AfterEach] version v1
  test/e2e/framework/framework.go:187
Aug 11 14:57:12.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "proxy-1872" for this suite. 08/11/23 14:57:12.678
{"msg":"PASSED [sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]","completed":192,"skipped":3596,"failed":0}
------------------------------
â€¢ [4.937 seconds]
[sig-network] Proxy
test/e2e/network/common/framework.go:23
  version v1
  test/e2e/network/proxy.go:74
    should proxy through a service and a pod  [Conformance]
    test/e2e/network/proxy.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] version v1
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:07.747
    Aug 11 14:57:07.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename proxy 08/11/23 14:57:07.748
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:07.763
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:07.766
    [It] should proxy through a service and a pod  [Conformance]
      test/e2e/network/proxy.go:101
    STEP: starting an echo server on multiple ports 08/11/23 14:57:07.785
    STEP: creating replication controller proxy-service-xr5ct in namespace proxy-1872 08/11/23 14:57:07.785
    I0811 14:57:07.796501      22 runners.go:193] Created replication controller with name: proxy-service-xr5ct, namespace: proxy-1872, replica count: 1
    I0811 14:57:08.846972      22 runners.go:193] proxy-service-xr5ct Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
    I0811 14:57:09.848084      22 runners.go:193] proxy-service-xr5ct Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 14:57:09.851: INFO: setup took 2.082502295s, starting test cases
    STEP: running 16 cases, 20 attempts per case, 320 total attempts 08/11/23 14:57:09.851
    Aug 11 14:57:09.866: INFO: (0) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 14.591163ms)
    Aug 11 14:57:09.866: INFO: (0) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 14.448479ms)
    Aug 11 14:57:09.866: INFO: (0) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 14.449499ms)
    Aug 11 14:57:09.867: INFO: (0) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 15.154121ms)
    Aug 11 14:57:09.867: INFO: (0) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 15.587815ms)
    Aug 11 14:57:09.868: INFO: (0) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 16.7209ms)
    Aug 11 14:57:09.869: INFO: (0) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 16.962467ms)
    Aug 11 14:57:09.869: INFO: (0) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 17.297477ms)
    Aug 11 14:57:09.871: INFO: (0) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 19.578938ms)
    Aug 11 14:57:09.871: INFO: (0) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 19.446054ms)
    Aug 11 14:57:09.871: INFO: (0) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 19.438473ms)
    Aug 11 14:57:09.872: INFO: (0) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 20.540478ms)
    Aug 11 14:57:09.876: INFO: (0) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 24.353926ms)
    Aug 11 14:57:09.876: INFO: (0) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 24.017476ms)
    Aug 11 14:57:09.877: INFO: (0) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 25.156761ms)
    Aug 11 14:57:09.877: INFO: (0) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 25.093469ms)
    Aug 11 14:57:09.886: INFO: (1) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.046812ms)
    Aug 11 14:57:09.889: INFO: (1) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.453306ms)
    Aug 11 14:57:09.889: INFO: (1) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.168547ms)
    Aug 11 14:57:09.889: INFO: (1) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 12.177849ms)
    Aug 11 14:57:09.890: INFO: (1) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 12.372285ms)
    Aug 11 14:57:09.890: INFO: (1) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.586401ms)
    Aug 11 14:57:09.890: INFO: (1) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 12.984664ms)
    Aug 11 14:57:09.890: INFO: (1) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.990104ms)
    Aug 11 14:57:09.891: INFO: (1) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 13.366636ms)
    Aug 11 14:57:09.891: INFO: (1) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.643095ms)
    Aug 11 14:57:09.891: INFO: (1) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 13.406927ms)
    Aug 11 14:57:09.892: INFO: (1) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 14.121109ms)
    Aug 11 14:57:09.892: INFO: (1) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 14.44779ms)
    Aug 11 14:57:09.893: INFO: (1) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 15.270925ms)
    Aug 11 14:57:09.893: INFO: (1) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.173183ms)
    Aug 11 14:57:09.894: INFO: (1) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 16.738351ms)
    Aug 11 14:57:09.905: INFO: (2) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.854008ms)
    Aug 11 14:57:09.905: INFO: (2) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.905379ms)
    Aug 11 14:57:09.905: INFO: (2) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 10.825517ms)
    Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 11.128956ms)
    Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.394505ms)
    Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.421175ms)
    Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.614552ms)
    Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 12.047955ms)
    Aug 11 14:57:09.906: INFO: (2) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.87259ms)
    Aug 11 14:57:09.907: INFO: (2) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 12.594971ms)
    Aug 11 14:57:09.907: INFO: (2) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 12.836339ms)
    Aug 11 14:57:09.908: INFO: (2) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 13.162319ms)
    Aug 11 14:57:09.910: INFO: (2) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 15.567644ms)
    Aug 11 14:57:09.910: INFO: (2) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 15.699018ms)
    Aug 11 14:57:09.910: INFO: (2) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 15.677638ms)
    Aug 11 14:57:09.917: INFO: (2) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 22.325235ms)
    Aug 11 14:57:09.925: INFO: (3) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 7.811093ms)
    Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.335932ms)
    Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.6342ms)
    Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 11.325012ms)
    Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.469066ms)
    Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.365184ms)
    Aug 11 14:57:09.928: INFO: (3) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.173997ms)
    Aug 11 14:57:09.929: INFO: (3) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.517959ms)
    Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 12.365035ms)
    Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 12.443807ms)
    Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.53677ms)
    Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 13.050256ms)
    Aug 11 14:57:09.930: INFO: (3) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.17594ms)
    Aug 11 14:57:09.931: INFO: (3) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.239152ms)
    Aug 11 14:57:09.932: INFO: (3) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.49133ms)
    Aug 11 14:57:09.934: INFO: (3) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.306068ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.846928ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.87569ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 12.021034ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 11.916651ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.043665ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.134917ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 12.191019ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 12.479988ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 12.344254ms)
    Aug 11 14:57:09.946: INFO: (4) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.564431ms)
    Aug 11 14:57:09.947: INFO: (4) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 13.460978ms)
    Aug 11 14:57:09.948: INFO: (4) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.945334ms)
    Aug 11 14:57:09.950: INFO: (4) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 16.022299ms)
    Aug 11 14:57:09.950: INFO: (4) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 16.184723ms)
    Aug 11 14:57:09.951: INFO: (4) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 16.7104ms)
    Aug 11 14:57:09.951: INFO: (4) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.889015ms)
    Aug 11 14:57:09.960: INFO: (5) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.162405ms)
    Aug 11 14:57:09.963: INFO: (5) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.045215ms)
    Aug 11 14:57:09.964: INFO: (5) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 13.144909ms)
    Aug 11 14:57:09.964: INFO: (5) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 13.298104ms)
    Aug 11 14:57:09.964: INFO: (5) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 13.514691ms)
    Aug 11 14:57:09.966: INFO: (5) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.969326ms)
    Aug 11 14:57:09.966: INFO: (5) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 15.453741ms)
    Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 15.819812ms)
    Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 15.775851ms)
    Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 16.06529ms)
    Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 16.038319ms)
    Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 16.371849ms)
    Aug 11 14:57:09.967: INFO: (5) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 16.70694ms)
    Aug 11 14:57:09.968: INFO: (5) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.705979ms)
    Aug 11 14:57:09.972: INFO: (5) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 20.624002ms)
    Aug 11 14:57:09.972: INFO: (5) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 21.20818ms)
    Aug 11 14:57:09.983: INFO: (6) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.889728ms)
    Aug 11 14:57:09.983: INFO: (6) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.475456ms)
    Aug 11 14:57:09.985: INFO: (6) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 12.216809ms)
    Aug 11 14:57:09.985: INFO: (6) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 12.456588ms)
    Aug 11 14:57:09.985: INFO: (6) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 12.486879ms)
    Aug 11 14:57:09.986: INFO: (6) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 13.517121ms)
    Aug 11 14:57:09.986: INFO: (6) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 14.073809ms)
    Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 13.939854ms)
    Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 14.042207ms)
    Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 14.559563ms)
    Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.309246ms)
    Aug 11 14:57:09.987: INFO: (6) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.478321ms)
    Aug 11 14:57:09.988: INFO: (6) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 15.226784ms)
    Aug 11 14:57:09.989: INFO: (6) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.478583ms)
    Aug 11 14:57:09.989: INFO: (6) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 16.976178ms)
    Aug 11 14:57:09.990: INFO: (6) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 17.34503ms)
    Aug 11 14:57:10.001: INFO: (7) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.467345ms)
    Aug 11 14:57:10.001: INFO: (7) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.234379ms)
    Aug 11 14:57:10.002: INFO: (7) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.618232ms)
    Aug 11 14:57:10.003: INFO: (7) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.660793ms)
    Aug 11 14:57:10.003: INFO: (7) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.272392ms)
    Aug 11 14:57:10.003: INFO: (7) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 12.473349ms)
    Aug 11 14:57:10.004: INFO: (7) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 13.391547ms)
    Aug 11 14:57:10.004: INFO: (7) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 12.957523ms)
    Aug 11 14:57:10.004: INFO: (7) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 13.138759ms)
    Aug 11 14:57:10.004: INFO: (7) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.913391ms)
    Aug 11 14:57:10.005: INFO: (7) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.853621ms)
    Aug 11 14:57:10.005: INFO: (7) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 14.395288ms)
    Aug 11 14:57:10.005: INFO: (7) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.805601ms)
    Aug 11 14:57:10.005: INFO: (7) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 14.723638ms)
    Aug 11 14:57:10.006: INFO: (7) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.617405ms)
    Aug 11 14:57:10.008: INFO: (7) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 16.898256ms)
    Aug 11 14:57:10.017: INFO: (8) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 9.344651ms)
    Aug 11 14:57:10.017: INFO: (8) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.388142ms)
    Aug 11 14:57:10.017: INFO: (8) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.458455ms)
    Aug 11 14:57:10.017: INFO: (8) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 9.673201ms)
    Aug 11 14:57:10.018: INFO: (8) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 9.855437ms)
    Aug 11 14:57:10.018: INFO: (8) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 10.210388ms)
    Aug 11 14:57:10.019: INFO: (8) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 10.461576ms)
    Aug 11 14:57:10.019: INFO: (8) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.479496ms)
    Aug 11 14:57:10.019: INFO: (8) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.621171ms)
    Aug 11 14:57:10.020: INFO: (8) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 12.456208ms)
    Aug 11 14:57:10.020: INFO: (8) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.435977ms)
    Aug 11 14:57:10.021: INFO: (8) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 13.142959ms)
    Aug 11 14:57:10.021: INFO: (8) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.070616ms)
    Aug 11 14:57:10.022: INFO: (8) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.529092ms)
    Aug 11 14:57:10.022: INFO: (8) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 14.320705ms)
    Aug 11 14:57:10.022: INFO: (8) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 14.239263ms)
    Aug 11 14:57:10.032: INFO: (9) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 8.68157ms)
    Aug 11 14:57:10.032: INFO: (9) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 8.752762ms)
    Aug 11 14:57:10.032: INFO: (9) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 9.442994ms)
    Aug 11 14:57:10.032: INFO: (9) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.272239ms)
    Aug 11 14:57:10.033: INFO: (9) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 10.719094ms)
    Aug 11 14:57:10.034: INFO: (9) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.167687ms)
    Aug 11 14:57:10.034: INFO: (9) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.154938ms)
    Aug 11 14:57:10.034: INFO: (9) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.400914ms)
    Aug 11 14:57:10.035: INFO: (9) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.88662ms)
    Aug 11 14:57:10.035: INFO: (9) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.931251ms)
    Aug 11 14:57:10.035: INFO: (9) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.051655ms)
    Aug 11 14:57:10.037: INFO: (9) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.13703ms)
    Aug 11 14:57:10.037: INFO: (9) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 13.990155ms)
    Aug 11 14:57:10.038: INFO: (9) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.705648ms)
    Aug 11 14:57:10.039: INFO: (9) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.207135ms)
    Aug 11 14:57:10.040: INFO: (9) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 17.0414ms)
    Aug 11 14:57:10.048: INFO: (10) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 7.71083ms)
    Aug 11 14:57:10.049: INFO: (10) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 8.808214ms)
    Aug 11 14:57:10.049: INFO: (10) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.278358ms)
    Aug 11 14:57:10.049: INFO: (10) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 9.148985ms)
    Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 9.232718ms)
    Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.416193ms)
    Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.343621ms)
    Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 9.762254ms)
    Aug 11 14:57:10.050: INFO: (10) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.124265ms)
    Aug 11 14:57:10.051: INFO: (10) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 10.377133ms)
    Aug 11 14:57:10.055: INFO: (10) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 15.031628ms)
    Aug 11 14:57:10.055: INFO: (10) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 15.340918ms)
    Aug 11 14:57:10.055: INFO: (10) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 15.238734ms)
    Aug 11 14:57:10.056: INFO: (10) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 15.579364ms)
    Aug 11 14:57:10.056: INFO: (10) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.079961ms)
    Aug 11 14:57:10.056: INFO: (10) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 16.167943ms)
    Aug 11 14:57:10.064: INFO: (11) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 7.032719ms)
    Aug 11 14:57:10.066: INFO: (11) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.789684ms)
    Aug 11 14:57:10.067: INFO: (11) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.031972ms)
    Aug 11 14:57:10.067: INFO: (11) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.94559ms)
    Aug 11 14:57:10.067: INFO: (11) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.487447ms)
    Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 10.734204ms)
    Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.205379ms)
    Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.199888ms)
    Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.480937ms)
    Aug 11 14:57:10.068: INFO: (11) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.56262ms)
    Aug 11 14:57:10.070: INFO: (11) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 13.218471ms)
    Aug 11 14:57:10.071: INFO: (11) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.954495ms)
    Aug 11 14:57:10.072: INFO: (11) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.975866ms)
    Aug 11 14:57:10.073: INFO: (11) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 15.838333ms)
    Aug 11 14:57:10.073: INFO: (11) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 16.118621ms)
    Aug 11 14:57:10.074: INFO: (11) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 16.844944ms)
    Aug 11 14:57:10.088: INFO: (12) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 14.218192ms)
    Aug 11 14:57:10.089: INFO: (12) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 14.546423ms)
    Aug 11 14:57:10.089: INFO: (12) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 14.816131ms)
    Aug 11 14:57:10.089: INFO: (12) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 14.906764ms)
    Aug 11 14:57:10.089: INFO: (12) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 15.178633ms)
    Aug 11 14:57:10.090: INFO: (12) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 16.146283ms)
    Aug 11 14:57:10.090: INFO: (12) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 16.236785ms)
    Aug 11 14:57:10.090: INFO: (12) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 16.182384ms)
    Aug 11 14:57:10.091: INFO: (12) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 16.474853ms)
    Aug 11 14:57:10.091: INFO: (12) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 16.593466ms)
    Aug 11 14:57:10.091: INFO: (12) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 17.638548ms)
    Aug 11 14:57:10.094: INFO: (12) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 19.657072ms)
    Aug 11 14:57:10.094: INFO: (12) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 20.589771ms)
    Aug 11 14:57:10.095: INFO: (12) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 21.396036ms)
    Aug 11 14:57:10.097: INFO: (12) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 22.8052ms)
    Aug 11 14:57:10.097: INFO: (12) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 23.216152ms)
    Aug 11 14:57:10.105: INFO: (13) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 7.106121ms)
    Aug 11 14:57:10.105: INFO: (13) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 8.012499ms)
    Aug 11 14:57:10.107: INFO: (13) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 9.144744ms)
    Aug 11 14:57:10.108: INFO: (13) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.409684ms)
    Aug 11 14:57:10.108: INFO: (13) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.26084ms)
    Aug 11 14:57:10.109: INFO: (13) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.054884ms)
    Aug 11 14:57:10.109: INFO: (13) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 11.055184ms)
    Aug 11 14:57:10.109: INFO: (13) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.313933ms)
    Aug 11 14:57:10.110: INFO: (13) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.020494ms)
    Aug 11 14:57:10.110: INFO: (13) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 12.131268ms)
    Aug 11 14:57:10.111: INFO: (13) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.278353ms)
    Aug 11 14:57:10.111: INFO: (13) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 13.507911ms)
    Aug 11 14:57:10.112: INFO: (13) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.055028ms)
    Aug 11 14:57:10.112: INFO: (13) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 14.519512ms)
    Aug 11 14:57:10.113: INFO: (13) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 15.483142ms)
    Aug 11 14:57:10.113: INFO: (13) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 15.73001ms)
    Aug 11 14:57:10.123: INFO: (14) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 9.33367ms)
    Aug 11 14:57:10.123: INFO: (14) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 9.882398ms)
    Aug 11 14:57:10.123: INFO: (14) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.906588ms)
    Aug 11 14:57:10.124: INFO: (14) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.022531ms)
    Aug 11 14:57:10.124: INFO: (14) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.28737ms)
    Aug 11 14:57:10.124: INFO: (14) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.675342ms)
    Aug 11 14:57:10.124: INFO: (14) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.043774ms)
    Aug 11 14:57:10.125: INFO: (14) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 11.140997ms)
    Aug 11 14:57:10.126: INFO: (14) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 12.278292ms)
    Aug 11 14:57:10.126: INFO: (14) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.099557ms)
    Aug 11 14:57:10.126: INFO: (14) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.1903ms)
    Aug 11 14:57:10.126: INFO: (14) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 12.598052ms)
    Aug 11 14:57:10.127: INFO: (14) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.411688ms)
    Aug 11 14:57:10.127: INFO: (14) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 13.371376ms)
    Aug 11 14:57:10.128: INFO: (14) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.603984ms)
    Aug 11 14:57:10.132: INFO: (14) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 18.200307ms)
    Aug 11 14:57:10.139: INFO: (15) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 7.382ms)
    Aug 11 14:57:10.141: INFO: (15) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.422843ms)
    Aug 11 14:57:10.142: INFO: (15) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 10.016632ms)
    Aug 11 14:57:10.142: INFO: (15) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.057143ms)
    Aug 11 14:57:10.142: INFO: (15) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 10.427825ms)
    Aug 11 14:57:10.142: INFO: (15) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.443445ms)
    Aug 11 14:57:10.143: INFO: (15) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.280521ms)
    Aug 11 14:57:10.143: INFO: (15) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.187228ms)
    Aug 11 14:57:10.143: INFO: (15) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 11.544939ms)
    Aug 11 14:57:10.144: INFO: (15) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.424107ms)
    Aug 11 14:57:10.144: INFO: (15) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 12.379225ms)
    Aug 11 14:57:10.145: INFO: (15) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 13.085477ms)
    Aug 11 14:57:10.146: INFO: (15) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.637594ms)
    Aug 11 14:57:10.146: INFO: (15) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 13.757908ms)
    Aug 11 14:57:10.146: INFO: (15) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.670936ms)
    Aug 11 14:57:10.147: INFO: (15) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.617385ms)
    Aug 11 14:57:10.155: INFO: (16) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 8.317149ms)
    Aug 11 14:57:10.157: INFO: (16) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 9.938699ms)
    Aug 11 14:57:10.157: INFO: (16) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 10.381793ms)
    Aug 11 14:57:10.158: INFO: (16) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 11.498968ms)
    Aug 11 14:57:10.159: INFO: (16) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 11.89488ms)
    Aug 11 14:57:10.160: INFO: (16) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 13.239252ms)
    Aug 11 14:57:10.160: INFO: (16) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 13.603293ms)
    Aug 11 14:57:10.160: INFO: (16) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 13.596293ms)
    Aug 11 14:57:10.160: INFO: (16) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 13.620514ms)
    Aug 11 14:57:10.161: INFO: (16) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 13.971284ms)
    Aug 11 14:57:10.161: INFO: (16) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.681227ms)
    Aug 11 14:57:10.162: INFO: (16) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.724468ms)
    Aug 11 14:57:10.162: INFO: (16) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 15.641537ms)
    Aug 11 14:57:10.163: INFO: (16) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 16.135922ms)
    Aug 11 14:57:10.163: INFO: (16) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 16.367049ms)
    Aug 11 14:57:10.163: INFO: (16) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 16.491763ms)
    Aug 11 14:57:10.173: INFO: (17) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 9.563208ms)
    Aug 11 14:57:10.173: INFO: (17) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 9.419974ms)
    Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.28962ms)
    Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 10.052923ms)
    Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.148606ms)
    Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.069984ms)
    Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 10.351423ms)
    Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.443045ms)
    Aug 11 14:57:10.174: INFO: (17) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 10.91801ms)
    Aug 11 14:57:10.175: INFO: (17) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.978801ms)
    Aug 11 14:57:10.175: INFO: (17) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.053394ms)
    Aug 11 14:57:10.176: INFO: (17) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.284903ms)
    Aug 11 14:57:10.176: INFO: (17) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 12.974373ms)
    Aug 11 14:57:10.177: INFO: (17) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 13.429919ms)
    Aug 11 14:57:10.177: INFO: (17) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 13.461599ms)
    Aug 11 14:57:10.177: INFO: (17) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 13.580032ms)
    Aug 11 14:57:10.187: INFO: (18) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 9.543617ms)
    Aug 11 14:57:10.187: INFO: (18) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.712322ms)
    Aug 11 14:57:10.187: INFO: (18) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.935829ms)
    Aug 11 14:57:10.187: INFO: (18) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 9.94985ms)
    Aug 11 14:57:10.188: INFO: (18) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 10.354593ms)
    Aug 11 14:57:10.188: INFO: (18) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 10.242859ms)
    Aug 11 14:57:10.188: INFO: (18) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 10.657211ms)
    Aug 11 14:57:10.188: INFO: (18) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 10.701843ms)
    Aug 11 14:57:10.189: INFO: (18) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 11.675433ms)
    Aug 11 14:57:10.189: INFO: (18) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 11.751836ms)
    Aug 11 14:57:10.190: INFO: (18) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 12.217201ms)
    Aug 11 14:57:10.190: INFO: (18) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 12.418316ms)
    Aug 11 14:57:10.191: INFO: (18) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 13.690486ms)
    Aug 11 14:57:10.191: INFO: (18) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 14.059417ms)
    Aug 11 14:57:10.191: INFO: (18) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 14.166371ms)
    Aug 11 14:57:10.192: INFO: (18) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 14.802071ms)
    Aug 11 14:57:10.201: INFO: (19) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 8.843225ms)
    Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">test<... (200; 9.198426ms)
    Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname1/proxy/: foo (200; 9.63665ms)
    Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 9.758924ms)
    Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:1080/proxy/rewriteme">... (200; 10.115185ms)
    Aug 11 14:57:10.202: INFO: (19) /api/v1/namespaces/proxy-1872/pods/http:proxy-service-xr5ct-p7wz2:162/proxy/: bar (200; 10.078534ms)
    Aug 11 14:57:10.203: INFO: (19) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:460/proxy/: tls baz (200; 10.825337ms)
    Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:443/proxy/tlsrewritem... (200; 11.643002ms)
    Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname1/proxy/: tls baz (200; 11.698595ms)
    Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/: <a href="/api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2/proxy/rewriteme">test</a> (200; 11.88277ms)
    Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/pods/https:proxy-service-xr5ct-p7wz2:462/proxy/: tls qux (200; 11.694334ms)
    Aug 11 14:57:10.204: INFO: (19) /api/v1/namespaces/proxy-1872/services/proxy-service-xr5ct:portname2/proxy/: bar (200; 11.88239ms)
    Aug 11 14:57:10.205: INFO: (19) /api/v1/namespaces/proxy-1872/pods/proxy-service-xr5ct-p7wz2:160/proxy/: foo (200; 12.612592ms)
    Aug 11 14:57:10.206: INFO: (19) /api/v1/namespaces/proxy-1872/services/https:proxy-service-xr5ct:tlsportname2/proxy/: tls qux (200; 13.469529ms)
    Aug 11 14:57:10.206: INFO: (19) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname2/proxy/: bar (200; 13.700706ms)
    Aug 11 14:57:10.207: INFO: (19) /api/v1/namespaces/proxy-1872/services/http:proxy-service-xr5ct:portname1/proxy/: foo (200; 14.568973ms)
    STEP: deleting ReplicationController proxy-service-xr5ct in namespace proxy-1872, will wait for the garbage collector to delete the pods 08/11/23 14:57:10.207
    Aug 11 14:57:10.271: INFO: Deleting ReplicationController proxy-service-xr5ct took: 10.437965ms
    Aug 11 14:57:10.373: INFO: Terminating ReplicationController proxy-service-xr5ct pods took: 101.156316ms
    [AfterEach] version v1
      test/e2e/framework/framework.go:187
    Aug 11 14:57:12.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "proxy-1872" for this suite. 08/11/23 14:57:12.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:12.686
Aug 11 14:57:12.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:57:12.687
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:12.701
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:12.703
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56
STEP: Creating configMap with name configmap-test-volume-28f34db2-cfe1-435d-a400-c9ca1fece4de 08/11/23 14:57:12.706
STEP: Creating a pod to test consume configMaps 08/11/23 14:57:12.71
Aug 11 14:57:12.717: INFO: Waiting up to 5m0s for pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159" in namespace "configmap-6076" to be "Succeeded or Failed"
Aug 11 14:57:12.723: INFO: Pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159": Phase="Pending", Reason="", readiness=false. Elapsed: 5.957205ms
Aug 11 14:57:14.728: INFO: Pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010839792s
Aug 11 14:57:16.728: INFO: Pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010634933s
STEP: Saw pod success 08/11/23 14:57:16.728
Aug 11 14:57:16.728: INFO: Pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159" satisfied condition "Succeeded or Failed"
Aug 11 14:57:16.731: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:57:16.739
Aug 11 14:57:16.752: INFO: Waiting for pod pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159 to disappear
Aug 11 14:57:16.755: INFO: Pod pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:57:16.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-6076" for this suite. 08/11/23 14:57:16.759
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":193,"skipped":3666,"failed":0}
------------------------------
â€¢ [4.079 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:56

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:12.686
    Aug 11 14:57:12.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:57:12.687
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:12.701
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:12.703
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:56
    STEP: Creating configMap with name configmap-test-volume-28f34db2-cfe1-435d-a400-c9ca1fece4de 08/11/23 14:57:12.706
    STEP: Creating a pod to test consume configMaps 08/11/23 14:57:12.71
    Aug 11 14:57:12.717: INFO: Waiting up to 5m0s for pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159" in namespace "configmap-6076" to be "Succeeded or Failed"
    Aug 11 14:57:12.723: INFO: Pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159": Phase="Pending", Reason="", readiness=false. Elapsed: 5.957205ms
    Aug 11 14:57:14.728: INFO: Pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010839792s
    Aug 11 14:57:16.728: INFO: Pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010634933s
    STEP: Saw pod success 08/11/23 14:57:16.728
    Aug 11 14:57:16.728: INFO: Pod "pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159" satisfied condition "Succeeded or Failed"
    Aug 11 14:57:16.731: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:57:16.739
    Aug 11 14:57:16.752: INFO: Waiting for pod pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159 to disappear
    Aug 11 14:57:16.755: INFO: Pod pod-configmaps-96b30fee-d611-4508-9785-1bd9ebb2b159 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:57:16.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-6076" for this suite. 08/11/23 14:57:16.759
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:16.765
Aug 11 14:57:16.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 14:57:16.766
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:16.781
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:16.785
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192
STEP: Creating a pod to test downward API volume plugin 08/11/23 14:57:16.788
Aug 11 14:57:16.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464" in namespace "downward-api-9049" to be "Succeeded or Failed"
Aug 11 14:57:16.804: INFO: Pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464": Phase="Pending", Reason="", readiness=false. Elapsed: 4.796709ms
Aug 11 14:57:18.808: INFO: Pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008930865s
Aug 11 14:57:20.810: INFO: Pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010881282s
STEP: Saw pod success 08/11/23 14:57:20.81
Aug 11 14:57:20.810: INFO: Pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464" satisfied condition "Succeeded or Failed"
Aug 11 14:57:20.813: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-58781886-4803-4702-8898-ae27c504b464 container client-container: <nil>
STEP: delete the pod 08/11/23 14:57:20.822
Aug 11 14:57:20.833: INFO: Waiting for pod downwardapi-volume-58781886-4803-4702-8898-ae27c504b464 to disappear
Aug 11 14:57:20.835: INFO: Pod downwardapi-volume-58781886-4803-4702-8898-ae27c504b464 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 14:57:20.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-9049" for this suite. 08/11/23 14:57:20.839
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]","completed":194,"skipped":3667,"failed":0}
------------------------------
â€¢ [4.080 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu limit [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:192

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:16.765
    Aug 11 14:57:16.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:57:16.766
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:16.781
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:16.785
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu limit [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:192
    STEP: Creating a pod to test downward API volume plugin 08/11/23 14:57:16.788
    Aug 11 14:57:16.799: INFO: Waiting up to 5m0s for pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464" in namespace "downward-api-9049" to be "Succeeded or Failed"
    Aug 11 14:57:16.804: INFO: Pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464": Phase="Pending", Reason="", readiness=false. Elapsed: 4.796709ms
    Aug 11 14:57:18.808: INFO: Pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008930865s
    Aug 11 14:57:20.810: INFO: Pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010881282s
    STEP: Saw pod success 08/11/23 14:57:20.81
    Aug 11 14:57:20.810: INFO: Pod "downwardapi-volume-58781886-4803-4702-8898-ae27c504b464" satisfied condition "Succeeded or Failed"
    Aug 11 14:57:20.813: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-58781886-4803-4702-8898-ae27c504b464 container client-container: <nil>
    STEP: delete the pod 08/11/23 14:57:20.822
    Aug 11 14:57:20.833: INFO: Waiting for pod downwardapi-volume-58781886-4803-4702-8898-ae27c504b464 to disappear
    Aug 11 14:57:20.835: INFO: Pod downwardapi-volume-58781886-4803-4702-8898-ae27c504b464 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 14:57:20.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-9049" for this suite. 08/11/23 14:57:20.839
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:20.846
Aug 11 14:57:20.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-runtime 08/11/23 14:57:20.847
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:20.862
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:20.864
[It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:247
STEP: create the container 08/11/23 14:57:20.867
STEP: wait for the container to reach Succeeded 08/11/23 14:57:20.874
STEP: get the container status 08/11/23 14:57:23.89
STEP: the container should be terminated 08/11/23 14:57:23.894
STEP: the termination message should be set 08/11/23 14:57:23.894
Aug 11 14:57:23.894: INFO: Expected: &{OK} to match Container's Termination Message: OK --
STEP: delete the container 08/11/23 14:57:23.894
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 11 14:57:23.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-2524" for this suite. 08/11/23 14:57:23.911
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]","completed":195,"skipped":3683,"failed":0}
------------------------------
â€¢ [3.071 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:20.846
    Aug 11 14:57:20.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-runtime 08/11/23 14:57:20.847
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:20.862
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:20.864
    [It] should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:247
    STEP: create the container 08/11/23 14:57:20.867
    STEP: wait for the container to reach Succeeded 08/11/23 14:57:20.874
    STEP: get the container status 08/11/23 14:57:23.89
    STEP: the container should be terminated 08/11/23 14:57:23.894
    STEP: the termination message should be set 08/11/23 14:57:23.894
    Aug 11 14:57:23.894: INFO: Expected: &{OK} to match Container's Termination Message: OK --
    STEP: delete the container 08/11/23 14:57:23.894
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 11 14:57:23.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-2524" for this suite. 08/11/23 14:57:23.911
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:23.917
Aug 11 14:57:23.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-webhook 08/11/23 14:57:23.918
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:23.934
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:23.937
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/11/23 14:57:23.939
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/11/23 14:57:24.131
STEP: Deploying the custom resource conversion webhook pod 08/11/23 14:57:24.14
STEP: Wait for the deployment to be ready 08/11/23 14:57:24.152
Aug 11 14:57:24.160: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
STEP: Deploying the webhook service 08/11/23 14:57:26.171
STEP: Verifying the service has paired with the endpoint 08/11/23 14:57:26.187
Aug 11 14:57:27.189: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149
Aug 11 14:57:27.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Creating a v1 custom resource 08/11/23 14:57:29.8
STEP: v2 custom resource should be converted 08/11/23 14:57:29.805
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:57:30.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5663" for this suite. 08/11/23 14:57:30.331
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]","completed":196,"skipped":3684,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.473 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert from CR v1 to CR v2 [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:149

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:23.917
    Aug 11 14:57:23.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-webhook 08/11/23 14:57:23.918
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:23.934
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:23.937
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/11/23 14:57:23.939
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/11/23 14:57:24.131
    STEP: Deploying the custom resource conversion webhook pod 08/11/23 14:57:24.14
    STEP: Wait for the deployment to be ready 08/11/23 14:57:24.152
    Aug 11 14:57:24.160: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
    STEP: Deploying the webhook service 08/11/23 14:57:26.171
    STEP: Verifying the service has paired with the endpoint 08/11/23 14:57:26.187
    Aug 11 14:57:27.189: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert from CR v1 to CR v2 [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:149
    Aug 11 14:57:27.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Creating a v1 custom resource 08/11/23 14:57:29.8
    STEP: v2 custom resource should be converted 08/11/23 14:57:29.805
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:57:30.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5663" for this suite. 08/11/23 14:57:30.331
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin]
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
[BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:30.391
Aug 11 14:57:30.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename certificates 08/11/23 14:57:30.393
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:30.41
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:30.414
[It] should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200
STEP: getting /apis 08/11/23 14:57:31.019
STEP: getting /apis/certificates.k8s.io 08/11/23 14:57:31.022
STEP: getting /apis/certificates.k8s.io/v1 08/11/23 14:57:31.023
STEP: creating 08/11/23 14:57:31.024
STEP: getting 08/11/23 14:57:31.041
STEP: listing 08/11/23 14:57:31.044
STEP: watching 08/11/23 14:57:31.047
Aug 11 14:57:31.047: INFO: starting watch
STEP: patching 08/11/23 14:57:31.048
STEP: updating 08/11/23 14:57:31.054
Aug 11 14:57:31.061: INFO: waiting for watch events with expected annotations
Aug 11 14:57:31.061: INFO: saw patched and updated annotations
STEP: getting /approval 08/11/23 14:57:31.061
STEP: patching /approval 08/11/23 14:57:31.064
STEP: updating /approval 08/11/23 14:57:31.07
STEP: getting /status 08/11/23 14:57:31.077
STEP: patching /status 08/11/23 14:57:31.079
STEP: updating /status 08/11/23 14:57:31.087
STEP: deleting 08/11/23 14:57:31.095
STEP: deleting a collection 08/11/23 14:57:31.108
[AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:57:31.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "certificates-1597" for this suite. 08/11/23 14:57:31.128
{"msg":"PASSED [sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]","completed":197,"skipped":3701,"failed":0}
------------------------------
â€¢ [0.743 seconds]
[sig-auth] Certificates API [Privileged:ClusterAdmin]
test/e2e/auth/framework.go:23
  should support CSR API operations [Conformance]
  test/e2e/auth/certificates.go:200

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:30.391
    Aug 11 14:57:30.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename certificates 08/11/23 14:57:30.393
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:30.41
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:30.414
    [It] should support CSR API operations [Conformance]
      test/e2e/auth/certificates.go:200
    STEP: getting /apis 08/11/23 14:57:31.019
    STEP: getting /apis/certificates.k8s.io 08/11/23 14:57:31.022
    STEP: getting /apis/certificates.k8s.io/v1 08/11/23 14:57:31.023
    STEP: creating 08/11/23 14:57:31.024
    STEP: getting 08/11/23 14:57:31.041
    STEP: listing 08/11/23 14:57:31.044
    STEP: watching 08/11/23 14:57:31.047
    Aug 11 14:57:31.047: INFO: starting watch
    STEP: patching 08/11/23 14:57:31.048
    STEP: updating 08/11/23 14:57:31.054
    Aug 11 14:57:31.061: INFO: waiting for watch events with expected annotations
    Aug 11 14:57:31.061: INFO: saw patched and updated annotations
    STEP: getting /approval 08/11/23 14:57:31.061
    STEP: patching /approval 08/11/23 14:57:31.064
    STEP: updating /approval 08/11/23 14:57:31.07
    STEP: getting /status 08/11/23 14:57:31.077
    STEP: patching /status 08/11/23 14:57:31.079
    STEP: updating /status 08/11/23 14:57:31.087
    STEP: deleting 08/11/23 14:57:31.095
    STEP: deleting a collection 08/11/23 14:57:31.108
    [AfterEach] [sig-auth] Certificates API [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:57:31.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "certificates-1597" for this suite. 08/11/23 14:57:31.128
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:31.135
Aug 11 14:57:31.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 14:57:31.136
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:31.153
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:31.156
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895
STEP: creating a Pod with a static label 08/11/23 14:57:31.167
STEP: watching for Pod to be ready 08/11/23 14:57:31.175
Aug 11 14:57:31.177: INFO: observed Pod pod-test in namespace pods-2773 in phase Pending with labels: map[test-pod-static:true] & conditions []
Aug 11 14:57:31.181: INFO: observed Pod pod-test in namespace pods-2773 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  }]
Aug 11 14:57:31.192: INFO: observed Pod pod-test in namespace pods-2773 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  }]
Aug 11 14:57:32.650: INFO: Found Pod pod-test in namespace pods-2773 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  }]
STEP: patching the Pod with a new Label and updated data 08/11/23 14:57:32.653
STEP: getting the Pod and ensuring that it's patched 08/11/23 14:57:32.667
STEP: replacing the Pod's status Ready condition to False 08/11/23 14:57:32.67
STEP: check the Pod again to ensure its Ready conditions are False 08/11/23 14:57:32.684
STEP: deleting the Pod via a Collection with a LabelSelector 08/11/23 14:57:32.684
STEP: watching for the Pod to be deleted 08/11/23 14:57:32.693
Aug 11 14:57:32.694: INFO: observed event type MODIFIED
Aug 11 14:57:34.656: INFO: observed event type MODIFIED
Aug 11 14:57:35.659: INFO: observed event type MODIFIED
Aug 11 14:57:35.667: INFO: observed event type MODIFIED
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 14:57:35.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-2773" for this suite. 08/11/23 14:57:35.678
{"msg":"PASSED [sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]","completed":198,"skipped":3711,"failed":0}
------------------------------
â€¢ [4.549 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should run through the lifecycle of Pods and PodStatus [Conformance]
  test/e2e/common/node/pods.go:895

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:31.135
    Aug 11 14:57:31.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 14:57:31.136
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:31.153
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:31.156
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should run through the lifecycle of Pods and PodStatus [Conformance]
      test/e2e/common/node/pods.go:895
    STEP: creating a Pod with a static label 08/11/23 14:57:31.167
    STEP: watching for Pod to be ready 08/11/23 14:57:31.175
    Aug 11 14:57:31.177: INFO: observed Pod pod-test in namespace pods-2773 in phase Pending with labels: map[test-pod-static:true] & conditions []
    Aug 11 14:57:31.181: INFO: observed Pod pod-test in namespace pods-2773 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  }]
    Aug 11 14:57:31.192: INFO: observed Pod pod-test in namespace pods-2773 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  }]
    Aug 11 14:57:32.650: INFO: Found Pod pod-test in namespace pods-2773 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:32 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:32 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 14:57:31 +0000 UTC  }]
    STEP: patching the Pod with a new Label and updated data 08/11/23 14:57:32.653
    STEP: getting the Pod and ensuring that it's patched 08/11/23 14:57:32.667
    STEP: replacing the Pod's status Ready condition to False 08/11/23 14:57:32.67
    STEP: check the Pod again to ensure its Ready conditions are False 08/11/23 14:57:32.684
    STEP: deleting the Pod via a Collection with a LabelSelector 08/11/23 14:57:32.684
    STEP: watching for the Pod to be deleted 08/11/23 14:57:32.693
    Aug 11 14:57:32.694: INFO: observed event type MODIFIED
    Aug 11 14:57:34.656: INFO: observed event type MODIFIED
    Aug 11 14:57:35.659: INFO: observed event type MODIFIED
    Aug 11 14:57:35.667: INFO: observed event type MODIFIED
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 14:57:35.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-2773" for this suite. 08/11/23 14:57:35.678
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:35.686
Aug 11 14:57:35.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:57:35.687
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:35.702
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:35.705
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88
STEP: Creating configMap with name configmap-test-volume-map-c6120f28-5850-4448-ad73-c782ec876d1b 08/11/23 14:57:35.707
STEP: Creating a pod to test consume configMaps 08/11/23 14:57:35.712
Aug 11 14:57:35.721: INFO: Waiting up to 5m0s for pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092" in namespace "configmap-9833" to be "Succeeded or Failed"
Aug 11 14:57:35.724: INFO: Pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657162ms
Aug 11 14:57:37.728: INFO: Pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007561721s
Aug 11 14:57:39.729: INFO: Pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008443185s
STEP: Saw pod success 08/11/23 14:57:39.729
Aug 11 14:57:39.729: INFO: Pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092" satisfied condition "Succeeded or Failed"
Aug 11 14:57:39.732: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:57:39.742
Aug 11 14:57:39.756: INFO: Waiting for pod pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092 to disappear
Aug 11 14:57:39.758: INFO: Pod pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092 no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:57:39.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-9833" for this suite. 08/11/23 14:57:39.762
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":199,"skipped":3746,"failed":0}
------------------------------
â€¢ [4.086 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:35.686
    Aug 11 14:57:35.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:57:35.687
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:35.702
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:35.705
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:88
    STEP: Creating configMap with name configmap-test-volume-map-c6120f28-5850-4448-ad73-c782ec876d1b 08/11/23 14:57:35.707
    STEP: Creating a pod to test consume configMaps 08/11/23 14:57:35.712
    Aug 11 14:57:35.721: INFO: Waiting up to 5m0s for pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092" in namespace "configmap-9833" to be "Succeeded or Failed"
    Aug 11 14:57:35.724: INFO: Pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.657162ms
    Aug 11 14:57:37.728: INFO: Pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007561721s
    Aug 11 14:57:39.729: INFO: Pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008443185s
    STEP: Saw pod success 08/11/23 14:57:39.729
    Aug 11 14:57:39.729: INFO: Pod "pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092" satisfied condition "Succeeded or Failed"
    Aug 11 14:57:39.732: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:57:39.742
    Aug 11 14:57:39.756: INFO: Waiting for pod pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092 to disappear
    Aug 11 14:57:39.758: INFO: Pod pod-configmaps-0babd22c-5240-41f9-999e-f2b84a4ad092 no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:57:39.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-9833" for this suite. 08/11/23 14:57:39.762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:39.773
Aug 11 14:57:39.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 14:57:39.774
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:39.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:39.792
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397
STEP: creating the pod 08/11/23 14:57:39.794
STEP: submitting the pod to kubernetes 08/11/23 14:57:39.794
Aug 11 14:57:39.803: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31" in namespace "pods-8588" to be "running and ready"
Aug 11 14:57:39.808: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Pending", Reason="", readiness=false. Elapsed: 5.212933ms
Aug 11 14:57:39.808: INFO: The phase of Pod pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:57:41.813: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Running", Reason="", readiness=true. Elapsed: 2.010473773s
Aug 11 14:57:41.813: INFO: The phase of Pod pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31 is Running (Ready = true)
Aug 11 14:57:41.813: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/11/23 14:57:41.817
STEP: updating the pod 08/11/23 14:57:41.82
Aug 11 14:57:42.335: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31"
Aug 11 14:57:42.335: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31" in namespace "pods-8588" to be "terminated with reason DeadlineExceeded"
Aug 11 14:57:42.339: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Running", Reason="", readiness=true. Elapsed: 3.313313ms
Aug 11 14:57:44.343: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Running", Reason="", readiness=true. Elapsed: 2.008024199s
Aug 11 14:57:46.343: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Running", Reason="", readiness=false. Elapsed: 4.008012542s
Aug 11 14:57:48.344: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.008166615s
Aug 11 14:57:48.344: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31" satisfied condition "terminated with reason DeadlineExceeded"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 14:57:48.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-8588" for this suite. 08/11/23 14:57:48.348
{"msg":"PASSED [sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]","completed":200,"skipped":3753,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.581 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:397

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:39.773
    Aug 11 14:57:39.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 14:57:39.774
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:39.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:39.792
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:397
    STEP: creating the pod 08/11/23 14:57:39.794
    STEP: submitting the pod to kubernetes 08/11/23 14:57:39.794
    Aug 11 14:57:39.803: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31" in namespace "pods-8588" to be "running and ready"
    Aug 11 14:57:39.808: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Pending", Reason="", readiness=false. Elapsed: 5.212933ms
    Aug 11 14:57:39.808: INFO: The phase of Pod pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:57:41.813: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Running", Reason="", readiness=true. Elapsed: 2.010473773s
    Aug 11 14:57:41.813: INFO: The phase of Pod pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31 is Running (Ready = true)
    Aug 11 14:57:41.813: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/11/23 14:57:41.817
    STEP: updating the pod 08/11/23 14:57:41.82
    Aug 11 14:57:42.335: INFO: Successfully updated pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31"
    Aug 11 14:57:42.335: INFO: Waiting up to 5m0s for pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31" in namespace "pods-8588" to be "terminated with reason DeadlineExceeded"
    Aug 11 14:57:42.339: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Running", Reason="", readiness=true. Elapsed: 3.313313ms
    Aug 11 14:57:44.343: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Running", Reason="", readiness=true. Elapsed: 2.008024199s
    Aug 11 14:57:46.343: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Running", Reason="", readiness=false. Elapsed: 4.008012542s
    Aug 11 14:57:48.344: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31": Phase="Failed", Reason="DeadlineExceeded", readiness=false. Elapsed: 6.008166615s
    Aug 11 14:57:48.344: INFO: Pod "pod-update-activedeadlineseconds-b0442164-0f1a-460d-9f45-f55869f8ff31" satisfied condition "terminated with reason DeadlineExceeded"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 14:57:48.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-8588" for this suite. 08/11/23 14:57:48.348
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:57:48.356
Aug 11 14:57:48.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-probe 08/11/23 14:57:48.357
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:48.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:48.374
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68
Aug 11 14:57:48.385: INFO: Waiting up to 5m0s for pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c" in namespace "container-probe-4341" to be "running and ready"
Aug 11 14:57:48.394: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.65924ms
Aug 11 14:57:48.394: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:57:50.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 2.012923219s
Aug 11 14:57:50.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:57:52.399: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 4.013914116s
Aug 11 14:57:52.399: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:57:54.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 6.013626544s
Aug 11 14:57:54.399: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:57:56.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 8.012880417s
Aug 11 14:57:56.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:57:58.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 10.013199074s
Aug 11 14:57:58.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:58:00.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 12.012990883s
Aug 11 14:58:00.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:58:02.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 14.01301902s
Aug 11 14:58:02.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:58:04.400: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 16.015171824s
Aug 11 14:58:04.400: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:58:06.399: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 18.013923622s
Aug 11 14:58:06.399: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:58:08.397: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 20.012196813s
Aug 11 14:58:08.397: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
Aug 11 14:58:10.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=true. Elapsed: 22.013250273s
Aug 11 14:58:10.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = true)
Aug 11 14:58:10.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c" satisfied condition "running and ready"
Aug 11 14:58:10.401: INFO: Container started at 2023-08-11 14:57:49 +0000 UTC, pod became ready at 2023-08-11 14:58:08 +0000 UTC
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 11 14:58:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4341" for this suite. 08/11/23 14:58:10.405
{"msg":"PASSED [sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]","completed":201,"skipped":3788,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.055 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:68

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:57:48.356
    Aug 11 14:57:48.356: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-probe 08/11/23 14:57:48.357
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:57:48.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:57:48.374
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:68
    Aug 11 14:57:48.385: INFO: Waiting up to 5m0s for pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c" in namespace "container-probe-4341" to be "running and ready"
    Aug 11 14:57:48.394: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Pending", Reason="", readiness=false. Elapsed: 8.65924ms
    Aug 11 14:57:48.394: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:57:50.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 2.012923219s
    Aug 11 14:57:50.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:57:52.399: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 4.013914116s
    Aug 11 14:57:52.399: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:57:54.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 6.013626544s
    Aug 11 14:57:54.399: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:57:56.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 8.012880417s
    Aug 11 14:57:56.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:57:58.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 10.013199074s
    Aug 11 14:57:58.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:58:00.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 12.012990883s
    Aug 11 14:58:00.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:58:02.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 14.01301902s
    Aug 11 14:58:02.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:58:04.400: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 16.015171824s
    Aug 11 14:58:04.400: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:58:06.399: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 18.013923622s
    Aug 11 14:58:06.399: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:58:08.397: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=false. Elapsed: 20.012196813s
    Aug 11 14:58:08.397: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = false)
    Aug 11 14:58:10.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c": Phase="Running", Reason="", readiness=true. Elapsed: 22.013250273s
    Aug 11 14:58:10.398: INFO: The phase of Pod test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c is Running (Ready = true)
    Aug 11 14:58:10.398: INFO: Pod "test-webserver-ae80cd5f-ada5-4948-bb1a-076bc7414f4c" satisfied condition "running and ready"
    Aug 11 14:58:10.401: INFO: Container started at 2023-08-11 14:57:49 +0000 UTC, pod became ready at 2023-08-11 14:58:08 +0000 UTC
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 11 14:58:10.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4341" for this suite. 08/11/23 14:58:10.405
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:10.414
Aug 11 14:58:10.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename endpointslice 08/11/23 14:58:10.414
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:10.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:10.435
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65
Aug 11 14:58:10.445: INFO: Endpoints addresses: [192.168.178.4 192.168.178.5 192.168.178.6] , ports: [6443]
Aug 11 14:58:10.445: INFO: EndpointSlices addresses: [192.168.178.4 192.168.178.5 192.168.178.6] , ports: [6443]
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Aug 11 14:58:10.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-4822" for this suite. 08/11/23 14:58:10.448
{"msg":"PASSED [sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]","completed":202,"skipped":3855,"failed":0}
------------------------------
â€¢ [0.040 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should have Endpoints and EndpointSlices pointing to API Server [Conformance]
  test/e2e/network/endpointslice.go:65

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:10.414
    Aug 11 14:58:10.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename endpointslice 08/11/23 14:58:10.414
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:10.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:10.435
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should have Endpoints and EndpointSlices pointing to API Server [Conformance]
      test/e2e/network/endpointslice.go:65
    Aug 11 14:58:10.445: INFO: Endpoints addresses: [192.168.178.4 192.168.178.5 192.168.178.6] , ports: [6443]
    Aug 11 14:58:10.445: INFO: EndpointSlices addresses: [192.168.178.4 192.168.178.5 192.168.178.6] , ports: [6443]
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Aug 11 14:58:10.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-4822" for this suite. 08/11/23 14:58:10.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:10.454
Aug 11 14:58:10.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:58:10.455
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:10.47
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:10.474
[It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118
STEP: Creating secret with name projected-secret-test-5b9e3203-6e4a-4256-877a-5b6e25a55fe3 08/11/23 14:58:10.477
STEP: Creating a pod to test consume secrets 08/11/23 14:58:10.482
Aug 11 14:58:10.493: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c" in namespace "projected-446" to be "Succeeded or Failed"
Aug 11 14:58:10.498: INFO: Pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.562402ms
Aug 11 14:58:12.502: INFO: Pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008551243s
Aug 11 14:58:14.504: INFO: Pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010142081s
STEP: Saw pod success 08/11/23 14:58:14.504
Aug 11 14:58:14.504: INFO: Pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c" satisfied condition "Succeeded or Failed"
Aug 11 14:58:14.506: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:58:14.514
Aug 11 14:58:14.527: INFO: Waiting for pod pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c to disappear
Aug 11 14:58:14.530: INFO: Pod pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 11 14:58:14.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-446" for this suite. 08/11/23 14:58:14.535
{"msg":"PASSED [sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]","completed":203,"skipped":3860,"failed":0}
------------------------------
â€¢ [4.089 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:118

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:10.454
    Aug 11 14:58:10.454: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:58:10.455
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:10.47
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:10.474
    [It] should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:118
    STEP: Creating secret with name projected-secret-test-5b9e3203-6e4a-4256-877a-5b6e25a55fe3 08/11/23 14:58:10.477
    STEP: Creating a pod to test consume secrets 08/11/23 14:58:10.482
    Aug 11 14:58:10.493: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c" in namespace "projected-446" to be "Succeeded or Failed"
    Aug 11 14:58:10.498: INFO: Pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 4.562402ms
    Aug 11 14:58:12.502: INFO: Pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008551243s
    Aug 11 14:58:14.504: INFO: Pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010142081s
    STEP: Saw pod success 08/11/23 14:58:14.504
    Aug 11 14:58:14.504: INFO: Pod "pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c" satisfied condition "Succeeded or Failed"
    Aug 11 14:58:14.506: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:58:14.514
    Aug 11 14:58:14.527: INFO: Waiting for pod pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c to disappear
    Aug 11 14:58:14.530: INFO: Pod pod-projected-secrets-6431605c-c621-4b13-97c3-a1f161ec0e2c no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 11 14:58:14.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-446" for this suite. 08/11/23 14:58:14.535
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace
  should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:14.544
Aug 11 14:58:14.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:58:14.545
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:14.56
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:14.563
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1732
[It] should update a single-container pod's image  [Conformance]
  test/e2e/kubectl/kubectl.go:1745
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/11/23 14:58:14.565
Aug 11 14:58:14.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8661 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
Aug 11 14:58:14.628: INFO: stderr: ""
Aug 11 14:58:14.628: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod is running 08/11/23 14:58:14.628
STEP: verifying the pod e2e-test-httpd-pod was created 08/11/23 14:58:19.681
Aug 11 14:58:19.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8661 get pod e2e-test-httpd-pod -o json'
Aug 11 14:58:19.739: INFO: stderr: ""
Aug 11 14:58:19.739: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-11T14:58:14Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8661\",\n        \"resourceVersion\": \"36431\",\n        \"uid\": \"acfc355a-889b-434c-b606-88efd31a55a6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-nwj67\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"constell-d93e7e1d-worker-d314547c-0lc3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-nwj67\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:58:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:58:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:58:15Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:58:14Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://27952da3b204fdf8a2f4186a1d05e93339e1341ff0ea3df2e1ca714851cb0511\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-11T14:58:15Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.178.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.0.226\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.0.226\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-11T14:58:14Z\"\n    }\n}\n"
STEP: replace the image in the pod 08/11/23 14:58:19.739
Aug 11 14:58:19.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8661 replace -f -'
Aug 11 14:58:20.595: INFO: stderr: ""
Aug 11 14:58:20.595: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 08/11/23 14:58:20.595
[AfterEach] Kubectl replace
  test/e2e/kubectl/kubectl.go:1736
Aug 11 14:58:20.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8661 delete pods e2e-test-httpd-pod'
Aug 11 14:58:21.796: INFO: stderr: ""
Aug 11 14:58:21.796: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:58:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-8661" for this suite. 08/11/23 14:58:21.801
{"msg":"PASSED [sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]","completed":204,"skipped":3878,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.264 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl replace
  test/e2e/kubectl/kubectl.go:1729
    should update a single-container pod's image  [Conformance]
    test/e2e/kubectl/kubectl.go:1745

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:14.544
    Aug 11 14:58:14.544: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:58:14.545
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:14.56
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:14.563
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1732
    [It] should update a single-container pod's image  [Conformance]
      test/e2e/kubectl/kubectl.go:1745
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/11/23 14:58:14.565
    Aug 11 14:58:14.566: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8661 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
    Aug 11 14:58:14.628: INFO: stderr: ""
    Aug 11 14:58:14.628: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod is running 08/11/23 14:58:14.628
    STEP: verifying the pod e2e-test-httpd-pod was created 08/11/23 14:58:19.681
    Aug 11 14:58:19.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8661 get pod e2e-test-httpd-pod -o json'
    Aug 11 14:58:19.739: INFO: stderr: ""
    Aug 11 14:58:19.739: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-11T14:58:14Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-8661\",\n        \"resourceVersion\": \"36431\",\n        \"uid\": \"acfc355a-889b-434c-b606-88efd31a55a6\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-nwj67\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"constell-d93e7e1d-worker-d314547c-0lc3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-nwj67\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:58:14Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:58:15Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:58:15Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-11T14:58:14Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://27952da3b204fdf8a2f4186a1d05e93339e1341ff0ea3df2e1ca714851cb0511\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-2\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-11T14:58:15Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.178.2\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.10.0.226\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.10.0.226\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-11T14:58:14Z\"\n    }\n}\n"
    STEP: replace the image in the pod 08/11/23 14:58:19.739
    Aug 11 14:58:19.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8661 replace -f -'
    Aug 11 14:58:20.595: INFO: stderr: ""
    Aug 11 14:58:20.595: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
    STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-2 08/11/23 14:58:20.595
    [AfterEach] Kubectl replace
      test/e2e/kubectl/kubectl.go:1736
    Aug 11 14:58:20.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-8661 delete pods e2e-test-httpd-pod'
    Aug 11 14:58:21.796: INFO: stderr: ""
    Aug 11 14:58:21.796: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:58:21.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-8661" for this suite. 08/11/23 14:58:21.801
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
[BeforeEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:21.81
Aug 11 14:58:21.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:58:21.811
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:21.825
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:21.827
[It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104
Aug 11 14:58:21.841: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5520 to be scheduled
[AfterEach] [sig-node] RuntimeClass
  test/e2e/framework/framework.go:187
Aug 11 14:58:21.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "runtimeclass-5520" for this suite. 08/11/23 14:58:21.857
{"msg":"PASSED [sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]","completed":205,"skipped":3898,"failed":0}
------------------------------
â€¢ [0.054 seconds]
[sig-node] RuntimeClass
test/e2e/common/node/framework.go:23
  should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
  test/e2e/common/node/runtimeclass.go:104

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:21.81
    Aug 11 14:58:21.810: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename runtimeclass 08/11/23 14:58:21.811
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:21.825
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:21.827
    [It] should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
      test/e2e/common/node/runtimeclass.go:104
    Aug 11 14:58:21.841: INFO: Waiting up to 1m20s for at least 1 pods in namespace runtimeclass-5520 to be scheduled
    [AfterEach] [sig-node] RuntimeClass
      test/e2e/framework/framework.go:187
    Aug 11 14:58:21.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "runtimeclass-5520" for this suite. 08/11/23 14:58:21.857
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:21.866
Aug 11 14:58:21.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename gc 08/11/23 14:58:21.867
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:21.88
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:21.883
[It] should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849
Aug 11 14:58:21.922: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5ac70216-2356-4850-92fa-f2b15bfe95bd", Controller:(*bool)(0xc003df59d6), BlockOwnerDeletion:(*bool)(0xc003df59d7)}}
Aug 11 14:58:21.929: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4b1cdbbc-9b3a-498b-aeb6-b057ff2fb92e", Controller:(*bool)(0xc003df5c66), BlockOwnerDeletion:(*bool)(0xc003df5c67)}}
Aug 11 14:58:21.936: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9b26cbb2-ceb4-4445-8a85-b0b25c241b5e", Controller:(*bool)(0xc003df5e7e), BlockOwnerDeletion:(*bool)(0xc003df5e7f)}}
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 11 14:58:26.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-2792" for this suite. 08/11/23 14:58:26.952
{"msg":"PASSED [sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]","completed":206,"skipped":3949,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.092 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not be blocked by dependency circle [Conformance]
  test/e2e/apimachinery/garbage_collector.go:849

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:21.866
    Aug 11 14:58:21.866: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename gc 08/11/23 14:58:21.867
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:21.88
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:21.883
    [It] should not be blocked by dependency circle [Conformance]
      test/e2e/apimachinery/garbage_collector.go:849
    Aug 11 14:58:21.922: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"5ac70216-2356-4850-92fa-f2b15bfe95bd", Controller:(*bool)(0xc003df59d6), BlockOwnerDeletion:(*bool)(0xc003df59d7)}}
    Aug 11 14:58:21.929: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4b1cdbbc-9b3a-498b-aeb6-b057ff2fb92e", Controller:(*bool)(0xc003df5c66), BlockOwnerDeletion:(*bool)(0xc003df5c67)}}
    Aug 11 14:58:21.936: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"9b26cbb2-ceb4-4445-8a85-b0b25c241b5e", Controller:(*bool)(0xc003df5e7e), BlockOwnerDeletion:(*bool)(0xc003df5e7f)}}
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 11 14:58:26.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-2792" for this suite. 08/11/23 14:58:26.952
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:26.959
Aug 11 14:58:26.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename disruption 08/11/23 14:58:26.96
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:26.98
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:26.982
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107
STEP: creating the pdb 08/11/23 14:58:26.985
STEP: Waiting for the pdb to be processed 08/11/23 14:58:26.991
STEP: updating the pdb 08/11/23 14:58:28.999
STEP: Waiting for the pdb to be processed 08/11/23 14:58:29.007
STEP: patching the pdb 08/11/23 14:58:31.018
STEP: Waiting for the pdb to be processed 08/11/23 14:58:31.027
STEP: Waiting for the pdb to be deleted 08/11/23 14:58:31.044
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 11 14:58:31.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-1136" for this suite. 08/11/23 14:58:31.051
{"msg":"PASSED [sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]","completed":207,"skipped":3961,"failed":0}
------------------------------
â€¢ [4.099 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should create a PodDisruptionBudget [Conformance]
  test/e2e/apps/disruption.go:107

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:26.959
    Aug 11 14:58:26.959: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename disruption 08/11/23 14:58:26.96
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:26.98
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:26.982
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should create a PodDisruptionBudget [Conformance]
      test/e2e/apps/disruption.go:107
    STEP: creating the pdb 08/11/23 14:58:26.985
    STEP: Waiting for the pdb to be processed 08/11/23 14:58:26.991
    STEP: updating the pdb 08/11/23 14:58:28.999
    STEP: Waiting for the pdb to be processed 08/11/23 14:58:29.007
    STEP: patching the pdb 08/11/23 14:58:31.018
    STEP: Waiting for the pdb to be processed 08/11/23 14:58:31.027
    STEP: Waiting for the pdb to be deleted 08/11/23 14:58:31.044
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 11 14:58:31.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-1136" for this suite. 08/11/23 14:58:31.051
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:31.058
Aug 11 14:58:31.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename disruption 08/11/23 14:58:31.059
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:31.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:31.077
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[It] should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346
STEP: Creating a pdb that targets all three pods in a test replica set 08/11/23 14:58:31.08
STEP: Waiting for the pdb to be processed 08/11/23 14:58:31.085
STEP: First trying to evict a pod which shouldn't be evictable 08/11/23 14:58:31.098
STEP: Waiting for all pods to be running 08/11/23 14:58:31.098
Aug 11 14:58:31.100: INFO: pods: 0 < 3
STEP: locating a running pod 08/11/23 14:58:33.105
STEP: Updating the pdb to allow a pod to be evicted 08/11/23 14:58:33.114
STEP: Waiting for the pdb to be processed 08/11/23 14:58:33.121
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/11/23 14:58:35.13
STEP: Waiting for all pods to be running 08/11/23 14:58:35.13
STEP: Waiting for the pdb to observed all healthy pods 08/11/23 14:58:35.134
STEP: Patching the pdb to disallow a pod to be evicted 08/11/23 14:58:35.155
STEP: Waiting for the pdb to be processed 08/11/23 14:58:35.177
STEP: Waiting for all pods to be running 08/11/23 14:58:37.185
STEP: locating a running pod 08/11/23 14:58:37.189
STEP: Deleting the pdb to allow a pod to be evicted 08/11/23 14:58:37.197
STEP: Waiting for the pdb to be deleted 08/11/23 14:58:37.203
STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/11/23 14:58:37.206
STEP: Waiting for all pods to be running 08/11/23 14:58:37.206
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 11 14:58:37.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-7402" for this suite. 08/11/23 14:58:37.228
{"msg":"PASSED [sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]","completed":208,"skipped":3962,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.179 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  should block an eviction until the PDB is updated to allow it [Conformance]
  test/e2e/apps/disruption.go:346

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:31.058
    Aug 11 14:58:31.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename disruption 08/11/23 14:58:31.059
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:31.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:31.077
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [It] should block an eviction until the PDB is updated to allow it [Conformance]
      test/e2e/apps/disruption.go:346
    STEP: Creating a pdb that targets all three pods in a test replica set 08/11/23 14:58:31.08
    STEP: Waiting for the pdb to be processed 08/11/23 14:58:31.085
    STEP: First trying to evict a pod which shouldn't be evictable 08/11/23 14:58:31.098
    STEP: Waiting for all pods to be running 08/11/23 14:58:31.098
    Aug 11 14:58:31.100: INFO: pods: 0 < 3
    STEP: locating a running pod 08/11/23 14:58:33.105
    STEP: Updating the pdb to allow a pod to be evicted 08/11/23 14:58:33.114
    STEP: Waiting for the pdb to be processed 08/11/23 14:58:33.121
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/11/23 14:58:35.13
    STEP: Waiting for all pods to be running 08/11/23 14:58:35.13
    STEP: Waiting for the pdb to observed all healthy pods 08/11/23 14:58:35.134
    STEP: Patching the pdb to disallow a pod to be evicted 08/11/23 14:58:35.155
    STEP: Waiting for the pdb to be processed 08/11/23 14:58:35.177
    STEP: Waiting for all pods to be running 08/11/23 14:58:37.185
    STEP: locating a running pod 08/11/23 14:58:37.189
    STEP: Deleting the pdb to allow a pod to be evicted 08/11/23 14:58:37.197
    STEP: Waiting for the pdb to be deleted 08/11/23 14:58:37.203
    STEP: Trying to evict the same pod we tried earlier which should now be evictable 08/11/23 14:58:37.206
    STEP: Waiting for all pods to be running 08/11/23 14:58:37.206
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 11 14:58:37.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-7402" for this suite. 08/11/23 14:58:37.228
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-node] Pods
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:37.238
Aug 11 14:58:37.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 14:58:37.239
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:37.254
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:37.257
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343
STEP: creating the pod 08/11/23 14:58:37.26
STEP: submitting the pod to kubernetes 08/11/23 14:58:37.26
Aug 11 14:58:37.269: INFO: Waiting up to 5m0s for pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc" in namespace "pods-7536" to be "running and ready"
Aug 11 14:58:37.272: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.813108ms
Aug 11 14:58:37.272: INFO: The phase of Pod pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:58:39.278: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008750099s
Aug 11 14:58:39.278: INFO: The phase of Pod pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc is Running (Ready = true)
Aug 11 14:58:39.278: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc" satisfied condition "running and ready"
STEP: verifying the pod is in kubernetes 08/11/23 14:58:39.281
STEP: updating the pod 08/11/23 14:58:39.285
Aug 11 14:58:39.797: INFO: Successfully updated pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc"
Aug 11 14:58:39.797: INFO: Waiting up to 5m0s for pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc" in namespace "pods-7536" to be "running"
Aug 11 14:58:39.800: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.829548ms
Aug 11 14:58:39.800: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc" satisfied condition "running"
STEP: verifying the updated pod is in kubernetes 08/11/23 14:58:39.8
Aug 11 14:58:39.803: INFO: Pod update OK
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 14:58:39.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-7536" for this suite. 08/11/23 14:58:39.811
{"msg":"PASSED [sig-node] Pods should be updated [NodeConformance] [Conformance]","completed":209,"skipped":3968,"failed":0}
------------------------------
â€¢ [2.579 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should be updated [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:343

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:37.238
    Aug 11 14:58:37.238: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 14:58:37.239
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:37.254
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:37.257
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should be updated [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:343
    STEP: creating the pod 08/11/23 14:58:37.26
    STEP: submitting the pod to kubernetes 08/11/23 14:58:37.26
    Aug 11 14:58:37.269: INFO: Waiting up to 5m0s for pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc" in namespace "pods-7536" to be "running and ready"
    Aug 11 14:58:37.272: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.813108ms
    Aug 11 14:58:37.272: INFO: The phase of Pod pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:58:39.278: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.008750099s
    Aug 11 14:58:39.278: INFO: The phase of Pod pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc is Running (Ready = true)
    Aug 11 14:58:39.278: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc" satisfied condition "running and ready"
    STEP: verifying the pod is in kubernetes 08/11/23 14:58:39.281
    STEP: updating the pod 08/11/23 14:58:39.285
    Aug 11 14:58:39.797: INFO: Successfully updated pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc"
    Aug 11 14:58:39.797: INFO: Waiting up to 5m0s for pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc" in namespace "pods-7536" to be "running"
    Aug 11 14:58:39.800: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc": Phase="Running", Reason="", readiness=true. Elapsed: 2.829548ms
    Aug 11 14:58:39.800: INFO: Pod "pod-update-9043332c-b558-4765-a4f0-59f5a6b484fc" satisfied condition "running"
    STEP: verifying the updated pod is in kubernetes 08/11/23 14:58:39.8
    Aug 11 14:58:39.803: INFO: Pod update OK
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 14:58:39.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-7536" for this suite. 08/11/23 14:58:39.811
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions
  should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:39.818
Aug 11 14:58:39.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:58:39.819
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:39.842
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:39.844
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if v1 is in available api versions  [Conformance]
  test/e2e/kubectl/kubectl.go:822
STEP: validating api versions 08/11/23 14:58:39.846
Aug 11 14:58:39.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-4733 api-versions'
Aug 11 14:58:39.895: INFO: stderr: ""
Aug 11 14:58:39.895: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnodemaintenance.medik8s.io/v1beta1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nupdate.edgeless.systems/v1alpha1\nv1\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:58:39.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-4733" for this suite. 08/11/23 14:58:39.9
{"msg":"PASSED [sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]","completed":210,"skipped":3983,"failed":0}
------------------------------
â€¢ [0.092 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl api-versions
  test/e2e/kubectl/kubectl.go:816
    should check if v1 is in available api versions  [Conformance]
    test/e2e/kubectl/kubectl.go:822

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:39.818
    Aug 11 14:58:39.818: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:58:39.819
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:39.842
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:39.844
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if v1 is in available api versions  [Conformance]
      test/e2e/kubectl/kubectl.go:822
    STEP: validating api versions 08/11/23 14:58:39.846
    Aug 11 14:58:39.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-4733 api-versions'
    Aug 11 14:58:39.895: INFO: stderr: ""
    Aug 11 14:58:39.895: INFO: stdout: "acme.cert-manager.io/v1\nadmissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nautoscaling/v2beta2\nbatch/v1\ncert-manager.io/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta1\nflowcontrol.apiserver.k8s.io/v1beta2\nnetworking.k8s.io/v1\nnode.k8s.io/v1\nnodemaintenance.medik8s.io/v1beta1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nstorage.k8s.io/v1beta1\nupdate.edgeless.systems/v1alpha1\nv1\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:58:39.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-4733" for this suite. 08/11/23 14:58:39.9
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:39.91
Aug 11 14:58:39.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pod-network-test 08/11/23 14:58:39.911
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:39.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:39.928
[It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:122
STEP: Performing setup for networking test in namespace pod-network-test-4728 08/11/23 14:58:39.931
STEP: creating a selector 08/11/23 14:58:39.931
STEP: Creating the service pods in kubernetes 08/11/23 14:58:39.931
Aug 11 14:58:39.931: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 11 14:58:39.955: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4728" to be "running and ready"
Aug 11 14:58:39.967: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.535749ms
Aug 11 14:58:39.967: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:58:41.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016303375s
Aug 11 14:58:41.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:58:43.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017664174s
Aug 11 14:58:43.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:58:45.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017702393s
Aug 11 14:58:45.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:58:47.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016392499s
Aug 11 14:58:47.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:58:49.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016682434s
Aug 11 14:58:49.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 14:58:51.971: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015919987s
Aug 11 14:58:51.971: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 11 14:58:51.971: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 11 14:58:51.974: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4728" to be "running and ready"
Aug 11 14:58:51.977: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.023304ms
Aug 11 14:58:51.977: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 11 14:58:51.977: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/11/23 14:58:51.98
Aug 11 14:58:51.994: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4728" to be "running"
Aug 11 14:58:52.000: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.967986ms
Aug 11 14:58:54.005: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011236746s
Aug 11 14:58:54.005: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 11 14:58:54.008: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4728" to be "running"
Aug 11 14:58:54.011: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.841388ms
Aug 11 14:58:54.011: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 11 14:58:54.014: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 11 14:58:54.014: INFO: Going to poll 10.10.0.32 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Aug 11 14:58:54.017: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.0.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4728 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:58:54.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:58:54.017: INFO: ExecWithOptions: Clientset creation
Aug 11 14:58:54.017: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4728/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.0.32+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 14:58:55.093: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 11 14:58:55.093: INFO: Going to poll 10.10.1.6 on port 8081 at least 0 times, with a maximum of 34 tries before failing
Aug 11 14:58:55.097: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.1.6 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4728 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 14:58:55.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 14:58:55.097: INFO: ExecWithOptions: Clientset creation
Aug 11 14:58:55.097: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4728/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.1.6+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 14:58:56.173: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Aug 11 14:58:56.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-4728" for this suite. 08/11/23 14:58:56.178
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]","completed":211,"skipped":3990,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.276 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:39.91
    Aug 11 14:58:39.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pod-network-test 08/11/23 14:58:39.911
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:39.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:39.928
    [It] should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:122
    STEP: Performing setup for networking test in namespace pod-network-test-4728 08/11/23 14:58:39.931
    STEP: creating a selector 08/11/23 14:58:39.931
    STEP: Creating the service pods in kubernetes 08/11/23 14:58:39.931
    Aug 11 14:58:39.931: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 11 14:58:39.955: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-4728" to be "running and ready"
    Aug 11 14:58:39.967: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 11.535749ms
    Aug 11 14:58:39.967: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:58:41.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.016303375s
    Aug 11 14:58:41.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:58:43.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.017664174s
    Aug 11 14:58:43.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:58:45.973: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.017702393s
    Aug 11 14:58:45.973: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:58:47.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.016392499s
    Aug 11 14:58:47.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:58:49.972: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.016682434s
    Aug 11 14:58:49.972: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 14:58:51.971: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.015919987s
    Aug 11 14:58:51.971: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 11 14:58:51.971: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 11 14:58:51.974: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-4728" to be "running and ready"
    Aug 11 14:58:51.977: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.023304ms
    Aug 11 14:58:51.977: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 11 14:58:51.977: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/11/23 14:58:51.98
    Aug 11 14:58:51.994: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-4728" to be "running"
    Aug 11 14:58:52.000: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.967986ms
    Aug 11 14:58:54.005: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.011236746s
    Aug 11 14:58:54.005: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 11 14:58:54.008: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-4728" to be "running"
    Aug 11 14:58:54.011: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.841388ms
    Aug 11 14:58:54.011: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 11 14:58:54.014: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 11 14:58:54.014: INFO: Going to poll 10.10.0.32 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Aug 11 14:58:54.017: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.0.32 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4728 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:58:54.017: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:58:54.017: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:58:54.017: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4728/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.0.32+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 14:58:55.093: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 11 14:58:55.093: INFO: Going to poll 10.10.1.6 on port 8081 at least 0 times, with a maximum of 34 tries before failing
    Aug 11 14:58:55.097: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.10.1.6 8081 | grep -v '^\s*$'] Namespace:pod-network-test-4728 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 14:58:55.097: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 14:58:55.097: INFO: ExecWithOptions: Clientset creation
    Aug 11 14:58:55.097: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-4728/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.10.1.6+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 14:58:56.173: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Aug 11 14:58:56.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-4728" for this suite. 08/11/23 14:58:56.178
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:58:56.187
Aug 11 14:58:56.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:58:56.188
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:56.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:56.206
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:58:56.212
Aug 11 14:58:56.221: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8694" to be "running and ready"
Aug 11 14:58:56.226: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.537882ms
Aug 11 14:58:56.227: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:58:58.231: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009875604s
Aug 11 14:58:58.231: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 11 14:58:58.231: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop exec hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:114
STEP: create the pod with lifecycle hook 08/11/23 14:58:58.235
Aug 11 14:58:58.241: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8694" to be "running and ready"
Aug 11 14:58:58.244: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.706374ms
Aug 11 14:58:58.244: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:59:00.248: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006606891s
Aug 11 14:59:00.248: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
Aug 11 14:59:00.248: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/11/23 14:59:00.25
Aug 11 14:59:00.257: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 11 14:59:00.260: INFO: Pod pod-with-prestop-exec-hook still exists
Aug 11 14:59:02.260: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
Aug 11 14:59:02.264: INFO: Pod pod-with-prestop-exec-hook no longer exists
STEP: check prestop hook 08/11/23 14:59:02.264
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Aug 11 14:59:02.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8694" for this suite. 08/11/23 14:59:02.285
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]","completed":212,"skipped":3994,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.105 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop exec hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:114

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:58:56.187
    Aug 11 14:58:56.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:58:56.188
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:58:56.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:58:56.206
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:58:56.212
    Aug 11 14:58:56.221: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8694" to be "running and ready"
    Aug 11 14:58:56.226: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.537882ms
    Aug 11 14:58:56.227: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:58:58.231: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.009875604s
    Aug 11 14:58:58.231: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 11 14:58:58.231: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop exec hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:114
    STEP: create the pod with lifecycle hook 08/11/23 14:58:58.235
    Aug 11 14:58:58.241: INFO: Waiting up to 5m0s for pod "pod-with-prestop-exec-hook" in namespace "container-lifecycle-hook-8694" to be "running and ready"
    Aug 11 14:58:58.244: INFO: Pod "pod-with-prestop-exec-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.706374ms
    Aug 11 14:58:58.244: INFO: The phase of Pod pod-with-prestop-exec-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:59:00.248: INFO: Pod "pod-with-prestop-exec-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.006606891s
    Aug 11 14:59:00.248: INFO: The phase of Pod pod-with-prestop-exec-hook is Running (Ready = true)
    Aug 11 14:59:00.248: INFO: Pod "pod-with-prestop-exec-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/11/23 14:59:00.25
    Aug 11 14:59:00.257: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 11 14:59:00.260: INFO: Pod pod-with-prestop-exec-hook still exists
    Aug 11 14:59:02.260: INFO: Waiting for pod pod-with-prestop-exec-hook to disappear
    Aug 11 14:59:02.264: INFO: Pod pod-with-prestop-exec-hook no longer exists
    STEP: check prestop hook 08/11/23 14:59:02.264
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Aug 11 14:59:02.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8694" for this suite. 08/11/23 14:59:02.285
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:02.293
Aug 11 14:59:02.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 14:59:02.294
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:02.311
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:02.318
[It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43
STEP: Creating a pod to test downward api env vars 08/11/23 14:59:02.32
Aug 11 14:59:02.330: INFO: Waiting up to 5m0s for pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8" in namespace "downward-api-2134" to be "Succeeded or Failed"
Aug 11 14:59:02.336: INFO: Pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089189ms
Aug 11 14:59:04.342: INFO: Pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012515575s
Aug 11 14:59:06.341: INFO: Pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011287663s
STEP: Saw pod success 08/11/23 14:59:06.341
Aug 11 14:59:06.341: INFO: Pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8" satisfied condition "Succeeded or Failed"
Aug 11 14:59:06.344: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8 container dapi-container: <nil>
STEP: delete the pod 08/11/23 14:59:06.353
Aug 11 14:59:06.367: INFO: Waiting for pod downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8 to disappear
Aug 11 14:59:06.371: INFO: Pod downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 11 14:59:06.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2134" for this suite. 08/11/23 14:59:06.374
{"msg":"PASSED [sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]","completed":213,"skipped":4014,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:02.293
    Aug 11 14:59:02.293: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 14:59:02.294
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:02.311
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:02.318
    [It] should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:43
    STEP: Creating a pod to test downward api env vars 08/11/23 14:59:02.32
    Aug 11 14:59:02.330: INFO: Waiting up to 5m0s for pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8" in namespace "downward-api-2134" to be "Succeeded or Failed"
    Aug 11 14:59:02.336: INFO: Pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8": Phase="Pending", Reason="", readiness=false. Elapsed: 6.089189ms
    Aug 11 14:59:04.342: INFO: Pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.012515575s
    Aug 11 14:59:06.341: INFO: Pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011287663s
    STEP: Saw pod success 08/11/23 14:59:06.341
    Aug 11 14:59:06.341: INFO: Pod "downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8" satisfied condition "Succeeded or Failed"
    Aug 11 14:59:06.344: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 14:59:06.353
    Aug 11 14:59:06.367: INFO: Waiting for pod downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8 to disappear
    Aug 11 14:59:06.371: INFO: Pod downward-api-8683c6aa-6a67-4a6d-9cab-904dd64848e8 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 11 14:59:06.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2134" for this suite. 08/11/23 14:59:06.374
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info
  should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:06.381
Aug 11 14:59:06.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 14:59:06.382
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:06.396
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:06.398
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
  test/e2e/kubectl/kubectl.go:1248
STEP: validating cluster-info 08/11/23 14:59:06.401
Aug 11 14:59:06.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5002 cluster-info'
Aug 11 14:59:06.456: INFO: stderr: ""
Aug 11 14:59:06.456: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 14:59:06.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5002" for this suite. 08/11/23 14:59:06.461
{"msg":"PASSED [sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]","completed":214,"skipped":4017,"failed":0}
------------------------------
â€¢ [0.088 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl cluster-info
  test/e2e/kubectl/kubectl.go:1242
    should check if Kubernetes control plane services is included in cluster-info  [Conformance]
    test/e2e/kubectl/kubectl.go:1248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:06.381
    Aug 11 14:59:06.381: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 14:59:06.382
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:06.396
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:06.398
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check if Kubernetes control plane services is included in cluster-info  [Conformance]
      test/e2e/kubectl/kubectl.go:1248
    STEP: validating cluster-info 08/11/23 14:59:06.401
    Aug 11 14:59:06.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5002 cluster-info'
    Aug 11 14:59:06.456: INFO: stderr: ""
    Aug 11 14:59:06.456: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 14:59:06.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5002" for this suite. 08/11/23 14:59:06.461
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:06.47
Aug 11 14:59:06.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename subpath 08/11/23 14:59:06.47
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:06.484
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:06.487
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 14:59:06.489
[It] should support subpaths with downward pod [Conformance]
  test/e2e/storage/subpath.go:92
STEP: Creating pod pod-subpath-test-downwardapi-kbww 08/11/23 14:59:06.498
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:59:06.498
Aug 11 14:59:06.506: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kbww" in namespace "subpath-5581" to be "Succeeded or Failed"
Aug 11 14:59:06.511: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Pending", Reason="", readiness=false. Elapsed: 4.639115ms
Aug 11 14:59:08.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 2.00870944s
Aug 11 14:59:10.516: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 4.009781348s
Aug 11 14:59:12.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 6.008312039s
Aug 11 14:59:14.516: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 8.009284297s
Aug 11 14:59:16.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 10.008911981s
Aug 11 14:59:18.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 12.008312179s
Aug 11 14:59:20.516: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 14.009360759s
Aug 11 14:59:22.514: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 16.007956212s
Aug 11 14:59:24.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 18.008686422s
Aug 11 14:59:26.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 20.008982857s
Aug 11 14:59:28.516: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=false. Elapsed: 22.009667515s
Aug 11 14:59:30.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008711662s
STEP: Saw pod success 08/11/23 14:59:30.515
Aug 11 14:59:30.515: INFO: Pod "pod-subpath-test-downwardapi-kbww" satisfied condition "Succeeded or Failed"
Aug 11 14:59:30.519: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-downwardapi-kbww container test-container-subpath-downwardapi-kbww: <nil>
STEP: delete the pod 08/11/23 14:59:30.528
Aug 11 14:59:30.546: INFO: Waiting for pod pod-subpath-test-downwardapi-kbww to disappear
Aug 11 14:59:30.549: INFO: Pod pod-subpath-test-downwardapi-kbww no longer exists
STEP: Deleting pod pod-subpath-test-downwardapi-kbww 08/11/23 14:59:30.549
Aug 11 14:59:30.549: INFO: Deleting pod "pod-subpath-test-downwardapi-kbww" in namespace "subpath-5581"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 11 14:59:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-5581" for this suite. 08/11/23 14:59:30.556
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]","completed":215,"skipped":4026,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.094 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with downward pod [Conformance]
    test/e2e/storage/subpath.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:06.47
    Aug 11 14:59:06.470: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename subpath 08/11/23 14:59:06.47
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:06.484
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:06.487
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 14:59:06.489
    [It] should support subpaths with downward pod [Conformance]
      test/e2e/storage/subpath.go:92
    STEP: Creating pod pod-subpath-test-downwardapi-kbww 08/11/23 14:59:06.498
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 14:59:06.498
    Aug 11 14:59:06.506: INFO: Waiting up to 5m0s for pod "pod-subpath-test-downwardapi-kbww" in namespace "subpath-5581" to be "Succeeded or Failed"
    Aug 11 14:59:06.511: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Pending", Reason="", readiness=false. Elapsed: 4.639115ms
    Aug 11 14:59:08.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 2.00870944s
    Aug 11 14:59:10.516: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 4.009781348s
    Aug 11 14:59:12.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 6.008312039s
    Aug 11 14:59:14.516: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 8.009284297s
    Aug 11 14:59:16.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 10.008911981s
    Aug 11 14:59:18.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 12.008312179s
    Aug 11 14:59:20.516: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 14.009360759s
    Aug 11 14:59:22.514: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 16.007956212s
    Aug 11 14:59:24.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 18.008686422s
    Aug 11 14:59:26.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=true. Elapsed: 20.008982857s
    Aug 11 14:59:28.516: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Running", Reason="", readiness=false. Elapsed: 22.009667515s
    Aug 11 14:59:30.515: INFO: Pod "pod-subpath-test-downwardapi-kbww": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.008711662s
    STEP: Saw pod success 08/11/23 14:59:30.515
    Aug 11 14:59:30.515: INFO: Pod "pod-subpath-test-downwardapi-kbww" satisfied condition "Succeeded or Failed"
    Aug 11 14:59:30.519: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-downwardapi-kbww container test-container-subpath-downwardapi-kbww: <nil>
    STEP: delete the pod 08/11/23 14:59:30.528
    Aug 11 14:59:30.546: INFO: Waiting for pod pod-subpath-test-downwardapi-kbww to disappear
    Aug 11 14:59:30.549: INFO: Pod pod-subpath-test-downwardapi-kbww no longer exists
    STEP: Deleting pod pod-subpath-test-downwardapi-kbww 08/11/23 14:59:30.549
    Aug 11 14:59:30.549: INFO: Deleting pod "pod-subpath-test-downwardapi-kbww" in namespace "subpath-5581"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 11 14:59:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-5581" for this suite. 08/11/23 14:59:30.556
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:30.564
Aug 11 14:59:30.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:59:30.565
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:30.578
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:30.581
[It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108
STEP: Creating configMap with name projected-configmap-test-volume-map-9cb46aa8-e602-4877-a75c-b82554ceb344 08/11/23 14:59:30.583
STEP: Creating a pod to test consume configMaps 08/11/23 14:59:30.588
Aug 11 14:59:30.596: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8" in namespace "projected-3911" to be "Succeeded or Failed"
Aug 11 14:59:30.602: INFO: Pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.733359ms
Aug 11 14:59:32.606: INFO: Pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009957748s
Aug 11 14:59:34.607: INFO: Pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010471681s
STEP: Saw pod success 08/11/23 14:59:34.607
Aug 11 14:59:34.607: INFO: Pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8" satisfied condition "Succeeded or Failed"
Aug 11 14:59:34.609: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:59:34.617
Aug 11 14:59:34.635: INFO: Waiting for pod pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8 to disappear
Aug 11 14:59:34.637: INFO: Pod pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 11 14:59:34.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-3911" for this suite. 08/11/23 14:59:34.641
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]","completed":216,"skipped":4026,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:108

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:30.564
    Aug 11 14:59:30.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:59:30.565
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:30.578
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:30.581
    [It] should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:108
    STEP: Creating configMap with name projected-configmap-test-volume-map-9cb46aa8-e602-4877-a75c-b82554ceb344 08/11/23 14:59:30.583
    STEP: Creating a pod to test consume configMaps 08/11/23 14:59:30.588
    Aug 11 14:59:30.596: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8" in namespace "projected-3911" to be "Succeeded or Failed"
    Aug 11 14:59:30.602: INFO: Pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.733359ms
    Aug 11 14:59:32.606: INFO: Pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009957748s
    Aug 11 14:59:34.607: INFO: Pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010471681s
    STEP: Saw pod success 08/11/23 14:59:34.607
    Aug 11 14:59:34.607: INFO: Pod "pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8" satisfied condition "Succeeded or Failed"
    Aug 11 14:59:34.609: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:59:34.617
    Aug 11 14:59:34.635: INFO: Waiting for pod pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8 to disappear
    Aug 11 14:59:34.637: INFO: Pod pod-projected-configmaps-633c7d57-7dd6-4ac5-8518-ad69907015a8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 11 14:59:34.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-3911" for this suite. 08/11/23 14:59:34.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:34.654
Aug 11 14:59:34.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 14:59:34.654
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:34.67
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:34.673
[It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55
STEP: Creating projection with secret that has name projected-secret-test-4540a4c8-8a40-4ded-8860-5297d8bdbc20 08/11/23 14:59:34.676
STEP: Creating a pod to test consume secrets 08/11/23 14:59:34.68
Aug 11 14:59:34.690: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e" in namespace "projected-559" to be "Succeeded or Failed"
Aug 11 14:59:34.694: INFO: Pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.15689ms
Aug 11 14:59:36.698: INFO: Pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008716889s
Aug 11 14:59:38.699: INFO: Pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009689795s
STEP: Saw pod success 08/11/23 14:59:38.699
Aug 11 14:59:38.700: INFO: Pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e" satisfied condition "Succeeded or Failed"
Aug 11 14:59:38.703: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:59:38.711
Aug 11 14:59:38.724: INFO: Waiting for pod pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e to disappear
Aug 11 14:59:38.727: INFO: Pod pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 11 14:59:38.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-559" for this suite. 08/11/23 14:59:38.732
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]","completed":217,"skipped":4131,"failed":0}
------------------------------
â€¢ [4.084 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:55

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:34.654
    Aug 11 14:59:34.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 14:59:34.654
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:34.67
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:34.673
    [It] should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:55
    STEP: Creating projection with secret that has name projected-secret-test-4540a4c8-8a40-4ded-8860-5297d8bdbc20 08/11/23 14:59:34.676
    STEP: Creating a pod to test consume secrets 08/11/23 14:59:34.68
    Aug 11 14:59:34.690: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e" in namespace "projected-559" to be "Succeeded or Failed"
    Aug 11 14:59:34.694: INFO: Pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.15689ms
    Aug 11 14:59:36.698: INFO: Pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008716889s
    Aug 11 14:59:38.699: INFO: Pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009689795s
    STEP: Saw pod success 08/11/23 14:59:38.699
    Aug 11 14:59:38.700: INFO: Pod "pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e" satisfied condition "Succeeded or Failed"
    Aug 11 14:59:38.703: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:59:38.711
    Aug 11 14:59:38.724: INFO: Waiting for pod pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e to disappear
    Aug 11 14:59:38.727: INFO: Pod pod-projected-secrets-6afd6dd2-70d1-48fe-bf6b-6f976d9b010e no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 11 14:59:38.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-559" for this suite. 08/11/23 14:59:38.732
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:38.738
Aug 11 14:59:38.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:59:38.739
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:38.754
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:38.756
[It] listing custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:85
Aug 11 14:59:38.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 14:59:45.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-4892" for this suite. 08/11/23 14:59:45.067
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]","completed":218,"skipped":4136,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.336 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    listing custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:85

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:38.738
    Aug 11 14:59:38.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 14:59:38.739
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:38.754
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:38.756
    [It] listing custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:85
    Aug 11 14:59:38.759: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 14:59:45.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-4892" for this suite. 08/11/23 14:59:45.067
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:45.075
Aug 11 14:59:45.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 14:59:45.076
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:45.091
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:45.093
[It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98
STEP: Creating secret with name secret-test-a752cb5c-d7bc-42af-a881-49547993cea0 08/11/23 14:59:45.115
STEP: Creating a pod to test consume secrets 08/11/23 14:59:45.12
Aug 11 14:59:45.129: INFO: Waiting up to 5m0s for pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d" in namespace "secrets-1800" to be "Succeeded or Failed"
Aug 11 14:59:45.132: INFO: Pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.755925ms
Aug 11 14:59:47.136: INFO: Pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007002255s
Aug 11 14:59:49.138: INFO: Pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008425295s
STEP: Saw pod success 08/11/23 14:59:49.138
Aug 11 14:59:49.138: INFO: Pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d" satisfied condition "Succeeded or Failed"
Aug 11 14:59:49.141: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 14:59:49.15
Aug 11 14:59:49.164: INFO: Waiting for pod pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d to disappear
Aug 11 14:59:49.167: INFO: Pod pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 11 14:59:49.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-1800" for this suite. 08/11/23 14:59:49.171
STEP: Destroying namespace "secret-namespace-4185" for this suite. 08/11/23 14:59:49.177
{"msg":"PASSED [sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]","completed":219,"skipped":4147,"failed":0}
------------------------------
â€¢ [4.108 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:45.075
    Aug 11 14:59:45.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 14:59:45.076
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:45.091
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:45.093
    [It] should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:98
    STEP: Creating secret with name secret-test-a752cb5c-d7bc-42af-a881-49547993cea0 08/11/23 14:59:45.115
    STEP: Creating a pod to test consume secrets 08/11/23 14:59:45.12
    Aug 11 14:59:45.129: INFO: Waiting up to 5m0s for pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d" in namespace "secrets-1800" to be "Succeeded or Failed"
    Aug 11 14:59:45.132: INFO: Pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.755925ms
    Aug 11 14:59:47.136: INFO: Pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007002255s
    Aug 11 14:59:49.138: INFO: Pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008425295s
    STEP: Saw pod success 08/11/23 14:59:49.138
    Aug 11 14:59:49.138: INFO: Pod "pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d" satisfied condition "Succeeded or Failed"
    Aug 11 14:59:49.141: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 14:59:49.15
    Aug 11 14:59:49.164: INFO: Waiting for pod pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d to disappear
    Aug 11 14:59:49.167: INFO: Pod pod-secrets-1bc45f1d-b97f-47cb-a6ca-7303d642f24d no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 14:59:49.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-1800" for this suite. 08/11/23 14:59:49.171
    STEP: Destroying namespace "secret-namespace-4185" for this suite. 08/11/23 14:59:49.177
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:49.187
Aug 11 14:59:49.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:59:49.188
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:49.203
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:49.205
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:59:49.211
Aug 11 14:59:49.218: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8793" to be "running and ready"
Aug 11 14:59:49.224: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.422499ms
Aug 11 14:59:49.224: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:59:51.230: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011095001s
Aug 11 14:59:51.230: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 11 14:59:51.230: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute poststart http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:130
STEP: create the pod with lifecycle hook 08/11/23 14:59:51.233
Aug 11 14:59:51.238: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8793" to be "running and ready"
Aug 11 14:59:51.241: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.775667ms
Aug 11 14:59:51.241: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 11 14:59:53.246: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007834942s
Aug 11 14:59:53.246: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
Aug 11 14:59:53.246: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
STEP: check poststart hook 08/11/23 14:59:53.249
STEP: delete the pod with lifecycle hook 08/11/23 14:59:53.257
Aug 11 14:59:53.264: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 11 14:59:53.267: INFO: Pod pod-with-poststart-http-hook still exists
Aug 11 14:59:55.268: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
Aug 11 14:59:55.272: INFO: Pod pod-with-poststart-http-hook no longer exists
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Aug 11 14:59:55.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-8793" for this suite. 08/11/23 14:59:55.276
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]","completed":220,"skipped":4223,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.096 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute poststart http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:130

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:49.187
    Aug 11 14:59:49.187: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 14:59:49.188
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:49.203
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:49.205
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 08/11/23 14:59:49.211
    Aug 11 14:59:49.218: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-8793" to be "running and ready"
    Aug 11 14:59:49.224: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 5.422499ms
    Aug 11 14:59:49.224: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:59:51.230: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011095001s
    Aug 11 14:59:51.230: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 11 14:59:51.230: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute poststart http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:130
    STEP: create the pod with lifecycle hook 08/11/23 14:59:51.233
    Aug 11 14:59:51.238: INFO: Waiting up to 5m0s for pod "pod-with-poststart-http-hook" in namespace "container-lifecycle-hook-8793" to be "running and ready"
    Aug 11 14:59:51.241: INFO: Pod "pod-with-poststart-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 2.775667ms
    Aug 11 14:59:51.241: INFO: The phase of Pod pod-with-poststart-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 14:59:53.246: INFO: Pod "pod-with-poststart-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.007834942s
    Aug 11 14:59:53.246: INFO: The phase of Pod pod-with-poststart-http-hook is Running (Ready = true)
    Aug 11 14:59:53.246: INFO: Pod "pod-with-poststart-http-hook" satisfied condition "running and ready"
    STEP: check poststart hook 08/11/23 14:59:53.249
    STEP: delete the pod with lifecycle hook 08/11/23 14:59:53.257
    Aug 11 14:59:53.264: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 11 14:59:53.267: INFO: Pod pod-with-poststart-http-hook still exists
    Aug 11 14:59:55.268: INFO: Waiting for pod pod-with-poststart-http-hook to disappear
    Aug 11 14:59:55.272: INFO: Pod pod-with-poststart-http-hook no longer exists
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Aug 11 14:59:55.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-8793" for this suite. 08/11/23 14:59:55.276
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:55.284
Aug 11 14:59:55.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 14:59:55.285
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:55.3
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:55.302
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46
STEP: Creating configMap with name configmap-test-volume-9a423266-39b6-45df-b79a-bdb6966d01df 08/11/23 14:59:55.305
STEP: Creating a pod to test consume configMaps 08/11/23 14:59:55.31
Aug 11 14:59:55.318: INFO: Waiting up to 5m0s for pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec" in namespace "configmap-7666" to be "Succeeded or Failed"
Aug 11 14:59:55.323: INFO: Pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104148ms
Aug 11 14:59:57.328: INFO: Pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010248765s
Aug 11 14:59:59.328: INFO: Pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010382705s
STEP: Saw pod success 08/11/23 14:59:59.328
Aug 11 14:59:59.328: INFO: Pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec" satisfied condition "Succeeded or Failed"
Aug 11 14:59:59.331: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec container agnhost-container: <nil>
STEP: delete the pod 08/11/23 14:59:59.34
Aug 11 14:59:59.353: INFO: Waiting for pod pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec to disappear
Aug 11 14:59:59.356: INFO: Pod pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec no longer exists
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 14:59:59.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-7666" for this suite. 08/11/23 14:59:59.36
{"msg":"PASSED [sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]","completed":221,"skipped":4244,"failed":0}
------------------------------
â€¢ [4.083 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:46

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:55.284
    Aug 11 14:59:55.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 14:59:55.285
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:55.3
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:55.302
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:46
    STEP: Creating configMap with name configmap-test-volume-9a423266-39b6-45df-b79a-bdb6966d01df 08/11/23 14:59:55.305
    STEP: Creating a pod to test consume configMaps 08/11/23 14:59:55.31
    Aug 11 14:59:55.318: INFO: Waiting up to 5m0s for pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec" in namespace "configmap-7666" to be "Succeeded or Failed"
    Aug 11 14:59:55.323: INFO: Pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec": Phase="Pending", Reason="", readiness=false. Elapsed: 5.104148ms
    Aug 11 14:59:57.328: INFO: Pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010248765s
    Aug 11 14:59:59.328: INFO: Pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010382705s
    STEP: Saw pod success 08/11/23 14:59:59.328
    Aug 11 14:59:59.328: INFO: Pod "pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec" satisfied condition "Succeeded or Failed"
    Aug 11 14:59:59.331: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 14:59:59.34
    Aug 11 14:59:59.353: INFO: Waiting for pod pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec to disappear
    Aug 11 14:59:59.356: INFO: Pod pod-configmaps-c17d5eda-d960-4f8f-bea2-11c8ab751aec no longer exists
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 14:59:59.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-7666" for this suite. 08/11/23 14:59:59.36
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 14:59:59.367
Aug 11 14:59:59.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 14:59:59.368
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:59.381
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:59.384
[It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216
STEP: Creating a pod to test emptydir 0777 on node default medium 08/11/23 14:59:59.386
Aug 11 14:59:59.394: INFO: Waiting up to 5m0s for pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992" in namespace "emptydir-3850" to be "Succeeded or Failed"
Aug 11 14:59:59.398: INFO: Pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992": Phase="Pending", Reason="", readiness=false. Elapsed: 3.732807ms
Aug 11 15:00:01.403: INFO: Pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008348517s
Aug 11 15:00:03.404: INFO: Pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009824449s
STEP: Saw pod success 08/11/23 15:00:03.404
Aug 11 15:00:03.404: INFO: Pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992" satisfied condition "Succeeded or Failed"
Aug 11 15:00:03.407: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-857d04f6-de67-4dd6-aa8a-24e5330a7992 container test-container: <nil>
STEP: delete the pod 08/11/23 15:00:03.415
Aug 11 15:00:03.430: INFO: Waiting for pod pod-857d04f6-de67-4dd6-aa8a-24e5330a7992 to disappear
Aug 11 15:00:03.433: INFO: Pod pod-857d04f6-de67-4dd6-aa8a-24e5330a7992 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 15:00:03.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-3850" for this suite. 08/11/23 15:00:03.437
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":222,"skipped":4246,"failed":0}
------------------------------
â€¢ [4.077 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 14:59:59.367
    Aug 11 14:59:59.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 14:59:59.368
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 14:59:59.381
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 14:59:59.384
    [It] should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:216
    STEP: Creating a pod to test emptydir 0777 on node default medium 08/11/23 14:59:59.386
    Aug 11 14:59:59.394: INFO: Waiting up to 5m0s for pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992" in namespace "emptydir-3850" to be "Succeeded or Failed"
    Aug 11 14:59:59.398: INFO: Pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992": Phase="Pending", Reason="", readiness=false. Elapsed: 3.732807ms
    Aug 11 15:00:01.403: INFO: Pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008348517s
    Aug 11 15:00:03.404: INFO: Pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009824449s
    STEP: Saw pod success 08/11/23 15:00:03.404
    Aug 11 15:00:03.404: INFO: Pod "pod-857d04f6-de67-4dd6-aa8a-24e5330a7992" satisfied condition "Succeeded or Failed"
    Aug 11 15:00:03.407: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-857d04f6-de67-4dd6-aa8a-24e5330a7992 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:00:03.415
    Aug 11 15:00:03.430: INFO: Waiting for pod pod-857d04f6-de67-4dd6-aa8a-24e5330a7992 to disappear
    Aug 11 15:00:03.433: INFO: Pod pod-857d04f6-de67-4dd6-aa8a-24e5330a7992 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 15:00:03.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-3850" for this suite. 08/11/23 15:00:03.437
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:00:03.446
Aug 11 15:00:03.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-probe 08/11/23 15:00:03.447
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:00:03.461
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:00:03.463
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211
STEP: Creating pod test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76 in namespace container-probe-4185 08/11/23 15:00:03.466
Aug 11 15:00:03.474: INFO: Waiting up to 5m0s for pod "test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76" in namespace "container-probe-4185" to be "not pending"
Aug 11 15:00:03.477: INFO: Pod "test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448396ms
Aug 11 15:00:05.481: INFO: Pod "test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76": Phase="Running", Reason="", readiness=true. Elapsed: 2.006974464s
Aug 11 15:00:05.481: INFO: Pod "test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76" satisfied condition "not pending"
Aug 11 15:00:05.481: INFO: Started pod test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76 in namespace container-probe-4185
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:00:05.482
Aug 11 15:00:05.485: INFO: Initial restart count of pod test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76 is 0
STEP: deleting the pod 08/11/23 15:04:06.069
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 11 15:04:06.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-4185" for this suite. 08/11/23 15:04:06.089
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":223,"skipped":4278,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.651 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:211

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:00:03.446
    Aug 11 15:00:03.446: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-probe 08/11/23 15:00:03.447
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:00:03.461
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:00:03.463
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:211
    STEP: Creating pod test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76 in namespace container-probe-4185 08/11/23 15:00:03.466
    Aug 11 15:00:03.474: INFO: Waiting up to 5m0s for pod "test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76" in namespace "container-probe-4185" to be "not pending"
    Aug 11 15:00:03.477: INFO: Pod "test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76": Phase="Pending", Reason="", readiness=false. Elapsed: 2.448396ms
    Aug 11 15:00:05.481: INFO: Pod "test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76": Phase="Running", Reason="", readiness=true. Elapsed: 2.006974464s
    Aug 11 15:00:05.481: INFO: Pod "test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76" satisfied condition "not pending"
    Aug 11 15:00:05.481: INFO: Started pod test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76 in namespace container-probe-4185
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:00:05.482
    Aug 11 15:00:05.485: INFO: Initial restart count of pod test-webserver-04c1f24d-2623-4cb1-908f-3262f6eedf76 is 0
    STEP: deleting the pod 08/11/23 15:04:06.069
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 11 15:04:06.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-4185" for this suite. 08/11/23 15:04:06.089
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:06.099
Aug 11 15:04:06.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 15:04:06.1
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:06.117
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:06.121
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082
STEP: Create a pod 08/11/23 15:04:06.123
Aug 11 15:04:06.133: INFO: Waiting up to 5m0s for pod "pod-jw96w" in namespace "pods-6827" to be "running"
Aug 11 15:04:06.139: INFO: Pod "pod-jw96w": Phase="Pending", Reason="", readiness=false. Elapsed: 5.79022ms
Aug 11 15:04:08.143: INFO: Pod "pod-jw96w": Phase="Running", Reason="", readiness=true. Elapsed: 2.010009858s
Aug 11 15:04:08.143: INFO: Pod "pod-jw96w" satisfied condition "running"
STEP: patching /status 08/11/23 15:04:08.143
Aug 11 15:04:08.151: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 15:04:08.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6827" for this suite. 08/11/23 15:04:08.155
{"msg":"PASSED [sig-node] Pods should patch a pod status [Conformance]","completed":224,"skipped":4294,"failed":0}
------------------------------
â€¢ [2.063 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should patch a pod status [Conformance]
  test/e2e/common/node/pods.go:1082

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:06.099
    Aug 11 15:04:06.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 15:04:06.1
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:06.117
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:06.121
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should patch a pod status [Conformance]
      test/e2e/common/node/pods.go:1082
    STEP: Create a pod 08/11/23 15:04:06.123
    Aug 11 15:04:06.133: INFO: Waiting up to 5m0s for pod "pod-jw96w" in namespace "pods-6827" to be "running"
    Aug 11 15:04:06.139: INFO: Pod "pod-jw96w": Phase="Pending", Reason="", readiness=false. Elapsed: 5.79022ms
    Aug 11 15:04:08.143: INFO: Pod "pod-jw96w": Phase="Running", Reason="", readiness=true. Elapsed: 2.010009858s
    Aug 11 15:04:08.143: INFO: Pod "pod-jw96w" satisfied condition "running"
    STEP: patching /status 08/11/23 15:04:08.143
    Aug 11 15:04:08.151: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 15:04:08.151: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6827" for this suite. 08/11/23 15:04:08.155
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-architecture] Conformance Tests
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
[BeforeEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:08.162
Aug 11 15:04:08.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename conformance-tests 08/11/23 15:04:08.163
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:08.176
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:08.179
[It] should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38
STEP: Getting node addresses 08/11/23 15:04:08.182
Aug 11 15:04:08.182: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
[AfterEach] [sig-architecture] Conformance Tests
  test/e2e/framework/framework.go:187
Aug 11 15:04:08.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "conformance-tests-9171" for this suite. 08/11/23 15:04:08.192
{"msg":"PASSED [sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]","completed":225,"skipped":4299,"failed":0}
------------------------------
â€¢ [0.037 seconds]
[sig-architecture] Conformance Tests
test/e2e/architecture/framework.go:23
  should have at least two untainted nodes [Conformance]
  test/e2e/architecture/conformance.go:38

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:08.162
    Aug 11 15:04:08.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename conformance-tests 08/11/23 15:04:08.163
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:08.176
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:08.179
    [It] should have at least two untainted nodes [Conformance]
      test/e2e/architecture/conformance.go:38
    STEP: Getting node addresses 08/11/23 15:04:08.182
    Aug 11 15:04:08.182: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    [AfterEach] [sig-architecture] Conformance Tests
      test/e2e/framework/framework.go:187
    Aug 11 15:04:08.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "conformance-tests-9171" for this suite. 08/11/23 15:04:08.192
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] Deployment
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:08.199
Aug 11 15:04:08.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename deployment 08/11/23 15:04:08.2
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:08.214
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:08.217
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105
Aug 11 15:04:08.220: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
Aug 11 15:04:08.230: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 11 15:04:13.235: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 15:04:13.235
Aug 11 15:04:13.235: INFO: Creating deployment "test-rolling-update-deployment"
Aug 11 15:04:13.246: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
Aug 11 15:04:13.258: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
Aug 11 15:04:15.267: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
Aug 11 15:04:15.270: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 15:04:15.280: INFO: Deployment "test-rolling-update-deployment":
&Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-971  94723f00-fdbb-47e7-abb8-a235389d3a99 39293 1 2023-08-11 15:04:13 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-11 15:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d476f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 15:04:13 +0000 UTC,LastTransitionTime:2023-08-11 15:04:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-08-11 15:04:14 +0000 UTC,LastTransitionTime:2023-08-11 15:04:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 11 15:04:15.283: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
&ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-971  c635b511-1939-4d41-b986-bae75acd93e6 39281 1 2023-08-11 15:04:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 94723f00-fdbb-47e7-abb8-a235389d3a99 0xc003d47bf7 0xc003d47bf8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94723f00-fdbb-47e7-abb8-a235389d3a99\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d47ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:04:15.283: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
Aug 11 15:04:15.283: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-971  10b3056a-034e-462c-be9d-8b4e715f3ea7 39291 2 2023-08-11 15:04:08 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 94723f00-fdbb-47e7-abb8-a235389d3a99 0xc003d47ac7 0xc003d47ac8}] [] [{e2e.test Update apps/v1 2023-08-11 15:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94723f00-fdbb-47e7-abb8-a235389d3a99\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d47b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:04:15.286: INFO: Pod "test-rolling-update-deployment-78f575d8ff-xmhdl" is available:
&Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-xmhdl test-rolling-update-deployment-78f575d8ff- deployment-971  e6cd4b48-5dbe-4823-a395-09910fc33575 39280 0 2023-08-11 15:04:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff c635b511-1939-4d41-b986-bae75acd93e6 0xc0034e0107 0xc0034e0108}] [] [{kube-controller-manager Update v1 2023-08-11 15:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c635b511-1939-4d41-b986-bae75acd93e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ppbqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ppbqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:04:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:04:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:04:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.104,StartTime:2023-08-11 15:04:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:04:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://037009ed9a66ead28a67f2747b1d9e5dfb07fc355599e479904ff870976c1520,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 11 15:04:15.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-971" for this suite. 08/11/23 15:04:15.291
{"msg":"PASSED [sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]","completed":226,"skipped":4300,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.098 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  RollingUpdateDeployment should delete old pods and create new ones [Conformance]
  test/e2e/apps/deployment.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:08.199
    Aug 11 15:04:08.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename deployment 08/11/23 15:04:08.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:08.214
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:08.217
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] RollingUpdateDeployment should delete old pods and create new ones [Conformance]
      test/e2e/apps/deployment.go:105
    Aug 11 15:04:08.220: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
    Aug 11 15:04:08.230: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 11 15:04:13.235: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 15:04:13.235
    Aug 11 15:04:13.235: INFO: Creating deployment "test-rolling-update-deployment"
    Aug 11 15:04:13.246: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
    Aug 11 15:04:13.258: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
    Aug 11 15:04:15.267: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
    Aug 11 15:04:15.270: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 15:04:15.280: INFO: Deployment "test-rolling-update-deployment":
    &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-971  94723f00-fdbb-47e7-abb8-a235389d3a99 39293 1 2023-08-11 15:04:13 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-11 15:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d476f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 15:04:13 +0000 UTC,LastTransitionTime:2023-08-11 15:04:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-78f575d8ff" has successfully progressed.,LastUpdateTime:2023-08-11 15:04:14 +0000 UTC,LastTransitionTime:2023-08-11 15:04:13 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 11 15:04:15.283: INFO: New ReplicaSet "test-rolling-update-deployment-78f575d8ff" of Deployment "test-rolling-update-deployment":
    &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-78f575d8ff  deployment-971  c635b511-1939-4d41-b986-bae75acd93e6 39281 1 2023-08-11 15:04:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 94723f00-fdbb-47e7-abb8-a235389d3a99 0xc003d47bf7 0xc003d47bf8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94723f00-fdbb-47e7-abb8-a235389d3a99\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 78f575d8ff,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003d47ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:04:15.283: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
    Aug 11 15:04:15.283: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-971  10b3056a-034e-462c-be9d-8b4e715f3ea7 39291 2 2023-08-11 15:04:08 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 94723f00-fdbb-47e7-abb8-a235389d3a99 0xc003d47ac7 0xc003d47ac8}] [] [{e2e.test Update apps/v1 2023-08-11 15:04:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"94723f00-fdbb-47e7-abb8-a235389d3a99\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003d47b88 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:04:15.286: INFO: Pod "test-rolling-update-deployment-78f575d8ff-xmhdl" is available:
    &Pod{ObjectMeta:{test-rolling-update-deployment-78f575d8ff-xmhdl test-rolling-update-deployment-78f575d8ff- deployment-971  e6cd4b48-5dbe-4823-a395-09910fc33575 39280 0 2023-08-11 15:04:13 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:78f575d8ff] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-78f575d8ff c635b511-1939-4d41-b986-bae75acd93e6 0xc0034e0107 0xc0034e0108}] [] [{kube-controller-manager Update v1 2023-08-11 15:04:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c635b511-1939-4d41-b986-bae75acd93e6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:04:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.104\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ppbqg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ppbqg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:04:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:04:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:04:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:04:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.104,StartTime:2023-08-11 15:04:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:04:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://037009ed9a66ead28a67f2747b1d9e5dfb07fc355599e479904ff870976c1520,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.104,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 11 15:04:15.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-971" for this suite. 08/11/23 15:04:15.291
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:15.298
Aug 11 15:04:15.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 15:04:15.299
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:15.313
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:15.316
[It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106
STEP: Creating a pod to test emptydir 0666 on tmpfs 08/11/23 15:04:15.32
Aug 11 15:04:15.329: INFO: Waiting up to 5m0s for pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e" in namespace "emptydir-5968" to be "Succeeded or Failed"
Aug 11 15:04:15.335: INFO: Pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.221653ms
Aug 11 15:04:17.340: INFO: Pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010440002s
Aug 11 15:04:19.339: INFO: Pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010045476s
STEP: Saw pod success 08/11/23 15:04:19.339
Aug 11 15:04:19.340: INFO: Pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e" satisfied condition "Succeeded or Failed"
Aug 11 15:04:19.343: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-1208f482-b4aa-4497-8d28-4c509a25962e container test-container: <nil>
STEP: delete the pod 08/11/23 15:04:19.368
Aug 11 15:04:19.379: INFO: Waiting for pod pod-1208f482-b4aa-4497-8d28-4c509a25962e to disappear
Aug 11 15:04:19.382: INFO: Pod pod-1208f482-b4aa-4497-8d28-4c509a25962e no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 15:04:19.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5968" for this suite. 08/11/23 15:04:19.386
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":227,"skipped":4314,"failed":0}
------------------------------
â€¢ [4.096 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:106

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:15.298
    Aug 11 15:04:15.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:04:15.299
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:15.313
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:15.316
    [It] should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:106
    STEP: Creating a pod to test emptydir 0666 on tmpfs 08/11/23 15:04:15.32
    Aug 11 15:04:15.329: INFO: Waiting up to 5m0s for pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e" in namespace "emptydir-5968" to be "Succeeded or Failed"
    Aug 11 15:04:15.335: INFO: Pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e": Phase="Pending", Reason="", readiness=false. Elapsed: 5.221653ms
    Aug 11 15:04:17.340: INFO: Pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010440002s
    Aug 11 15:04:19.339: INFO: Pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010045476s
    STEP: Saw pod success 08/11/23 15:04:19.339
    Aug 11 15:04:19.340: INFO: Pod "pod-1208f482-b4aa-4497-8d28-4c509a25962e" satisfied condition "Succeeded or Failed"
    Aug 11 15:04:19.343: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-1208f482-b4aa-4497-8d28-4c509a25962e container test-container: <nil>
    STEP: delete the pod 08/11/23 15:04:19.368
    Aug 11 15:04:19.379: INFO: Waiting for pod pod-1208f482-b4aa-4497-8d28-4c509a25962e to disappear
    Aug 11 15:04:19.382: INFO: Pod pod-1208f482-b4aa-4497-8d28-4c509a25962e no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 15:04:19.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5968" for this suite. 08/11/23 15:04:19.386
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-network] IngressClass API
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
[BeforeEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:19.395
Aug 11 15:04:19.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename ingressclass 08/11/23 15:04:19.396
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:19.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:19.417
[BeforeEach] [sig-network] IngressClass API
  test/e2e/network/ingressclass.go:211
[It]  should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223
STEP: getting /apis 08/11/23 15:04:19.42
STEP: getting /apis/networking.k8s.io 08/11/23 15:04:19.422
STEP: getting /apis/networking.k8s.iov1 08/11/23 15:04:19.423
STEP: creating 08/11/23 15:04:19.425
STEP: getting 08/11/23 15:04:19.442
STEP: listing 08/11/23 15:04:19.445
STEP: watching 08/11/23 15:04:19.449
Aug 11 15:04:19.449: INFO: starting watch
STEP: patching 08/11/23 15:04:19.45
STEP: updating 08/11/23 15:04:19.455
Aug 11 15:04:19.460: INFO: waiting for watch events with expected annotations
Aug 11 15:04:19.460: INFO: saw patched and updated annotations
STEP: deleting 08/11/23 15:04:19.461
STEP: deleting a collection 08/11/23 15:04:19.472
[AfterEach] [sig-network] IngressClass API
  test/e2e/framework/framework.go:187
Aug 11 15:04:19.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "ingressclass-1984" for this suite. 08/11/23 15:04:19.493
{"msg":"PASSED [sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]","completed":228,"skipped":4315,"failed":0}
------------------------------
â€¢ [0.106 seconds]
[sig-network] IngressClass API
test/e2e/network/common/framework.go:23
   should support creating IngressClass API operations [Conformance]
  test/e2e/network/ingressclass.go:223

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:19.395
    Aug 11 15:04:19.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename ingressclass 08/11/23 15:04:19.396
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:19.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:19.417
    [BeforeEach] [sig-network] IngressClass API
      test/e2e/network/ingressclass.go:211
    [It]  should support creating IngressClass API operations [Conformance]
      test/e2e/network/ingressclass.go:223
    STEP: getting /apis 08/11/23 15:04:19.42
    STEP: getting /apis/networking.k8s.io 08/11/23 15:04:19.422
    STEP: getting /apis/networking.k8s.iov1 08/11/23 15:04:19.423
    STEP: creating 08/11/23 15:04:19.425
    STEP: getting 08/11/23 15:04:19.442
    STEP: listing 08/11/23 15:04:19.445
    STEP: watching 08/11/23 15:04:19.449
    Aug 11 15:04:19.449: INFO: starting watch
    STEP: patching 08/11/23 15:04:19.45
    STEP: updating 08/11/23 15:04:19.455
    Aug 11 15:04:19.460: INFO: waiting for watch events with expected annotations
    Aug 11 15:04:19.460: INFO: saw patched and updated annotations
    STEP: deleting 08/11/23 15:04:19.461
    STEP: deleting a collection 08/11/23 15:04:19.472
    [AfterEach] [sig-network] IngressClass API
      test/e2e/framework/framework.go:187
    Aug 11 15:04:19.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "ingressclass-1984" for this suite. 08/11/23 15:04:19.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:19.502
Aug 11 15:04:19.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 15:04:19.503
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:19.518
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:19.52
[It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/11/23 15:04:19.523
Aug 11 15:04:19.533: INFO: Waiting up to 5m0s for pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd" in namespace "emptydir-1814" to be "Succeeded or Failed"
Aug 11 15:04:19.549: INFO: Pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015379ms
Aug 11 15:04:21.554: INFO: Pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020950958s
Aug 11 15:04:23.555: INFO: Pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022357219s
STEP: Saw pod success 08/11/23 15:04:23.555
Aug 11 15:04:23.555: INFO: Pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd" satisfied condition "Succeeded or Failed"
Aug 11 15:04:23.559: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd container test-container: <nil>
STEP: delete the pod 08/11/23 15:04:23.567
Aug 11 15:04:23.583: INFO: Waiting for pod pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd to disappear
Aug 11 15:04:23.586: INFO: Pod pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 15:04:23.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-1814" for this suite. 08/11/23 15:04:23.59
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":229,"skipped":4327,"failed":0}
------------------------------
â€¢ [4.094 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:126

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:19.502
    Aug 11 15:04:19.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:04:19.503
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:19.518
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:19.52
    [It] should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:126
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/11/23 15:04:19.523
    Aug 11 15:04:19.533: INFO: Waiting up to 5m0s for pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd" in namespace "emptydir-1814" to be "Succeeded or Failed"
    Aug 11 15:04:19.549: INFO: Pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd": Phase="Pending", Reason="", readiness=false. Elapsed: 16.015379ms
    Aug 11 15:04:21.554: INFO: Pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd": Phase="Pending", Reason="", readiness=false. Elapsed: 2.020950958s
    Aug 11 15:04:23.555: INFO: Pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.022357219s
    STEP: Saw pod success 08/11/23 15:04:23.555
    Aug 11 15:04:23.555: INFO: Pod "pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd" satisfied condition "Succeeded or Failed"
    Aug 11 15:04:23.559: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd container test-container: <nil>
    STEP: delete the pod 08/11/23 15:04:23.567
    Aug 11 15:04:23.583: INFO: Waiting for pod pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd to disappear
    Aug 11 15:04:23.586: INFO: Pod pod-dd73f5f6-a973-44c6-9b5c-01eda29748fd no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 15:04:23.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-1814" for this suite. 08/11/23 15:04:23.59
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem
  should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:23.596
Aug 11 15:04:23.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename security-context-test 08/11/23 15:04:23.597
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:23.613
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:23.616
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:485
Aug 11 15:04:23.626: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8" in namespace "security-context-test-7896" to be "Succeeded or Failed"
Aug 11 15:04:23.631: INFO: Pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.109799ms
Aug 11 15:04:25.636: INFO: Pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009900136s
Aug 11 15:04:27.636: INFO: Pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010086199s
Aug 11 15:04:27.636: INFO: Pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 11 15:04:27.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-7896" for this suite. 08/11/23 15:04:27.641
{"msg":"PASSED [sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]","completed":230,"skipped":4327,"failed":0}
------------------------------
â€¢ [4.052 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with readOnlyRootFilesystem
  test/e2e/common/node/security_context.go:429
    should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:485

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:23.596
    Aug 11 15:04:23.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename security-context-test 08/11/23 15:04:23.597
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:23.613
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:23.616
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:485
    Aug 11 15:04:23.626: INFO: Waiting up to 5m0s for pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8" in namespace "security-context-test-7896" to be "Succeeded or Failed"
    Aug 11 15:04:23.631: INFO: Pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.109799ms
    Aug 11 15:04:25.636: INFO: Pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009900136s
    Aug 11 15:04:27.636: INFO: Pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010086199s
    Aug 11 15:04:27.636: INFO: Pod "busybox-readonly-false-7690a5cf-6edb-4fef-8099-1b771bfe2aa8" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 11 15:04:27.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-7896" for this suite. 08/11/23 15:04:27.641
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Pods
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:27.648
Aug 11 15:04:27.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 15:04:27.65
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:27.665
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:27.667
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535
Aug 11 15:04:27.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: creating the pod 08/11/23 15:04:27.67
STEP: submitting the pod to kubernetes 08/11/23 15:04:27.67
Aug 11 15:04:27.679: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af" in namespace "pods-3225" to be "running and ready"
Aug 11 15:04:27.683: INFO: Pod "pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.916602ms
Aug 11 15:04:27.683: INFO: The phase of Pod pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:04:29.688: INFO: Pod "pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af": Phase="Running", Reason="", readiness=true. Elapsed: 2.008751429s
Aug 11 15:04:29.688: INFO: The phase of Pod pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af is Running (Ready = true)
Aug 11 15:04:29.688: INFO: Pod "pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 15:04:29.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-3225" for this suite. 08/11/23 15:04:29.776
{"msg":"PASSED [sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]","completed":231,"skipped":4332,"failed":0}
------------------------------
â€¢ [2.134 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support remote command execution over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:535

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:27.648
    Aug 11 15:04:27.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 15:04:27.65
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:27.665
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:27.667
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support remote command execution over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:535
    Aug 11 15:04:27.670: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: creating the pod 08/11/23 15:04:27.67
    STEP: submitting the pod to kubernetes 08/11/23 15:04:27.67
    Aug 11 15:04:27.679: INFO: Waiting up to 5m0s for pod "pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af" in namespace "pods-3225" to be "running and ready"
    Aug 11 15:04:27.683: INFO: Pod "pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af": Phase="Pending", Reason="", readiness=false. Elapsed: 3.916602ms
    Aug 11 15:04:27.683: INFO: The phase of Pod pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:04:29.688: INFO: Pod "pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af": Phase="Running", Reason="", readiness=true. Elapsed: 2.008751429s
    Aug 11 15:04:29.688: INFO: The phase of Pod pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af is Running (Ready = true)
    Aug 11 15:04:29.688: INFO: Pod "pod-exec-websocket-78134623-6769-4adf-88dd-b0b41f4768af" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 15:04:29.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-3225" for this suite. 08/11/23 15:04:29.776
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server
  should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:29.788
Aug 11 15:04:29.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 15:04:29.789
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:29.807
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:29.81
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should support proxy with --port 0  [Conformance]
  test/e2e/kubectl/kubectl.go:1785
STEP: starting the proxy server 08/11/23 15:04:29.814
Aug 11 15:04:29.814: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5746 proxy -p 0 --disable-filter'
STEP: curling proxy /api/ output 08/11/23 15:04:29.859
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 15:04:29.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-5746" for this suite. 08/11/23 15:04:29.872
{"msg":"PASSED [sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]","completed":232,"skipped":4363,"failed":0}
------------------------------
â€¢ [0.091 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Proxy server
  test/e2e/kubectl/kubectl.go:1778
    should support proxy with --port 0  [Conformance]
    test/e2e/kubectl/kubectl.go:1785

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:29.788
    Aug 11 15:04:29.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:04:29.789
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:29.807
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:29.81
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should support proxy with --port 0  [Conformance]
      test/e2e/kubectl/kubectl.go:1785
    STEP: starting the proxy server 08/11/23 15:04:29.814
    Aug 11 15:04:29.814: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-5746 proxy -p 0 --disable-filter'
    STEP: curling proxy /api/ output 08/11/23 15:04:29.859
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 15:04:29.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-5746" for this suite. 08/11/23 15:04:29.872
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:29.879
Aug 11 15:04:29.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 15:04:29.88
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:29.897
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:29.9
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:04:29.902
Aug 11 15:04:29.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a" in namespace "downward-api-6948" to be "Succeeded or Failed"
Aug 11 15:04:29.916: INFO: Pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.380376ms
Aug 11 15:04:31.921: INFO: Pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008870512s
Aug 11 15:04:33.921: INFO: Pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009383084s
STEP: Saw pod success 08/11/23 15:04:33.921
Aug 11 15:04:33.922: INFO: Pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a" satisfied condition "Succeeded or Failed"
Aug 11 15:04:33.924: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a container client-container: <nil>
STEP: delete the pod 08/11/23 15:04:33.934
Aug 11 15:04:33.948: INFO: Waiting for pod downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a to disappear
Aug 11 15:04:33.951: INFO: Pod downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 15:04:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-6948" for this suite. 08/11/23 15:04:33.955
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]","completed":233,"skipped":4373,"failed":0}
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:248

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:29.879
    Aug 11 15:04:29.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:04:29.88
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:29.897
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:29.9
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:248
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:04:29.902
    Aug 11 15:04:29.912: INFO: Waiting up to 5m0s for pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a" in namespace "downward-api-6948" to be "Succeeded or Failed"
    Aug 11 15:04:29.916: INFO: Pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.380376ms
    Aug 11 15:04:31.921: INFO: Pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008870512s
    Aug 11 15:04:33.921: INFO: Pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009383084s
    STEP: Saw pod success 08/11/23 15:04:33.921
    Aug 11 15:04:33.922: INFO: Pod "downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a" satisfied condition "Succeeded or Failed"
    Aug 11 15:04:33.924: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a container client-container: <nil>
    STEP: delete the pod 08/11/23 15:04:33.934
    Aug 11 15:04:33.948: INFO: Waiting for pod downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a to disappear
    Aug 11 15:04:33.951: INFO: Pod downwardapi-volume-cf94f9d5-fef8-41b1-9919-eeb189dfd28a no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 15:04:33.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-6948" for this suite. 08/11/23 15:04:33.955
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:33.961
Aug 11 15:04:33.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 15:04:33.962
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:33.978
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:33.982
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:04:33.985
Aug 11 15:04:33.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985" in namespace "projected-4584" to be "Succeeded or Failed"
Aug 11 15:04:33.999: INFO: Pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985": Phase="Pending", Reason="", readiness=false. Elapsed: 5.917514ms
Aug 11 15:04:36.004: INFO: Pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011105233s
Aug 11 15:04:38.004: INFO: Pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010909905s
STEP: Saw pod success 08/11/23 15:04:38.004
Aug 11 15:04:38.004: INFO: Pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985" satisfied condition "Succeeded or Failed"
Aug 11 15:04:38.007: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985 container client-container: <nil>
STEP: delete the pod 08/11/23 15:04:38.018
Aug 11 15:04:38.033: INFO: Waiting for pod downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985 to disappear
Aug 11 15:04:38.037: INFO: Pod downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 15:04:38.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4584" for this suite. 08/11/23 15:04:38.041
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":234,"skipped":4375,"failed":0}
------------------------------
â€¢ [4.088 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:33.961
    Aug 11 15:04:33.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 15:04:33.962
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:33.978
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:33.982
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:260
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:04:33.985
    Aug 11 15:04:33.993: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985" in namespace "projected-4584" to be "Succeeded or Failed"
    Aug 11 15:04:33.999: INFO: Pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985": Phase="Pending", Reason="", readiness=false. Elapsed: 5.917514ms
    Aug 11 15:04:36.004: INFO: Pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011105233s
    Aug 11 15:04:38.004: INFO: Pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010909905s
    STEP: Saw pod success 08/11/23 15:04:38.004
    Aug 11 15:04:38.004: INFO: Pod "downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985" satisfied condition "Succeeded or Failed"
    Aug 11 15:04:38.007: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985 container client-container: <nil>
    STEP: delete the pod 08/11/23 15:04:38.018
    Aug 11 15:04:38.033: INFO: Waiting for pod downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985 to disappear
    Aug 11 15:04:38.037: INFO: Pod downwardapi-volume-e6941d72-a428-464d-a938-e44573ec5985 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 15:04:38.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4584" for this suite. 08/11/23 15:04:38.041
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:04:38.05
Aug 11 15:04:38.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename dns 08/11/23 15:04:38.051
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:38.068
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:38.072
[It] should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137
STEP: Creating a test headless service 08/11/23 15:04:38.076
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8021.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8021.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 192.153.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.153.192_udp@PTR;check="$$(dig +tcp +noall +answer +search 192.153.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.153.192_tcp@PTR;sleep 1; done
 08/11/23 15:04:38.099
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8021.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8021.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 192.153.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.153.192_udp@PTR;check="$$(dig +tcp +noall +answer +search 192.153.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.153.192_tcp@PTR;sleep 1; done
 08/11/23 15:04:38.099
STEP: creating a pod to probe DNS 08/11/23 15:04:38.099
STEP: submitting the pod to kubernetes 08/11/23 15:04:38.1
Aug 11 15:04:38.112: INFO: Waiting up to 15m0s for pod "dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f" in namespace "dns-8021" to be "running"
Aug 11 15:04:38.118: INFO: Pod "dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.4394ms
Aug 11 15:04:40.124: INFO: Pod "dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f": Phase="Running", Reason="", readiness=true. Elapsed: 2.01200999s
Aug 11 15:04:40.124: INFO: Pod "dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:04:40.124
STEP: looking for the results for each expected name from probers 08/11/23 15:04:40.127
Aug 11 15:04:40.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:40.143: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:40.149: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:40.154: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:40.180: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:40.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:40.192: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:40.197: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:40.219: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

Aug 11 15:04:45.227: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:45.233: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:45.238: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:45.244: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:45.273: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:45.280: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:45.286: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:45.292: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:45.318: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

Aug 11 15:04:50.227: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:50.233: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:50.238: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:50.244: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:50.274: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:50.279: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:50.285: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:50.291: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:50.316: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

Aug 11 15:04:55.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:55.236: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:55.241: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:55.246: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:55.272: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:55.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:55.283: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:55.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:04:55.310: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

Aug 11 15:05:00.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:00.236: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:00.241: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:00.247: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:00.275: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:00.282: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:00.287: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:00.292: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:00.314: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

Aug 11 15:05:05.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:05.237: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:05.243: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:05.248: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:05.281: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:05.288: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:05.294: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:05.301: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
Aug 11 15:05:05.324: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

Aug 11 15:05:10.318: INFO: DNS probes using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f succeeded

STEP: deleting the pod 08/11/23 15:05:10.318
STEP: deleting the test service 08/11/23 15:05:10.34
STEP: deleting the test headless service 08/11/23 15:05:10.371
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 11 15:05:10.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-8021" for this suite. 08/11/23 15:05:10.393
{"msg":"PASSED [sig-network] DNS should provide DNS for services  [Conformance]","completed":235,"skipped":4389,"failed":0}
------------------------------
â€¢ [SLOW TEST] [32.349 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for services  [Conformance]
  test/e2e/network/dns.go:137

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:04:38.05
    Aug 11 15:04:38.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename dns 08/11/23 15:04:38.051
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:04:38.068
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:04:38.072
    [It] should provide DNS for services  [Conformance]
      test/e2e/network/dns.go:137
    STEP: Creating a test headless service 08/11/23 15:04:38.076
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8021.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8021.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 192.153.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.153.192_udp@PTR;check="$$(dig +tcp +noall +answer +search 192.153.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.153.192_tcp@PTR;sleep 1; done
     08/11/23 15:04:38.099
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-8021.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-8021.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-8021.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-8021.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-8021.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 192.153.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.153.192_udp@PTR;check="$$(dig +tcp +noall +answer +search 192.153.110.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.110.153.192_tcp@PTR;sleep 1; done
     08/11/23 15:04:38.099
    STEP: creating a pod to probe DNS 08/11/23 15:04:38.099
    STEP: submitting the pod to kubernetes 08/11/23 15:04:38.1
    Aug 11 15:04:38.112: INFO: Waiting up to 15m0s for pod "dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f" in namespace "dns-8021" to be "running"
    Aug 11 15:04:38.118: INFO: Pod "dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f": Phase="Pending", Reason="", readiness=false. Elapsed: 6.4394ms
    Aug 11 15:04:40.124: INFO: Pod "dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f": Phase="Running", Reason="", readiness=true. Elapsed: 2.01200999s
    Aug 11 15:04:40.124: INFO: Pod "dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:04:40.124
    STEP: looking for the results for each expected name from probers 08/11/23 15:04:40.127
    Aug 11 15:04:40.138: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:40.143: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:40.149: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:40.154: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:40.180: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:40.187: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:40.192: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:40.197: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:40.219: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

    Aug 11 15:04:45.227: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:45.233: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:45.238: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:45.244: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:45.273: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:45.280: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:45.286: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:45.292: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:45.318: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

    Aug 11 15:04:50.227: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:50.233: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:50.238: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:50.244: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:50.274: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:50.279: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:50.285: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:50.291: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:50.316: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

    Aug 11 15:04:55.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:55.236: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:55.241: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:55.246: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:55.272: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:55.277: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:55.283: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:55.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:04:55.310: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

    Aug 11 15:05:00.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:00.236: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:00.241: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:00.247: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:00.275: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:00.282: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:00.287: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:00.292: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:00.314: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

    Aug 11 15:05:05.230: INFO: Unable to read wheezy_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:05.237: INFO: Unable to read wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:05.243: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:05.248: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:05.281: INFO: Unable to read jessie_udp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:05.288: INFO: Unable to read jessie_tcp@dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:05.294: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:05.301: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local from pod dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f: the server could not find the requested resource (get pods dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f)
    Aug 11 15:05:05.324: INFO: Lookups using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f failed for: [wheezy_udp@dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@dns-test-service.dns-8021.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_udp@dns-test-service.dns-8021.svc.cluster.local jessie_tcp@dns-test-service.dns-8021.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-8021.svc.cluster.local]

    Aug 11 15:05:10.318: INFO: DNS probes using dns-8021/dns-test-01a3c1dd-34cd-423d-8f5f-a25926bd520f succeeded

    STEP: deleting the pod 08/11/23 15:05:10.318
    STEP: deleting the test service 08/11/23 15:05:10.34
    STEP: deleting the test headless service 08/11/23 15:05:10.371
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 11 15:05:10.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-8021" for this suite. 08/11/23 15:05:10.393
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:05:10.4
Aug 11 15:05:10.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename statefulset 08/11/23 15:05:10.402
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:10.419
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:10.422
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2813 08/11/23 15:05:10.425
[It] Should recreate evicted statefulset [Conformance]
  test/e2e/apps/statefulset.go:737
STEP: Looking for a node to schedule stateful set and pod 08/11/23 15:05:10.43
STEP: Creating pod with conflicting port in namespace statefulset-2813 08/11/23 15:05:10.437
STEP: Waiting until pod test-pod will start running in namespace statefulset-2813 08/11/23 15:05:10.446
Aug 11 15:05:10.446: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2813" to be "running"
Aug 11 15:05:10.449: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.107517ms
Aug 11 15:05:12.456: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010029549s
Aug 11 15:05:12.456: INFO: Pod "test-pod" satisfied condition "running"
STEP: Creating statefulset with conflicting port in namespace statefulset-2813 08/11/23 15:05:12.456
STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2813 08/11/23 15:05:12.465
Aug 11 15:05:12.489: INFO: Observed stateful pod in namespace: statefulset-2813, name: ss-0, uid: 6dda9658-380e-4ca0-8655-deb0e06d63a0, status phase: Pending. Waiting for statefulset controller to delete.
Aug 11 15:05:12.505: INFO: Observed stateful pod in namespace: statefulset-2813, name: ss-0, uid: 6dda9658-380e-4ca0-8655-deb0e06d63a0, status phase: Failed. Waiting for statefulset controller to delete.
Aug 11 15:05:12.521: INFO: Observed stateful pod in namespace: statefulset-2813, name: ss-0, uid: 6dda9658-380e-4ca0-8655-deb0e06d63a0, status phase: Failed. Waiting for statefulset controller to delete.
Aug 11 15:05:12.523: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2813
STEP: Removing pod with conflicting port in namespace statefulset-2813 08/11/23 15:05:12.523
STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2813 and will be in running state 08/11/23 15:05:12.539
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 11 15:05:14.549: INFO: Deleting all statefulset in ns statefulset-2813
Aug 11 15:05:14.551: INFO: Scaling statefulset ss to 0
Aug 11 15:05:24.571: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:05:24.574: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 11 15:05:24.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2813" for this suite. 08/11/23 15:05:24.592
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]","completed":236,"skipped":4393,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.199 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Should recreate evicted statefulset [Conformance]
    test/e2e/apps/statefulset.go:737

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:05:10.4
    Aug 11 15:05:10.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename statefulset 08/11/23 15:05:10.402
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:10.419
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:10.422
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2813 08/11/23 15:05:10.425
    [It] Should recreate evicted statefulset [Conformance]
      test/e2e/apps/statefulset.go:737
    STEP: Looking for a node to schedule stateful set and pod 08/11/23 15:05:10.43
    STEP: Creating pod with conflicting port in namespace statefulset-2813 08/11/23 15:05:10.437
    STEP: Waiting until pod test-pod will start running in namespace statefulset-2813 08/11/23 15:05:10.446
    Aug 11 15:05:10.446: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "statefulset-2813" to be "running"
    Aug 11 15:05:10.449: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.107517ms
    Aug 11 15:05:12.456: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010029549s
    Aug 11 15:05:12.456: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Creating statefulset with conflicting port in namespace statefulset-2813 08/11/23 15:05:12.456
    STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2813 08/11/23 15:05:12.465
    Aug 11 15:05:12.489: INFO: Observed stateful pod in namespace: statefulset-2813, name: ss-0, uid: 6dda9658-380e-4ca0-8655-deb0e06d63a0, status phase: Pending. Waiting for statefulset controller to delete.
    Aug 11 15:05:12.505: INFO: Observed stateful pod in namespace: statefulset-2813, name: ss-0, uid: 6dda9658-380e-4ca0-8655-deb0e06d63a0, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 11 15:05:12.521: INFO: Observed stateful pod in namespace: statefulset-2813, name: ss-0, uid: 6dda9658-380e-4ca0-8655-deb0e06d63a0, status phase: Failed. Waiting for statefulset controller to delete.
    Aug 11 15:05:12.523: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2813
    STEP: Removing pod with conflicting port in namespace statefulset-2813 08/11/23 15:05:12.523
    STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2813 and will be in running state 08/11/23 15:05:12.539
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 11 15:05:14.549: INFO: Deleting all statefulset in ns statefulset-2813
    Aug 11 15:05:14.551: INFO: Scaling statefulset ss to 0
    Aug 11 15:05:24.571: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:05:24.574: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 11 15:05:24.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2813" for this suite. 08/11/23 15:05:24.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:05:24.6
Aug 11 15:05:24.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename var-expansion 08/11/23 15:05:24.601
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:24.616
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:24.618
[It] should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72
STEP: Creating a pod to test substitution in container's command 08/11/23 15:05:24.621
Aug 11 15:05:24.630: INFO: Waiting up to 5m0s for pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d" in namespace "var-expansion-6976" to be "Succeeded or Failed"
Aug 11 15:05:24.633: INFO: Pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20698ms
Aug 11 15:05:26.638: INFO: Pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00814891s
Aug 11 15:05:28.637: INFO: Pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007084524s
STEP: Saw pod success 08/11/23 15:05:28.637
Aug 11 15:05:28.637: INFO: Pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d" satisfied condition "Succeeded or Failed"
Aug 11 15:05:28.641: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod var-expansion-f452a421-74a7-494a-b1fa-4a224454064d container dapi-container: <nil>
STEP: delete the pod 08/11/23 15:05:28.65
Aug 11 15:05:28.664: INFO: Waiting for pod var-expansion-f452a421-74a7-494a-b1fa-4a224454064d to disappear
Aug 11 15:05:28.667: INFO: Pod var-expansion-f452a421-74a7-494a-b1fa-4a224454064d no longer exists
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 11 15:05:28.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-6976" for this suite. 08/11/23 15:05:28.671
{"msg":"PASSED [sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]","completed":237,"skipped":4403,"failed":0}
------------------------------
â€¢ [4.078 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should allow substituting values in a container's command [NodeConformance] [Conformance]
  test/e2e/common/node/expansion.go:72

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:05:24.6
    Aug 11 15:05:24.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename var-expansion 08/11/23 15:05:24.601
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:24.616
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:24.618
    [It] should allow substituting values in a container's command [NodeConformance] [Conformance]
      test/e2e/common/node/expansion.go:72
    STEP: Creating a pod to test substitution in container's command 08/11/23 15:05:24.621
    Aug 11 15:05:24.630: INFO: Waiting up to 5m0s for pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d" in namespace "var-expansion-6976" to be "Succeeded or Failed"
    Aug 11 15:05:24.633: INFO: Pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.20698ms
    Aug 11 15:05:26.638: INFO: Pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.00814891s
    Aug 11 15:05:28.637: INFO: Pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007084524s
    STEP: Saw pod success 08/11/23 15:05:28.637
    Aug 11 15:05:28.637: INFO: Pod "var-expansion-f452a421-74a7-494a-b1fa-4a224454064d" satisfied condition "Succeeded or Failed"
    Aug 11 15:05:28.641: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod var-expansion-f452a421-74a7-494a-b1fa-4a224454064d container dapi-container: <nil>
    STEP: delete the pod 08/11/23 15:05:28.65
    Aug 11 15:05:28.664: INFO: Waiting for pod var-expansion-f452a421-74a7-494a-b1fa-4a224454064d to disappear
    Aug 11 15:05:28.667: INFO: Pod var-expansion-f452a421-74a7-494a-b1fa-4a224454064d no longer exists
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 11 15:05:28.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-6976" for this suite. 08/11/23 15:05:28.671
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] Projected combined
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
[BeforeEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:05:28.679
Aug 11 15:05:28.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 15:05:28.68
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:28.696
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:28.698
[It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43
STEP: Creating configMap with name configmap-projected-all-test-volume-5c2f503e-8b7d-425c-9914-f5c315ef7e2d 08/11/23 15:05:28.701
STEP: Creating secret with name secret-projected-all-test-volume-c5f83088-95ac-4d5f-a9e1-fc53797587fb 08/11/23 15:05:28.706
STEP: Creating a pod to test Check all projections for projected volume plugin 08/11/23 15:05:28.712
Aug 11 15:05:28.721: INFO: Waiting up to 5m0s for pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb" in namespace "projected-5490" to be "Succeeded or Failed"
Aug 11 15:05:28.726: INFO: Pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515412ms
Aug 11 15:05:30.732: INFO: Pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01070838s
Aug 11 15:05:32.732: INFO: Pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010890352s
STEP: Saw pod success 08/11/23 15:05:32.732
Aug 11 15:05:32.732: INFO: Pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb" satisfied condition "Succeeded or Failed"
Aug 11 15:05:32.735: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb container projected-all-volume-test: <nil>
STEP: delete the pod 08/11/23 15:05:32.743
Aug 11 15:05:32.758: INFO: Waiting for pod projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb to disappear
Aug 11 15:05:32.761: INFO: Pod projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb no longer exists
[AfterEach] [sig-storage] Projected combined
  test/e2e/framework/framework.go:187
Aug 11 15:05:32.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5490" for this suite. 08/11/23 15:05:32.765
{"msg":"PASSED [sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]","completed":238,"skipped":4407,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-storage] Projected combined
test/e2e/common/storage/framework.go:23
  should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
  test/e2e/common/storage/projected_combined.go:43

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:05:28.679
    Aug 11 15:05:28.679: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 15:05:28.68
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:28.696
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:28.698
    [It] should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
      test/e2e/common/storage/projected_combined.go:43
    STEP: Creating configMap with name configmap-projected-all-test-volume-5c2f503e-8b7d-425c-9914-f5c315ef7e2d 08/11/23 15:05:28.701
    STEP: Creating secret with name secret-projected-all-test-volume-c5f83088-95ac-4d5f-a9e1-fc53797587fb 08/11/23 15:05:28.706
    STEP: Creating a pod to test Check all projections for projected volume plugin 08/11/23 15:05:28.712
    Aug 11 15:05:28.721: INFO: Waiting up to 5m0s for pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb" in namespace "projected-5490" to be "Succeeded or Failed"
    Aug 11 15:05:28.726: INFO: Pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.515412ms
    Aug 11 15:05:30.732: INFO: Pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01070838s
    Aug 11 15:05:32.732: INFO: Pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010890352s
    STEP: Saw pod success 08/11/23 15:05:32.732
    Aug 11 15:05:32.732: INFO: Pod "projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb" satisfied condition "Succeeded or Failed"
    Aug 11 15:05:32.735: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb container projected-all-volume-test: <nil>
    STEP: delete the pod 08/11/23 15:05:32.743
    Aug 11 15:05:32.758: INFO: Waiting for pod projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb to disappear
    Aug 11 15:05:32.761: INFO: Pod projected-volume-bdc89ccd-c71b-4283-903c-37bee0456acb no longer exists
    [AfterEach] [sig-storage] Projected combined
      test/e2e/framework/framework.go:187
    Aug 11 15:05:32.761: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5490" for this suite. 08/11/23 15:05:32.765
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:05:32.772
Aug 11 15:05:32.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 15:05:32.773
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:32.789
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:32.792
[It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316
STEP: Counting existing ResourceQuota 08/11/23 15:05:49.799
STEP: Creating a ResourceQuota 08/11/23 15:05:54.804
STEP: Ensuring resource quota status is calculated 08/11/23 15:05:54.809
STEP: Creating a ConfigMap 08/11/23 15:05:56.815
STEP: Ensuring resource quota status captures configMap creation 08/11/23 15:05:56.828
STEP: Deleting a ConfigMap 08/11/23 15:05:58.831
STEP: Ensuring resource quota status released usage 08/11/23 15:05:58.837
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 15:06:00.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-8354" for this suite. 08/11/23 15:06:00.847
{"msg":"PASSED [sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]","completed":239,"skipped":4412,"failed":0}
------------------------------
â€¢ [SLOW TEST] [28.081 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should create a ResourceQuota and capture the life of a configMap. [Conformance]
  test/e2e/apimachinery/resource_quota.go:316

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:05:32.772
    Aug 11 15:05:32.772: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 15:05:32.773
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:05:32.789
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:05:32.792
    [It] should create a ResourceQuota and capture the life of a configMap. [Conformance]
      test/e2e/apimachinery/resource_quota.go:316
    STEP: Counting existing ResourceQuota 08/11/23 15:05:49.799
    STEP: Creating a ResourceQuota 08/11/23 15:05:54.804
    STEP: Ensuring resource quota status is calculated 08/11/23 15:05:54.809
    STEP: Creating a ConfigMap 08/11/23 15:05:56.815
    STEP: Ensuring resource quota status captures configMap creation 08/11/23 15:05:56.828
    STEP: Deleting a ConfigMap 08/11/23 15:05:58.831
    STEP: Ensuring resource quota status released usage 08/11/23 15:05:58.837
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 15:06:00.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-8354" for this suite. 08/11/23 15:06:00.847
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:06:00.854
Aug 11 15:06:00.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename gc 08/11/23 15:06:00.856
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:00.872
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:00.874
[It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735
STEP: create the rc1 08/11/23 15:06:00.881
STEP: create the rc2 08/11/23 15:06:00.892
STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/11/23 15:06:05.909
STEP: delete the rc simpletest-rc-to-be-deleted 08/11/23 15:06:06.364
STEP: wait for the rc to be deleted 08/11/23 15:06:06.371
Aug 11 15:06:11.385: INFO: 81 pods remaining
Aug 11 15:06:11.385: INFO: 73 pods has nil DeletionTimestamp
Aug 11 15:06:11.385: INFO: 
Aug 11 15:06:16.382: INFO: 60 pods remaining
Aug 11 15:06:16.382: INFO: 50 pods has nil DeletionTimestamp
Aug 11 15:06:16.382: INFO: 
STEP: Gathering metrics 08/11/23 15:06:21.387
Aug 11 15:06:21.427: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
Aug 11 15:06:21.431: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.350524ms
Aug 11 15:06:21.431: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
Aug 11 15:06:21.431: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
Aug 11 15:06:21.494: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

Aug 11 15:06:21.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-24q76" in namespace "gc-7011"
Aug 11 15:06:21.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-44tbl" in namespace "gc-7011"
Aug 11 15:06:21.528: INFO: Deleting pod "simpletest-rc-to-be-deleted-4brsf" in namespace "gc-7011"
Aug 11 15:06:21.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ktfh" in namespace "gc-7011"
Aug 11 15:06:21.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lxj2" in namespace "gc-7011"
Aug 11 15:06:21.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qsr8" in namespace "gc-7011"
Aug 11 15:06:21.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qvx7" in namespace "gc-7011"
Aug 11 15:06:21.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-4sw78" in namespace "gc-7011"
Aug 11 15:06:21.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-546pt" in namespace "gc-7011"
Aug 11 15:06:21.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-5krqr" in namespace "gc-7011"
Aug 11 15:06:21.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-5t4s4" in namespace "gc-7011"
Aug 11 15:06:21.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-5t6sw" in namespace "gc-7011"
Aug 11 15:06:21.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wpfc" in namespace "gc-7011"
Aug 11 15:06:21.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-67nzs" in namespace "gc-7011"
Aug 11 15:06:21.749: INFO: Deleting pod "simpletest-rc-to-be-deleted-6flc9" in namespace "gc-7011"
Aug 11 15:06:21.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jrb6" in namespace "gc-7011"
Aug 11 15:06:21.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rkvg" in namespace "gc-7011"
Aug 11 15:06:21.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tr2c" in namespace "gc-7011"
Aug 11 15:06:21.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-7t59b" in namespace "gc-7011"
Aug 11 15:06:21.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v8dx" in namespace "gc-7011"
Aug 11 15:06:21.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qsgv" in namespace "gc-7011"
Aug 11 15:06:21.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tj8l" in namespace "gc-7011"
Aug 11 15:06:21.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-8v6tp" in namespace "gc-7011"
Aug 11 15:06:21.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-965t7" in namespace "gc-7011"
Aug 11 15:06:21.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kpnb" in namespace "gc-7011"
Aug 11 15:06:21.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mzxd" in namespace "gc-7011"
Aug 11 15:06:21.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x26r" in namespace "gc-7011"
Aug 11 15:06:21.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8mkl" in namespace "gc-7011"
Aug 11 15:06:21.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqhgl" in namespace "gc-7011"
Aug 11 15:06:21.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdjhz" in namespace "gc-7011"
Aug 11 15:06:22.005: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmqcr" in namespace "gc-7011"
Aug 11 15:06:22.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-cn642" in namespace "gc-7011"
Aug 11 15:06:22.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr8ns" in namespace "gc-7011"
Aug 11 15:06:22.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxk4k" in namespace "gc-7011"
Aug 11 15:06:22.056: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh7cj" in namespace "gc-7011"
Aug 11 15:06:22.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtf6n" in namespace "gc-7011"
Aug 11 15:06:22.088: INFO: Deleting pod "simpletest-rc-to-be-deleted-dznxj" in namespace "gc-7011"
Aug 11 15:06:22.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-f88fm" in namespace "gc-7011"
Aug 11 15:06:22.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbslj" in namespace "gc-7011"
Aug 11 15:06:22.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-fcvft" in namespace "gc-7011"
Aug 11 15:06:22.148: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgnzm" in namespace "gc-7011"
Aug 11 15:06:22.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft5n7" in namespace "gc-7011"
Aug 11 15:06:22.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx6p5" in namespace "gc-7011"
Aug 11 15:06:22.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4fcs" in namespace "gc-7011"
Aug 11 15:06:22.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfx6l" in namespace "gc-7011"
Aug 11 15:06:22.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjfcj" in namespace "gc-7011"
Aug 11 15:06:22.232: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkj62" in namespace "gc-7011"
Aug 11 15:06:22.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6pjf" in namespace "gc-7011"
Aug 11 15:06:22.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8x9v" in namespace "gc-7011"
Aug 11 15:06:22.266: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcmhg" in namespace "gc-7011"
[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 11 15:06:22.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-7011" for this suite. 08/11/23 15:06:22.284
{"msg":"PASSED [sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]","completed":240,"skipped":4415,"failed":0}
------------------------------
â€¢ [SLOW TEST] [21.441 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
  test/e2e/apimachinery/garbage_collector.go:735

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:06:00.854
    Aug 11 15:06:00.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename gc 08/11/23 15:06:00.856
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:00.872
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:00.874
    [It] should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
      test/e2e/apimachinery/garbage_collector.go:735
    STEP: create the rc1 08/11/23 15:06:00.881
    STEP: create the rc2 08/11/23 15:06:00.892
    STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well 08/11/23 15:06:05.909
    STEP: delete the rc simpletest-rc-to-be-deleted 08/11/23 15:06:06.364
    STEP: wait for the rc to be deleted 08/11/23 15:06:06.371
    Aug 11 15:06:11.385: INFO: 81 pods remaining
    Aug 11 15:06:11.385: INFO: 73 pods has nil DeletionTimestamp
    Aug 11 15:06:11.385: INFO: 
    Aug 11 15:06:16.382: INFO: 60 pods remaining
    Aug 11 15:06:16.382: INFO: 50 pods has nil DeletionTimestamp
    Aug 11 15:06:16.382: INFO: 
    STEP: Gathering metrics 08/11/23 15:06:21.387
    Aug 11 15:06:21.427: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
    Aug 11 15:06:21.431: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.350524ms
    Aug 11 15:06:21.431: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
    Aug 11 15:06:21.431: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
    Aug 11 15:06:21.494: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    Aug 11 15:06:21.494: INFO: Deleting pod "simpletest-rc-to-be-deleted-24q76" in namespace "gc-7011"
    Aug 11 15:06:21.511: INFO: Deleting pod "simpletest-rc-to-be-deleted-44tbl" in namespace "gc-7011"
    Aug 11 15:06:21.528: INFO: Deleting pod "simpletest-rc-to-be-deleted-4brsf" in namespace "gc-7011"
    Aug 11 15:06:21.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ktfh" in namespace "gc-7011"
    Aug 11 15:06:21.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lxj2" in namespace "gc-7011"
    Aug 11 15:06:21.570: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qsr8" in namespace "gc-7011"
    Aug 11 15:06:21.588: INFO: Deleting pod "simpletest-rc-to-be-deleted-4qvx7" in namespace "gc-7011"
    Aug 11 15:06:21.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-4sw78" in namespace "gc-7011"
    Aug 11 15:06:21.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-546pt" in namespace "gc-7011"
    Aug 11 15:06:21.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-5krqr" in namespace "gc-7011"
    Aug 11 15:06:21.668: INFO: Deleting pod "simpletest-rc-to-be-deleted-5t4s4" in namespace "gc-7011"
    Aug 11 15:06:21.696: INFO: Deleting pod "simpletest-rc-to-be-deleted-5t6sw" in namespace "gc-7011"
    Aug 11 15:06:21.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wpfc" in namespace "gc-7011"
    Aug 11 15:06:21.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-67nzs" in namespace "gc-7011"
    Aug 11 15:06:21.749: INFO: Deleting pod "simpletest-rc-to-be-deleted-6flc9" in namespace "gc-7011"
    Aug 11 15:06:21.771: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jrb6" in namespace "gc-7011"
    Aug 11 15:06:21.782: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rkvg" in namespace "gc-7011"
    Aug 11 15:06:21.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tr2c" in namespace "gc-7011"
    Aug 11 15:06:21.809: INFO: Deleting pod "simpletest-rc-to-be-deleted-7t59b" in namespace "gc-7011"
    Aug 11 15:06:21.822: INFO: Deleting pod "simpletest-rc-to-be-deleted-7v8dx" in namespace "gc-7011"
    Aug 11 15:06:21.836: INFO: Deleting pod "simpletest-rc-to-be-deleted-8qsgv" in namespace "gc-7011"
    Aug 11 15:06:21.848: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tj8l" in namespace "gc-7011"
    Aug 11 15:06:21.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-8v6tp" in namespace "gc-7011"
    Aug 11 15:06:21.875: INFO: Deleting pod "simpletest-rc-to-be-deleted-965t7" in namespace "gc-7011"
    Aug 11 15:06:21.894: INFO: Deleting pod "simpletest-rc-to-be-deleted-9kpnb" in namespace "gc-7011"
    Aug 11 15:06:21.911: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mzxd" in namespace "gc-7011"
    Aug 11 15:06:21.924: INFO: Deleting pod "simpletest-rc-to-be-deleted-9x26r" in namespace "gc-7011"
    Aug 11 15:06:21.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-b8mkl" in namespace "gc-7011"
    Aug 11 15:06:21.971: INFO: Deleting pod "simpletest-rc-to-be-deleted-bqhgl" in namespace "gc-7011"
    Aug 11 15:06:21.986: INFO: Deleting pod "simpletest-rc-to-be-deleted-cdjhz" in namespace "gc-7011"
    Aug 11 15:06:22.005: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmqcr" in namespace "gc-7011"
    Aug 11 15:06:22.019: INFO: Deleting pod "simpletest-rc-to-be-deleted-cn642" in namespace "gc-7011"
    Aug 11 15:06:22.031: INFO: Deleting pod "simpletest-rc-to-be-deleted-cr8ns" in namespace "gc-7011"
    Aug 11 15:06:22.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-cxk4k" in namespace "gc-7011"
    Aug 11 15:06:22.056: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh7cj" in namespace "gc-7011"
    Aug 11 15:06:22.070: INFO: Deleting pod "simpletest-rc-to-be-deleted-dtf6n" in namespace "gc-7011"
    Aug 11 15:06:22.088: INFO: Deleting pod "simpletest-rc-to-be-deleted-dznxj" in namespace "gc-7011"
    Aug 11 15:06:22.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-f88fm" in namespace "gc-7011"
    Aug 11 15:06:22.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-fbslj" in namespace "gc-7011"
    Aug 11 15:06:22.133: INFO: Deleting pod "simpletest-rc-to-be-deleted-fcvft" in namespace "gc-7011"
    Aug 11 15:06:22.148: INFO: Deleting pod "simpletest-rc-to-be-deleted-fgnzm" in namespace "gc-7011"
    Aug 11 15:06:22.161: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft5n7" in namespace "gc-7011"
    Aug 11 15:06:22.175: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx6p5" in namespace "gc-7011"
    Aug 11 15:06:22.188: INFO: Deleting pod "simpletest-rc-to-be-deleted-g4fcs" in namespace "gc-7011"
    Aug 11 15:06:22.203: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfx6l" in namespace "gc-7011"
    Aug 11 15:06:22.218: INFO: Deleting pod "simpletest-rc-to-be-deleted-gjfcj" in namespace "gc-7011"
    Aug 11 15:06:22.232: INFO: Deleting pod "simpletest-rc-to-be-deleted-gkj62" in namespace "gc-7011"
    Aug 11 15:06:22.244: INFO: Deleting pod "simpletest-rc-to-be-deleted-h6pjf" in namespace "gc-7011"
    Aug 11 15:06:22.255: INFO: Deleting pod "simpletest-rc-to-be-deleted-h8x9v" in namespace "gc-7011"
    Aug 11 15:06:22.266: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcmhg" in namespace "gc-7011"
    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 11 15:06:22.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-7011" for this suite. 08/11/23 15:06:22.284
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:06:22.299
Aug 11 15:06:22.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:06:22.303
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:22.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:22.322
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should have an terminated reason [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:110
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 11 15:06:30.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-6687" for this suite. 08/11/23 15:06:30.359
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]","completed":241,"skipped":4434,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.067 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should have an terminated reason [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:110

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:06:22.299
    Aug 11 15:06:22.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:06:22.303
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:22.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:22.322
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should have an terminated reason [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:110
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 11 15:06:30.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-6687" for this suite. 08/11/23 15:06:30.359
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:06:30.367
Aug 11 15:06:30.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:06:30.368
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:30.384
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:30.387
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523
STEP: creating a service nodeport-service with the type=NodePort in namespace services-7048 08/11/23 15:06:30.389
STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/11/23 15:06:30.41
STEP: creating service externalsvc in namespace services-7048 08/11/23 15:06:30.41
STEP: creating replication controller externalsvc in namespace services-7048 08/11/23 15:06:30.434
I0811 15:06:30.442622      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7048, replica count: 2
I0811 15:06:33.494395      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
STEP: changing the NodePort service to type=ExternalName 08/11/23 15:06:33.498
Aug 11 15:06:33.530: INFO: Creating new exec pod
Aug 11 15:06:33.540: INFO: Waiting up to 5m0s for pod "execpodnb76b" in namespace "services-7048" to be "running"
Aug 11 15:06:33.546: INFO: Pod "execpodnb76b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.4407ms
Aug 11 15:06:35.552: INFO: Pod "execpodnb76b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011562197s
Aug 11 15:06:35.552: INFO: Pod "execpodnb76b" satisfied condition "running"
Aug 11 15:06:35.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-7048 exec execpodnb76b -- /bin/sh -x -c nslookup nodeport-service.services-7048.svc.cluster.local'
Aug 11 15:06:35.739: INFO: stderr: "+ nslookup nodeport-service.services-7048.svc.cluster.local\n"
Aug 11 15:06:35.739: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-7048.svc.cluster.local\tcanonical name = externalsvc.services-7048.svc.cluster.local.\nName:\texternalsvc.services-7048.svc.cluster.local\nAddress: 10.102.23.134\n\n"
STEP: deleting ReplicationController externalsvc in namespace services-7048, will wait for the garbage collector to delete the pods 08/11/23 15:06:35.739
Aug 11 15:06:35.801: INFO: Deleting ReplicationController externalsvc took: 7.362329ms
Aug 11 15:06:35.902: INFO: Terminating ReplicationController externalsvc pods took: 100.91278ms
Aug 11 15:06:38.031: INFO: Cleaning up the NodePort to ExternalName test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:06:38.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-7048" for this suite. 08/11/23 15:06:38.05
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]","completed":242,"skipped":4441,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.689 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from NodePort to ExternalName [Conformance]
  test/e2e/network/service.go:1523

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:06:30.367
    Aug 11 15:06:30.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:06:30.368
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:30.384
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:30.387
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from NodePort to ExternalName [Conformance]
      test/e2e/network/service.go:1523
    STEP: creating a service nodeport-service with the type=NodePort in namespace services-7048 08/11/23 15:06:30.389
    STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service 08/11/23 15:06:30.41
    STEP: creating service externalsvc in namespace services-7048 08/11/23 15:06:30.41
    STEP: creating replication controller externalsvc in namespace services-7048 08/11/23 15:06:30.434
    I0811 15:06:30.442622      22 runners.go:193] Created replication controller with name: externalsvc, namespace: services-7048, replica count: 2
    I0811 15:06:33.494395      22 runners.go:193] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    STEP: changing the NodePort service to type=ExternalName 08/11/23 15:06:33.498
    Aug 11 15:06:33.530: INFO: Creating new exec pod
    Aug 11 15:06:33.540: INFO: Waiting up to 5m0s for pod "execpodnb76b" in namespace "services-7048" to be "running"
    Aug 11 15:06:33.546: INFO: Pod "execpodnb76b": Phase="Pending", Reason="", readiness=false. Elapsed: 6.4407ms
    Aug 11 15:06:35.552: INFO: Pod "execpodnb76b": Phase="Running", Reason="", readiness=true. Elapsed: 2.011562197s
    Aug 11 15:06:35.552: INFO: Pod "execpodnb76b" satisfied condition "running"
    Aug 11 15:06:35.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-7048 exec execpodnb76b -- /bin/sh -x -c nslookup nodeport-service.services-7048.svc.cluster.local'
    Aug 11 15:06:35.739: INFO: stderr: "+ nslookup nodeport-service.services-7048.svc.cluster.local\n"
    Aug 11 15:06:35.739: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-7048.svc.cluster.local\tcanonical name = externalsvc.services-7048.svc.cluster.local.\nName:\texternalsvc.services-7048.svc.cluster.local\nAddress: 10.102.23.134\n\n"
    STEP: deleting ReplicationController externalsvc in namespace services-7048, will wait for the garbage collector to delete the pods 08/11/23 15:06:35.739
    Aug 11 15:06:35.801: INFO: Deleting ReplicationController externalsvc took: 7.362329ms
    Aug 11 15:06:35.902: INFO: Terminating ReplicationController externalsvc pods took: 100.91278ms
    Aug 11 15:06:38.031: INFO: Cleaning up the NodePort to ExternalName test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:06:38.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-7048" for this suite. 08/11/23 15:06:38.05
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:06:38.057
Aug 11 15:06:38.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:06:38.058
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:38.074
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:38.076
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157
STEP: creating service in namespace services-4219 08/11/23 15:06:38.079
STEP: creating service affinity-clusterip in namespace services-4219 08/11/23 15:06:38.079
STEP: creating replication controller affinity-clusterip in namespace services-4219 08/11/23 15:06:38.101
I0811 15:06:38.108901      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4219, replica count: 3
I0811 15:06:41.160387      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 15:06:41.167: INFO: Creating new exec pod
Aug 11 15:06:41.175: INFO: Waiting up to 5m0s for pod "execpod-affinityc5f4d" in namespace "services-4219" to be "running"
Aug 11 15:06:41.183: INFO: Pod "execpod-affinityc5f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.682529ms
Aug 11 15:06:43.188: INFO: Pod "execpod-affinityc5f4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012707433s
Aug 11 15:06:43.188: INFO: Pod "execpod-affinityc5f4d" satisfied condition "running"
Aug 11 15:06:44.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4219 exec execpod-affinityc5f4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
Aug 11 15:06:44.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
Aug 11 15:06:44.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:06:44.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4219 exec execpod-affinityc5f4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.109.250 80'
Aug 11 15:06:44.454: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.109.250 80\nConnection to 10.109.109.250 80 port [tcp/http] succeeded!\n"
Aug 11 15:06:44.454: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:06:44.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4219 exec execpod-affinityc5f4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.109.250:80/ ; done'
Aug 11 15:06:44.618: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n"
Aug 11 15:06:44.619: INFO: stdout: "\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8"
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
Aug 11 15:06:44.619: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip in namespace services-4219, will wait for the garbage collector to delete the pods 08/11/23 15:06:44.635
Aug 11 15:06:44.698: INFO: Deleting ReplicationController affinity-clusterip took: 7.599686ms
Aug 11 15:06:44.798: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.555319ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:06:46.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4219" for this suite. 08/11/23 15:06:46.43
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]","completed":243,"skipped":4453,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.380 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2157

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:06:38.057
    Aug 11 15:06:38.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:06:38.058
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:38.074
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:38.076
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2157
    STEP: creating service in namespace services-4219 08/11/23 15:06:38.079
    STEP: creating service affinity-clusterip in namespace services-4219 08/11/23 15:06:38.079
    STEP: creating replication controller affinity-clusterip in namespace services-4219 08/11/23 15:06:38.101
    I0811 15:06:38.108901      22 runners.go:193] Created replication controller with name: affinity-clusterip, namespace: services-4219, replica count: 3
    I0811 15:06:41.160387      22 runners.go:193] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 15:06:41.167: INFO: Creating new exec pod
    Aug 11 15:06:41.175: INFO: Waiting up to 5m0s for pod "execpod-affinityc5f4d" in namespace "services-4219" to be "running"
    Aug 11 15:06:41.183: INFO: Pod "execpod-affinityc5f4d": Phase="Pending", Reason="", readiness=false. Elapsed: 7.682529ms
    Aug 11 15:06:43.188: INFO: Pod "execpod-affinityc5f4d": Phase="Running", Reason="", readiness=true. Elapsed: 2.012707433s
    Aug 11 15:06:43.188: INFO: Pod "execpod-affinityc5f4d" satisfied condition "running"
    Aug 11 15:06:44.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4219 exec execpod-affinityc5f4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
    Aug 11 15:06:44.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
    Aug 11 15:06:44.329: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:06:44.330: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4219 exec execpod-affinityc5f4d -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.109.250 80'
    Aug 11 15:06:44.454: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.109.250 80\nConnection to 10.109.109.250 80 port [tcp/http] succeeded!\n"
    Aug 11 15:06:44.454: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:06:44.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4219 exec execpod-affinityc5f4d -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.109.109.250:80/ ; done'
    Aug 11 15:06:44.618: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.109.109.250:80/\n"
    Aug 11 15:06:44.619: INFO: stdout: "\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8\naffinity-clusterip-fwkc8"
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Received response from host: affinity-clusterip-fwkc8
    Aug 11 15:06:44.619: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip in namespace services-4219, will wait for the garbage collector to delete the pods 08/11/23 15:06:44.635
    Aug 11 15:06:44.698: INFO: Deleting ReplicationController affinity-clusterip took: 7.599686ms
    Aug 11 15:06:44.798: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.555319ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:06:46.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4219" for this suite. 08/11/23 15:06:46.43
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:06:46.438
Aug 11 15:06:46.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replication-controller 08/11/23 15:06:46.439
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:46.454
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:46.457
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109
STEP: creating a ReplicationController 08/11/23 15:06:46.463
STEP: waiting for RC to be added 08/11/23 15:06:46.468
STEP: waiting for available Replicas 08/11/23 15:06:46.469
STEP: patching ReplicationController 08/11/23 15:06:47.231
STEP: waiting for RC to be modified 08/11/23 15:06:47.239
STEP: patching ReplicationController status 08/11/23 15:06:47.239
STEP: waiting for RC to be modified 08/11/23 15:06:47.245
STEP: waiting for available Replicas 08/11/23 15:06:47.245
STEP: fetching ReplicationController status 08/11/23 15:06:47.25
STEP: patching ReplicationController scale 08/11/23 15:06:47.254
STEP: waiting for RC to be modified 08/11/23 15:06:47.261
STEP: waiting for ReplicationController's scale to be the max amount 08/11/23 15:06:47.261
STEP: fetching ReplicationController; ensuring that it's patched 08/11/23 15:06:49.021
STEP: updating ReplicationController status 08/11/23 15:06:49.025
STEP: waiting for RC to be modified 08/11/23 15:06:49.031
STEP: listing all ReplicationControllers 08/11/23 15:06:49.032
STEP: checking that ReplicationController has expected values 08/11/23 15:06:49.037
STEP: deleting ReplicationControllers by collection 08/11/23 15:06:49.037
STEP: waiting for ReplicationController to have a DELETED watchEvent 08/11/23 15:06:49.045
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 11 15:06:49.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3862" for this suite. 08/11/23 15:06:49.087
{"msg":"PASSED [sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]","completed":244,"skipped":4458,"failed":0}
------------------------------
â€¢ [2.656 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should test the lifecycle of a ReplicationController [Conformance]
  test/e2e/apps/rc.go:109

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:06:46.438
    Aug 11 15:06:46.438: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replication-controller 08/11/23 15:06:46.439
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:46.454
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:46.457
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should test the lifecycle of a ReplicationController [Conformance]
      test/e2e/apps/rc.go:109
    STEP: creating a ReplicationController 08/11/23 15:06:46.463
    STEP: waiting for RC to be added 08/11/23 15:06:46.468
    STEP: waiting for available Replicas 08/11/23 15:06:46.469
    STEP: patching ReplicationController 08/11/23 15:06:47.231
    STEP: waiting for RC to be modified 08/11/23 15:06:47.239
    STEP: patching ReplicationController status 08/11/23 15:06:47.239
    STEP: waiting for RC to be modified 08/11/23 15:06:47.245
    STEP: waiting for available Replicas 08/11/23 15:06:47.245
    STEP: fetching ReplicationController status 08/11/23 15:06:47.25
    STEP: patching ReplicationController scale 08/11/23 15:06:47.254
    STEP: waiting for RC to be modified 08/11/23 15:06:47.261
    STEP: waiting for ReplicationController's scale to be the max amount 08/11/23 15:06:47.261
    STEP: fetching ReplicationController; ensuring that it's patched 08/11/23 15:06:49.021
    STEP: updating ReplicationController status 08/11/23 15:06:49.025
    STEP: waiting for RC to be modified 08/11/23 15:06:49.031
    STEP: listing all ReplicationControllers 08/11/23 15:06:49.032
    STEP: checking that ReplicationController has expected values 08/11/23 15:06:49.037
    STEP: deleting ReplicationControllers by collection 08/11/23 15:06:49.037
    STEP: waiting for ReplicationController to have a DELETED watchEvent 08/11/23 15:06:49.045
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 11 15:06:49.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3862" for this suite. 08/11/23 15:06:49.087
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:06:49.095
Aug 11 15:06:49.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename init-container 08/11/23 15:06:49.096
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:49.112
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:49.115
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457
STEP: creating the pod 08/11/23 15:06:49.118
Aug 11 15:06:49.118: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 11 15:06:53.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-7571" for this suite. 08/11/23 15:06:53.255
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]","completed":245,"skipped":4484,"failed":0}
------------------------------
â€¢ [4.166 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
  test/e2e/common/node/init_container.go:457

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:06:49.095
    Aug 11 15:06:49.095: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename init-container 08/11/23 15:06:49.096
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:49.112
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:49.115
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
      test/e2e/common/node/init_container.go:457
    STEP: creating the pod 08/11/23 15:06:49.118
    Aug 11 15:06:49.118: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 11 15:06:53.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-7571" for this suite. 08/11/23 15:06:53.255
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:06:53.263
Aug 11 15:06:53.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 15:06:53.264
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:53.28
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:53.282
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443
Aug 11 15:06:53.292: INFO: Waiting up to 5m0s for pod "server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb" in namespace "pods-6365" to be "running and ready"
Aug 11 15:06:53.298: INFO: Pod "server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.381688ms
Aug 11 15:06:53.298: INFO: The phase of Pod server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:06:55.303: INFO: Pod "server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010339818s
Aug 11 15:06:55.303: INFO: The phase of Pod server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb is Running (Ready = true)
Aug 11 15:06:55.303: INFO: Pod "server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb" satisfied condition "running and ready"
Aug 11 15:06:55.329: INFO: Waiting up to 5m0s for pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb" in namespace "pods-6365" to be "Succeeded or Failed"
Aug 11 15:06:55.335: INFO: Pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079649ms
Aug 11 15:06:57.339: INFO: Pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01043463s
Aug 11 15:06:59.340: INFO: Pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010957473s
STEP: Saw pod success 08/11/23 15:06:59.34
Aug 11 15:06:59.340: INFO: Pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb" satisfied condition "Succeeded or Failed"
Aug 11 15:06:59.344: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod client-envvars-118d55ac-157d-4050-a926-3f613abc98cb container env3cont: <nil>
STEP: delete the pod 08/11/23 15:06:59.353
Aug 11 15:06:59.370: INFO: Waiting for pod client-envvars-118d55ac-157d-4050-a926-3f613abc98cb to disappear
Aug 11 15:06:59.373: INFO: Pod client-envvars-118d55ac-157d-4050-a926-3f613abc98cb no longer exists
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 15:06:59.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-6365" for this suite. 08/11/23 15:06:59.377
{"msg":"PASSED [sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]","completed":246,"skipped":4522,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.120 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should contain environment variables for services [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:06:53.263
    Aug 11 15:06:53.263: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 15:06:53.264
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:53.28
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:53.282
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should contain environment variables for services [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:443
    Aug 11 15:06:53.292: INFO: Waiting up to 5m0s for pod "server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb" in namespace "pods-6365" to be "running and ready"
    Aug 11 15:06:53.298: INFO: Pod "server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.381688ms
    Aug 11 15:06:53.298: INFO: The phase of Pod server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:06:55.303: INFO: Pod "server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb": Phase="Running", Reason="", readiness=true. Elapsed: 2.010339818s
    Aug 11 15:06:55.303: INFO: The phase of Pod server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb is Running (Ready = true)
    Aug 11 15:06:55.303: INFO: Pod "server-envvars-77b71184-de95-4384-b2d0-5a74b98142eb" satisfied condition "running and ready"
    Aug 11 15:06:55.329: INFO: Waiting up to 5m0s for pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb" in namespace "pods-6365" to be "Succeeded or Failed"
    Aug 11 15:06:55.335: INFO: Pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb": Phase="Pending", Reason="", readiness=false. Elapsed: 6.079649ms
    Aug 11 15:06:57.339: INFO: Pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.01043463s
    Aug 11 15:06:59.340: INFO: Pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010957473s
    STEP: Saw pod success 08/11/23 15:06:59.34
    Aug 11 15:06:59.340: INFO: Pod "client-envvars-118d55ac-157d-4050-a926-3f613abc98cb" satisfied condition "Succeeded or Failed"
    Aug 11 15:06:59.344: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod client-envvars-118d55ac-157d-4050-a926-3f613abc98cb container env3cont: <nil>
    STEP: delete the pod 08/11/23 15:06:59.353
    Aug 11 15:06:59.370: INFO: Waiting for pod client-envvars-118d55ac-157d-4050-a926-3f613abc98cb to disappear
    Aug 11 15:06:59.373: INFO: Pod client-envvars-118d55ac-157d-4050-a926-3f613abc98cb no longer exists
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 15:06:59.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-6365" for this suite. 08/11/23 15:06:59.377
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:06:59.384
Aug 11 15:06:59.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename dns 08/11/23 15:06:59.385
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:59.402
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:59.405
[It] should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50
STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/11/23 15:06:59.408
STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
 08/11/23 15:06:59.408
STEP: creating a pod to probe DNS 08/11/23 15:06:59.408
STEP: submitting the pod to kubernetes 08/11/23 15:06:59.408
Aug 11 15:06:59.417: INFO: Waiting up to 15m0s for pod "dns-test-07673f7c-6af6-4a01-bdef-cd2138401028" in namespace "dns-9397" to be "running"
Aug 11 15:06:59.423: INFO: Pod "dns-test-07673f7c-6af6-4a01-bdef-cd2138401028": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7999ms
Aug 11 15:07:01.428: INFO: Pod "dns-test-07673f7c-6af6-4a01-bdef-cd2138401028": Phase="Running", Reason="", readiness=true. Elapsed: 2.010509513s
Aug 11 15:07:01.428: INFO: Pod "dns-test-07673f7c-6af6-4a01-bdef-cd2138401028" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:07:01.428
STEP: looking for the results for each expected name from probers 08/11/23 15:07:01.431
Aug 11 15:07:01.465: INFO: DNS probes using dns-9397/dns-test-07673f7c-6af6-4a01-bdef-cd2138401028 succeeded

STEP: deleting the pod 08/11/23 15:07:01.465
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 11 15:07:01.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-9397" for this suite. 08/11/23 15:07:01.484
{"msg":"PASSED [sig-network] DNS should provide DNS for the cluster  [Conformance]","completed":247,"skipped":4528,"failed":0}
------------------------------
â€¢ [2.108 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for the cluster  [Conformance]
  test/e2e/network/dns.go:50

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:06:59.384
    Aug 11 15:06:59.384: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename dns 08/11/23 15:06:59.385
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:06:59.402
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:06:59.405
    [It] should provide DNS for the cluster  [Conformance]
      test/e2e/network/dns.go:50
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/11/23 15:06:59.408
    STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
     08/11/23 15:06:59.408
    STEP: creating a pod to probe DNS 08/11/23 15:06:59.408
    STEP: submitting the pod to kubernetes 08/11/23 15:06:59.408
    Aug 11 15:06:59.417: INFO: Waiting up to 15m0s for pod "dns-test-07673f7c-6af6-4a01-bdef-cd2138401028" in namespace "dns-9397" to be "running"
    Aug 11 15:06:59.423: INFO: Pod "dns-test-07673f7c-6af6-4a01-bdef-cd2138401028": Phase="Pending", Reason="", readiness=false. Elapsed: 5.7999ms
    Aug 11 15:07:01.428: INFO: Pod "dns-test-07673f7c-6af6-4a01-bdef-cd2138401028": Phase="Running", Reason="", readiness=true. Elapsed: 2.010509513s
    Aug 11 15:07:01.428: INFO: Pod "dns-test-07673f7c-6af6-4a01-bdef-cd2138401028" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:07:01.428
    STEP: looking for the results for each expected name from probers 08/11/23 15:07:01.431
    Aug 11 15:07:01.465: INFO: DNS probes using dns-9397/dns-test-07673f7c-6af6-4a01-bdef-cd2138401028 succeeded

    STEP: deleting the pod 08/11/23 15:07:01.465
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 11 15:07:01.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-9397" for this suite. 08/11/23 15:07:01.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:07:01.492
Aug 11 15:07:01.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:07:01.493
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:01.513
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:01.517
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216
STEP: creating service in namespace services-2886 08/11/23 15:07:01.52
STEP: creating service affinity-nodeport-transition in namespace services-2886 08/11/23 15:07:01.521
STEP: creating replication controller affinity-nodeport-transition in namespace services-2886 08/11/23 15:07:01.544
I0811 15:07:01.552593      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2886, replica count: 3
I0811 15:07:04.604077      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 15:07:04.615: INFO: Creating new exec pod
Aug 11 15:07:04.623: INFO: Waiting up to 5m0s for pod "execpod-affinity6pb6m" in namespace "services-2886" to be "running"
Aug 11 15:07:04.629: INFO: Pod "execpod-affinity6pb6m": Phase="Pending", Reason="", readiness=false. Elapsed: 5.724668ms
Aug 11 15:07:06.635: INFO: Pod "execpod-affinity6pb6m": Phase="Running", Reason="", readiness=true. Elapsed: 2.011949438s
Aug 11 15:07:06.635: INFO: Pod "execpod-affinity6pb6m" satisfied condition "running"
Aug 11 15:07:07.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
Aug 11 15:07:07.787: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
Aug 11 15:07:07.787: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:07:07.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.165.226 80'
Aug 11 15:07:07.924: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.165.226 80\nConnection to 10.109.165.226 80 port [tcp/http] succeeded!\n"
Aug 11 15:07:07.924: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:07:07.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.2 31463'
Aug 11 15:07:08.064: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.2 31463\nConnection to 192.168.178.2 31463 port [tcp/*] succeeded!\n"
Aug 11 15:07:08.064: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:07:08.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.3 31463'
Aug 11 15:07:08.207: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.3 31463\nConnection to 192.168.178.3 31463 port [tcp/*] succeeded!\n"
Aug 11 15:07:08.207: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:07:08.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31463/ ; done'
Aug 11 15:07:08.416: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n"
Aug 11 15:07:08.416: INFO: stdout: "\naffinity-nodeport-transition-4ptx4\naffinity-nodeport-transition-x8bv5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-4ptx4\naffinity-nodeport-transition-x8bv5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-4ptx4\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-x8bv5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-x8bv5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-4ptx4\naffinity-nodeport-transition-4ptx4"
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-x8bv5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-x8bv5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-x8bv5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-x8bv5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
Aug 11 15:07:08.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31463/ ; done'
Aug 11 15:07:08.619: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n"
Aug 11 15:07:08.619: INFO: stdout: "\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5"
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
Aug 11 15:07:08.619: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2886, will wait for the garbage collector to delete the pods 08/11/23 15:07:08.632
Aug 11 15:07:08.694: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.335799ms
Aug 11 15:07:08.795: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.902431ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:07:10.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-2886" for this suite. 08/11/23 15:07:10.429
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]","completed":248,"skipped":4539,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.944 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2216

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:07:01.492
    Aug 11 15:07:01.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:07:01.493
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:01.513
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:01.517
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2216
    STEP: creating service in namespace services-2886 08/11/23 15:07:01.52
    STEP: creating service affinity-nodeport-transition in namespace services-2886 08/11/23 15:07:01.521
    STEP: creating replication controller affinity-nodeport-transition in namespace services-2886 08/11/23 15:07:01.544
    I0811 15:07:01.552593      22 runners.go:193] Created replication controller with name: affinity-nodeport-transition, namespace: services-2886, replica count: 3
    I0811 15:07:04.604077      22 runners.go:193] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 15:07:04.615: INFO: Creating new exec pod
    Aug 11 15:07:04.623: INFO: Waiting up to 5m0s for pod "execpod-affinity6pb6m" in namespace "services-2886" to be "running"
    Aug 11 15:07:04.629: INFO: Pod "execpod-affinity6pb6m": Phase="Pending", Reason="", readiness=false. Elapsed: 5.724668ms
    Aug 11 15:07:06.635: INFO: Pod "execpod-affinity6pb6m": Phase="Running", Reason="", readiness=true. Elapsed: 2.011949438s
    Aug 11 15:07:06.635: INFO: Pod "execpod-affinity6pb6m" satisfied condition "running"
    Aug 11 15:07:07.642: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
    Aug 11 15:07:07.787: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
    Aug 11 15:07:07.787: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:07:07.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.165.226 80'
    Aug 11 15:07:07.924: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.165.226 80\nConnection to 10.109.165.226 80 port [tcp/http] succeeded!\n"
    Aug 11 15:07:07.924: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:07:07.924: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.2 31463'
    Aug 11 15:07:08.064: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.2 31463\nConnection to 192.168.178.2 31463 port [tcp/*] succeeded!\n"
    Aug 11 15:07:08.064: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:07:08.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.3 31463'
    Aug 11 15:07:08.207: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.3 31463\nConnection to 192.168.178.3 31463 port [tcp/*] succeeded!\n"
    Aug 11 15:07:08.207: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:07:08.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31463/ ; done'
    Aug 11 15:07:08.416: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n"
    Aug 11 15:07:08.416: INFO: stdout: "\naffinity-nodeport-transition-4ptx4\naffinity-nodeport-transition-x8bv5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-4ptx4\naffinity-nodeport-transition-x8bv5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-4ptx4\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-x8bv5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-x8bv5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-4ptx4\naffinity-nodeport-transition-4ptx4"
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-x8bv5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-x8bv5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-x8bv5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-x8bv5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
    Aug 11 15:07:08.416: INFO: Received response from host: affinity-nodeport-transition-4ptx4
    Aug 11 15:07:08.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-2886 exec execpod-affinity6pb6m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.178.2:31463/ ; done'
    Aug 11 15:07:08.619: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.178.2:31463/\n"
    Aug 11 15:07:08.619: INFO: stdout: "\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5\naffinity-nodeport-transition-j4mr5"
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Received response from host: affinity-nodeport-transition-j4mr5
    Aug 11 15:07:08.619: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-2886, will wait for the garbage collector to delete the pods 08/11/23 15:07:08.632
    Aug 11 15:07:08.694: INFO: Deleting ReplicationController affinity-nodeport-transition took: 8.335799ms
    Aug 11 15:07:08.795: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.902431ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:07:10.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-2886" for this suite. 08/11/23 15:07:10.429
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases
  should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:07:10.439
Aug 11 15:07:10.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:07:10.44
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:10.455
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:10.458
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should write entries to /etc/hosts [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:148
STEP: Waiting for pod completion 08/11/23 15:07:10.468
Aug 11 15:07:10.468: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb" in namespace "kubelet-test-7595" to be "completed"
Aug 11 15:07:10.474: INFO: Pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.454421ms
Aug 11 15:07:12.478: INFO: Pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010313448s
Aug 11 15:07:14.479: INFO: Pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011162391s
Aug 11 15:07:14.479: INFO: Pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb" satisfied condition "completed"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 11 15:07:14.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-7595" for this suite. 08/11/23 15:07:14.493
{"msg":"PASSED [sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]","completed":249,"skipped":4575,"failed":0}
------------------------------
â€¢ [4.061 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling an agnhost Pod with hostAliases
  test/e2e/common/node/kubelet.go:140
    should write entries to /etc/hosts [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:148

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:07:10.439
    Aug 11 15:07:10.439: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:07:10.44
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:10.455
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:10.458
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should write entries to /etc/hosts [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:148
    STEP: Waiting for pod completion 08/11/23 15:07:10.468
    Aug 11 15:07:10.468: INFO: Waiting up to 3m0s for pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb" in namespace "kubelet-test-7595" to be "completed"
    Aug 11 15:07:10.474: INFO: Pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb": Phase="Pending", Reason="", readiness=false. Elapsed: 5.454421ms
    Aug 11 15:07:12.478: INFO: Pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010313448s
    Aug 11 15:07:14.479: INFO: Pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011162391s
    Aug 11 15:07:14.479: INFO: Pod "agnhost-host-aliasesaa953138-dcb7-4cc5-9d45-ef04c72bdcbb" satisfied condition "completed"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 11 15:07:14.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-7595" for this suite. 08/11/23 15:07:14.493
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:07:14.501
Aug 11 15:07:14.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename gc 08/11/23 15:07:14.502
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:14.519
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:14.522
[It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550
STEP: create the deployment 08/11/23 15:07:14.524
STEP: Wait for the Deployment to create new ReplicaSet 08/11/23 15:07:14.53
STEP: delete the deployment 08/11/23 15:07:14.647
STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/11/23 15:07:14.656
STEP: Gathering metrics 08/11/23 15:07:15.178
Aug 11 15:07:15.206: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
Aug 11 15:07:15.210: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.662114ms
Aug 11 15:07:15.210: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
Aug 11 15:07:15.210: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
Aug 11 15:07:15.264: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 11 15:07:15.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-4000" for this suite. 08/11/23 15:07:15.269
{"msg":"PASSED [sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]","completed":250,"skipped":4583,"failed":0}
------------------------------
â€¢ [0.777 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
  test/e2e/apimachinery/garbage_collector.go:550

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:07:14.501
    Aug 11 15:07:14.501: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename gc 08/11/23 15:07:14.502
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:14.519
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:14.522
    [It] should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
      test/e2e/apimachinery/garbage_collector.go:550
    STEP: create the deployment 08/11/23 15:07:14.524
    STEP: Wait for the Deployment to create new ReplicaSet 08/11/23 15:07:14.53
    STEP: delete the deployment 08/11/23 15:07:14.647
    STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs 08/11/23 15:07:14.656
    STEP: Gathering metrics 08/11/23 15:07:15.178
    Aug 11 15:07:15.206: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
    Aug 11 15:07:15.210: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 3.662114ms
    Aug 11 15:07:15.210: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
    Aug 11 15:07:15.210: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
    Aug 11 15:07:15.264: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 11 15:07:15.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-4000" for this suite. 08/11/23 15:07:15.269
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:07:15.279
Aug 11 15:07:15.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 15:07:15.281
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:15.294
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:15.297
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 15:07:15.313
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:07:15.522
STEP: Deploying the webhook pod 08/11/23 15:07:15.537
STEP: Wait for the deployment to be ready 08/11/23 15:07:15.559
Aug 11 15:07:15.582: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:07:17.594
STEP: Verifying the service has paired with the endpoint 08/11/23 15:07:17.609
Aug 11 15:07:18.610: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581
STEP: Listing all of the created validation webhooks 08/11/23 15:07:18.687
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:07:18.726
STEP: Deleting the collection of validation webhooks 08/11/23 15:07:18.761
STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:07:18.811
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:07:18.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-5588" for this suite. 08/11/23 15:07:18.827
STEP: Destroying namespace "webhook-5588-markers" for this suite. 08/11/23 15:07:18.836
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]","completed":251,"skipped":4604,"failed":0}
------------------------------
â€¢ [3.607 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  listing validating webhooks should work [Conformance]
  test/e2e/apimachinery/webhook.go:581

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:07:15.279
    Aug 11 15:07:15.279: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 15:07:15.281
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:15.294
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:15.297
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 15:07:15.313
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:07:15.522
    STEP: Deploying the webhook pod 08/11/23 15:07:15.537
    STEP: Wait for the deployment to be ready 08/11/23 15:07:15.559
    Aug 11 15:07:15.582: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:07:17.594
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:07:17.609
    Aug 11 15:07:18.610: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] listing validating webhooks should work [Conformance]
      test/e2e/apimachinery/webhook.go:581
    STEP: Listing all of the created validation webhooks 08/11/23 15:07:18.687
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:07:18.726
    STEP: Deleting the collection of validation webhooks 08/11/23 15:07:18.761
    STEP: Creating a configMap that does not comply to the validation webhook rules 08/11/23 15:07:18.811
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:07:18.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-5588" for this suite. 08/11/23 15:07:18.827
    STEP: Destroying namespace "webhook-5588-markers" for this suite. 08/11/23 15:07:18.836
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:07:18.887
Aug 11 15:07:18.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 15:07:18.888
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:18.907
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:18.91
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:07:18.913
Aug 11 15:07:18.923: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc" in namespace "downward-api-8302" to be "Succeeded or Failed"
Aug 11 15:07:18.928: INFO: Pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.750178ms
Aug 11 15:07:20.933: INFO: Pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009569545s
Aug 11 15:07:22.933: INFO: Pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009587514s
STEP: Saw pod success 08/11/23 15:07:22.933
Aug 11 15:07:22.933: INFO: Pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc" satisfied condition "Succeeded or Failed"
Aug 11 15:07:22.936: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc container client-container: <nil>
STEP: delete the pod 08/11/23 15:07:22.946
Aug 11 15:07:22.960: INFO: Waiting for pod downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc to disappear
Aug 11 15:07:22.963: INFO: Pod downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 15:07:22.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8302" for this suite. 08/11/23 15:07:22.968
{"msg":"PASSED [sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]","completed":252,"skipped":4605,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:07:18.887
    Aug 11 15:07:18.887: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:07:18.888
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:18.907
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:18.91
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:220
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:07:18.913
    Aug 11 15:07:18.923: INFO: Waiting up to 5m0s for pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc" in namespace "downward-api-8302" to be "Succeeded or Failed"
    Aug 11 15:07:18.928: INFO: Pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc": Phase="Pending", Reason="", readiness=false. Elapsed: 4.750178ms
    Aug 11 15:07:20.933: INFO: Pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009569545s
    Aug 11 15:07:22.933: INFO: Pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009587514s
    STEP: Saw pod success 08/11/23 15:07:22.933
    Aug 11 15:07:22.933: INFO: Pod "downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc" satisfied condition "Succeeded or Failed"
    Aug 11 15:07:22.936: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc container client-container: <nil>
    STEP: delete the pod 08/11/23 15:07:22.946
    Aug 11 15:07:22.960: INFO: Waiting for pod downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc to disappear
    Aug 11 15:07:22.963: INFO: Pod downwardapi-volume-d251ee78-3cfc-47c8-af7b-076e131891bc no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 15:07:22.963: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8302" for this suite. 08/11/23 15:07:22.968
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:07:22.976
Aug 11 15:07:22.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-probe 08/11/23 15:07:22.977
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:22.992
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:22.995
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131
STEP: Creating pod busybox-e0658207-730d-4296-82fc-8cfb751559bf in namespace container-probe-2699 08/11/23 15:07:22.998
Aug 11 15:07:23.007: INFO: Waiting up to 5m0s for pod "busybox-e0658207-730d-4296-82fc-8cfb751559bf" in namespace "container-probe-2699" to be "not pending"
Aug 11 15:07:23.012: INFO: Pod "busybox-e0658207-730d-4296-82fc-8cfb751559bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.691628ms
Aug 11 15:07:25.017: INFO: Pod "busybox-e0658207-730d-4296-82fc-8cfb751559bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010474733s
Aug 11 15:07:25.017: INFO: Pod "busybox-e0658207-730d-4296-82fc-8cfb751559bf" satisfied condition "not pending"
Aug 11 15:07:25.017: INFO: Started pod busybox-e0658207-730d-4296-82fc-8cfb751559bf in namespace container-probe-2699
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:07:25.017
Aug 11 15:07:25.021: INFO: Initial restart count of pod busybox-e0658207-730d-4296-82fc-8cfb751559bf is 0
Aug 11 15:08:15.151: INFO: Restart count of pod container-probe-2699/busybox-e0658207-730d-4296-82fc-8cfb751559bf is now 1 (50.129611811s elapsed)
STEP: deleting the pod 08/11/23 15:08:15.151
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 11 15:08:15.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-2699" for this suite. 08/11/23 15:08:15.17
{"msg":"PASSED [sig-node] Probing container should be restarted with a exec \"cat /tmp/health\" liveness probe [NodeConformance] [Conformance]","completed":253,"skipped":4637,"failed":0}
------------------------------
â€¢ [SLOW TEST] [52.201 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:07:22.976
    Aug 11 15:07:22.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-probe 08/11/23 15:07:22.977
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:07:22.992
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:07:22.995
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:131
    STEP: Creating pod busybox-e0658207-730d-4296-82fc-8cfb751559bf in namespace container-probe-2699 08/11/23 15:07:22.998
    Aug 11 15:07:23.007: INFO: Waiting up to 5m0s for pod "busybox-e0658207-730d-4296-82fc-8cfb751559bf" in namespace "container-probe-2699" to be "not pending"
    Aug 11 15:07:23.012: INFO: Pod "busybox-e0658207-730d-4296-82fc-8cfb751559bf": Phase="Pending", Reason="", readiness=false. Elapsed: 5.691628ms
    Aug 11 15:07:25.017: INFO: Pod "busybox-e0658207-730d-4296-82fc-8cfb751559bf": Phase="Running", Reason="", readiness=true. Elapsed: 2.010474733s
    Aug 11 15:07:25.017: INFO: Pod "busybox-e0658207-730d-4296-82fc-8cfb751559bf" satisfied condition "not pending"
    Aug 11 15:07:25.017: INFO: Started pod busybox-e0658207-730d-4296-82fc-8cfb751559bf in namespace container-probe-2699
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:07:25.017
    Aug 11 15:07:25.021: INFO: Initial restart count of pod busybox-e0658207-730d-4296-82fc-8cfb751559bf is 0
    Aug 11 15:08:15.151: INFO: Restart count of pod container-probe-2699/busybox-e0658207-730d-4296-82fc-8cfb751559bf is now 1 (50.129611811s elapsed)
    STEP: deleting the pod 08/11/23 15:08:15.151
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 11 15:08:15.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-2699" for this suite. 08/11/23 15:08:15.17
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged
  should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:08:15.177
Aug 11 15:08:15.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename security-context-test 08/11/23 15:08:15.179
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:15.195
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:15.199
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:527
Aug 11 15:08:15.210: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225" in namespace "security-context-test-3264" to be "Succeeded or Failed"
Aug 11 15:08:15.215: INFO: Pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225": Phase="Pending", Reason="", readiness=false. Elapsed: 4.725427ms
Aug 11 15:08:17.220: INFO: Pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010099621s
Aug 11 15:08:19.221: INFO: Pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010391837s
Aug 11 15:08:19.221: INFO: Pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225" satisfied condition "Succeeded or Failed"
Aug 11 15:08:19.231: INFO: Got logs for pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225": "ip: RTNETLINK answers: Operation not permitted\n"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 11 15:08:19.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-3264" for this suite. 08/11/23 15:08:19.236
{"msg":"PASSED [sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]","completed":254,"skipped":4642,"failed":0}
------------------------------
â€¢ [4.066 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  When creating a pod with privileged
  test/e2e/common/node/security_context.go:490
    should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:527

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:08:15.177
    Aug 11 15:08:15.178: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename security-context-test 08/11/23 15:08:15.179
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:15.195
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:15.199
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:527
    Aug 11 15:08:15.210: INFO: Waiting up to 5m0s for pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225" in namespace "security-context-test-3264" to be "Succeeded or Failed"
    Aug 11 15:08:15.215: INFO: Pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225": Phase="Pending", Reason="", readiness=false. Elapsed: 4.725427ms
    Aug 11 15:08:17.220: INFO: Pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010099621s
    Aug 11 15:08:19.221: INFO: Pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010391837s
    Aug 11 15:08:19.221: INFO: Pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225" satisfied condition "Succeeded or Failed"
    Aug 11 15:08:19.231: INFO: Got logs for pod "busybox-privileged-false-6101ab4f-e154-42de-9fef-1d5397ebc225": "ip: RTNETLINK answers: Operation not permitted\n"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 11 15:08:19.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-3264" for this suite. 08/11/23 15:08:19.236
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:08:19.245
Aug 11 15:08:19.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 15:08:19.246
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:19.26
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:19.263
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 15:08:19.278
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:08:19.614
STEP: Deploying the webhook pod 08/11/23 15:08:19.623
STEP: Wait for the deployment to be ready 08/11/23 15:08:19.636
Aug 11 15:08:19.649: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:08:21.661
STEP: Verifying the service has paired with the endpoint 08/11/23 15:08:21.678
Aug 11 15:08:22.678: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208
STEP: Registering the webhook via the AdmissionRegistration API 08/11/23 15:08:22.683
STEP: create a pod 08/11/23 15:08:22.71
Aug 11 15:08:22.719: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3184" to be "running"
Aug 11 15:08:22.723: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.87598ms
Aug 11 15:08:24.729: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009104919s
Aug 11 15:08:24.729: INFO: Pod "to-be-attached-pod" satisfied condition "running"
STEP: 'kubectl attach' the pod, should be denied by the webhook 08/11/23 15:08:24.729
Aug 11 15:08:24.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=webhook-3184 attach --namespace=webhook-3184 to-be-attached-pod -i -c=container1'
Aug 11 15:08:24.816: INFO: rc: 1
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:08:24.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-3184" for this suite. 08/11/23 15:08:24.827
STEP: Destroying namespace "webhook-3184-markers" for this suite. 08/11/23 15:08:24.833
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]","completed":255,"skipped":4659,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.641 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny attaching pod [Conformance]
  test/e2e/apimachinery/webhook.go:208

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:08:19.245
    Aug 11 15:08:19.245: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 15:08:19.246
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:19.26
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:19.263
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 15:08:19.278
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:08:19.614
    STEP: Deploying the webhook pod 08/11/23 15:08:19.623
    STEP: Wait for the deployment to be ready 08/11/23 15:08:19.636
    Aug 11 15:08:19.649: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:08:21.661
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:08:21.678
    Aug 11 15:08:22.678: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny attaching pod [Conformance]
      test/e2e/apimachinery/webhook.go:208
    STEP: Registering the webhook via the AdmissionRegistration API 08/11/23 15:08:22.683
    STEP: create a pod 08/11/23 15:08:22.71
    Aug 11 15:08:22.719: INFO: Waiting up to 5m0s for pod "to-be-attached-pod" in namespace "webhook-3184" to be "running"
    Aug 11 15:08:22.723: INFO: Pod "to-be-attached-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.87598ms
    Aug 11 15:08:24.729: INFO: Pod "to-be-attached-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009104919s
    Aug 11 15:08:24.729: INFO: Pod "to-be-attached-pod" satisfied condition "running"
    STEP: 'kubectl attach' the pod, should be denied by the webhook 08/11/23 15:08:24.729
    Aug 11 15:08:24.729: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=webhook-3184 attach --namespace=webhook-3184 to-be-attached-pod -i -c=container1'
    Aug 11 15:08:24.816: INFO: rc: 1
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:08:24.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-3184" for this suite. 08/11/23 15:08:24.827
    STEP: Destroying namespace "webhook-3184-markers" for this suite. 08/11/23 15:08:24.833
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod
  should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:08:24.886
Aug 11 15:08:24.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 15:08:24.887
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:24.912
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:24.915
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1698
[It] should create a pod from an image when restart is Never  [Conformance]
  test/e2e/kubectl/kubectl.go:1711
STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/11/23 15:08:24.922
Aug 11 15:08:24.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-1986 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
Aug 11 15:08:24.992: INFO: stderr: ""
Aug 11 15:08:24.992: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
STEP: verifying the pod e2e-test-httpd-pod was created 08/11/23 15:08:24.992
[AfterEach] Kubectl run pod
  test/e2e/kubectl/kubectl.go:1702
Aug 11 15:08:24.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-1986 delete pods e2e-test-httpd-pod'
Aug 11 15:08:27.528: INFO: stderr: ""
Aug 11 15:08:27.528: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 15:08:27.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-1986" for this suite. 08/11/23 15:08:27.532
{"msg":"PASSED [sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]","completed":256,"skipped":4676,"failed":0}
------------------------------
â€¢ [2.653 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl run pod
  test/e2e/kubectl/kubectl.go:1695
    should create a pod from an image when restart is Never  [Conformance]
    test/e2e/kubectl/kubectl.go:1711

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:08:24.886
    Aug 11 15:08:24.886: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:08:24.887
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:24.912
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:24.915
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1698
    [It] should create a pod from an image when restart is Never  [Conformance]
      test/e2e/kubectl/kubectl.go:1711
    STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-2 08/11/23 15:08:24.922
    Aug 11 15:08:24.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-1986 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-2'
    Aug 11 15:08:24.992: INFO: stderr: ""
    Aug 11 15:08:24.992: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
    STEP: verifying the pod e2e-test-httpd-pod was created 08/11/23 15:08:24.992
    [AfterEach] Kubectl run pod
      test/e2e/kubectl/kubectl.go:1702
    Aug 11 15:08:24.996: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-1986 delete pods e2e-test-httpd-pod'
    Aug 11 15:08:27.528: INFO: stderr: ""
    Aug 11 15:08:27.528: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 15:08:27.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-1986" for this suite. 08/11/23 15:08:27.532
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
[BeforeEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:08:27.541
Aug 11 15:08:27.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename events 08/11/23 15:08:27.542
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:27.558
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:27.561
[It] should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57
STEP: creating a test event 08/11/23 15:08:27.564
STEP: listing all events in all namespaces 08/11/23 15:08:27.571
STEP: patching the test event 08/11/23 15:08:27.575
STEP: fetching the test event 08/11/23 15:08:27.584
STEP: updating the test event 08/11/23 15:08:27.587
STEP: getting the test event 08/11/23 15:08:27.597
STEP: deleting the test event 08/11/23 15:08:27.6
STEP: listing all events in all namespaces 08/11/23 15:08:27.606
[AfterEach] [sig-instrumentation] Events
  test/e2e/framework/framework.go:187
Aug 11 15:08:27.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-3140" for this suite. 08/11/23 15:08:27.613
{"msg":"PASSED [sig-instrumentation] Events should manage the lifecycle of an event [Conformance]","completed":257,"skipped":4710,"failed":0}
------------------------------
â€¢ [0.078 seconds]
[sig-instrumentation] Events
test/e2e/instrumentation/common/framework.go:23
  should manage the lifecycle of an event [Conformance]
  test/e2e/instrumentation/core_events.go:57

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:08:27.541
    Aug 11 15:08:27.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename events 08/11/23 15:08:27.542
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:27.558
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:27.561
    [It] should manage the lifecycle of an event [Conformance]
      test/e2e/instrumentation/core_events.go:57
    STEP: creating a test event 08/11/23 15:08:27.564
    STEP: listing all events in all namespaces 08/11/23 15:08:27.571
    STEP: patching the test event 08/11/23 15:08:27.575
    STEP: fetching the test event 08/11/23 15:08:27.584
    STEP: updating the test event 08/11/23 15:08:27.587
    STEP: getting the test event 08/11/23 15:08:27.597
    STEP: deleting the test event 08/11/23 15:08:27.6
    STEP: listing all events in all namespaces 08/11/23 15:08:27.606
    [AfterEach] [sig-instrumentation] Events
      test/e2e/framework/framework.go:187
    Aug 11 15:08:27.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-3140" for this suite. 08/11/23 15:08:27.613
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:08:27.621
Aug 11 15:08:27.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 15:08:27.622
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:27.637
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:27.64
[It] binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174
STEP: Creating configMap with name configmap-test-upd-fb87a9e1-7162-4a5f-8f21-90d153c9497f 08/11/23 15:08:27.648
STEP: Creating the pod 08/11/23 15:08:27.655
Aug 11 15:08:27.663: INFO: Waiting up to 5m0s for pod "pod-configmaps-63190ad0-7bae-4838-9933-52bd2520d3bd" in namespace "configmap-4464" to be "running"
Aug 11 15:08:27.670: INFO: Pod "pod-configmaps-63190ad0-7bae-4838-9933-52bd2520d3bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.785591ms
Aug 11 15:08:29.675: INFO: Pod "pod-configmaps-63190ad0-7bae-4838-9933-52bd2520d3bd": Phase="Running", Reason="", readiness=false. Elapsed: 2.011479845s
Aug 11 15:08:29.675: INFO: Pod "pod-configmaps-63190ad0-7bae-4838-9933-52bd2520d3bd" satisfied condition "running"
STEP: Waiting for pod with text data 08/11/23 15:08:29.675
STEP: Waiting for pod with binary data 08/11/23 15:08:29.686
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 15:08:29.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4464" for this suite. 08/11/23 15:08:29.698
{"msg":"PASSED [sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]","completed":258,"skipped":4787,"failed":0}
------------------------------
â€¢ [2.083 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  binary data should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:174

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:08:27.621
    Aug 11 15:08:27.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 15:08:27.622
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:27.637
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:27.64
    [It] binary data should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:174
    STEP: Creating configMap with name configmap-test-upd-fb87a9e1-7162-4a5f-8f21-90d153c9497f 08/11/23 15:08:27.648
    STEP: Creating the pod 08/11/23 15:08:27.655
    Aug 11 15:08:27.663: INFO: Waiting up to 5m0s for pod "pod-configmaps-63190ad0-7bae-4838-9933-52bd2520d3bd" in namespace "configmap-4464" to be "running"
    Aug 11 15:08:27.670: INFO: Pod "pod-configmaps-63190ad0-7bae-4838-9933-52bd2520d3bd": Phase="Pending", Reason="", readiness=false. Elapsed: 6.785591ms
    Aug 11 15:08:29.675: INFO: Pod "pod-configmaps-63190ad0-7bae-4838-9933-52bd2520d3bd": Phase="Running", Reason="", readiness=false. Elapsed: 2.011479845s
    Aug 11 15:08:29.675: INFO: Pod "pod-configmaps-63190ad0-7bae-4838-9933-52bd2520d3bd" satisfied condition "running"
    STEP: Waiting for pod with text data 08/11/23 15:08:29.675
    STEP: Waiting for pod with binary data 08/11/23 15:08:29.686
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 15:08:29.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4464" for this suite. 08/11/23 15:08:29.698
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-network] Services
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:08:29.704
Aug 11 15:08:29.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:08:29.705
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:29.72
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:29.723
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791
STEP: creating service endpoint-test2 in namespace services-163 08/11/23 15:08:29.725
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[] 08/11/23 15:08:29.739
Aug 11 15:08:29.749: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[]
STEP: Creating pod pod1 in namespace services-163 08/11/23 15:08:29.749
Aug 11 15:08:29.758: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-163" to be "running and ready"
Aug 11 15:08:29.761: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.798497ms
Aug 11 15:08:29.761: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:08:31.766: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007211141s
Aug 11 15:08:31.766: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 11 15:08:31.766: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[pod1:[80]] 08/11/23 15:08:31.769
Aug 11 15:08:31.777: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[pod1:[80]]
STEP: Checking if the Service forwards traffic to pod1 08/11/23 15:08:31.777
Aug 11 15:08:31.777: INFO: Creating new exec pod
Aug 11 15:08:31.783: INFO: Waiting up to 5m0s for pod "execpodswrr2" in namespace "services-163" to be "running"
Aug 11 15:08:31.785: INFO: Pod "execpodswrr2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.577731ms
Aug 11 15:08:33.791: INFO: Pod "execpodswrr2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008186951s
Aug 11 15:08:33.791: INFO: Pod "execpodswrr2" satisfied condition "running"
Aug 11 15:08:34.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 11 15:08:34.927: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 11 15:08:34.927: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:08:34.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.50.93 80'
Aug 11 15:08:35.055: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.50.93 80\nConnection to 10.108.50.93 80 port [tcp/http] succeeded!\n"
Aug 11 15:08:35.055: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Creating pod pod2 in namespace services-163 08/11/23 15:08:35.055
Aug 11 15:08:35.063: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-163" to be "running and ready"
Aug 11 15:08:35.066: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.808318ms
Aug 11 15:08:35.066: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:08:37.070: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007200121s
Aug 11 15:08:37.070: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 11 15:08:37.070: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[pod1:[80] pod2:[80]] 08/11/23 15:08:37.073
Aug 11 15:08:37.084: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[pod1:[80] pod2:[80]]
STEP: Checking if the Service forwards traffic to pod1 and pod2 08/11/23 15:08:37.084
Aug 11 15:08:38.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 11 15:08:38.215: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 11 15:08:38.215: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:08:38.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.50.93 80'
Aug 11 15:08:38.353: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.50.93 80\nConnection to 10.108.50.93 80 port [tcp/http] succeeded!\n"
Aug 11 15:08:38.353: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod1 in namespace services-163 08/11/23 15:08:38.353
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[pod2:[80]] 08/11/23 15:08:38.366
Aug 11 15:08:39.389: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[pod2:[80]]
STEP: Checking if the Service forwards traffic to pod2 08/11/23 15:08:39.389
Aug 11 15:08:40.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
Aug 11 15:08:40.525: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
Aug 11 15:08:40.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:08:40.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.50.93 80'
Aug 11 15:08:40.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.50.93 80\nConnection to 10.108.50.93 80 port [tcp/http] succeeded!\n"
Aug 11 15:08:40.645: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
STEP: Deleting pod pod2 in namespace services-163 08/11/23 15:08:40.645
STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[] 08/11/23 15:08:40.668
Aug 11 15:08:40.678: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[]
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:08:40.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-163" for this suite. 08/11/23 15:08:40.711
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should serve a basic endpoint from pods  [Conformance]","completed":259,"skipped":4792,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.015 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should serve a basic endpoint from pods  [Conformance]
  test/e2e/network/service.go:791

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:08:29.704
    Aug 11 15:08:29.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:08:29.705
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:29.72
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:29.723
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should serve a basic endpoint from pods  [Conformance]
      test/e2e/network/service.go:791
    STEP: creating service endpoint-test2 in namespace services-163 08/11/23 15:08:29.725
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[] 08/11/23 15:08:29.739
    Aug 11 15:08:29.749: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[]
    STEP: Creating pod pod1 in namespace services-163 08/11/23 15:08:29.749
    Aug 11 15:08:29.758: INFO: Waiting up to 5m0s for pod "pod1" in namespace "services-163" to be "running and ready"
    Aug 11 15:08:29.761: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 2.798497ms
    Aug 11 15:08:29.761: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:08:31.766: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.007211141s
    Aug 11 15:08:31.766: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 11 15:08:31.766: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[pod1:[80]] 08/11/23 15:08:31.769
    Aug 11 15:08:31.777: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[pod1:[80]]
    STEP: Checking if the Service forwards traffic to pod1 08/11/23 15:08:31.777
    Aug 11 15:08:31.777: INFO: Creating new exec pod
    Aug 11 15:08:31.783: INFO: Waiting up to 5m0s for pod "execpodswrr2" in namespace "services-163" to be "running"
    Aug 11 15:08:31.785: INFO: Pod "execpodswrr2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.577731ms
    Aug 11 15:08:33.791: INFO: Pod "execpodswrr2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008186951s
    Aug 11 15:08:33.791: INFO: Pod "execpodswrr2" satisfied condition "running"
    Aug 11 15:08:34.792: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Aug 11 15:08:34.927: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 11 15:08:34.927: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:08:34.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.50.93 80'
    Aug 11 15:08:35.055: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.50.93 80\nConnection to 10.108.50.93 80 port [tcp/http] succeeded!\n"
    Aug 11 15:08:35.055: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Creating pod pod2 in namespace services-163 08/11/23 15:08:35.055
    Aug 11 15:08:35.063: INFO: Waiting up to 5m0s for pod "pod2" in namespace "services-163" to be "running and ready"
    Aug 11 15:08:35.066: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.808318ms
    Aug 11 15:08:35.066: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:08:37.070: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007200121s
    Aug 11 15:08:37.070: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 11 15:08:37.070: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[pod1:[80] pod2:[80]] 08/11/23 15:08:37.073
    Aug 11 15:08:37.084: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[pod1:[80] pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod1 and pod2 08/11/23 15:08:37.084
    Aug 11 15:08:38.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Aug 11 15:08:38.215: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 11 15:08:38.215: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:08:38.215: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.50.93 80'
    Aug 11 15:08:38.353: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.50.93 80\nConnection to 10.108.50.93 80 port [tcp/http] succeeded!\n"
    Aug 11 15:08:38.353: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod1 in namespace services-163 08/11/23 15:08:38.353
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[pod2:[80]] 08/11/23 15:08:38.366
    Aug 11 15:08:39.389: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[pod2:[80]]
    STEP: Checking if the Service forwards traffic to pod2 08/11/23 15:08:39.389
    Aug 11 15:08:40.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
    Aug 11 15:08:40.525: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
    Aug 11 15:08:40.525: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:08:40.525: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-163 exec execpodswrr2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.50.93 80'
    Aug 11 15:08:40.645: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.50.93 80\nConnection to 10.108.50.93 80 port [tcp/http] succeeded!\n"
    Aug 11 15:08:40.645: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    STEP: Deleting pod pod2 in namespace services-163 08/11/23 15:08:40.645
    STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-163 to expose endpoints map[] 08/11/23 15:08:40.668
    Aug 11 15:08:40.678: INFO: successfully validated that service endpoint-test2 in namespace services-163 exposes endpoints map[]
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:08:40.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-163" for this suite. 08/11/23 15:08:40.711
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Job
  should delete a job [Conformance]
  test/e2e/apps/job.go:309
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:08:40.72
Aug 11 15:08:40.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename job 08/11/23 15:08:40.721
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:40.74
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:40.744
[It] should delete a job [Conformance]
  test/e2e/apps/job.go:309
STEP: Creating a job 08/11/23 15:08:40.746
STEP: Ensuring active pods == parallelism 08/11/23 15:08:40.756
STEP: delete a job 08/11/23 15:08:42.761
STEP: deleting Job.batch foo in namespace job-5408, will wait for the garbage collector to delete the pods 08/11/23 15:08:42.761
Aug 11 15:08:42.821: INFO: Deleting Job.batch foo took: 6.666618ms
Aug 11 15:08:42.922: INFO: Terminating Job.batch foo pods took: 100.561399ms
STEP: Ensuring job was deleted 08/11/23 15:09:14.722
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 11 15:09:14.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-5408" for this suite. 08/11/23 15:09:14.731
{"msg":"PASSED [sig-apps] Job should delete a job [Conformance]","completed":260,"skipped":4804,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.017 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should delete a job [Conformance]
  test/e2e/apps/job.go:309

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:08:40.72
    Aug 11 15:08:40.720: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename job 08/11/23 15:08:40.721
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:08:40.74
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:08:40.744
    [It] should delete a job [Conformance]
      test/e2e/apps/job.go:309
    STEP: Creating a job 08/11/23 15:08:40.746
    STEP: Ensuring active pods == parallelism 08/11/23 15:08:40.756
    STEP: delete a job 08/11/23 15:08:42.761
    STEP: deleting Job.batch foo in namespace job-5408, will wait for the garbage collector to delete the pods 08/11/23 15:08:42.761
    Aug 11 15:08:42.821: INFO: Deleting Job.batch foo took: 6.666618ms
    Aug 11 15:08:42.922: INFO: Terminating Job.batch foo pods took: 100.561399ms
    STEP: Ensuring job was deleted 08/11/23 15:09:14.722
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 11 15:09:14.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-5408" for this suite. 08/11/23 15:09:14.731
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:09:14.739
Aug 11 15:09:14.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 15:09:14.74
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:09:14.756
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:09:14.759
[It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88
STEP: Creating configMap with name projected-configmap-test-volume-map-bc486e97-f374-49fd-832f-59f37f4c6d76 08/11/23 15:09:14.761
STEP: Creating a pod to test consume configMaps 08/11/23 15:09:14.768
Aug 11 15:09:14.777: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716" in namespace "projected-512" to be "Succeeded or Failed"
Aug 11 15:09:14.782: INFO: Pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529871ms
Aug 11 15:09:16.787: INFO: Pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009563624s
Aug 11 15:09:18.788: INFO: Pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011362206s
STEP: Saw pod success 08/11/23 15:09:18.789
Aug 11 15:09:18.789: INFO: Pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716" satisfied condition "Succeeded or Failed"
Aug 11 15:09:18.792: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 15:09:18.8
Aug 11 15:09:18.818: INFO: Waiting for pod pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716 to disappear
Aug 11 15:09:18.821: INFO: Pod pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 11 15:09:18.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-512" for this suite. 08/11/23 15:09:18.824
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]","completed":261,"skipped":4824,"failed":0}
------------------------------
â€¢ [4.093 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:09:14.739
    Aug 11 15:09:14.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 15:09:14.74
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:09:14.756
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:09:14.759
    [It] should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:88
    STEP: Creating configMap with name projected-configmap-test-volume-map-bc486e97-f374-49fd-832f-59f37f4c6d76 08/11/23 15:09:14.761
    STEP: Creating a pod to test consume configMaps 08/11/23 15:09:14.768
    Aug 11 15:09:14.777: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716" in namespace "projected-512" to be "Succeeded or Failed"
    Aug 11 15:09:14.782: INFO: Pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716": Phase="Pending", Reason="", readiness=false. Elapsed: 4.529871ms
    Aug 11 15:09:16.787: INFO: Pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009563624s
    Aug 11 15:09:18.788: INFO: Pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011362206s
    STEP: Saw pod success 08/11/23 15:09:18.789
    Aug 11 15:09:18.789: INFO: Pod "pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716" satisfied condition "Succeeded or Failed"
    Aug 11 15:09:18.792: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 15:09:18.8
    Aug 11 15:09:18.818: INFO: Waiting for pod pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716 to disappear
    Aug 11 15:09:18.821: INFO: Pod pod-projected-configmaps-f75435ee-bfa9-4ac4-afb0-a619f3a2f716 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 11 15:09:18.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-512" for this suite. 08/11/23 15:09:18.824
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] ResourceQuota
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
[BeforeEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:09:18.832
Aug 11 15:09:18.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename resourcequota 08/11/23 15:09:18.833
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:09:18.849
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:09:18.851
[It] should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680
STEP: Creating a ResourceQuota with terminating scope 08/11/23 15:09:18.854
STEP: Ensuring ResourceQuota status is calculated 08/11/23 15:09:18.859
STEP: Creating a ResourceQuota with not terminating scope 08/11/23 15:09:20.865
STEP: Ensuring ResourceQuota status is calculated 08/11/23 15:09:20.871
STEP: Creating a long running pod 08/11/23 15:09:22.876
STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/11/23 15:09:22.89
STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/11/23 15:09:24.896
STEP: Deleting the pod 08/11/23 15:09:26.9
STEP: Ensuring resource quota status released the pod usage 08/11/23 15:09:26.915
STEP: Creating a terminating pod 08/11/23 15:09:28.919
STEP: Ensuring resource quota with terminating scope captures the pod usage 08/11/23 15:09:28.934
STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/11/23 15:09:30.939
STEP: Deleting the pod 08/11/23 15:09:32.944
STEP: Ensuring resource quota status released the pod usage 08/11/23 15:09:32.962
[AfterEach] [sig-api-machinery] ResourceQuota
  test/e2e/framework/framework.go:187
Aug 11 15:09:34.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "resourcequota-1560" for this suite. 08/11/23 15:09:34.971
{"msg":"PASSED [sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]","completed":262,"skipped":4824,"failed":0}
------------------------------
â€¢ [SLOW TEST] [16.146 seconds]
[sig-api-machinery] ResourceQuota
test/e2e/apimachinery/framework.go:23
  should verify ResourceQuota with terminating scopes. [Conformance]
  test/e2e/apimachinery/resource_quota.go:680

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:09:18.832
    Aug 11 15:09:18.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename resourcequota 08/11/23 15:09:18.833
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:09:18.849
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:09:18.851
    [It] should verify ResourceQuota with terminating scopes. [Conformance]
      test/e2e/apimachinery/resource_quota.go:680
    STEP: Creating a ResourceQuota with terminating scope 08/11/23 15:09:18.854
    STEP: Ensuring ResourceQuota status is calculated 08/11/23 15:09:18.859
    STEP: Creating a ResourceQuota with not terminating scope 08/11/23 15:09:20.865
    STEP: Ensuring ResourceQuota status is calculated 08/11/23 15:09:20.871
    STEP: Creating a long running pod 08/11/23 15:09:22.876
    STEP: Ensuring resource quota with not terminating scope captures the pod usage 08/11/23 15:09:22.89
    STEP: Ensuring resource quota with terminating scope ignored the pod usage 08/11/23 15:09:24.896
    STEP: Deleting the pod 08/11/23 15:09:26.9
    STEP: Ensuring resource quota status released the pod usage 08/11/23 15:09:26.915
    STEP: Creating a terminating pod 08/11/23 15:09:28.919
    STEP: Ensuring resource quota with terminating scope captures the pod usage 08/11/23 15:09:28.934
    STEP: Ensuring resource quota with not terminating scope ignored the pod usage 08/11/23 15:09:30.939
    STEP: Deleting the pod 08/11/23 15:09:32.944
    STEP: Ensuring resource quota status released the pod usage 08/11/23 15:09:32.962
    [AfterEach] [sig-api-machinery] ResourceQuota
      test/e2e/framework/framework.go:187
    Aug 11 15:09:34.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "resourcequota-1560" for this suite. 08/11/23 15:09:34.971
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:09:34.98
Aug 11 15:09:34.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename cronjob 08/11/23 15:09:34.981
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:09:34.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:09:34.998
[It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124
STEP: Creating a ForbidConcurrent cronjob 08/11/23 15:09:35.001
STEP: Ensuring a job is scheduled 08/11/23 15:09:35.008
STEP: Ensuring exactly one is scheduled 08/11/23 15:10:01.013
STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/11/23 15:10:01.016
STEP: Ensuring no more jobs are scheduled 08/11/23 15:10:01.019
STEP: Removing cronjob 08/11/23 15:15:01.029
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 11 15:15:01.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-5504" for this suite. 08/11/23 15:15:01.039
{"msg":"PASSED [sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]","completed":263,"skipped":4853,"failed":0}
------------------------------
â€¢ [SLOW TEST] [326.071 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
  test/e2e/apps/cronjob.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:09:34.98
    Aug 11 15:09:34.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename cronjob 08/11/23 15:09:34.981
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:09:34.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:09:34.998
    [It] should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
      test/e2e/apps/cronjob.go:124
    STEP: Creating a ForbidConcurrent cronjob 08/11/23 15:09:35.001
    STEP: Ensuring a job is scheduled 08/11/23 15:09:35.008
    STEP: Ensuring exactly one is scheduled 08/11/23 15:10:01.013
    STEP: Ensuring exactly one running job exists by listing jobs explicitly 08/11/23 15:10:01.016
    STEP: Ensuring no more jobs are scheduled 08/11/23 15:10:01.019
    STEP: Removing cronjob 08/11/23 15:15:01.029
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 11 15:15:01.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-5504" for this suite. 08/11/23 15:15:01.039
  << End Captured GinkgoWriter Output
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:15:01.053
Aug 11 15:15:01.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 15:15:01.054
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:15:01.077
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:15:01.08
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:15:01.082
Aug 11 15:15:01.091: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d" in namespace "projected-5882" to be "Succeeded or Failed"
Aug 11 15:15:01.095: INFO: Pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031525ms
Aug 11 15:15:03.101: INFO: Pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009074549s
Aug 11 15:15:05.101: INFO: Pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00983286s
STEP: Saw pod success 08/11/23 15:15:05.101
Aug 11 15:15:05.101: INFO: Pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d" satisfied condition "Succeeded or Failed"
Aug 11 15:15:05.104: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d container client-container: <nil>
STEP: delete the pod 08/11/23 15:15:05.125
Aug 11 15:15:05.143: INFO: Waiting for pod downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d to disappear
Aug 11 15:15:05.146: INFO: Pod downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 15:15:05.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5882" for this suite. 08/11/23 15:15:05.15
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]","completed":264,"skipped":4859,"failed":0}
------------------------------
â€¢ [4.104 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:15:01.053
    Aug 11 15:15:01.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 15:15:01.054
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:15:01.077
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:15:01.08
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:234
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:15:01.082
    Aug 11 15:15:01.091: INFO: Waiting up to 5m0s for pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d" in namespace "projected-5882" to be "Succeeded or Failed"
    Aug 11 15:15:01.095: INFO: Pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d": Phase="Pending", Reason="", readiness=false. Elapsed: 4.031525ms
    Aug 11 15:15:03.101: INFO: Pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009074549s
    Aug 11 15:15:05.101: INFO: Pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00983286s
    STEP: Saw pod success 08/11/23 15:15:05.101
    Aug 11 15:15:05.101: INFO: Pod "downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d" satisfied condition "Succeeded or Failed"
    Aug 11 15:15:05.104: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d container client-container: <nil>
    STEP: delete the pod 08/11/23 15:15:05.125
    Aug 11 15:15:05.143: INFO: Waiting for pod downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d to disappear
    Aug 11 15:15:05.146: INFO: Pod downwardapi-volume-9b1c763c-5de0-418f-b025-cf51c163751d no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 15:15:05.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5882" for this suite. 08/11/23 15:15:05.15
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:15:05.159
Aug 11 15:15:05.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename statefulset 08/11/23 15:15:05.16
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:15:05.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:15:05.176
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9159 08/11/23 15:15:05.179
[It] should perform canary updates and phased rolling updates of template modifications [Conformance]
  test/e2e/apps/statefulset.go:315
STEP: Creating a new StatefulSet 08/11/23 15:15:05.185
Aug 11 15:15:05.198: INFO: Found 0 stateful pods, waiting for 3
Aug 11 15:15:15.206: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:15:15.206: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:15:15.206: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 08/11/23 15:15:15.215
Aug 11 15:15:15.235: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/11/23 15:15:15.235
STEP: Not applying an update when the partition is greater than the number of replicas 08/11/23 15:15:25.256
STEP: Performing a canary update 08/11/23 15:15:25.256
Aug 11 15:15:25.277: INFO: Updating stateful set ss2
Aug 11 15:15:25.287: INFO: Waiting for Pod statefulset-9159/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
STEP: Restoring Pods to the correct revision when they are deleted 08/11/23 15:15:35.3
Aug 11 15:15:35.338: INFO: Found 2 stateful pods, waiting for 3
Aug 11 15:15:45.348: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:15:45.348: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:15:45.348: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Performing a phased rolling update 08/11/23 15:15:45.356
Aug 11 15:15:45.377: INFO: Updating stateful set ss2
Aug 11 15:15:45.385: INFO: Waiting for Pod statefulset-9159/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
Aug 11 15:15:55.421: INFO: Updating stateful set ss2
Aug 11 15:15:55.439: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
Aug 11 15:15:55.439: INFO: Waiting for Pod statefulset-9159/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 11 15:16:05.452: INFO: Deleting all statefulset in ns statefulset-9159
Aug 11 15:16:05.455: INFO: Scaling statefulset ss2 to 0
Aug 11 15:16:15.477: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:16:15.480: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 11 15:16:15.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9159" for this suite. 08/11/23 15:16:15.5
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]","completed":265,"skipped":4893,"failed":0}
------------------------------
â€¢ [SLOW TEST] [70.350 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform canary updates and phased rolling updates of template modifications [Conformance]
    test/e2e/apps/statefulset.go:315

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:15:05.159
    Aug 11 15:15:05.159: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename statefulset 08/11/23 15:15:05.16
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:15:05.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:15:05.176
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9159 08/11/23 15:15:05.179
    [It] should perform canary updates and phased rolling updates of template modifications [Conformance]
      test/e2e/apps/statefulset.go:315
    STEP: Creating a new StatefulSet 08/11/23 15:15:05.185
    Aug 11 15:15:05.198: INFO: Found 0 stateful pods, waiting for 3
    Aug 11 15:15:15.206: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:15:15.206: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:15:15.206: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 08/11/23 15:15:15.215
    Aug 11 15:15:15.235: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/11/23 15:15:15.235
    STEP: Not applying an update when the partition is greater than the number of replicas 08/11/23 15:15:25.256
    STEP: Performing a canary update 08/11/23 15:15:25.256
    Aug 11 15:15:25.277: INFO: Updating stateful set ss2
    Aug 11 15:15:25.287: INFO: Waiting for Pod statefulset-9159/ss2-2 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    STEP: Restoring Pods to the correct revision when they are deleted 08/11/23 15:15:35.3
    Aug 11 15:15:35.338: INFO: Found 2 stateful pods, waiting for 3
    Aug 11 15:15:45.348: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:15:45.348: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:15:45.348: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Performing a phased rolling update 08/11/23 15:15:45.356
    Aug 11 15:15:45.377: INFO: Updating stateful set ss2
    Aug 11 15:15:45.385: INFO: Waiting for Pod statefulset-9159/ss2-1 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    Aug 11 15:15:55.421: INFO: Updating stateful set ss2
    Aug 11 15:15:55.439: INFO: Waiting for StatefulSet statefulset-9159/ss2 to complete update
    Aug 11 15:15:55.439: INFO: Waiting for Pod statefulset-9159/ss2-0 to have revision ss2-5d8c6ff87d update revision ss2-6557876d87
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 11 15:16:05.452: INFO: Deleting all statefulset in ns statefulset-9159
    Aug 11 15:16:05.455: INFO: Scaling statefulset ss2 to 0
    Aug 11 15:16:15.477: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:16:15.480: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 11 15:16:15.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9159" for this suite. 08/11/23 15:16:15.5
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes
  should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
[BeforeEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:16:15.509
Aug 11 15:16:15.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename subpath 08/11/23 15:16:15.51
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:15.524
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:15.527
[BeforeEach] Atomic writer volumes
  test/e2e/storage/subpath.go:40
STEP: Setting up data 08/11/23 15:16:15.531
[It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
  test/e2e/storage/subpath.go:80
STEP: Creating pod pod-subpath-test-configmap-hjjd 08/11/23 15:16:15.544
STEP: Creating a pod to test atomic-volume-subpath 08/11/23 15:16:15.544
Aug 11 15:16:15.553: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hjjd" in namespace "subpath-8684" to be "Succeeded or Failed"
Aug 11 15:16:15.560: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.645098ms
Aug 11 15:16:17.569: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 2.016148049s
Aug 11 15:16:19.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 4.013035309s
Aug 11 15:16:21.565: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 6.01184108s
Aug 11 15:16:23.564: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 8.011509616s
Aug 11 15:16:25.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 10.01271497s
Aug 11 15:16:27.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 12.013506181s
Aug 11 15:16:29.567: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 14.013732735s
Aug 11 15:16:31.565: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 16.012076081s
Aug 11 15:16:33.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 18.013033627s
Aug 11 15:16:35.565: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 20.011749853s
Aug 11 15:16:37.565: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=false. Elapsed: 22.011871113s
Aug 11 15:16:39.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012915843s
STEP: Saw pod success 08/11/23 15:16:39.566
Aug 11 15:16:39.566: INFO: Pod "pod-subpath-test-configmap-hjjd" satisfied condition "Succeeded or Failed"
Aug 11 15:16:39.569: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-configmap-hjjd container test-container-subpath-configmap-hjjd: <nil>
STEP: delete the pod 08/11/23 15:16:39.591
Aug 11 15:16:39.606: INFO: Waiting for pod pod-subpath-test-configmap-hjjd to disappear
Aug 11 15:16:39.609: INFO: Pod pod-subpath-test-configmap-hjjd no longer exists
STEP: Deleting pod pod-subpath-test-configmap-hjjd 08/11/23 15:16:39.609
Aug 11 15:16:39.609: INFO: Deleting pod "pod-subpath-test-configmap-hjjd" in namespace "subpath-8684"
[AfterEach] [sig-storage] Subpath
  test/e2e/framework/framework.go:187
Aug 11 15:16:39.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "subpath-8684" for this suite. 08/11/23 15:16:39.615
{"msg":"PASSED [sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]","completed":266,"skipped":4902,"failed":0}
------------------------------
â€¢ [SLOW TEST] [24.113 seconds]
[sig-storage] Subpath
test/e2e/storage/utils/framework.go:23
  Atomic writer volumes
  test/e2e/storage/subpath.go:36
    should support subpaths with configmap pod with mountPath of existing file [Conformance]
    test/e2e/storage/subpath.go:80

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:16:15.509
    Aug 11 15:16:15.509: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename subpath 08/11/23 15:16:15.51
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:15.524
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:15.527
    [BeforeEach] Atomic writer volumes
      test/e2e/storage/subpath.go:40
    STEP: Setting up data 08/11/23 15:16:15.531
    [It] should support subpaths with configmap pod with mountPath of existing file [Conformance]
      test/e2e/storage/subpath.go:80
    STEP: Creating pod pod-subpath-test-configmap-hjjd 08/11/23 15:16:15.544
    STEP: Creating a pod to test atomic-volume-subpath 08/11/23 15:16:15.544
    Aug 11 15:16:15.553: INFO: Waiting up to 5m0s for pod "pod-subpath-test-configmap-hjjd" in namespace "subpath-8684" to be "Succeeded or Failed"
    Aug 11 15:16:15.560: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Pending", Reason="", readiness=false. Elapsed: 7.645098ms
    Aug 11 15:16:17.569: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 2.016148049s
    Aug 11 15:16:19.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 4.013035309s
    Aug 11 15:16:21.565: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 6.01184108s
    Aug 11 15:16:23.564: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 8.011509616s
    Aug 11 15:16:25.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 10.01271497s
    Aug 11 15:16:27.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 12.013506181s
    Aug 11 15:16:29.567: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 14.013732735s
    Aug 11 15:16:31.565: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 16.012076081s
    Aug 11 15:16:33.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 18.013033627s
    Aug 11 15:16:35.565: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=true. Elapsed: 20.011749853s
    Aug 11 15:16:37.565: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Running", Reason="", readiness=false. Elapsed: 22.011871113s
    Aug 11 15:16:39.566: INFO: Pod "pod-subpath-test-configmap-hjjd": Phase="Succeeded", Reason="", readiness=false. Elapsed: 24.012915843s
    STEP: Saw pod success 08/11/23 15:16:39.566
    Aug 11 15:16:39.566: INFO: Pod "pod-subpath-test-configmap-hjjd" satisfied condition "Succeeded or Failed"
    Aug 11 15:16:39.569: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-subpath-test-configmap-hjjd container test-container-subpath-configmap-hjjd: <nil>
    STEP: delete the pod 08/11/23 15:16:39.591
    Aug 11 15:16:39.606: INFO: Waiting for pod pod-subpath-test-configmap-hjjd to disappear
    Aug 11 15:16:39.609: INFO: Pod pod-subpath-test-configmap-hjjd no longer exists
    STEP: Deleting pod pod-subpath-test-configmap-hjjd 08/11/23 15:16:39.609
    Aug 11 15:16:39.609: INFO: Deleting pod "pod-subpath-test-configmap-hjjd" in namespace "subpath-8684"
    [AfterEach] [sig-storage] Subpath
      test/e2e/framework/framework.go:187
    Aug 11 15:16:39.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "subpath-8684" for this suite. 08/11/23 15:16:39.615
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container
  should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:16:39.622
Aug 11 15:16:39.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:16:39.624
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:39.639
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:39.642
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:184
Aug 11 15:16:39.654: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18" in namespace "kubelet-test-91" to be "running and ready"
Aug 11 15:16:39.658: INFO: Pod "busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277473ms
Aug 11 15:16:39.658: INFO: The phase of Pod busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:16:41.662: INFO: Pod "busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18": Phase="Running", Reason="", readiness=true. Elapsed: 2.008383627s
Aug 11 15:16:41.662: INFO: The phase of Pod busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18 is Running (Ready = true)
Aug 11 15:16:41.662: INFO: Pod "busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 11 15:16:41.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-91" for this suite. 08/11/23 15:16:41.679
{"msg":"PASSED [sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]","completed":267,"skipped":4905,"failed":0}
------------------------------
â€¢ [2.064 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a read only busybox container
  test/e2e/common/node/kubelet.go:175
    should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:16:39.622
    Aug 11 15:16:39.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:16:39.624
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:39.639
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:39.642
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:184
    Aug 11 15:16:39.654: INFO: Waiting up to 5m0s for pod "busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18" in namespace "kubelet-test-91" to be "running and ready"
    Aug 11 15:16:39.658: INFO: Pod "busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18": Phase="Pending", Reason="", readiness=false. Elapsed: 4.277473ms
    Aug 11 15:16:39.658: INFO: The phase of Pod busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:16:41.662: INFO: Pod "busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18": Phase="Running", Reason="", readiness=true. Elapsed: 2.008383627s
    Aug 11 15:16:41.662: INFO: The phase of Pod busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18 is Running (Ready = true)
    Aug 11 15:16:41.662: INFO: Pod "busybox-readonly-fs0253d833-be3c-4bbe-9921-056b41ae2c18" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 11 15:16:41.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-91" for this suite. 08/11/23 15:16:41.679
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial]
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:16:41.688
Aug 11 15:16:41.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename controllerrevisions 08/11/23 15:16:41.69
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:41.705
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:41.708
[BeforeEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:93
[It] should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124
STEP: Creating DaemonSet "e2e-52cc2-daemon-set" 08/11/23 15:16:41.728
STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 15:16:41.736
Aug 11 15:16:41.741: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:16:41.741: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:16:41.741: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:16:41.747: INFO: Number of nodes with available pods controlled by daemonset e2e-52cc2-daemon-set: 0
Aug 11 15:16:41.747: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 15:16:42.753: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:16:42.753: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:16:42.753: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
Aug 11 15:16:42.762: INFO: Number of nodes with available pods controlled by daemonset e2e-52cc2-daemon-set: 2
Aug 11 15:16:42.762: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-52cc2-daemon-set
STEP: Confirm DaemonSet "e2e-52cc2-daemon-set" successfully created with "daemonset-name=e2e-52cc2-daemon-set" label 08/11/23 15:16:42.764
STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-52cc2-daemon-set" 08/11/23 15:16:42.77
Aug 11 15:16:42.774: INFO: Located ControllerRevision: "e2e-52cc2-daemon-set-ccd88d6d6"
STEP: Patching ControllerRevision "e2e-52cc2-daemon-set-ccd88d6d6" 08/11/23 15:16:42.777
Aug 11 15:16:42.784: INFO: e2e-52cc2-daemon-set-ccd88d6d6 has been patched
STEP: Create a new ControllerRevision 08/11/23 15:16:42.784
Aug 11 15:16:42.791: INFO: Created ControllerRevision: e2e-52cc2-daemon-set-649b65488d
STEP: Confirm that there are two ControllerRevisions 08/11/23 15:16:42.791
Aug 11 15:16:42.792: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 15:16:42.795: INFO: Found 2 ControllerRevisions
STEP: Deleting ControllerRevision "e2e-52cc2-daemon-set-ccd88d6d6" 08/11/23 15:16:42.795
STEP: Confirm that there is only one ControllerRevision 08/11/23 15:16:42.801
Aug 11 15:16:42.801: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 15:16:42.805: INFO: Found 1 ControllerRevisions
STEP: Updating ControllerRevision "e2e-52cc2-daemon-set-649b65488d" 08/11/23 15:16:42.808
Aug 11 15:16:42.817: INFO: e2e-52cc2-daemon-set-649b65488d has been updated
STEP: Generate another ControllerRevision by patching the Daemonset 08/11/23 15:16:42.817
W0811 15:16:42.827478      22 warnings.go:70] unknown field "updateStrategy"
STEP: Confirm that there are two ControllerRevisions 08/11/23 15:16:42.827
Aug 11 15:16:42.827: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 15:16:43.833: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 15:16:43.837: INFO: Found 2 ControllerRevisions
STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-52cc2-daemon-set-649b65488d=updated" 08/11/23 15:16:43.837
STEP: Confirm that there is only one ControllerRevision 08/11/23 15:16:43.844
Aug 11 15:16:43.844: INFO: Requesting list of ControllerRevisions to confirm quantity
Aug 11 15:16:43.847: INFO: Found 1 ControllerRevisions
Aug 11 15:16:43.850: INFO: ControllerRevision "e2e-52cc2-daemon-set-5f847b79bf" has revision 3
[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/apps/controller_revision.go:58
STEP: Deleting DaemonSet "e2e-52cc2-daemon-set" 08/11/23 15:16:43.853
STEP: deleting DaemonSet.extensions e2e-52cc2-daemon-set in namespace controllerrevisions-2545, will wait for the garbage collector to delete the pods 08/11/23 15:16:43.853
Aug 11 15:16:43.915: INFO: Deleting DaemonSet.extensions e2e-52cc2-daemon-set took: 7.939596ms
Aug 11 15:16:44.015: INFO: Terminating DaemonSet.extensions e2e-52cc2-daemon-set pods took: 100.160397ms
Aug 11 15:16:45.721: INFO: Number of nodes with available pods controlled by daemonset e2e-52cc2-daemon-set: 0
Aug 11 15:16:45.721: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-52cc2-daemon-set
Aug 11 15:16:45.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47206"},"items":null}

Aug 11 15:16:45.727: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47206"},"items":null}

[AfterEach] [sig-apps] ControllerRevision [Serial]
  test/e2e/framework/framework.go:187
Aug 11 15:16:45.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "controllerrevisions-2545" for this suite. 08/11/23 15:16:45.747
{"msg":"PASSED [sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]","completed":268,"skipped":4955,"failed":0}
------------------------------
â€¢ [4.065 seconds]
[sig-apps] ControllerRevision [Serial]
test/e2e/apps/framework.go:23
  should manage the lifecycle of a ControllerRevision [Conformance]
  test/e2e/apps/controller_revision.go:124

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:16:41.688
    Aug 11 15:16:41.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename controllerrevisions 08/11/23 15:16:41.69
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:41.705
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:41.708
    [BeforeEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:93
    [It] should manage the lifecycle of a ControllerRevision [Conformance]
      test/e2e/apps/controller_revision.go:124
    STEP: Creating DaemonSet "e2e-52cc2-daemon-set" 08/11/23 15:16:41.728
    STEP: Check that daemon pods launch on every node of the cluster. 08/11/23 15:16:41.736
    Aug 11 15:16:41.741: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:16:41.741: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:16:41.741: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:16:41.747: INFO: Number of nodes with available pods controlled by daemonset e2e-52cc2-daemon-set: 0
    Aug 11 15:16:41.747: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 15:16:42.753: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-1v0j with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:16:42.753: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-3t35 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:16:42.753: INFO: DaemonSet pods can't tolerate node constell-d93e7e1d-control-plane-6ef148f4-d3pw with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
    Aug 11 15:16:42.762: INFO: Number of nodes with available pods controlled by daemonset e2e-52cc2-daemon-set: 2
    Aug 11 15:16:42.762: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-52cc2-daemon-set
    STEP: Confirm DaemonSet "e2e-52cc2-daemon-set" successfully created with "daemonset-name=e2e-52cc2-daemon-set" label 08/11/23 15:16:42.764
    STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-52cc2-daemon-set" 08/11/23 15:16:42.77
    Aug 11 15:16:42.774: INFO: Located ControllerRevision: "e2e-52cc2-daemon-set-ccd88d6d6"
    STEP: Patching ControllerRevision "e2e-52cc2-daemon-set-ccd88d6d6" 08/11/23 15:16:42.777
    Aug 11 15:16:42.784: INFO: e2e-52cc2-daemon-set-ccd88d6d6 has been patched
    STEP: Create a new ControllerRevision 08/11/23 15:16:42.784
    Aug 11 15:16:42.791: INFO: Created ControllerRevision: e2e-52cc2-daemon-set-649b65488d
    STEP: Confirm that there are two ControllerRevisions 08/11/23 15:16:42.791
    Aug 11 15:16:42.792: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 15:16:42.795: INFO: Found 2 ControllerRevisions
    STEP: Deleting ControllerRevision "e2e-52cc2-daemon-set-ccd88d6d6" 08/11/23 15:16:42.795
    STEP: Confirm that there is only one ControllerRevision 08/11/23 15:16:42.801
    Aug 11 15:16:42.801: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 15:16:42.805: INFO: Found 1 ControllerRevisions
    STEP: Updating ControllerRevision "e2e-52cc2-daemon-set-649b65488d" 08/11/23 15:16:42.808
    Aug 11 15:16:42.817: INFO: e2e-52cc2-daemon-set-649b65488d has been updated
    STEP: Generate another ControllerRevision by patching the Daemonset 08/11/23 15:16:42.817
    W0811 15:16:42.827478      22 warnings.go:70] unknown field "updateStrategy"
    STEP: Confirm that there are two ControllerRevisions 08/11/23 15:16:42.827
    Aug 11 15:16:42.827: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 15:16:43.833: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 15:16:43.837: INFO: Found 2 ControllerRevisions
    STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-52cc2-daemon-set-649b65488d=updated" 08/11/23 15:16:43.837
    STEP: Confirm that there is only one ControllerRevision 08/11/23 15:16:43.844
    Aug 11 15:16:43.844: INFO: Requesting list of ControllerRevisions to confirm quantity
    Aug 11 15:16:43.847: INFO: Found 1 ControllerRevisions
    Aug 11 15:16:43.850: INFO: ControllerRevision "e2e-52cc2-daemon-set-5f847b79bf" has revision 3
    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/apps/controller_revision.go:58
    STEP: Deleting DaemonSet "e2e-52cc2-daemon-set" 08/11/23 15:16:43.853
    STEP: deleting DaemonSet.extensions e2e-52cc2-daemon-set in namespace controllerrevisions-2545, will wait for the garbage collector to delete the pods 08/11/23 15:16:43.853
    Aug 11 15:16:43.915: INFO: Deleting DaemonSet.extensions e2e-52cc2-daemon-set took: 7.939596ms
    Aug 11 15:16:44.015: INFO: Terminating DaemonSet.extensions e2e-52cc2-daemon-set pods took: 100.160397ms
    Aug 11 15:16:45.721: INFO: Number of nodes with available pods controlled by daemonset e2e-52cc2-daemon-set: 0
    Aug 11 15:16:45.721: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-52cc2-daemon-set
    Aug 11 15:16:45.724: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"47206"},"items":null}

    Aug 11 15:16:45.727: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"47206"},"items":null}

    [AfterEach] [sig-apps] ControllerRevision [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 15:16:45.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "controllerrevisions-2545" for this suite. 08/11/23 15:16:45.747
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:16:45.756
Aug 11 15:16:45.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename deployment 08/11/23 15:16:45.757
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:45.772
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:45.775
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160
Aug 11 15:16:45.779: INFO: Creating deployment "webserver-deployment"
Aug 11 15:16:45.784: INFO: Waiting for observed generation 1
Aug 11 15:16:47.794: INFO: Waiting for all required pods to come up
Aug 11 15:16:47.802: INFO: Pod name httpd: Found 10 pods out of 10
STEP: ensuring each pod is running 08/11/23 15:16:47.802
Aug 11 15:16:47.802: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rxnff" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.802: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2vkxk" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4twzs" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-76hz4" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bqw8v" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-fdsw2" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gvvv9" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n7xjq" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-lqh95" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pmjks" in namespace "deployment-203" to be "running"
Aug 11 15:16:47.805: INFO: Pod "webserver-deployment-845c8977d9-4twzs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791037ms
Aug 11 15:16:47.805: INFO: Pod "webserver-deployment-845c8977d9-2vkxk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010694ms
Aug 11 15:16:47.806: INFO: Pod "webserver-deployment-845c8977d9-bqw8v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154488ms
Aug 11 15:16:47.806: INFO: Pod "webserver-deployment-845c8977d9-rxnff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.995914ms
Aug 11 15:16:47.806: INFO: Pod "webserver-deployment-845c8977d9-76hz4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.654113ms
Aug 11 15:16:47.807: INFO: Pod "webserver-deployment-845c8977d9-gvvv9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310354ms
Aug 11 15:16:47.808: INFO: Pod "webserver-deployment-845c8977d9-pmjks": Phase="Pending", Reason="", readiness=false. Elapsed: 4.327625ms
Aug 11 15:16:47.808: INFO: Pod "webserver-deployment-845c8977d9-fdsw2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.681995ms
Aug 11 15:16:47.808: INFO: Pod "webserver-deployment-845c8977d9-lqh95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.458409ms
Aug 11 15:16:47.808: INFO: Pod "webserver-deployment-845c8977d9-n7xjq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.581463ms
Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-bqw8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.006764557s
Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-bqw8v" satisfied condition "running"
Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-2vkxk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007356506s
Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-4twzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.007208231s
Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-4twzs" satisfied condition "running"
Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-rxnff": Phase="Running", Reason="", readiness=true. Elapsed: 2.007661165s
Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-rxnff" satisfied condition "running"
Aug 11 15:16:49.811: INFO: Pod "webserver-deployment-845c8977d9-76hz4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00782063s
Aug 11 15:16:49.811: INFO: Pod "webserver-deployment-845c8977d9-76hz4" satisfied condition "running"
Aug 11 15:16:49.811: INFO: Pod "webserver-deployment-845c8977d9-pmjks": Phase="Running", Reason="", readiness=true. Elapsed: 2.007995745s
Aug 11 15:16:49.811: INFO: Pod "webserver-deployment-845c8977d9-pmjks" satisfied condition "running"
Aug 11 15:16:49.812: INFO: Pod "webserver-deployment-845c8977d9-fdsw2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009469411s
Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-fdsw2" satisfied condition "running"
Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-gvvv9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00943324s
Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-gvvv9" satisfied condition "running"
Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-n7xjq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009382239s
Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-n7xjq" satisfied condition "running"
Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-lqh95": Phase="Running", Reason="", readiness=true. Elapsed: 2.009469102s
Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-lqh95" satisfied condition "running"
Aug 11 15:16:51.810: INFO: Pod "webserver-deployment-845c8977d9-2vkxk": Phase="Running", Reason="", readiness=true. Elapsed: 4.00760004s
Aug 11 15:16:51.810: INFO: Pod "webserver-deployment-845c8977d9-2vkxk" satisfied condition "running"
Aug 11 15:16:51.810: INFO: Waiting for deployment "webserver-deployment" to complete
Aug 11 15:16:51.817: INFO: Updating deployment "webserver-deployment" with a non-existent image
Aug 11 15:16:51.828: INFO: Updating deployment webserver-deployment
Aug 11 15:16:51.828: INFO: Waiting for observed generation 2
Aug 11 15:16:53.839: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
Aug 11 15:16:53.843: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
Aug 11 15:16:53.846: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 11 15:16:53.854: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
Aug 11 15:16:53.854: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
Aug 11 15:16:53.856: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
Aug 11 15:16:53.861: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
Aug 11 15:16:53.861: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
Aug 11 15:16:53.870: INFO: Updating deployment webserver-deployment
Aug 11 15:16:53.870: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
Aug 11 15:16:53.877: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
Aug 11 15:16:53.883: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 15:16:53.905: INFO: Deployment "webserver-deployment":
&Deployment{ObjectMeta:{webserver-deployment  deployment-203  43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b 47511 3 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00358ae28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-08-11 15:16:51 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 15:16:53 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

Aug 11 15:16:53.916: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
&ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-203  a3e0ff10-29a0-4073-a3fb-42c090199f22 47507 3 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b 0xc00358b257 0xc00358b258}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00358b2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:16:53.916: INFO: All old ReplicaSets of Deployment "webserver-deployment":
Aug 11 15:16:53.916: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-203  991556de-8045-4ace-b4ce-580b463b9bde 47504 3 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b 0xc00358b357 0xc00358b358}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00358b3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:16:53.931: INFO: Pod "webserver-deployment-69b7448995-4xwjx" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-4xwjx webserver-deployment-69b7448995- deployment-203  4b5f0b7a-dadc-406b-9c5e-a5ed3abd819f 47499 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057af5a7 0xc0057af5a8}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jd84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jd84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.154,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.931: INFO: Pod "webserver-deployment-69b7448995-5rpsk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-5rpsk webserver-deployment-69b7448995- deployment-203  10d9d6d7-8bd2-42ee-94d0-34dde5aa6c4d 47527 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057af7c0 0xc0057af7c1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhxvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhxvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.931: INFO: Pod "webserver-deployment-69b7448995-9j4mx" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-9j4mx webserver-deployment-69b7448995- deployment-203  b8b31cee-f68c-4585-b2b9-d296b513c18c 47528 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057af907 0xc0057af908}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fdvf6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fdvf6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-crnss" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-crnss webserver-deployment-69b7448995- deployment-203  19b871bb-d627-4613-b219-e3be9470d659 47526 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057afa77 0xc0057afa78}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddcsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddcsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-csfl9" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-csfl9 webserver-deployment-69b7448995- deployment-203  50441bcc-141b-4c53-beb0-d1b9f68825a9 47516 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057afbc7 0xc0057afbc8}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p9h8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9h8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-hc9fg" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-hc9fg webserver-deployment-69b7448995- deployment-203  b7ea11e5-f1ae-4456-836b-11126c6cb72f 47429 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057afd50 0xc0057afd51}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfbjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfbjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-kp4mj" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-kp4mj webserver-deployment-69b7448995- deployment-203  3632f306-65d9-4d03-b4c7-52e4e67dddfd 47460 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057aff37 0xc0057aff38}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5zq6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zq6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-nkm6z" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-nkm6z webserver-deployment-69b7448995- deployment-203  a72c4d1b-733e-41ae-8ece-1070726c3fc9 47525 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e0107 0xc0034e0108}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7w5z8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7w5z8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-p2rmd" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-p2rmd webserver-deployment-69b7448995- deployment-203  9a01739c-841a-4d70-bf6b-b024bb16305f 47524 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e0270 0xc0034e0271}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5g2kh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5g2kh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-69b7448995-pmb5b" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-pmb5b webserver-deployment-69b7448995- deployment-203  789b1840-2a9a-439c-9bd1-886311c3fa2b 47458 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e03e0 0xc0034e03e1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s2vhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s2vhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-69b7448995-qrcg5" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-qrcg5 webserver-deployment-69b7448995- deployment-203  ad76aadc-ddd8-4c1b-bece-a4f9f69a71e3 47529 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e05b7 0xc0034e05b8}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmwth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmwth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-69b7448995-zdhtk" is not available:
&Pod{ObjectMeta:{webserver-deployment-69b7448995-zdhtk webserver-deployment-69b7448995- deployment-203  d795746d-d798-4c50-ac8d-4f0de13dd052 47501 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e0707 0xc0034e0708}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4x6nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4x6nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.149,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-845c8977d9-76hz4" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-76hz4 webserver-deployment-845c8977d9- deployment-203  4b4154ed-e5a0-4487-afdd-78ae6497a0cd 47378 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0910 0xc0034e0911}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8bls4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8bls4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.154,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2148429332b26e510b619a4c24090188919935eee00fac8c404eec55734b9a46,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-845c8977d9-8pzlz" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-8pzlz webserver-deployment-845c8977d9- deployment-203  46e5e3cd-2d37-4935-9bb6-af4db2cc27f4 47531 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0ae0 0xc0034e0ae1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bqck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bqck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.934: INFO: Pod "webserver-deployment-845c8977d9-bqw8v" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bqw8v webserver-deployment-845c8977d9- deployment-203  c863f6c5-dc5d-499a-a120-1f96ba54cae1 47351 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0c17 0xc0034e0c18}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tmbd4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tmbd4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.67,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bf6ea6f71241824e406d656a4b1ef7d64e88ef88fa97d132cc7ee8b95014094b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.934: INFO: Pod "webserver-deployment-845c8977d9-bwhws" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-bwhws webserver-deployment-845c8977d9- deployment-203  32511c2b-f177-4184-bedc-01755ae01020 47523 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0df0 0xc0034e0df1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdjwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdjwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.934: INFO: Pod "webserver-deployment-845c8977d9-fdsw2" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-fdsw2 webserver-deployment-845c8977d9- deployment-203  7b5e1b3f-54bc-49ad-8c25-7b27951a8967 47374 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0f27 0xc0034e0f28}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vhlts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vhlts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.195,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c0b00062655165025fb911c050c7e8b495dcbce095babd077c4d8ab3ae4f073e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.934: INFO: Pod "webserver-deployment-845c8977d9-gvvv9" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-gvvv9 webserver-deployment-845c8977d9- deployment-203  ff215fd5-43ef-4a3a-9ffe-f84d64393db7 47371 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1110 0xc0034e1111}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssp55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssp55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.57,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://dc8a857178774bda1d2e8d9612355a8cb570e0a60a65a576117358c84f6d9f09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.935: INFO: Pod "webserver-deployment-845c8977d9-hhg98" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-hhg98 webserver-deployment-845c8977d9- deployment-203  a677f5bd-c8ee-4b8c-aab8-f01cd2756707 47513 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e12e0 0xc0034e12e1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9l5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9l5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.935: INFO: Pod "webserver-deployment-845c8977d9-lg2b6" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lg2b6 webserver-deployment-845c8977d9- deployment-203  83358bee-b5fc-4317-baba-4fa8e018de47 47520 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1440 0xc0034e1441}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7xrf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7xrf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.935: INFO: Pod "webserver-deployment-845c8977d9-lqh95" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-lqh95 webserver-deployment-845c8977d9- deployment-203  52120d6e-eb87-40a0-918f-ca3c3ee031ff 47343 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1577 0xc0034e1578}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbqc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbqc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.194,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3da480a0717c2019750617714419b853f38573d38c7e07e15eb19fcc7fe7d612,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.935: INFO: Pod "webserver-deployment-845c8977d9-n7xjq" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-n7xjq webserver-deployment-845c8977d9- deployment-203  05f46ce0-d371-4485-8e75-4d4b90c7d6e4 47362 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1760 0xc0034e1761}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7fq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7fq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.26,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4f84e2f5e410dec89790438c2497328ac3fcbd52149af6cc79783c15da2ad691,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-pl8vm" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pl8vm webserver-deployment-845c8977d9- deployment-203  c08278ac-3256-4007-a225-dfc86ff89a4d 47517 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1930 0xc0034e1931}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdjs9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdjs9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-pmjks" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-pmjks webserver-deployment-845c8977d9- deployment-203  e331e577-eb8c-4e6f-8eeb-cd0b2ae4eb90 47356 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1a80 0xc0034e1a81}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47ckb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47ckb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.162,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0a73505cacad7a3c06a1d01426a840558e84e190e4475c20c93e1e19945c841f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-rpsfc" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rpsfc webserver-deployment-845c8977d9- deployment-203  8f695068-f1f9-48ad-aa85-e427ad0e25fe 47530 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1c50 0xc0034e1c51}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-htjjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-htjjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-rxnff" is available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-rxnff webserver-deployment-845c8977d9- deployment-203  0485eef8-a130-426a-aaab-954be5ffd439 47366 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1d87 0xc0034e1d88}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qz99j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qz99j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.14,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://89896914e5249a5ff8bcb1bf3bd9e3f63e3b9cd0a7fceff7b2b194bf307d2674,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-xkt6t" is not available:
&Pod{ObjectMeta:{webserver-deployment-845c8977d9-xkt6t webserver-deployment-845c8977d9- deployment-203  c45baead-e366-429c-bd60-49a1fcea10a8 47518 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1f60 0xc0034e1f61}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgt6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgt6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 11 15:16:53.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-203" for this suite. 08/11/23 15:16:53.943
{"msg":"PASSED [sig-apps] Deployment deployment should support proportional scaling [Conformance]","completed":269,"skipped":5014,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.211 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should support proportional scaling [Conformance]
  test/e2e/apps/deployment.go:160

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:16:45.756
    Aug 11 15:16:45.756: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename deployment 08/11/23 15:16:45.757
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:45.772
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:45.775
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should support proportional scaling [Conformance]
      test/e2e/apps/deployment.go:160
    Aug 11 15:16:45.779: INFO: Creating deployment "webserver-deployment"
    Aug 11 15:16:45.784: INFO: Waiting for observed generation 1
    Aug 11 15:16:47.794: INFO: Waiting for all required pods to come up
    Aug 11 15:16:47.802: INFO: Pod name httpd: Found 10 pods out of 10
    STEP: ensuring each pod is running 08/11/23 15:16:47.802
    Aug 11 15:16:47.802: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-rxnff" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.802: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-2vkxk" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-4twzs" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-76hz4" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-bqw8v" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-fdsw2" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-gvvv9" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-n7xjq" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-lqh95" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.803: INFO: Waiting up to 5m0s for pod "webserver-deployment-845c8977d9-pmjks" in namespace "deployment-203" to be "running"
    Aug 11 15:16:47.805: INFO: Pod "webserver-deployment-845c8977d9-4twzs": Phase="Pending", Reason="", readiness=false. Elapsed: 2.791037ms
    Aug 11 15:16:47.805: INFO: Pod "webserver-deployment-845c8977d9-2vkxk": Phase="Pending", Reason="", readiness=false. Elapsed: 3.010694ms
    Aug 11 15:16:47.806: INFO: Pod "webserver-deployment-845c8977d9-bqw8v": Phase="Pending", Reason="", readiness=false. Elapsed: 3.154488ms
    Aug 11 15:16:47.806: INFO: Pod "webserver-deployment-845c8977d9-rxnff": Phase="Pending", Reason="", readiness=false. Elapsed: 3.995914ms
    Aug 11 15:16:47.806: INFO: Pod "webserver-deployment-845c8977d9-76hz4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.654113ms
    Aug 11 15:16:47.807: INFO: Pod "webserver-deployment-845c8977d9-gvvv9": Phase="Pending", Reason="", readiness=false. Elapsed: 4.310354ms
    Aug 11 15:16:47.808: INFO: Pod "webserver-deployment-845c8977d9-pmjks": Phase="Pending", Reason="", readiness=false. Elapsed: 4.327625ms
    Aug 11 15:16:47.808: INFO: Pod "webserver-deployment-845c8977d9-fdsw2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.681995ms
    Aug 11 15:16:47.808: INFO: Pod "webserver-deployment-845c8977d9-lqh95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.458409ms
    Aug 11 15:16:47.808: INFO: Pod "webserver-deployment-845c8977d9-n7xjq": Phase="Pending", Reason="", readiness=false. Elapsed: 4.581463ms
    Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-bqw8v": Phase="Running", Reason="", readiness=true. Elapsed: 2.006764557s
    Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-bqw8v" satisfied condition "running"
    Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-2vkxk": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007356506s
    Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-4twzs": Phase="Running", Reason="", readiness=true. Elapsed: 2.007208231s
    Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-4twzs" satisfied condition "running"
    Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-rxnff": Phase="Running", Reason="", readiness=true. Elapsed: 2.007661165s
    Aug 11 15:16:49.810: INFO: Pod "webserver-deployment-845c8977d9-rxnff" satisfied condition "running"
    Aug 11 15:16:49.811: INFO: Pod "webserver-deployment-845c8977d9-76hz4": Phase="Running", Reason="", readiness=true. Elapsed: 2.00782063s
    Aug 11 15:16:49.811: INFO: Pod "webserver-deployment-845c8977d9-76hz4" satisfied condition "running"
    Aug 11 15:16:49.811: INFO: Pod "webserver-deployment-845c8977d9-pmjks": Phase="Running", Reason="", readiness=true. Elapsed: 2.007995745s
    Aug 11 15:16:49.811: INFO: Pod "webserver-deployment-845c8977d9-pmjks" satisfied condition "running"
    Aug 11 15:16:49.812: INFO: Pod "webserver-deployment-845c8977d9-fdsw2": Phase="Running", Reason="", readiness=true. Elapsed: 2.009469411s
    Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-fdsw2" satisfied condition "running"
    Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-gvvv9": Phase="Running", Reason="", readiness=true. Elapsed: 2.00943324s
    Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-gvvv9" satisfied condition "running"
    Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-n7xjq": Phase="Running", Reason="", readiness=true. Elapsed: 2.009382239s
    Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-n7xjq" satisfied condition "running"
    Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-lqh95": Phase="Running", Reason="", readiness=true. Elapsed: 2.009469102s
    Aug 11 15:16:49.813: INFO: Pod "webserver-deployment-845c8977d9-lqh95" satisfied condition "running"
    Aug 11 15:16:51.810: INFO: Pod "webserver-deployment-845c8977d9-2vkxk": Phase="Running", Reason="", readiness=true. Elapsed: 4.00760004s
    Aug 11 15:16:51.810: INFO: Pod "webserver-deployment-845c8977d9-2vkxk" satisfied condition "running"
    Aug 11 15:16:51.810: INFO: Waiting for deployment "webserver-deployment" to complete
    Aug 11 15:16:51.817: INFO: Updating deployment "webserver-deployment" with a non-existent image
    Aug 11 15:16:51.828: INFO: Updating deployment webserver-deployment
    Aug 11 15:16:51.828: INFO: Waiting for observed generation 2
    Aug 11 15:16:53.839: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
    Aug 11 15:16:53.843: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
    Aug 11 15:16:53.846: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 11 15:16:53.854: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
    Aug 11 15:16:53.854: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
    Aug 11 15:16:53.856: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
    Aug 11 15:16:53.861: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
    Aug 11 15:16:53.861: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
    Aug 11 15:16:53.870: INFO: Updating deployment webserver-deployment
    Aug 11 15:16:53.870: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
    Aug 11 15:16:53.877: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
    Aug 11 15:16:53.883: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 15:16:53.905: INFO: Deployment "webserver-deployment":
    &Deployment{ObjectMeta:{webserver-deployment  deployment-203  43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b 47511 3 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00358ae28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-69b7448995" is progressing.,LastUpdateTime:2023-08-11 15:16:51 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-11 15:16:53 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

    Aug 11 15:16:53.916: INFO: New ReplicaSet "webserver-deployment-69b7448995" of Deployment "webserver-deployment":
    &ReplicaSet{ObjectMeta:{webserver-deployment-69b7448995  deployment-203  a3e0ff10-29a0-4073-a3fb-42c090199f22 47507 3 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b 0xc00358b257 0xc00358b258}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 69b7448995,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00358b2f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:16:53.916: INFO: All old ReplicaSets of Deployment "webserver-deployment":
    Aug 11 15:16:53.916: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-845c8977d9  deployment-203  991556de-8045-4ace-b4ce-580b463b9bde 47504 3 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b 0xc00358b357 0xc00358b358}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43a8e12d-a750-4b26-ba4d-7f2eb8a1ba3b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 845c8977d9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00358b3e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:16:53.931: INFO: Pod "webserver-deployment-69b7448995-4xwjx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-4xwjx webserver-deployment-69b7448995- deployment-203  4b5f0b7a-dadc-406b-9c5e-a5ed3abd819f 47499 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057af5a7 0xc0057af5a8}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7jd84,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7jd84,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.154,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.931: INFO: Pod "webserver-deployment-69b7448995-5rpsk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-5rpsk webserver-deployment-69b7448995- deployment-203  10d9d6d7-8bd2-42ee-94d0-34dde5aa6c4d 47527 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057af7c0 0xc0057af7c1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bhxvb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bhxvb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.931: INFO: Pod "webserver-deployment-69b7448995-9j4mx" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-9j4mx webserver-deployment-69b7448995- deployment-203  b8b31cee-f68c-4585-b2b9-d296b513c18c 47528 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057af907 0xc0057af908}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fdvf6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fdvf6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-crnss" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-crnss webserver-deployment-69b7448995- deployment-203  19b871bb-d627-4613-b219-e3be9470d659 47526 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057afa77 0xc0057afa78}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ddcsv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ddcsv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-csfl9" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-csfl9 webserver-deployment-69b7448995- deployment-203  50441bcc-141b-4c53-beb0-d1b9f68825a9 47516 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057afbc7 0xc0057afbc8}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p9h8d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p9h8d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-hc9fg" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-hc9fg webserver-deployment-69b7448995- deployment-203  b7ea11e5-f1ae-4456-836b-11126c6cb72f 47429 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057afd50 0xc0057afd51}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfbjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfbjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-kp4mj" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-kp4mj webserver-deployment-69b7448995- deployment-203  3632f306-65d9-4d03-b4c7-52e4e67dddfd 47460 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0057aff37 0xc0057aff38}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5zq6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5zq6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-nkm6z" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-nkm6z webserver-deployment-69b7448995- deployment-203  a72c4d1b-733e-41ae-8ece-1070726c3fc9 47525 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e0107 0xc0034e0108}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7w5z8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7w5z8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.932: INFO: Pod "webserver-deployment-69b7448995-p2rmd" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-p2rmd webserver-deployment-69b7448995- deployment-203  9a01739c-841a-4d70-bf6b-b024bb16305f 47524 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e0270 0xc0034e0271}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5g2kh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5g2kh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-69b7448995-pmb5b" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-pmb5b webserver-deployment-69b7448995- deployment-203  789b1840-2a9a-439c-9bd1-886311c3fa2b 47458 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e03e0 0xc0034e03e1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s2vhk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s2vhk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-69b7448995-qrcg5" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-qrcg5 webserver-deployment-69b7448995- deployment-203  ad76aadc-ddd8-4c1b-bece-a4f9f69a71e3 47529 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e05b7 0xc0034e05b8}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qmwth,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qmwth,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-69b7448995-zdhtk" is not available:
    &Pod{ObjectMeta:{webserver-deployment-69b7448995-zdhtk webserver-deployment-69b7448995- deployment-203  d795746d-d798-4c50-ac8d-4f0de13dd052 47501 0 2023-08-11 15:16:51 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:69b7448995] map[] [{apps/v1 ReplicaSet webserver-deployment-69b7448995 a3e0ff10-29a0-4073-a3fb-42c090199f22 0xc0034e0707 0xc0034e0708}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:51 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a3e0ff10-29a0-4073-a3fb-42c090199f22\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4x6nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4x6nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:51 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.149,StartTime:2023-08-11 15:16:51 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.149,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-845c8977d9-76hz4" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-76hz4 webserver-deployment-845c8977d9- deployment-203  4b4154ed-e5a0-4487-afdd-78ae6497a0cd 47378 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0910 0xc0034e0911}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.154\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8bls4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8bls4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.154,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://2148429332b26e510b619a4c24090188919935eee00fac8c404eec55734b9a46,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.154,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.933: INFO: Pod "webserver-deployment-845c8977d9-8pzlz" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-8pzlz webserver-deployment-845c8977d9- deployment-203  46e5e3cd-2d37-4935-9bb6-af4db2cc27f4 47531 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0ae0 0xc0034e0ae1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2bqck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2bqck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.934: INFO: Pod "webserver-deployment-845c8977d9-bqw8v" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bqw8v webserver-deployment-845c8977d9- deployment-203  c863f6c5-dc5d-499a-a120-1f96ba54cae1 47351 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0c17 0xc0034e0c18}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.67\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tmbd4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tmbd4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.67,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://bf6ea6f71241824e406d656a4b1ef7d64e88ef88fa97d132cc7ee8b95014094b,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.67,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.934: INFO: Pod "webserver-deployment-845c8977d9-bwhws" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-bwhws webserver-deployment-845c8977d9- deployment-203  32511c2b-f177-4184-bedc-01755ae01020 47523 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0df0 0xc0034e0df1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zdjwb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zdjwb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.934: INFO: Pod "webserver-deployment-845c8977d9-fdsw2" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-fdsw2 webserver-deployment-845c8977d9- deployment-203  7b5e1b3f-54bc-49ad-8c25-7b27951a8967 47374 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e0f27 0xc0034e0f28}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vhlts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vhlts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.195,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://c0b00062655165025fb911c050c7e8b495dcbce095babd077c4d8ab3ae4f073e,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.195,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.934: INFO: Pod "webserver-deployment-845c8977d9-gvvv9" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-gvvv9 webserver-deployment-845c8977d9- deployment-203  ff215fd5-43ef-4a3a-9ffe-f84d64393db7 47371 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1110 0xc0034e1111}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:49 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.57\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ssp55,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ssp55,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.57,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://dc8a857178774bda1d2e8d9612355a8cb570e0a60a65a576117358c84f6d9f09,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.57,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.935: INFO: Pod "webserver-deployment-845c8977d9-hhg98" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-hhg98 webserver-deployment-845c8977d9- deployment-203  a677f5bd-c8ee-4b8c-aab8-f01cd2756707 47513 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e12e0 0xc0034e12e1}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q9l5p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q9l5p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.935: INFO: Pod "webserver-deployment-845c8977d9-lg2b6" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lg2b6 webserver-deployment-845c8977d9- deployment-203  83358bee-b5fc-4317-baba-4fa8e018de47 47520 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1440 0xc0034e1441}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t7xrf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t7xrf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.935: INFO: Pod "webserver-deployment-845c8977d9-lqh95" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-lqh95 webserver-deployment-845c8977d9- deployment-203  52120d6e-eb87-40a0-918f-ca3c3ee031ff 47343 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1577 0xc0034e1578}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dbqc6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dbqc6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.194,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://3da480a0717c2019750617714419b853f38573d38c7e07e15eb19fcc7fe7d612,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.194,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.935: INFO: Pod "webserver-deployment-845c8977d9-n7xjq" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-n7xjq webserver-deployment-845c8977d9- deployment-203  05f46ce0-d371-4485-8e75-4d4b90c7d6e4 47362 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1760 0xc0034e1761}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.1.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7fq9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7fq9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-wzlp,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.3,PodIP:10.10.1.26,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://4f84e2f5e410dec89790438c2497328ac3fcbd52149af6cc79783c15da2ad691,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.1.26,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-pl8vm" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pl8vm webserver-deployment-845c8977d9- deployment-203  c08278ac-3256-4007-a225-dfc86ff89a4d 47517 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1930 0xc0034e1931}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hdjs9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hdjs9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-pmjks" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-pmjks webserver-deployment-845c8977d9- deployment-203  e331e577-eb8c-4e6f-8eeb-cd0b2ae4eb90 47356 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1a80 0xc0034e1a81}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-47ckb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-47ckb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.162,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:47 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://0a73505cacad7a3c06a1d01426a840558e84e190e4475c20c93e1e19945c841f,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.162,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-rpsfc" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rpsfc webserver-deployment-845c8977d9- deployment-203  8f695068-f1f9-48ad-aa85-e427ad0e25fe 47530 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1c50 0xc0034e1c51}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-htjjx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-htjjx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-rxnff" is available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-rxnff webserver-deployment-845c8977d9- deployment-203  0485eef8-a130-426a-aaab-954be5ffd439 47366 0 2023-08-11 15:16:45 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1d87 0xc0034e1d88}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:16:48 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qz99j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qz99j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.14,StartTime:2023-08-11 15:16:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:16:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://89896914e5249a5ff8bcb1bf3bd9e3f63e3b9cd0a7fceff7b2b194bf307d2674,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.14,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    Aug 11 15:16:53.936: INFO: Pod "webserver-deployment-845c8977d9-xkt6t" is not available:
    &Pod{ObjectMeta:{webserver-deployment-845c8977d9-xkt6t webserver-deployment-845c8977d9- deployment-203  c45baead-e366-429c-bd60-49a1fcea10a8 47518 0 2023-08-11 15:16:53 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:845c8977d9] map[] [{apps/v1 ReplicaSet webserver-deployment-845c8977d9 991556de-8045-4ace-b4ce-580b463b9bde 0xc0034e1f60 0xc0034e1f61}] [] [{kube-controller-manager Update v1 2023-08-11 15:16:53 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"991556de-8045-4ace-b4ce-580b463b9bde\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bgt6x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bgt6x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:16:53 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 11 15:16:53.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-203" for this suite. 08/11/23 15:16:53.943
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:16:53.971
Aug 11 15:16:53.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 15:16:53.972
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:53.996
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:53.999
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 15:16:54.013
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:16:54.367
STEP: Deploying the webhook pod 08/11/23 15:16:54.377
STEP: Wait for the deployment to be ready 08/11/23 15:16:54.389
Aug 11 15:16:54.402: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
Aug 11 15:16:56.413: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:16:58.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:00.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:02.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
Aug 11 15:17:04.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
STEP: Deploying the webhook service 08/11/23 15:17:06.418
STEP: Verifying the service has paired with the endpoint 08/11/23 15:17:06.438
Aug 11 15:17:07.438: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196
STEP: Registering the webhook via the AdmissionRegistration API 08/11/23 15:17:07.442
STEP: create a pod that should be denied by the webhook 08/11/23 15:17:07.466
STEP: create a pod that causes the webhook to hang 08/11/23 15:17:07.492
STEP: create a configmap that should be denied by the webhook 08/11/23 15:17:17.505
STEP: create a configmap that should be admitted by the webhook 08/11/23 15:17:17.59
STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/11/23 15:17:17.608
STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/11/23 15:17:17.619
STEP: create a namespace that bypass the webhook 08/11/23 15:17:17.627
STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/11/23 15:17:17.635
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:17:17.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-7727" for this suite. 08/11/23 15:17:17.679
STEP: Destroying namespace "webhook-7727-markers" for this suite. 08/11/23 15:17:17.69
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]","completed":270,"skipped":5030,"failed":0}
------------------------------
â€¢ [SLOW TEST] [23.769 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to deny pod and configmap creation [Conformance]
  test/e2e/apimachinery/webhook.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:16:53.971
    Aug 11 15:16:53.971: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 15:16:53.972
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:16:53.996
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:16:53.999
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 15:16:54.013
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:16:54.367
    STEP: Deploying the webhook pod 08/11/23 15:16:54.377
    STEP: Wait for the deployment to be ready 08/11/23 15:16:54.389
    Aug 11 15:16:54.402: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    Aug 11 15:16:56.413: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:16:58.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:00.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:02.418: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    Aug 11 15:17:04.417: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 16, 54, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-5d85dd8cdb\" is progressing."}}, CollisionCount:(*int32)(nil)}
    STEP: Deploying the webhook service 08/11/23 15:17:06.418
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:17:06.438
    Aug 11 15:17:07.438: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should be able to deny pod and configmap creation [Conformance]
      test/e2e/apimachinery/webhook.go:196
    STEP: Registering the webhook via the AdmissionRegistration API 08/11/23 15:17:07.442
    STEP: create a pod that should be denied by the webhook 08/11/23 15:17:07.466
    STEP: create a pod that causes the webhook to hang 08/11/23 15:17:07.492
    STEP: create a configmap that should be denied by the webhook 08/11/23 15:17:17.505
    STEP: create a configmap that should be admitted by the webhook 08/11/23 15:17:17.59
    STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook 08/11/23 15:17:17.608
    STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook 08/11/23 15:17:17.619
    STEP: create a namespace that bypass the webhook 08/11/23 15:17:17.627
    STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace 08/11/23 15:17:17.635
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:17:17.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-7727" for this suite. 08/11/23 15:17:17.679
    STEP: Destroying namespace "webhook-7727-markers" for this suite. 08/11/23 15:17:17.69
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:17:17.742
Aug 11 15:17:17.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 15:17:17.743
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:17.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:17.764
[It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156
STEP: Creating a pod to test emptydir volume type on node default medium 08/11/23 15:17:17.767
Aug 11 15:17:17.776: INFO: Waiting up to 5m0s for pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37" in namespace "emptydir-6363" to be "Succeeded or Failed"
Aug 11 15:17:17.782: INFO: Pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37": Phase="Pending", Reason="", readiness=false. Elapsed: 5.228903ms
Aug 11 15:17:19.787: INFO: Pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010745831s
Aug 11 15:17:21.787: INFO: Pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010187271s
STEP: Saw pod success 08/11/23 15:17:21.787
Aug 11 15:17:21.787: INFO: Pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37" satisfied condition "Succeeded or Failed"
Aug 11 15:17:21.790: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37 container test-container: <nil>
STEP: delete the pod 08/11/23 15:17:21.799
Aug 11 15:17:21.812: INFO: Waiting for pod pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37 to disappear
Aug 11 15:17:21.816: INFO: Pod pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 15:17:21.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-6363" for this suite. 08/11/23 15:17:21.82
{"msg":"PASSED [sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]","completed":271,"skipped":5048,"failed":0}
------------------------------
â€¢ [4.084 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:156

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:17:17.742
    Aug 11 15:17:17.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:17:17.743
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:17.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:17.764
    [It] volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:156
    STEP: Creating a pod to test emptydir volume type on node default medium 08/11/23 15:17:17.767
    Aug 11 15:17:17.776: INFO: Waiting up to 5m0s for pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37" in namespace "emptydir-6363" to be "Succeeded or Failed"
    Aug 11 15:17:17.782: INFO: Pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37": Phase="Pending", Reason="", readiness=false. Elapsed: 5.228903ms
    Aug 11 15:17:19.787: INFO: Pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010745831s
    Aug 11 15:17:21.787: INFO: Pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010187271s
    STEP: Saw pod success 08/11/23 15:17:21.787
    Aug 11 15:17:21.787: INFO: Pod "pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37" satisfied condition "Succeeded or Failed"
    Aug 11 15:17:21.790: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:17:21.799
    Aug 11 15:17:21.812: INFO: Waiting for pod pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37 to disappear
    Aug 11 15:17:21.816: INFO: Pod pod-cb13ca1e-6351-4b06-b7f0-f80ad4c74b37 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 15:17:21.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-6363" for this suite. 08/11/23 15:17:21.82
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:17:21.828
Aug 11 15:17:21.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:17:21.829
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:21.844
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:21.847
[It] works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235
Aug 11 15:17:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 15:17:26.083
Aug 11 15:17:26.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 --namespace=crd-publish-openapi-5438 create -f -'
Aug 11 15:17:26.669: INFO: stderr: ""
Aug 11 15:17:26.669: INFO: stdout: "e2e-test-crd-publish-openapi-4012-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 11 15:17:26.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 --namespace=crd-publish-openapi-5438 delete e2e-test-crd-publish-openapi-4012-crds test-cr'
Aug 11 15:17:26.752: INFO: stderr: ""
Aug 11 15:17:26.752: INFO: stdout: "e2e-test-crd-publish-openapi-4012-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
Aug 11 15:17:26.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 --namespace=crd-publish-openapi-5438 apply -f -'
Aug 11 15:17:26.975: INFO: stderr: ""
Aug 11 15:17:26.975: INFO: stdout: "e2e-test-crd-publish-openapi-4012-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
Aug 11 15:17:26.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 --namespace=crd-publish-openapi-5438 delete e2e-test-crd-publish-openapi-4012-crds test-cr'
Aug 11 15:17:27.038: INFO: stderr: ""
Aug 11 15:17:27.038: INFO: stdout: "e2e-test-crd-publish-openapi-4012-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
STEP: kubectl explain works to explain CR 08/11/23 15:17:27.038
Aug 11 15:17:27.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 explain e2e-test-crd-publish-openapi-4012-crds'
Aug 11 15:17:27.588: INFO: stderr: ""
Aug 11 15:17:27.588: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4012-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:17:30.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5438" for this suite. 08/11/23 15:17:30.36
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]","completed":272,"skipped":5086,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.537 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for CRD preserving unknown fields in an embedded object [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:235

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:17:21.828
    Aug 11 15:17:21.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:17:21.829
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:21.844
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:21.847
    [It] works for CRD preserving unknown fields in an embedded object [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:235
    Aug 11 15:17:21.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties 08/11/23 15:17:26.083
    Aug 11 15:17:26.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 --namespace=crd-publish-openapi-5438 create -f -'
    Aug 11 15:17:26.669: INFO: stderr: ""
    Aug 11 15:17:26.669: INFO: stdout: "e2e-test-crd-publish-openapi-4012-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 11 15:17:26.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 --namespace=crd-publish-openapi-5438 delete e2e-test-crd-publish-openapi-4012-crds test-cr'
    Aug 11 15:17:26.752: INFO: stderr: ""
    Aug 11 15:17:26.752: INFO: stdout: "e2e-test-crd-publish-openapi-4012-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    Aug 11 15:17:26.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 --namespace=crd-publish-openapi-5438 apply -f -'
    Aug 11 15:17:26.975: INFO: stderr: ""
    Aug 11 15:17:26.975: INFO: stdout: "e2e-test-crd-publish-openapi-4012-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
    Aug 11 15:17:26.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 --namespace=crd-publish-openapi-5438 delete e2e-test-crd-publish-openapi-4012-crds test-cr'
    Aug 11 15:17:27.038: INFO: stderr: ""
    Aug 11 15:17:27.038: INFO: stdout: "e2e-test-crd-publish-openapi-4012-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
    STEP: kubectl explain works to explain CR 08/11/23 15:17:27.038
    Aug 11 15:17:27.038: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=crd-publish-openapi-5438 explain e2e-test-crd-publish-openapi-4012-crds'
    Aug 11 15:17:27.588: INFO: stderr: ""
    Aug 11 15:17:27.588: INFO: stdout: "KIND:     e2e-test-crd-publish-openapi-4012-crd\nVERSION:  crd-publish-openapi-test-unknown-in-nested.example.com/v1\n\nDESCRIPTION:\n     preserve-unknown-properties in nested field for Testing\n\nFIELDS:\n   apiVersion\t<string>\n     APIVersion defines the versioned schema of this representation of an\n     object. Servers should convert recognized schemas to the latest internal\n     value, and may reject unrecognized values. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n   kind\t<string>\n     Kind is a string value representing the REST resource this object\n     represents. Servers may infer this from the endpoint the client submits\n     requests to. Cannot be updated. In CamelCase. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n   metadata\t<Object>\n     Standard object's metadata. More info:\n     https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n   spec\t<>\n     Specification of Waldo\n\n   status\t<Object>\n     Status of Waldo\n\n"
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:17:30.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5438" for this suite. 08/11/23 15:17:30.36
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:17:30.367
Aug 11 15:17:30.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-probe 08/11/23 15:17:30.368
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:30.379
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:30.382
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180
STEP: Creating pod liveness-5d02388e-072d-414d-9a29-bf5059f77681 in namespace container-probe-5363 08/11/23 15:17:30.384
Aug 11 15:17:30.390: INFO: Waiting up to 5m0s for pod "liveness-5d02388e-072d-414d-9a29-bf5059f77681" in namespace "container-probe-5363" to be "not pending"
Aug 11 15:17:30.396: INFO: Pod "liveness-5d02388e-072d-414d-9a29-bf5059f77681": Phase="Pending", Reason="", readiness=false. Elapsed: 5.408248ms
Aug 11 15:17:32.399: INFO: Pod "liveness-5d02388e-072d-414d-9a29-bf5059f77681": Phase="Running", Reason="", readiness=true. Elapsed: 2.008571553s
Aug 11 15:17:32.399: INFO: Pod "liveness-5d02388e-072d-414d-9a29-bf5059f77681" satisfied condition "not pending"
Aug 11 15:17:32.399: INFO: Started pod liveness-5d02388e-072d-414d-9a29-bf5059f77681 in namespace container-probe-5363
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:17:32.399
Aug 11 15:17:32.401: INFO: Initial restart count of pod liveness-5d02388e-072d-414d-9a29-bf5059f77681 is 0
STEP: deleting the pod 08/11/23 15:21:32.907
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 11 15:21:32.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-5363" for this suite. 08/11/23 15:21:32.92
{"msg":"PASSED [sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]","completed":273,"skipped":5102,"failed":0}
------------------------------
â€¢ [SLOW TEST] [242.560 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:180

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:17:30.367
    Aug 11 15:17:30.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-probe 08/11/23 15:17:30.368
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:17:30.379
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:17:30.382
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:180
    STEP: Creating pod liveness-5d02388e-072d-414d-9a29-bf5059f77681 in namespace container-probe-5363 08/11/23 15:17:30.384
    Aug 11 15:17:30.390: INFO: Waiting up to 5m0s for pod "liveness-5d02388e-072d-414d-9a29-bf5059f77681" in namespace "container-probe-5363" to be "not pending"
    Aug 11 15:17:30.396: INFO: Pod "liveness-5d02388e-072d-414d-9a29-bf5059f77681": Phase="Pending", Reason="", readiness=false. Elapsed: 5.408248ms
    Aug 11 15:17:32.399: INFO: Pod "liveness-5d02388e-072d-414d-9a29-bf5059f77681": Phase="Running", Reason="", readiness=true. Elapsed: 2.008571553s
    Aug 11 15:17:32.399: INFO: Pod "liveness-5d02388e-072d-414d-9a29-bf5059f77681" satisfied condition "not pending"
    Aug 11 15:17:32.399: INFO: Started pod liveness-5d02388e-072d-414d-9a29-bf5059f77681 in namespace container-probe-5363
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:17:32.399
    Aug 11 15:17:32.401: INFO: Initial restart count of pod liveness-5d02388e-072d-414d-9a29-bf5059f77681 is 0
    STEP: deleting the pod 08/11/23 15:21:32.907
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 11 15:21:32.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-5363" for this suite. 08/11/23 15:21:32.92
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:21:32.927
Aug 11 15:21:32.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 15:21:32.928
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:21:32.942
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:21:32.945
[It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96
STEP: Creating a pod to test emptydir 0644 on tmpfs 08/11/23 15:21:32.947
Aug 11 15:21:32.955: INFO: Waiting up to 5m0s for pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218" in namespace "emptydir-5347" to be "Succeeded or Failed"
Aug 11 15:21:32.958: INFO: Pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218": Phase="Pending", Reason="", readiness=false. Elapsed: 3.399746ms
Aug 11 15:21:34.962: INFO: Pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007489299s
Aug 11 15:21:36.962: INFO: Pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007346441s
STEP: Saw pod success 08/11/23 15:21:36.962
Aug 11 15:21:36.962: INFO: Pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218" satisfied condition "Succeeded or Failed"
Aug 11 15:21:36.964: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-93125b72-2089-4165-8bf1-e7dfcfe54218 container test-container: <nil>
STEP: delete the pod 08/11/23 15:21:36.984
Aug 11 15:21:36.995: INFO: Waiting for pod pod-93125b72-2089-4165-8bf1-e7dfcfe54218 to disappear
Aug 11 15:21:36.997: INFO: Pod pod-93125b72-2089-4165-8bf1-e7dfcfe54218 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 15:21:36.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5347" for this suite. 08/11/23 15:21:37
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]","completed":274,"skipped":5106,"failed":0}
------------------------------
â€¢ [4.079 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:96

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:21:32.927
    Aug 11 15:21:32.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:21:32.928
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:21:32.942
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:21:32.945
    [It] should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:96
    STEP: Creating a pod to test emptydir 0644 on tmpfs 08/11/23 15:21:32.947
    Aug 11 15:21:32.955: INFO: Waiting up to 5m0s for pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218" in namespace "emptydir-5347" to be "Succeeded or Failed"
    Aug 11 15:21:32.958: INFO: Pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218": Phase="Pending", Reason="", readiness=false. Elapsed: 3.399746ms
    Aug 11 15:21:34.962: INFO: Pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007489299s
    Aug 11 15:21:36.962: INFO: Pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.007346441s
    STEP: Saw pod success 08/11/23 15:21:36.962
    Aug 11 15:21:36.962: INFO: Pod "pod-93125b72-2089-4165-8bf1-e7dfcfe54218" satisfied condition "Succeeded or Failed"
    Aug 11 15:21:36.964: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-93125b72-2089-4165-8bf1-e7dfcfe54218 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:21:36.984
    Aug 11 15:21:36.995: INFO: Waiting for pod pod-93125b72-2089-4165-8bf1-e7dfcfe54218 to disappear
    Aug 11 15:21:36.997: INFO: Pod pod-93125b72-2089-4165-8bf1-e7dfcfe54218 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 15:21:36.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5347" for this suite. 08/11/23 15:21:37
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application
  should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:21:37.009
Aug 11 15:21:37.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 15:21:37.01
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:21:37.022
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:21:37.024
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create and stop a working application  [Conformance]
  test/e2e/kubectl/kubectl.go:392
STEP: creating all guestbook components 08/11/23 15:21:37.027
Aug 11 15:21:37.027: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-replica
  labels:
    app: agnhost
    role: replica
    tier: backend
spec:
  ports:
  - port: 6379
  selector:
    app: agnhost
    role: replica
    tier: backend

Aug 11 15:21:37.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
Aug 11 15:21:37.694: INFO: stderr: ""
Aug 11 15:21:37.694: INFO: stdout: "service/agnhost-replica created\n"
Aug 11 15:21:37.694: INFO: apiVersion: v1
kind: Service
metadata:
  name: agnhost-primary
  labels:
    app: agnhost
    role: primary
    tier: backend
spec:
  ports:
  - port: 6379
    targetPort: 6379
  selector:
    app: agnhost
    role: primary
    tier: backend

Aug 11 15:21:37.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
Aug 11 15:21:38.367: INFO: stderr: ""
Aug 11 15:21:38.367: INFO: stdout: "service/agnhost-primary created\n"
Aug 11 15:21:38.367: INFO: apiVersion: v1
kind: Service
metadata:
  name: frontend
  labels:
    app: guestbook
    tier: frontend
spec:
  # if your cluster supports it, uncomment the following to automatically create
  # an external load-balanced IP for the frontend service.
  # type: LoadBalancer
  ports:
  - port: 80
  selector:
    app: guestbook
    tier: frontend

Aug 11 15:21:38.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
Aug 11 15:21:38.599: INFO: stderr: ""
Aug 11 15:21:38.599: INFO: stdout: "service/frontend created\n"
Aug 11 15:21:38.599: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  selector:
    matchLabels:
      app: guestbook
      tier: frontend
  template:
    metadata:
      labels:
        app: guestbook
        tier: frontend
    spec:
      containers:
      - name: guestbook-frontend
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--backend-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 80

Aug 11 15:21:38.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
Aug 11 15:21:38.827: INFO: stderr: ""
Aug 11 15:21:38.827: INFO: stdout: "deployment.apps/frontend created\n"
Aug 11 15:21:38.827: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-primary
spec:
  replicas: 1
  selector:
    matchLabels:
      app: agnhost
      role: primary
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      containers:
      - name: primary
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 11 15:21:38.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
Aug 11 15:21:39.058: INFO: stderr: ""
Aug 11 15:21:39.058: INFO: stdout: "deployment.apps/agnhost-primary created\n"
Aug 11 15:21:39.058: INFO: apiVersion: apps/v1
kind: Deployment
metadata:
  name: agnhost-replica
spec:
  replicas: 2
  selector:
    matchLabels:
      app: agnhost
      role: replica
      tier: backend
  template:
    metadata:
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      containers:
      - name: replica
        image: registry.k8s.io/e2e-test-images/agnhost:2.40
        args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
        resources:
          requests:
            cpu: 100m
            memory: 100Mi
        ports:
        - containerPort: 6379

Aug 11 15:21:39.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
Aug 11 15:21:39.316: INFO: stderr: ""
Aug 11 15:21:39.316: INFO: stdout: "deployment.apps/agnhost-replica created\n"
STEP: validating guestbook app 08/11/23 15:21:39.317
Aug 11 15:21:39.317: INFO: Waiting for all frontend pods to be Running.
Aug 11 15:21:44.368: INFO: Waiting for frontend to serve content.
Aug 11 15:21:44.389: INFO: Trying to add a new entry to the guestbook.
Aug 11 15:21:44.403: INFO: Verifying that added entry can be retrieved.
STEP: using delete to clean up resources 08/11/23 15:21:44.414
Aug 11 15:21:44.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
Aug 11 15:21:44.493: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:21:44.493: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:21:44.493
Aug 11 15:21:44.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
Aug 11 15:21:44.577: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:21:44.577: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:21:44.578
Aug 11 15:21:44.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
Aug 11 15:21:44.662: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:21:44.662: INFO: stdout: "service \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:21:44.662
Aug 11 15:21:44.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
Aug 11 15:21:44.729: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:21:44.730: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:21:44.73
Aug 11 15:21:44.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
Aug 11 15:21:44.807: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:21:44.807: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
STEP: using delete to clean up resources 08/11/23 15:21:44.807
Aug 11 15:21:44.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
Aug 11 15:21:44.883: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:21:44.883: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 15:21:44.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7487" for this suite. 08/11/23 15:21:44.887
{"msg":"PASSED [sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]","completed":275,"skipped":5151,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.890 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Guestbook application
  test/e2e/kubectl/kubectl.go:367
    should create and stop a working application  [Conformance]
    test/e2e/kubectl/kubectl.go:392

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:21:37.009
    Aug 11 15:21:37.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:21:37.01
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:21:37.022
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:21:37.024
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create and stop a working application  [Conformance]
      test/e2e/kubectl/kubectl.go:392
    STEP: creating all guestbook components 08/11/23 15:21:37.027
    Aug 11 15:21:37.027: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-replica
      labels:
        app: agnhost
        role: replica
        tier: backend
    spec:
      ports:
      - port: 6379
      selector:
        app: agnhost
        role: replica
        tier: backend

    Aug 11 15:21:37.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
    Aug 11 15:21:37.694: INFO: stderr: ""
    Aug 11 15:21:37.694: INFO: stdout: "service/agnhost-replica created\n"
    Aug 11 15:21:37.694: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: agnhost-primary
      labels:
        app: agnhost
        role: primary
        tier: backend
    spec:
      ports:
      - port: 6379
        targetPort: 6379
      selector:
        app: agnhost
        role: primary
        tier: backend

    Aug 11 15:21:37.694: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
    Aug 11 15:21:38.367: INFO: stderr: ""
    Aug 11 15:21:38.367: INFO: stdout: "service/agnhost-primary created\n"
    Aug 11 15:21:38.367: INFO: apiVersion: v1
    kind: Service
    metadata:
      name: frontend
      labels:
        app: guestbook
        tier: frontend
    spec:
      # if your cluster supports it, uncomment the following to automatically create
      # an external load-balanced IP for the frontend service.
      # type: LoadBalancer
      ports:
      - port: 80
      selector:
        app: guestbook
        tier: frontend

    Aug 11 15:21:38.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
    Aug 11 15:21:38.599: INFO: stderr: ""
    Aug 11 15:21:38.599: INFO: stdout: "service/frontend created\n"
    Aug 11 15:21:38.599: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: frontend
    spec:
      replicas: 3
      selector:
        matchLabels:
          app: guestbook
          tier: frontend
      template:
        metadata:
          labels:
            app: guestbook
            tier: frontend
        spec:
          containers:
          - name: guestbook-frontend
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--backend-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 80

    Aug 11 15:21:38.599: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
    Aug 11 15:21:38.827: INFO: stderr: ""
    Aug 11 15:21:38.827: INFO: stdout: "deployment.apps/frontend created\n"
    Aug 11 15:21:38.827: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-primary
    spec:
      replicas: 1
      selector:
        matchLabels:
          app: agnhost
          role: primary
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: primary
            tier: backend
        spec:
          containers:
          - name: primary
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 11 15:21:38.827: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
    Aug 11 15:21:39.058: INFO: stderr: ""
    Aug 11 15:21:39.058: INFO: stdout: "deployment.apps/agnhost-primary created\n"
    Aug 11 15:21:39.058: INFO: apiVersion: apps/v1
    kind: Deployment
    metadata:
      name: agnhost-replica
    spec:
      replicas: 2
      selector:
        matchLabels:
          app: agnhost
          role: replica
          tier: backend
      template:
        metadata:
          labels:
            app: agnhost
            role: replica
            tier: backend
        spec:
          containers:
          - name: replica
            image: registry.k8s.io/e2e-test-images/agnhost:2.40
            args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
            resources:
              requests:
                cpu: 100m
                memory: 100Mi
            ports:
            - containerPort: 6379

    Aug 11 15:21:39.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 create -f -'
    Aug 11 15:21:39.316: INFO: stderr: ""
    Aug 11 15:21:39.316: INFO: stdout: "deployment.apps/agnhost-replica created\n"
    STEP: validating guestbook app 08/11/23 15:21:39.317
    Aug 11 15:21:39.317: INFO: Waiting for all frontend pods to be Running.
    Aug 11 15:21:44.368: INFO: Waiting for frontend to serve content.
    Aug 11 15:21:44.389: INFO: Trying to add a new entry to the guestbook.
    Aug 11 15:21:44.403: INFO: Verifying that added entry can be retrieved.
    STEP: using delete to clean up resources 08/11/23 15:21:44.414
    Aug 11 15:21:44.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
    Aug 11 15:21:44.493: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:21:44.493: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:21:44.493
    Aug 11 15:21:44.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
    Aug 11 15:21:44.577: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:21:44.577: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:21:44.578
    Aug 11 15:21:44.578: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
    Aug 11 15:21:44.662: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:21:44.662: INFO: stdout: "service \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:21:44.662
    Aug 11 15:21:44.662: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
    Aug 11 15:21:44.729: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:21:44.730: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:21:44.73
    Aug 11 15:21:44.730: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
    Aug 11 15:21:44.807: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:21:44.807: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
    STEP: using delete to clean up resources 08/11/23 15:21:44.807
    Aug 11 15:21:44.807: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7487 delete --grace-period=0 --force -f -'
    Aug 11 15:21:44.883: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:21:44.883: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 15:21:44.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7487" for this suite. 08/11/23 15:21:44.887
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-node] PreStop
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
[BeforeEach] [sig-node] PreStop
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:21:44.9
Aug 11 15:21:44.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename prestop 08/11/23 15:21:44.901
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:21:44.914
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:21:44.917
[BeforeEach] [sig-node] PreStop
  test/e2e/node/pre_stop.go:159
[It] should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168
STEP: Creating server pod server in namespace prestop-7847 08/11/23 15:21:44.92
STEP: Waiting for pods to come up. 08/11/23 15:21:44.929
Aug 11 15:21:44.929: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-7847" to be "running"
Aug 11 15:21:44.933: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.260661ms
Aug 11 15:21:46.937: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007311474s
Aug 11 15:21:48.936: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.007109134s
Aug 11 15:21:48.937: INFO: Pod "server" satisfied condition "running"
STEP: Creating tester pod tester in namespace prestop-7847 08/11/23 15:21:48.939
Aug 11 15:21:48.944: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-7847" to be "running"
Aug 11 15:21:48.946: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188488ms
Aug 11 15:21:50.950: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006071015s
Aug 11 15:21:50.950: INFO: Pod "tester" satisfied condition "running"
STEP: Deleting pre-stop pod 08/11/23 15:21:50.95
Aug 11 15:21:55.970: INFO: Saw: {
	"Hostname": "server",
	"Sent": null,
	"Received": {
		"prestop": 1
	},
	"Errors": null,
	"Log": [
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
	],
	"StillContactingPeers": true
}
STEP: Deleting the server pod 08/11/23 15:21:55.97
[AfterEach] [sig-node] PreStop
  test/e2e/framework/framework.go:187
Aug 11 15:21:55.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "prestop-7847" for this suite. 08/11/23 15:21:55.984
{"msg":"PASSED [sig-node] PreStop should call prestop when killing a pod  [Conformance]","completed":276,"skipped":5158,"failed":0}
------------------------------
â€¢ [SLOW TEST] [11.091 seconds]
[sig-node] PreStop
test/e2e/node/framework.go:23
  should call prestop when killing a pod  [Conformance]
  test/e2e/node/pre_stop.go:168

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] PreStop
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:21:44.9
    Aug 11 15:21:44.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename prestop 08/11/23 15:21:44.901
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:21:44.914
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:21:44.917
    [BeforeEach] [sig-node] PreStop
      test/e2e/node/pre_stop.go:159
    [It] should call prestop when killing a pod  [Conformance]
      test/e2e/node/pre_stop.go:168
    STEP: Creating server pod server in namespace prestop-7847 08/11/23 15:21:44.92
    STEP: Waiting for pods to come up. 08/11/23 15:21:44.929
    Aug 11 15:21:44.929: INFO: Waiting up to 5m0s for pod "server" in namespace "prestop-7847" to be "running"
    Aug 11 15:21:44.933: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 3.260661ms
    Aug 11 15:21:46.937: INFO: Pod "server": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007311474s
    Aug 11 15:21:48.936: INFO: Pod "server": Phase="Running", Reason="", readiness=true. Elapsed: 4.007109134s
    Aug 11 15:21:48.937: INFO: Pod "server" satisfied condition "running"
    STEP: Creating tester pod tester in namespace prestop-7847 08/11/23 15:21:48.939
    Aug 11 15:21:48.944: INFO: Waiting up to 5m0s for pod "tester" in namespace "prestop-7847" to be "running"
    Aug 11 15:21:48.946: INFO: Pod "tester": Phase="Pending", Reason="", readiness=false. Elapsed: 2.188488ms
    Aug 11 15:21:50.950: INFO: Pod "tester": Phase="Running", Reason="", readiness=true. Elapsed: 2.006071015s
    Aug 11 15:21:50.950: INFO: Pod "tester" satisfied condition "running"
    STEP: Deleting pre-stop pod 08/11/23 15:21:50.95
    Aug 11 15:21:55.970: INFO: Saw: {
    	"Hostname": "server",
    	"Sent": null,
    	"Received": {
    		"prestop": 1
    	},
    	"Errors": null,
    	"Log": [
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
    		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
    	],
    	"StillContactingPeers": true
    }
    STEP: Deleting the server pod 08/11/23 15:21:55.97
    [AfterEach] [sig-node] PreStop
      test/e2e/framework/framework.go:187
    Aug 11 15:21:55.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "prestop-7847" for this suite. 08/11/23 15:21:55.984
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:21:55.991
Aug 11 15:21:55.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename deployment 08/11/23 15:21:55.993
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:21:56.005
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:21:56.008
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185
STEP: creating a Deployment 08/11/23 15:21:56.013
STEP: waiting for Deployment to be created 08/11/23 15:21:56.022
STEP: waiting for all Replicas to be Ready 08/11/23 15:21:56.024
Aug 11 15:21:56.025: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 15:21:56.025: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 15:21:56.033: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 15:21:56.033: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 15:21:56.047: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 15:21:56.047: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 15:21:56.076: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 15:21:56.076: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
Aug 11 15:21:57.245: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 11 15:21:57.245: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment-static:true]
Aug 11 15:21:57.513: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment-static:true]
STEP: patching the Deployment 08/11/23 15:21:57.513
W0811 15:21:57.523773      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 11 15:21:57.525: INFO: observed event type ADDED
STEP: waiting for Replicas to scale 08/11/23 15:21:57.525
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:57.535: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:57.535: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:57.557: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:57.557: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:57.568: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:21:57.568: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:21:57.574: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:21:57.574: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:21:58.257: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:58.257: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:21:58.277: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
STEP: listing Deployments 08/11/23 15:21:58.277
Aug 11 15:21:58.280: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
STEP: updating the Deployment 08/11/23 15:21:58.28
Aug 11 15:21:58.290: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
STEP: fetching the DeploymentStatus 08/11/23 15:21:58.29
Aug 11 15:21:58.299: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:21:58.302: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:21:58.321: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:21:58.341: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:21:58.354: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:21:59.268: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:21:59.282: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:21:59.290: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:21:59.298: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:21:59.306: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
Aug 11 15:22:00.581: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
STEP: patching the DeploymentStatus 08/11/23 15:22:00.605
STEP: fetching the DeploymentStatus 08/11/23 15:22:00.611
Aug 11 15:22:00.614: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:22:00.616: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:22:00.616: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:22:00.616: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
Aug 11 15:22:00.616: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 3
STEP: deleting the Deployment 08/11/23 15:22:00.616
Aug 11 15:22:00.625: INFO: observed event type MODIFIED
Aug 11 15:22:00.625: INFO: observed event type MODIFIED
Aug 11 15:22:00.625: INFO: observed event type MODIFIED
Aug 11 15:22:00.625: INFO: observed event type MODIFIED
Aug 11 15:22:00.626: INFO: observed event type MODIFIED
Aug 11 15:22:00.626: INFO: observed event type MODIFIED
Aug 11 15:22:00.626: INFO: observed event type MODIFIED
Aug 11 15:22:00.626: INFO: observed event type MODIFIED
Aug 11 15:22:00.626: INFO: observed event type MODIFIED
Aug 11 15:22:00.626: INFO: observed event type MODIFIED
Aug 11 15:22:00.626: INFO: observed event type MODIFIED
Aug 11 15:22:00.626: INFO: observed event type MODIFIED
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 15:22:00.630: INFO: Log out all the ReplicaSets if there is no deployment created
Aug 11 15:22:00.632: INFO: ReplicaSet "test-deployment-54cc775c4b":
&ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-74  451c376c-fccb-46c2-8fb1-f3d1dd2437e1 50045 4 2023-08-11 15:21:57 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7de4f23f-cf53-49e4-a3b9-cb0005d39559 0xc0034f87c7 0xc0034f87c8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:22:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7de4f23f-cf53-49e4-a3b9-cb0005d39559\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:22:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034f8850 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

Aug 11 15:22:00.637: INFO: pod: "test-deployment-54cc775c4b-tztxb":
&Pod{ObjectMeta:{test-deployment-54cc775c4b-tztxb test-deployment-54cc775c4b- deployment-74  95ac9f9a-6472-4b06-b28b-1ecf9e48a9b7 50041 0 2023-08-11 15:21:57 +0000 UTC 2023-08-11 15:22:01 +0000 UTC 0xc0034f8bb8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 451c376c-fccb-46c2-8fb1-f3d1dd2437e1 0xc0034f8be7 0xc0034f8be8}] [] [{kube-controller-manager Update v1 2023-08-11 15:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"451c376c-fccb-46c2-8fb1-f3d1dd2437e1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:21:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jh58b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jh58b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.150,StartTime:2023-08-11 15:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:21:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://8c1fdc63b37cad259429f65dfae0009836ff616d71cf3c1fc50f7c7ce02d473c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 11 15:22:00.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-74" for this suite. 08/11/23 15:22:00.644
{"msg":"PASSED [sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]","completed":277,"skipped":5160,"failed":0}
------------------------------
â€¢ [4.658 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should run the lifecycle of a Deployment [Conformance]
  test/e2e/apps/deployment.go:185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:21:55.991
    Aug 11 15:21:55.991: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename deployment 08/11/23 15:21:55.993
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:21:56.005
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:21:56.008
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should run the lifecycle of a Deployment [Conformance]
      test/e2e/apps/deployment.go:185
    STEP: creating a Deployment 08/11/23 15:21:56.013
    STEP: waiting for Deployment to be created 08/11/23 15:21:56.022
    STEP: waiting for all Replicas to be Ready 08/11/23 15:21:56.024
    Aug 11 15:21:56.025: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 15:21:56.025: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 15:21:56.033: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 15:21:56.033: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 15:21:56.047: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 15:21:56.047: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 15:21:56.076: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 15:21:56.076: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0 and labels map[test-deployment-static:true]
    Aug 11 15:21:57.245: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 11 15:21:57.245: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment-static:true]
    Aug 11 15:21:57.513: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment-static:true]
    STEP: patching the Deployment 08/11/23 15:21:57.513
    W0811 15:21:57.523773      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 11 15:21:57.525: INFO: observed event type ADDED
    STEP: waiting for Replicas to scale 08/11/23 15:21:57.525
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 0
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:57.527: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:57.535: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:57.535: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:57.557: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:57.557: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:57.568: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:21:57.568: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:21:57.574: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:21:57.574: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:21:58.257: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:58.257: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:21:58.277: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    STEP: listing Deployments 08/11/23 15:21:58.277
    Aug 11 15:21:58.280: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
    STEP: updating the Deployment 08/11/23 15:21:58.28
    Aug 11 15:21:58.290: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    STEP: fetching the DeploymentStatus 08/11/23 15:21:58.29
    Aug 11 15:21:58.299: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:21:58.302: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:21:58.321: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:21:58.341: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:21:58.354: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:21:59.268: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:21:59.282: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:21:59.290: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:21:59.298: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:21:59.306: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
    Aug 11 15:22:00.581: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
    STEP: patching the DeploymentStatus 08/11/23 15:22:00.605
    STEP: fetching the DeploymentStatus 08/11/23 15:22:00.611
    Aug 11 15:22:00.614: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 1
    Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:22:00.615: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:22:00.616: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:22:00.616: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:22:00.616: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 2
    Aug 11 15:22:00.616: INFO: observed Deployment test-deployment in namespace deployment-74 with ReadyReplicas 3
    STEP: deleting the Deployment 08/11/23 15:22:00.616
    Aug 11 15:22:00.625: INFO: observed event type MODIFIED
    Aug 11 15:22:00.625: INFO: observed event type MODIFIED
    Aug 11 15:22:00.625: INFO: observed event type MODIFIED
    Aug 11 15:22:00.625: INFO: observed event type MODIFIED
    Aug 11 15:22:00.626: INFO: observed event type MODIFIED
    Aug 11 15:22:00.626: INFO: observed event type MODIFIED
    Aug 11 15:22:00.626: INFO: observed event type MODIFIED
    Aug 11 15:22:00.626: INFO: observed event type MODIFIED
    Aug 11 15:22:00.626: INFO: observed event type MODIFIED
    Aug 11 15:22:00.626: INFO: observed event type MODIFIED
    Aug 11 15:22:00.626: INFO: observed event type MODIFIED
    Aug 11 15:22:00.626: INFO: observed event type MODIFIED
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 15:22:00.630: INFO: Log out all the ReplicaSets if there is no deployment created
    Aug 11 15:22:00.632: INFO: ReplicaSet "test-deployment-54cc775c4b":
    &ReplicaSet{ObjectMeta:{test-deployment-54cc775c4b  deployment-74  451c376c-fccb-46c2-8fb1-f3d1dd2437e1 50045 4 2023-08-11 15:21:57 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7de4f23f-cf53-49e4-a3b9-cb0005d39559 0xc0034f87c7 0xc0034f87c8}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:22:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7de4f23f-cf53-49e4-a3b9-cb0005d39559\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:22:00 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 54cc775c4b,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.8 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034f8850 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

    Aug 11 15:22:00.637: INFO: pod: "test-deployment-54cc775c4b-tztxb":
    &Pod{ObjectMeta:{test-deployment-54cc775c4b-tztxb test-deployment-54cc775c4b- deployment-74  95ac9f9a-6472-4b06-b28b-1ecf9e48a9b7 50041 0 2023-08-11 15:21:57 +0000 UTC 2023-08-11 15:22:01 +0000 UTC 0xc0034f8bb8 map[pod-template-hash:54cc775c4b test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-54cc775c4b 451c376c-fccb-46c2-8fb1-f3d1dd2437e1 0xc0034f8be7 0xc0034f8be8}] [] [{kube-controller-manager Update v1 2023-08-11 15:21:57 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"451c376c-fccb-46c2-8fb1-f3d1dd2437e1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:21:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.150\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jh58b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.8,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jh58b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:21:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:21:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:21:57 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.150,StartTime:2023-08-11 15:21:57 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:21:58 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.8,ImageID:registry.k8s.io/pause@sha256:9001185023633d17a2f98ff69b6ff2615b8ea02a825adffa40422f51dfdcde9d,ContainerID:containerd://8c1fdc63b37cad259429f65dfae0009836ff616d71cf3c1fc50f7c7ce02d473c,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.150,},},EphemeralContainerStatuses:[]ContainerStatus{},},}

    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 11 15:22:00.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-74" for this suite. 08/11/23 15:22:00.644
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:22:00.65
Aug 11 15:22:00.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-webhook 08/11/23 15:22:00.651
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:00.663
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:00.665
[BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:128
STEP: Setting up server cert 08/11/23 15:22:00.667
STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/11/23 15:22:00.886
STEP: Deploying the custom resource conversion webhook pod 08/11/23 15:22:00.894
STEP: Wait for the deployment to be ready 08/11/23 15:22:00.906
Aug 11 15:22:00.912: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:22:02.921
STEP: Verifying the service has paired with the endpoint 08/11/23 15:22:02.934
Aug 11 15:22:03.935: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
[It] should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184
Aug 11 15:22:03.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Creating a v1 custom resource 08/11/23 15:22:06.529
STEP: Create a v2 custom resource 08/11/23 15:22:06.545
STEP: List CRs in v1 08/11/23 15:22:06.594
STEP: List CRs in v2 08/11/23 15:22:06.601
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:22:07.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-webhook-5522" for this suite. 08/11/23 15:22:07.123
[AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/crd_conversion_webhook.go:139
{"msg":"PASSED [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]","completed":278,"skipped":5161,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.516 seconds]
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should be able to convert a non homogeneous list of CRs [Conformance]
  test/e2e/apimachinery/crd_conversion_webhook.go:184

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:22:00.65
    Aug 11 15:22:00.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-webhook 08/11/23 15:22:00.651
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:00.663
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:00.665
    [BeforeEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:128
    STEP: Setting up server cert 08/11/23 15:22:00.667
    STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication 08/11/23 15:22:00.886
    STEP: Deploying the custom resource conversion webhook pod 08/11/23 15:22:00.894
    STEP: Wait for the deployment to be ready 08/11/23 15:22:00.906
    Aug 11 15:22:00.912: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:22:02.921
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:22:02.934
    Aug 11 15:22:03.935: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
    [It] should be able to convert a non homogeneous list of CRs [Conformance]
      test/e2e/apimachinery/crd_conversion_webhook.go:184
    Aug 11 15:22:03.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Creating a v1 custom resource 08/11/23 15:22:06.529
    STEP: Create a v2 custom resource 08/11/23 15:22:06.545
    STEP: List CRs in v1 08/11/23 15:22:06.594
    STEP: List CRs in v2 08/11/23 15:22:06.601
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:22:07.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-webhook-5522" for this suite. 08/11/23 15:22:07.123
    [AfterEach] [sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/crd_conversion_webhook.go:139
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
[BeforeEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:22:07.169
Aug 11 15:22:07.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir 08/11/23 15:22:07.17
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:07.189
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:07.192
[It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196
STEP: Creating a pod to test emptydir 0644 on node default medium 08/11/23 15:22:07.196
Aug 11 15:22:07.205: INFO: Waiting up to 5m0s for pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851" in namespace "emptydir-5870" to be "Succeeded or Failed"
Aug 11 15:22:07.211: INFO: Pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851": Phase="Pending", Reason="", readiness=false. Elapsed: 5.239283ms
Aug 11 15:22:09.215: INFO: Pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009712929s
Aug 11 15:22:11.216: INFO: Pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010367836s
STEP: Saw pod success 08/11/23 15:22:11.216
Aug 11 15:22:11.216: INFO: Pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851" satisfied condition "Succeeded or Failed"
Aug 11 15:22:11.219: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-74ef7239-a703-40e8-9d03-83e6efbf4851 container test-container: <nil>
STEP: delete the pod 08/11/23 15:22:11.227
Aug 11 15:22:11.242: INFO: Waiting for pod pod-74ef7239-a703-40e8-9d03-83e6efbf4851 to disappear
Aug 11 15:22:11.244: INFO: Pod pod-74ef7239-a703-40e8-9d03-83e6efbf4851 no longer exists
[AfterEach] [sig-storage] EmptyDir volumes
  test/e2e/framework/framework.go:187
Aug 11 15:22:11.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-5870" for this suite. 08/11/23 15:22:11.248
{"msg":"PASSED [sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]","completed":279,"skipped":5229,"failed":0}
------------------------------
â€¢ [4.086 seconds]
[sig-storage] EmptyDir volumes
test/e2e/common/storage/framework.go:23
  should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/empty_dir.go:196

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:22:07.169
    Aug 11 15:22:07.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir 08/11/23 15:22:07.17
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:07.189
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:07.192
    [It] should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/empty_dir.go:196
    STEP: Creating a pod to test emptydir 0644 on node default medium 08/11/23 15:22:07.196
    Aug 11 15:22:07.205: INFO: Waiting up to 5m0s for pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851" in namespace "emptydir-5870" to be "Succeeded or Failed"
    Aug 11 15:22:07.211: INFO: Pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851": Phase="Pending", Reason="", readiness=false. Elapsed: 5.239283ms
    Aug 11 15:22:09.215: INFO: Pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009712929s
    Aug 11 15:22:11.216: INFO: Pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010367836s
    STEP: Saw pod success 08/11/23 15:22:11.216
    Aug 11 15:22:11.216: INFO: Pod "pod-74ef7239-a703-40e8-9d03-83e6efbf4851" satisfied condition "Succeeded or Failed"
    Aug 11 15:22:11.219: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-74ef7239-a703-40e8-9d03-83e6efbf4851 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:22:11.227
    Aug 11 15:22:11.242: INFO: Waiting for pod pod-74ef7239-a703-40e8-9d03-83e6efbf4851 to disappear
    Aug 11 15:22:11.244: INFO: Pod pod-74ef7239-a703-40e8-9d03-83e6efbf4851 no longer exists
    [AfterEach] [sig-storage] EmptyDir volumes
      test/e2e/framework/framework.go:187
    Aug 11 15:22:11.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-5870" for this suite. 08/11/23 15:22:11.248
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:22:11.257
Aug 11 15:22:11.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replicaset 08/11/23 15:22:11.258
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:11.27
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:11.273
[It] Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154
Aug 11 15:22:11.288: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 11 15:22:16.292: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 15:22:16.292
STEP: Scaling up "test-rs" replicaset  08/11/23 15:22:16.292
Aug 11 15:22:16.303: INFO: Updating replica set "test-rs"
STEP: patching the ReplicaSet 08/11/23 15:22:16.303
W0811 15:22:16.310107      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
Aug 11 15:22:16.312: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 1, AvailableReplicas 1
Aug 11 15:22:16.326: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 1, AvailableReplicas 1
Aug 11 15:22:16.344: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 1, AvailableReplicas 1
Aug 11 15:22:16.354: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 1, AvailableReplicas 1
Aug 11 15:22:17.303: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 2, AvailableReplicas 2
Aug 11 15:22:17.583: INFO: observed Replicaset test-rs in namespace replicaset-7003 with ReadyReplicas 3 found true
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 11 15:22:17.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-7003" for this suite. 08/11/23 15:22:17.587
{"msg":"PASSED [sig-apps] ReplicaSet Replace and Patch tests [Conformance]","completed":280,"skipped":5260,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.337 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replace and Patch tests [Conformance]
  test/e2e/apps/replica_set.go:154

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:22:11.257
    Aug 11 15:22:11.257: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replicaset 08/11/23 15:22:11.258
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:11.27
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:11.273
    [It] Replace and Patch tests [Conformance]
      test/e2e/apps/replica_set.go:154
    Aug 11 15:22:11.288: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 11 15:22:16.292: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 15:22:16.292
    STEP: Scaling up "test-rs" replicaset  08/11/23 15:22:16.292
    Aug 11 15:22:16.303: INFO: Updating replica set "test-rs"
    STEP: patching the ReplicaSet 08/11/23 15:22:16.303
    W0811 15:22:16.310107      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
    Aug 11 15:22:16.312: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 1, AvailableReplicas 1
    Aug 11 15:22:16.326: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 1, AvailableReplicas 1
    Aug 11 15:22:16.344: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 1, AvailableReplicas 1
    Aug 11 15:22:16.354: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 1, AvailableReplicas 1
    Aug 11 15:22:17.303: INFO: observed ReplicaSet test-rs in namespace replicaset-7003 with ReadyReplicas 2, AvailableReplicas 2
    Aug 11 15:22:17.583: INFO: observed Replicaset test-rs in namespace replicaset-7003 with ReadyReplicas 3 found true
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 11 15:22:17.583: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-7003" for this suite. 08/11/23 15:22:17.587
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:22:17.594
Aug 11 15:22:17.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:22:17.595
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:17.606
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:17.608
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404
STEP: creating a service externalname-service with the type=ExternalName in namespace services-4149 08/11/23 15:22:17.611
STEP: changing the ExternalName service to type=ClusterIP 08/11/23 15:22:17.615
STEP: creating replication controller externalname-service in namespace services-4149 08/11/23 15:22:17.632
I0811 15:22:17.638776      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4149, replica count: 2
I0811 15:22:20.690213      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 15:22:20.690: INFO: Creating new exec pod
Aug 11 15:22:20.695: INFO: Waiting up to 5m0s for pod "execpod8tlp2" in namespace "services-4149" to be "running"
Aug 11 15:22:20.699: INFO: Pod "execpod8tlp2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349774ms
Aug 11 15:22:22.704: INFO: Pod "execpod8tlp2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008171851s
Aug 11 15:22:22.704: INFO: Pod "execpod8tlp2" satisfied condition "running"
Aug 11 15:22:23.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4149 exec execpod8tlp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 11 15:22:23.866: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 11 15:22:23.866: INFO: stdout: ""
Aug 11 15:22:24.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4149 exec execpod8tlp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 11 15:22:25.015: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 11 15:22:25.015: INFO: stdout: ""
Aug 11 15:22:25.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4149 exec execpod8tlp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 11 15:22:26.000: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 11 15:22:26.000: INFO: stdout: "externalname-service-6n4kb"
Aug 11 15:22:26.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4149 exec execpod8tlp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.92.104 80'
Aug 11 15:22:26.142: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.92.104 80\nConnection to 10.107.92.104 80 port [tcp/http] succeeded!\n"
Aug 11 15:22:26.142: INFO: stdout: "externalname-service-6n4kb"
Aug 11 15:22:26.142: INFO: Cleaning up the ExternalName to ClusterIP test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:22:26.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4149" for this suite. 08/11/23 15:22:26.167
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]","completed":281,"skipped":5260,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.579 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to ClusterIP [Conformance]
  test/e2e/network/service.go:1404

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:22:17.594
    Aug 11 15:22:17.594: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:22:17.595
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:17.606
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:17.608
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to ClusterIP [Conformance]
      test/e2e/network/service.go:1404
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-4149 08/11/23 15:22:17.611
    STEP: changing the ExternalName service to type=ClusterIP 08/11/23 15:22:17.615
    STEP: creating replication controller externalname-service in namespace services-4149 08/11/23 15:22:17.632
    I0811 15:22:17.638776      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-4149, replica count: 2
    I0811 15:22:20.690213      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 15:22:20.690: INFO: Creating new exec pod
    Aug 11 15:22:20.695: INFO: Waiting up to 5m0s for pod "execpod8tlp2" in namespace "services-4149" to be "running"
    Aug 11 15:22:20.699: INFO: Pod "execpod8tlp2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.349774ms
    Aug 11 15:22:22.704: INFO: Pod "execpod8tlp2": Phase="Running", Reason="", readiness=true. Elapsed: 2.008171851s
    Aug 11 15:22:22.704: INFO: Pod "execpod8tlp2" satisfied condition "running"
    Aug 11 15:22:23.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4149 exec execpod8tlp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Aug 11 15:22:23.866: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 11 15:22:23.866: INFO: stdout: ""
    Aug 11 15:22:24.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4149 exec execpod8tlp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Aug 11 15:22:25.015: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 11 15:22:25.015: INFO: stdout: ""
    Aug 11 15:22:25.866: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4149 exec execpod8tlp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Aug 11 15:22:26.000: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 11 15:22:26.000: INFO: stdout: "externalname-service-6n4kb"
    Aug 11 15:22:26.000: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-4149 exec execpod8tlp2 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.92.104 80'
    Aug 11 15:22:26.142: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.92.104 80\nConnection to 10.107.92.104 80 port [tcp/http] succeeded!\n"
    Aug 11 15:22:26.142: INFO: stdout: "externalname-service-6n4kb"
    Aug 11 15:22:26.142: INFO: Cleaning up the ExternalName to ClusterIP test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:22:26.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4149" for this suite. 08/11/23 15:22:26.167
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:22:26.181
Aug 11 15:22:26.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 15:22:26.182
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:26.196
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:26.199
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844
STEP: Create set of pods 08/11/23 15:22:26.202
Aug 11 15:22:26.210: INFO: created test-pod-1
Aug 11 15:22:26.217: INFO: created test-pod-2
Aug 11 15:22:26.226: INFO: created test-pod-3
STEP: waiting for all 3 pods to be running 08/11/23 15:22:26.226
Aug 11 15:22:26.226: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9685' to be running and ready
Aug 11 15:22:26.244: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 11 15:22:26.244: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 11 15:22:26.244: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
Aug 11 15:22:26.244: INFO: 0 / 3 pods in namespace 'pods-9685' are running and ready (0 seconds elapsed)
Aug 11 15:22:26.244: INFO: expected 0 pod replicas in namespace 'pods-9685', 0 are Running and Ready.
Aug 11 15:22:26.244: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
Aug 11 15:22:26.244: INFO: test-pod-1  constell-d93e7e1d-worker-d314547c-0lc3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC  }]
Aug 11 15:22:26.244: INFO: test-pod-2  constell-d93e7e1d-worker-d314547c-0lc3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC  }]
Aug 11 15:22:26.244: INFO: test-pod-3  constell-d93e7e1d-worker-d314547c-0lc3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC  }]
Aug 11 15:22:26.244: INFO: 
Aug 11 15:22:28.252: INFO: 3 / 3 pods in namespace 'pods-9685' are running and ready (2 seconds elapsed)
Aug 11 15:22:28.252: INFO: expected 0 pod replicas in namespace 'pods-9685', 0 are Running and Ready.
STEP: waiting for all pods to be deleted 08/11/23 15:22:28.27
Aug 11 15:22:28.274: INFO: Pod quantity 3 is different from expected quantity 0
Aug 11 15:22:29.277: INFO: Pod quantity 3 is different from expected quantity 0
Aug 11 15:22:30.278: INFO: Pod quantity 3 is different from expected quantity 0
Aug 11 15:22:31.278: INFO: Pod quantity 1 is different from expected quantity 0
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 15:22:32.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-9685" for this suite. 08/11/23 15:22:32.281
{"msg":"PASSED [sig-node] Pods should delete a collection of pods [Conformance]","completed":282,"skipped":5367,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.105 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should delete a collection of pods [Conformance]
  test/e2e/common/node/pods.go:844

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:22:26.181
    Aug 11 15:22:26.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 15:22:26.182
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:26.196
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:26.199
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should delete a collection of pods [Conformance]
      test/e2e/common/node/pods.go:844
    STEP: Create set of pods 08/11/23 15:22:26.202
    Aug 11 15:22:26.210: INFO: created test-pod-1
    Aug 11 15:22:26.217: INFO: created test-pod-2
    Aug 11 15:22:26.226: INFO: created test-pod-3
    STEP: waiting for all 3 pods to be running 08/11/23 15:22:26.226
    Aug 11 15:22:26.226: INFO: Waiting up to 5m0s for all pods (need at least 3) in namespace 'pods-9685' to be running and ready
    Aug 11 15:22:26.244: INFO: The status of Pod test-pod-1 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 11 15:22:26.244: INFO: The status of Pod test-pod-2 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 11 15:22:26.244: INFO: The status of Pod test-pod-3 is Pending (Ready = false), waiting for it to be either Running (with Ready = true) or Failed
    Aug 11 15:22:26.244: INFO: 0 / 3 pods in namespace 'pods-9685' are running and ready (0 seconds elapsed)
    Aug 11 15:22:26.244: INFO: expected 0 pod replicas in namespace 'pods-9685', 0 are Running and Ready.
    Aug 11 15:22:26.244: INFO: POD         NODE                                    PHASE    GRACE  CONDITIONS
    Aug 11 15:22:26.244: INFO: test-pod-1  constell-d93e7e1d-worker-d314547c-0lc3  Pending         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC ContainersNotReady containers with unready status: [token-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC  }]
    Aug 11 15:22:26.244: INFO: test-pod-2  constell-d93e7e1d-worker-d314547c-0lc3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC  }]
    Aug 11 15:22:26.244: INFO: test-pod-3  constell-d93e7e1d-worker-d314547c-0lc3  Pending         [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-11 15:22:26 +0000 UTC  }]
    Aug 11 15:22:26.244: INFO: 
    Aug 11 15:22:28.252: INFO: 3 / 3 pods in namespace 'pods-9685' are running and ready (2 seconds elapsed)
    Aug 11 15:22:28.252: INFO: expected 0 pod replicas in namespace 'pods-9685', 0 are Running and Ready.
    STEP: waiting for all pods to be deleted 08/11/23 15:22:28.27
    Aug 11 15:22:28.274: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 11 15:22:29.277: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 11 15:22:30.278: INFO: Pod quantity 3 is different from expected quantity 0
    Aug 11 15:22:31.278: INFO: Pod quantity 1 is different from expected quantity 0
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 15:22:32.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-9685" for this suite. 08/11/23 15:22:32.281
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:22:32.287
Aug 11 15:22:32.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 15:22:32.288
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:32.299
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:32.302
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 15:22:32.318
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:22:32.492
STEP: Deploying the webhook pod 08/11/23 15:22:32.499
STEP: Wait for the deployment to be ready 08/11/23 15:22:32.511
Aug 11 15:22:32.523: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:22:34.533
STEP: Verifying the service has paired with the endpoint 08/11/23 15:22:34.546
Aug 11 15:22:35.547: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380
STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/11/23 15:22:35.55
STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 15:22:35.55
STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/11/23 15:22:35.572
STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/11/23 15:22:36.583
STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 15:22:36.583
STEP: Having no error when timeout is longer than webhook latency 08/11/23 15:22:37.615
STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 15:22:37.615
STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/11/23 15:22:42.658
STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 15:22:42.659
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:22:47.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-8412" for this suite. 08/11/23 15:22:47.695
STEP: Destroying namespace "webhook-8412-markers" for this suite. 08/11/23 15:22:47.701
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]","completed":283,"skipped":5390,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.454 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should honor timeout [Conformance]
  test/e2e/apimachinery/webhook.go:380

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:22:32.287
    Aug 11 15:22:32.287: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 15:22:32.288
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:32.299
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:32.302
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 15:22:32.318
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:22:32.492
    STEP: Deploying the webhook pod 08/11/23 15:22:32.499
    STEP: Wait for the deployment to be ready 08/11/23 15:22:32.511
    Aug 11 15:22:32.523: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:22:34.533
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:22:34.546
    Aug 11 15:22:35.547: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should honor timeout [Conformance]
      test/e2e/apimachinery/webhook.go:380
    STEP: Setting timeout (1s) shorter than webhook latency (5s) 08/11/23 15:22:35.55
    STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 15:22:35.55
    STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) 08/11/23 15:22:35.572
    STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore 08/11/23 15:22:36.583
    STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 15:22:36.583
    STEP: Having no error when timeout is longer than webhook latency 08/11/23 15:22:37.615
    STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 15:22:37.615
    STEP: Having no error when timeout is empty (defaulted to 10s in v1) 08/11/23 15:22:42.658
    STEP: Registering slow webhook via the AdmissionRegistration API 08/11/23 15:22:42.659
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:22:47.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-8412" for this suite. 08/11/23 15:22:47.695
    STEP: Destroying namespace "webhook-8412-markers" for this suite. 08/11/23 15:22:47.701
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:22:47.744
Aug 11 15:22:47.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replication-controller 08/11/23 15:22:47.745
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:47.761
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:47.764
[BeforeEach] [sig-apps] ReplicationController
  test/e2e/apps/rc.go:56
[It] should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91
STEP: Given a Pod with a 'name' label pod-adoption is created 08/11/23 15:22:47.767
Aug 11 15:22:47.773: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3325" to be "running and ready"
Aug 11 15:22:47.779: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.13362ms
Aug 11 15:22:47.779: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:22:49.782: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008919935s
Aug 11 15:22:49.782: INFO: The phase of Pod pod-adoption is Running (Ready = true)
Aug 11 15:22:49.782: INFO: Pod "pod-adoption" satisfied condition "running and ready"
STEP: When a replication controller with a matching selector is created 08/11/23 15:22:49.785
STEP: Then the orphan pod is adopted 08/11/23 15:22:49.79
[AfterEach] [sig-apps] ReplicationController
  test/e2e/framework/framework.go:187
Aug 11 15:22:50.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replication-controller-3325" for this suite. 08/11/23 15:22:50.799
{"msg":"PASSED [sig-apps] ReplicationController should adopt matching pods on creation [Conformance]","completed":284,"skipped":5395,"failed":0}
------------------------------
â€¢ [3.061 seconds]
[sig-apps] ReplicationController
test/e2e/apps/framework.go:23
  should adopt matching pods on creation [Conformance]
  test/e2e/apps/rc.go:91

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:22:47.744
    Aug 11 15:22:47.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replication-controller 08/11/23 15:22:47.745
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:47.761
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:47.764
    [BeforeEach] [sig-apps] ReplicationController
      test/e2e/apps/rc.go:56
    [It] should adopt matching pods on creation [Conformance]
      test/e2e/apps/rc.go:91
    STEP: Given a Pod with a 'name' label pod-adoption is created 08/11/23 15:22:47.767
    Aug 11 15:22:47.773: INFO: Waiting up to 5m0s for pod "pod-adoption" in namespace "replication-controller-3325" to be "running and ready"
    Aug 11 15:22:47.779: INFO: Pod "pod-adoption": Phase="Pending", Reason="", readiness=false. Elapsed: 5.13362ms
    Aug 11 15:22:47.779: INFO: The phase of Pod pod-adoption is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:22:49.782: INFO: Pod "pod-adoption": Phase="Running", Reason="", readiness=true. Elapsed: 2.008919935s
    Aug 11 15:22:49.782: INFO: The phase of Pod pod-adoption is Running (Ready = true)
    Aug 11 15:22:49.782: INFO: Pod "pod-adoption" satisfied condition "running and ready"
    STEP: When a replication controller with a matching selector is created 08/11/23 15:22:49.785
    STEP: Then the orphan pod is adopted 08/11/23 15:22:49.79
    [AfterEach] [sig-apps] ReplicationController
      test/e2e/framework/framework.go:187
    Aug 11 15:22:50.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replication-controller-3325" for this suite. 08/11/23 15:22:50.799
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:22:50.805
Aug 11 15:22:50.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename init-container 08/11/23 15:22:50.807
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:50.818
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:50.82
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333
STEP: creating the pod 08/11/23 15:22:50.822
Aug 11 15:22:50.822: INFO: PodSpec: initContainers in spec.initContainers
Aug 11 15:23:35.511: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-da91fb75-c821-4e5f-9bca-37105061f605", GenerateName:"", Namespace:"init-container-9916", SelfLink:"", UID:"e11f7804-b2fc-4ae3-a0f7-be8f3552ef40", ResourceVersion:"51253", Generation:0, CreationTimestamp:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"822773667"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004dfea08), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 11, 15, 23, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004dfea50), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-nsq6z", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0041b2480), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nsq6z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nsq6z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nsq6z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00454d3a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"constell-d93e7e1d-worker-d314547c-0lc3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0000d0d20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00454d420)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00454d440)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00454d448), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00454d44c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000d9da90), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.178.2", PodIP:"10.10.0.188", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.0.188"}}, StartTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000d0e00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000d0ee0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://899a140bd1f51782c26f0dc87be225f3cf93a449b03e4ddc7930954caccd632e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041b2500), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041b24e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00454d4cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 11 15:23:35.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9916" for this suite. 08/11/23 15:23:35.516
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]","completed":285,"skipped":5397,"failed":0}
------------------------------
â€¢ [SLOW TEST] [44.715 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should not start app containers if init containers fail on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:22:50.805
    Aug 11 15:22:50.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename init-container 08/11/23 15:22:50.807
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:22:50.818
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:22:50.82
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:333
    STEP: creating the pod 08/11/23 15:22:50.822
    Aug 11 15:22:50.822: INFO: PodSpec: initContainers in spec.initContainers
    Aug 11 15:23:35.511: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-da91fb75-c821-4e5f-9bca-37105061f605", GenerateName:"", Namespace:"init-container-9916", SelfLink:"", UID:"e11f7804-b2fc-4ae3-a0f7-be8f3552ef40", ResourceVersion:"51253", Generation:0, CreationTimestamp:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"822773667"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004dfea08), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 11, 15, 23, 35, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004dfea50), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-nsq6z", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0041b2480), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nsq6z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil)}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nsq6z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.8", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}}, VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-nsq6z", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc00454d3a8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"constell-d93e7e1d-worker-d314547c-0lc3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0000d0d20), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00454d420)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc00454d440)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc00454d448), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc00454d44c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000d9da90), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.178.2", PodIP:"10.10.0.188", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.10.0.188"}}, StartTime:time.Date(2023, time.August, 11, 15, 22, 50, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000d0e00)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0000d0ee0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:c318242786b139d18676b1c09a0ad7f15fc17f8f16a5b2e625cd0dc8c9703daf", ContainerID:"containerd://899a140bd1f51782c26f0dc87be225f3cf93a449b03e4ddc7930954caccd632e", Started:(*bool)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041b2500), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-2", ImageID:"", ContainerID:"", Started:(*bool)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0041b24e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.8", ImageID:"", ContainerID:"", Started:(*bool)(0xc00454d4cf)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil)}}
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 11 15:23:35.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9916" for this suite. 08/11/23 15:23:35.516
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:23:35.522
Aug 11 15:23:35.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:23:35.523
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:35.536
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:35.538
[It] updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390
STEP: set up a multi version CRD 08/11/23 15:23:35.541
Aug 11 15:23:35.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: rename a version 08/11/23 15:23:42.681
STEP: check the new version name is served 08/11/23 15:23:42.695
STEP: check the old version name is removed 08/11/23 15:23:45.932
STEP: check the other version is not changed 08/11/23 15:23:47.385
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:23:52.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-4869" for this suite. 08/11/23 15:23:52.996
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]","completed":286,"skipped":5407,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.482 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  updates the published spec when one version gets renamed [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:390

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:23:35.522
    Aug 11 15:23:35.522: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:23:35.523
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:35.536
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:35.538
    [It] updates the published spec when one version gets renamed [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:390
    STEP: set up a multi version CRD 08/11/23 15:23:35.541
    Aug 11 15:23:35.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: rename a version 08/11/23 15:23:42.681
    STEP: check the new version name is served 08/11/23 15:23:42.695
    STEP: check the old version name is removed 08/11/23 15:23:45.932
    STEP: check the other version is not changed 08/11/23 15:23:47.385
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:23:52.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-4869" for this suite. 08/11/23 15:23:52.996
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] Downward API volume
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:23:53.005
Aug 11 15:23:53.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 15:23:53.006
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:53.03
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:53.034
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:23:53.037
Aug 11 15:23:53.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0" in namespace "downward-api-2344" to be "Succeeded or Failed"
Aug 11 15:23:53.053: INFO: Pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.623516ms
Aug 11 15:23:55.058: INFO: Pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010921727s
Aug 11 15:23:57.058: INFO: Pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011333567s
STEP: Saw pod success 08/11/23 15:23:57.058
Aug 11 15:23:57.058: INFO: Pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0" satisfied condition "Succeeded or Failed"
Aug 11 15:23:57.061: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0 container client-container: <nil>
STEP: delete the pod 08/11/23 15:23:57.079
Aug 11 15:23:57.091: INFO: Waiting for pod downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0 to disappear
Aug 11 15:23:57.095: INFO: Pod downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 15:23:57.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2344" for this suite. 08/11/23 15:23:57.099
{"msg":"PASSED [sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]","completed":287,"skipped":5409,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:23:53.005
    Aug 11 15:23:53.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:23:53.006
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:53.03
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:53.034
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:67
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:23:53.037
    Aug 11 15:23:53.047: INFO: Waiting up to 5m0s for pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0" in namespace "downward-api-2344" to be "Succeeded or Failed"
    Aug 11 15:23:53.053: INFO: Pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 6.623516ms
    Aug 11 15:23:55.058: INFO: Pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010921727s
    Aug 11 15:23:57.058: INFO: Pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011333567s
    STEP: Saw pod success 08/11/23 15:23:57.058
    Aug 11 15:23:57.058: INFO: Pod "downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0" satisfied condition "Succeeded or Failed"
    Aug 11 15:23:57.061: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0 container client-container: <nil>
    STEP: delete the pod 08/11/23 15:23:57.079
    Aug 11 15:23:57.091: INFO: Waiting for pod downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0 to disappear
    Aug 11 15:23:57.095: INFO: Pod downwardapi-volume-5bd82c5b-f844-48ed-a4e5-d84223245ef0 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 15:23:57.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2344" for this suite. 08/11/23 15:23:57.099
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:23:57.108
Aug 11 15:23:57.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename events 08/11/23 15:23:57.109
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:57.125
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:57.128
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207
STEP: Create set of events 08/11/23 15:23:57.131
STEP: get a list of Events with a label in the current namespace 08/11/23 15:23:57.153
STEP: delete a list of events 08/11/23 15:23:57.158
Aug 11 15:23:57.159: INFO: requesting DeleteCollection of events
STEP: check that the list of events matches the requested quantity 08/11/23 15:23:57.182
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Aug 11 15:23:57.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-1642" for this suite. 08/11/23 15:23:57.19
{"msg":"PASSED [sig-instrumentation] Events API should delete a collection of events [Conformance]","completed":288,"skipped":5418,"failed":0}
------------------------------
â€¢ [0.090 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should delete a collection of events [Conformance]
  test/e2e/instrumentation/events.go:207

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:23:57.108
    Aug 11 15:23:57.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename events 08/11/23 15:23:57.109
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:57.125
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:57.128
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should delete a collection of events [Conformance]
      test/e2e/instrumentation/events.go:207
    STEP: Create set of events 08/11/23 15:23:57.131
    STEP: get a list of Events with a label in the current namespace 08/11/23 15:23:57.153
    STEP: delete a list of events 08/11/23 15:23:57.158
    Aug 11 15:23:57.159: INFO: requesting DeleteCollection of events
    STEP: check that the list of events matches the requested quantity 08/11/23 15:23:57.182
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Aug 11 15:23:57.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-1642" for this suite. 08/11/23 15:23:57.19
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:23:57.199
Aug 11 15:23:57.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename dns 08/11/23 15:23:57.2
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:57.217
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:57.22
[It] should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333
STEP: Creating a test externalName service 08/11/23 15:23:57.223
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
 08/11/23 15:23:57.228
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
 08/11/23 15:23:57.229
STEP: creating a pod to probe DNS 08/11/23 15:23:57.229
STEP: submitting the pod to kubernetes 08/11/23 15:23:57.229
Aug 11 15:23:57.239: INFO: Waiting up to 15m0s for pod "dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5" in namespace "dns-1950" to be "running"
Aug 11 15:23:57.245: INFO: Pod "dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209683ms
Aug 11 15:23:59.250: INFO: Pod "dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010565696s
Aug 11 15:23:59.250: INFO: Pod "dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:23:59.25
STEP: looking for the results for each expected name from probers 08/11/23 15:23:59.253
Aug 11 15:23:59.270: INFO: DNS probes using dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5 succeeded

STEP: deleting the pod 08/11/23 15:23:59.27
STEP: changing the externalName to bar.example.com 08/11/23 15:23:59.283
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
 08/11/23 15:23:59.293
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
 08/11/23 15:23:59.293
STEP: creating a second pod to probe DNS 08/11/23 15:23:59.293
STEP: submitting the pod to kubernetes 08/11/23 15:23:59.293
Aug 11 15:23:59.302: INFO: Waiting up to 15m0s for pod "dns-test-13853965-7305-4500-b40c-58ae864be540" in namespace "dns-1950" to be "running"
Aug 11 15:23:59.313: INFO: Pod "dns-test-13853965-7305-4500-b40c-58ae864be540": Phase="Pending", Reason="", readiness=false. Elapsed: 10.826327ms
Aug 11 15:24:01.318: INFO: Pod "dns-test-13853965-7305-4500-b40c-58ae864be540": Phase="Running", Reason="", readiness=true. Elapsed: 2.015648404s
Aug 11 15:24:01.318: INFO: Pod "dns-test-13853965-7305-4500-b40c-58ae864be540" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:24:01.318
STEP: looking for the results for each expected name from probers 08/11/23 15:24:01.321
Aug 11 15:24:01.333: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:01.340: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:01.340: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

Aug 11 15:24:06.352: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:06.360: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:06.360: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

Aug 11 15:24:11.352: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:11.360: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:11.360: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

Aug 11 15:24:16.348: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:16.355: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:16.355: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

Aug 11 15:24:21.350: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:21.357: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:21.357: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

Aug 11 15:24:26.352: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
' instead of 'bar.example.com.'
Aug 11 15:24:26.360: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains '' instead of 'bar.example.com.'
Aug 11 15:24:26.360: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

Aug 11 15:24:31.360: INFO: DNS probes using dns-test-13853965-7305-4500-b40c-58ae864be540 succeeded

STEP: deleting the pod 08/11/23 15:24:31.36
STEP: changing the service to type=ClusterIP 08/11/23 15:24:31.375
STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
 08/11/23 15:24:31.403
STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
 08/11/23 15:24:31.403
STEP: creating a third pod to probe DNS 08/11/23 15:24:31.403
STEP: submitting the pod to kubernetes 08/11/23 15:24:31.406
Aug 11 15:24:31.418: INFO: Waiting up to 15m0s for pod "dns-test-a6dbb601-8007-4e51-a691-f63031919dbe" in namespace "dns-1950" to be "running"
Aug 11 15:24:31.424: INFO: Pod "dns-test-a6dbb601-8007-4e51-a691-f63031919dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.446971ms
Aug 11 15:24:33.429: INFO: Pod "dns-test-a6dbb601-8007-4e51-a691-f63031919dbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.01167914s
Aug 11 15:24:33.429: INFO: Pod "dns-test-a6dbb601-8007-4e51-a691-f63031919dbe" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:24:33.429
STEP: looking for the results for each expected name from probers 08/11/23 15:24:33.433
Aug 11 15:24:33.451: INFO: DNS probes using dns-test-a6dbb601-8007-4e51-a691-f63031919dbe succeeded

STEP: deleting the pod 08/11/23 15:24:33.451
STEP: deleting the test externalName service 08/11/23 15:24:33.464
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 11 15:24:33.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-1950" for this suite. 08/11/23 15:24:33.494
{"msg":"PASSED [sig-network] DNS should provide DNS for ExternalName services [Conformance]","completed":289,"skipped":5425,"failed":0}
------------------------------
â€¢ [SLOW TEST] [36.302 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide DNS for ExternalName services [Conformance]
  test/e2e/network/dns.go:333

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:23:57.199
    Aug 11 15:23:57.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename dns 08/11/23 15:23:57.2
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:23:57.217
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:23:57.22
    [It] should provide DNS for ExternalName services [Conformance]
      test/e2e/network/dns.go:333
    STEP: Creating a test externalName service 08/11/23 15:23:57.223
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
     08/11/23 15:23:57.228
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
     08/11/23 15:23:57.229
    STEP: creating a pod to probe DNS 08/11/23 15:23:57.229
    STEP: submitting the pod to kubernetes 08/11/23 15:23:57.229
    Aug 11 15:23:57.239: INFO: Waiting up to 15m0s for pod "dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5" in namespace "dns-1950" to be "running"
    Aug 11 15:23:57.245: INFO: Pod "dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.209683ms
    Aug 11 15:23:59.250: INFO: Pod "dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5": Phase="Running", Reason="", readiness=true. Elapsed: 2.010565696s
    Aug 11 15:23:59.250: INFO: Pod "dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:23:59.25
    STEP: looking for the results for each expected name from probers 08/11/23 15:23:59.253
    Aug 11 15:23:59.270: INFO: DNS probes using dns-test-99fd93cc-d281-45f2-9d36-6f3f6edcdfa5 succeeded

    STEP: deleting the pod 08/11/23 15:23:59.27
    STEP: changing the externalName to bar.example.com 08/11/23 15:23:59.283
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
     08/11/23 15:23:59.293
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
     08/11/23 15:23:59.293
    STEP: creating a second pod to probe DNS 08/11/23 15:23:59.293
    STEP: submitting the pod to kubernetes 08/11/23 15:23:59.293
    Aug 11 15:23:59.302: INFO: Waiting up to 15m0s for pod "dns-test-13853965-7305-4500-b40c-58ae864be540" in namespace "dns-1950" to be "running"
    Aug 11 15:23:59.313: INFO: Pod "dns-test-13853965-7305-4500-b40c-58ae864be540": Phase="Pending", Reason="", readiness=false. Elapsed: 10.826327ms
    Aug 11 15:24:01.318: INFO: Pod "dns-test-13853965-7305-4500-b40c-58ae864be540": Phase="Running", Reason="", readiness=true. Elapsed: 2.015648404s
    Aug 11 15:24:01.318: INFO: Pod "dns-test-13853965-7305-4500-b40c-58ae864be540" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:24:01.318
    STEP: looking for the results for each expected name from probers 08/11/23 15:24:01.321
    Aug 11 15:24:01.333: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:01.340: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:01.340: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

    Aug 11 15:24:06.352: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:06.360: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:06.360: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

    Aug 11 15:24:11.352: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:11.360: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:11.360: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

    Aug 11 15:24:16.348: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:16.355: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:16.355: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

    Aug 11 15:24:21.350: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:21.357: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:21.357: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

    Aug 11 15:24:26.352: INFO: File wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains 'foo.example.com.
    ' instead of 'bar.example.com.'
    Aug 11 15:24:26.360: INFO: File jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local from pod  dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 contains '' instead of 'bar.example.com.'
    Aug 11 15:24:26.360: INFO: Lookups using dns-1950/dns-test-13853965-7305-4500-b40c-58ae864be540 failed for: [wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local]

    Aug 11 15:24:31.360: INFO: DNS probes using dns-test-13853965-7305-4500-b40c-58ae864be540 succeeded

    STEP: deleting the pod 08/11/23 15:24:31.36
    STEP: changing the service to type=ClusterIP 08/11/23 15:24:31.375
    STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
     08/11/23 15:24:31.403
    STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1950.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1950.svc.cluster.local; sleep 1; done
     08/11/23 15:24:31.403
    STEP: creating a third pod to probe DNS 08/11/23 15:24:31.403
    STEP: submitting the pod to kubernetes 08/11/23 15:24:31.406
    Aug 11 15:24:31.418: INFO: Waiting up to 15m0s for pod "dns-test-a6dbb601-8007-4e51-a691-f63031919dbe" in namespace "dns-1950" to be "running"
    Aug 11 15:24:31.424: INFO: Pod "dns-test-a6dbb601-8007-4e51-a691-f63031919dbe": Phase="Pending", Reason="", readiness=false. Elapsed: 6.446971ms
    Aug 11 15:24:33.429: INFO: Pod "dns-test-a6dbb601-8007-4e51-a691-f63031919dbe": Phase="Running", Reason="", readiness=true. Elapsed: 2.01167914s
    Aug 11 15:24:33.429: INFO: Pod "dns-test-a6dbb601-8007-4e51-a691-f63031919dbe" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:24:33.429
    STEP: looking for the results for each expected name from probers 08/11/23 15:24:33.433
    Aug 11 15:24:33.451: INFO: DNS probes using dns-test-a6dbb601-8007-4e51-a691-f63031919dbe succeeded

    STEP: deleting the pod 08/11/23 15:24:33.451
    STEP: deleting the test externalName service 08/11/23 15:24:33.464
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 11 15:24:33.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-1950" for this suite. 08/11/23 15:24:33.494
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance]
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:24:33.503
Aug 11 15:24:33.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename init-container 08/11/23 15:24:33.504
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:33.523
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:33.526
[BeforeEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/common/node/init_container.go:164
[It] should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254
STEP: creating the pod 08/11/23 15:24:33.529
Aug 11 15:24:33.529: INFO: PodSpec: initContainers in spec.initContainers
[AfterEach] [sig-node] InitContainer [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 11 15:24:36.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "init-container-9664" for this suite. 08/11/23 15:24:36.669
{"msg":"PASSED [sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]","completed":290,"skipped":5439,"failed":0}
------------------------------
â€¢ [3.173 seconds]
[sig-node] InitContainer [NodeConformance]
test/e2e/common/node/framework.go:23
  should invoke init containers on a RestartAlways pod [Conformance]
  test/e2e/common/node/init_container.go:254

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:24:33.503
    Aug 11 15:24:33.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename init-container 08/11/23 15:24:33.504
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:33.523
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:33.526
    [BeforeEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/common/node/init_container.go:164
    [It] should invoke init containers on a RestartAlways pod [Conformance]
      test/e2e/common/node/init_container.go:254
    STEP: creating the pod 08/11/23 15:24:33.529
    Aug 11 15:24:33.529: INFO: PodSpec: initContainers in spec.initContainers
    [AfterEach] [sig-node] InitContainer [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 11 15:24:36.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "init-container-9664" for this suite. 08/11/23 15:24:36.669
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:24:36.677
Aug 11 15:24:36.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename security-context 08/11/23 15:24:36.679
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:36.695
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:36.699
[It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132
STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/11/23 15:24:36.702
Aug 11 15:24:36.712: INFO: Waiting up to 5m0s for pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899" in namespace "security-context-6564" to be "Succeeded or Failed"
Aug 11 15:24:36.716: INFO: Pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292983ms
Aug 11 15:24:38.721: INFO: Pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008531551s
Aug 11 15:24:40.722: INFO: Pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009543549s
STEP: Saw pod success 08/11/23 15:24:40.722
Aug 11 15:24:40.722: INFO: Pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899" satisfied condition "Succeeded or Failed"
Aug 11 15:24:40.726: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod security-context-4235ca8c-a472-49b8-bde3-f785afe78899 container test-container: <nil>
STEP: delete the pod 08/11/23 15:24:40.736
Aug 11 15:24:40.751: INFO: Waiting for pod security-context-4235ca8c-a472-49b8-bde3-f785afe78899 to disappear
Aug 11 15:24:40.754: INFO: Pod security-context-4235ca8c-a472-49b8-bde3-f785afe78899 no longer exists
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 11 15:24:40.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-6564" for this suite. 08/11/23 15:24:40.758
{"msg":"PASSED [sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]","completed":291,"skipped":5442,"failed":0}
------------------------------
â€¢ [4.087 seconds]
[sig-node] Security Context
test/e2e/node/framework.go:23
  should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
  test/e2e/node/security_context.go:132

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:24:36.677
    Aug 11 15:24:36.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename security-context 08/11/23 15:24:36.679
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:36.695
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:36.699
    [It] should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
      test/e2e/node/security_context.go:132
    STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser 08/11/23 15:24:36.702
    Aug 11 15:24:36.712: INFO: Waiting up to 5m0s for pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899" in namespace "security-context-6564" to be "Succeeded or Failed"
    Aug 11 15:24:36.716: INFO: Pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899": Phase="Pending", Reason="", readiness=false. Elapsed: 4.292983ms
    Aug 11 15:24:38.721: INFO: Pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008531551s
    Aug 11 15:24:40.722: INFO: Pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009543549s
    STEP: Saw pod success 08/11/23 15:24:40.722
    Aug 11 15:24:40.722: INFO: Pod "security-context-4235ca8c-a472-49b8-bde3-f785afe78899" satisfied condition "Succeeded or Failed"
    Aug 11 15:24:40.726: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod security-context-4235ca8c-a472-49b8-bde3-f785afe78899 container test-container: <nil>
    STEP: delete the pod 08/11/23 15:24:40.736
    Aug 11 15:24:40.751: INFO: Waiting for pod security-context-4235ca8c-a472-49b8-bde3-f785afe78899 to disappear
    Aug 11 15:24:40.754: INFO: Pod security-context-4235ca8c-a472-49b8-bde3-f785afe78899 no longer exists
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 11 15:24:40.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-6564" for this suite. 08/11/23 15:24:40.758
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:24:40.768
Aug 11 15:24:40.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replicaset 08/11/23 15:24:40.769
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:40.786
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:40.789
[It] should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165
STEP: Create a ReplicaSet 08/11/23 15:24:40.792
STEP: Verify that the required pods have come up 08/11/23 15:24:40.798
Aug 11 15:24:40.801: INFO: Pod name sample-pod: Found 0 pods out of 3
Aug 11 15:24:45.806: INFO: Pod name sample-pod: Found 3 pods out of 3
STEP: ensuring each pod is running 08/11/23 15:24:45.806
Aug 11 15:24:45.811: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
STEP: Listing all ReplicaSets 08/11/23 15:24:45.811
STEP: DeleteCollection of the ReplicaSets 08/11/23 15:24:45.815
STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/11/23 15:24:45.823
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 11 15:24:45.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-6788" for this suite. 08/11/23 15:24:45.837
{"msg":"PASSED [sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]","completed":292,"skipped":5498,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.082 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should list and delete a collection of ReplicaSets [Conformance]
  test/e2e/apps/replica_set.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:24:40.768
    Aug 11 15:24:40.768: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replicaset 08/11/23 15:24:40.769
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:40.786
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:40.789
    [It] should list and delete a collection of ReplicaSets [Conformance]
      test/e2e/apps/replica_set.go:165
    STEP: Create a ReplicaSet 08/11/23 15:24:40.792
    STEP: Verify that the required pods have come up 08/11/23 15:24:40.798
    Aug 11 15:24:40.801: INFO: Pod name sample-pod: Found 0 pods out of 3
    Aug 11 15:24:45.806: INFO: Pod name sample-pod: Found 3 pods out of 3
    STEP: ensuring each pod is running 08/11/23 15:24:45.806
    Aug 11 15:24:45.811: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
    STEP: Listing all ReplicaSets 08/11/23 15:24:45.811
    STEP: DeleteCollection of the ReplicaSets 08/11/23 15:24:45.815
    STEP: After DeleteCollection verify that ReplicaSets have been deleted 08/11/23 15:24:45.823
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 11 15:24:45.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-6788" for this suite. 08/11/23 15:24:45.837
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label
  should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:24:45.851
Aug 11 15:24:45.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 15:24:45.852
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:45.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:45.878
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[BeforeEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1492
STEP: creating the pod 08/11/23 15:24:45.881
Aug 11 15:24:45.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 create -f -'
Aug 11 15:24:46.402: INFO: stderr: ""
Aug 11 15:24:46.402: INFO: stdout: "pod/pause created\n"
Aug 11 15:24:46.402: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
Aug 11 15:24:46.402: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3183" to be "running and ready"
Aug 11 15:24:46.407: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741598ms
Aug 11 15:24:46.407: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'constell-d93e7e1d-worker-d314547c-0lc3' to be 'Running' but was 'Pending'
Aug 11 15:24:48.412: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010058561s
Aug 11 15:24:48.412: INFO: Pod "pause" satisfied condition "running and ready"
Aug 11 15:24:48.412: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
[It] should update the label on a resource  [Conformance]
  test/e2e/kubectl/kubectl.go:1507
STEP: adding the label testing-label with value testing-label-value to a pod 08/11/23 15:24:48.412
Aug 11 15:24:48.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 label pods pause testing-label=testing-label-value'
Aug 11 15:24:48.481: INFO: stderr: ""
Aug 11 15:24:48.481: INFO: stdout: "pod/pause labeled\n"
STEP: verifying the pod has the label testing-label with the value testing-label-value 08/11/23 15:24:48.481
Aug 11 15:24:48.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 get pod pause -L testing-label'
Aug 11 15:24:48.537: INFO: stderr: ""
Aug 11 15:24:48.538: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
STEP: removing the label testing-label of a pod 08/11/23 15:24:48.538
Aug 11 15:24:48.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 label pods pause testing-label-'
Aug 11 15:24:48.605: INFO: stderr: ""
Aug 11 15:24:48.605: INFO: stdout: "pod/pause unlabeled\n"
STEP: verifying the pod doesn't have the label testing-label 08/11/23 15:24:48.605
Aug 11 15:24:48.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 get pod pause -L testing-label'
Aug 11 15:24:48.660: INFO: stderr: ""
Aug 11 15:24:48.660: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
[AfterEach] Kubectl label
  test/e2e/kubectl/kubectl.go:1498
STEP: using delete to clean up resources 08/11/23 15:24:48.66
Aug 11 15:24:48.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 delete --grace-period=0 --force -f -'
Aug 11 15:24:48.736: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
Aug 11 15:24:48.736: INFO: stdout: "pod \"pause\" force deleted\n"
Aug 11 15:24:48.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 get rc,svc -l name=pause --no-headers'
Aug 11 15:24:48.800: INFO: stderr: "No resources found in kubectl-3183 namespace.\n"
Aug 11 15:24:48.800: INFO: stdout: ""
Aug 11 15:24:48.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
Aug 11 15:24:48.859: INFO: stderr: ""
Aug 11 15:24:48.859: INFO: stdout: ""
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 15:24:48.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-3183" for this suite. 08/11/23 15:24:48.865
{"msg":"PASSED [sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]","completed":293,"skipped":5517,"failed":0}
------------------------------
â€¢ [3.022 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl label
  test/e2e/kubectl/kubectl.go:1490
    should update the label on a resource  [Conformance]
    test/e2e/kubectl/kubectl.go:1507

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:24:45.851
    Aug 11 15:24:45.851: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:24:45.852
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:45.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:45.878
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [BeforeEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1492
    STEP: creating the pod 08/11/23 15:24:45.881
    Aug 11 15:24:45.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 create -f -'
    Aug 11 15:24:46.402: INFO: stderr: ""
    Aug 11 15:24:46.402: INFO: stdout: "pod/pause created\n"
    Aug 11 15:24:46.402: INFO: Waiting up to 5m0s for 1 pods to be running and ready: [pause]
    Aug 11 15:24:46.402: INFO: Waiting up to 5m0s for pod "pause" in namespace "kubectl-3183" to be "running and ready"
    Aug 11 15:24:46.407: INFO: Pod "pause": Phase="Pending", Reason="", readiness=false. Elapsed: 4.741598ms
    Aug 11 15:24:46.407: INFO: Error evaluating pod condition running and ready: want pod 'pause' on 'constell-d93e7e1d-worker-d314547c-0lc3' to be 'Running' but was 'Pending'
    Aug 11 15:24:48.412: INFO: Pod "pause": Phase="Running", Reason="", readiness=true. Elapsed: 2.010058561s
    Aug 11 15:24:48.412: INFO: Pod "pause" satisfied condition "running and ready"
    Aug 11 15:24:48.412: INFO: Wanted all 1 pods to be running and ready. Result: true. Pods: [pause]
    [It] should update the label on a resource  [Conformance]
      test/e2e/kubectl/kubectl.go:1507
    STEP: adding the label testing-label with value testing-label-value to a pod 08/11/23 15:24:48.412
    Aug 11 15:24:48.412: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 label pods pause testing-label=testing-label-value'
    Aug 11 15:24:48.481: INFO: stderr: ""
    Aug 11 15:24:48.481: INFO: stdout: "pod/pause labeled\n"
    STEP: verifying the pod has the label testing-label with the value testing-label-value 08/11/23 15:24:48.481
    Aug 11 15:24:48.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 get pod pause -L testing-label'
    Aug 11 15:24:48.537: INFO: stderr: ""
    Aug 11 15:24:48.538: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
    STEP: removing the label testing-label of a pod 08/11/23 15:24:48.538
    Aug 11 15:24:48.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 label pods pause testing-label-'
    Aug 11 15:24:48.605: INFO: stderr: ""
    Aug 11 15:24:48.605: INFO: stdout: "pod/pause unlabeled\n"
    STEP: verifying the pod doesn't have the label testing-label 08/11/23 15:24:48.605
    Aug 11 15:24:48.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 get pod pause -L testing-label'
    Aug 11 15:24:48.660: INFO: stderr: ""
    Aug 11 15:24:48.660: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
    [AfterEach] Kubectl label
      test/e2e/kubectl/kubectl.go:1498
    STEP: using delete to clean up resources 08/11/23 15:24:48.66
    Aug 11 15:24:48.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 delete --grace-period=0 --force -f -'
    Aug 11 15:24:48.736: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
    Aug 11 15:24:48.736: INFO: stdout: "pod \"pause\" force deleted\n"
    Aug 11 15:24:48.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 get rc,svc -l name=pause --no-headers'
    Aug 11 15:24:48.800: INFO: stderr: "No resources found in kubectl-3183 namespace.\n"
    Aug 11 15:24:48.800: INFO: stdout: ""
    Aug 11 15:24:48.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-3183 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
    Aug 11 15:24:48.859: INFO: stderr: ""
    Aug 11 15:24:48.859: INFO: stdout: ""
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 15:24:48.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-3183" for this suite. 08/11/23 15:24:48.865
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events API
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:24:48.874
Aug 11 15:24:48.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename events 08/11/23 15:24:48.875
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:48.892
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:48.895
[BeforeEach] [sig-instrumentation] Events API
  test/e2e/instrumentation/events.go:84
[It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98
STEP: creating a test event 08/11/23 15:24:48.898
STEP: listing events in all namespaces 08/11/23 15:24:48.905
STEP: listing events in test namespace 08/11/23 15:24:48.909
STEP: listing events with field selection filtering on source 08/11/23 15:24:48.912
STEP: listing events with field selection filtering on reportingController 08/11/23 15:24:48.915
STEP: getting the test event 08/11/23 15:24:48.918
STEP: patching the test event 08/11/23 15:24:48.923
STEP: getting the test event 08/11/23 15:24:48.933
STEP: updating the test event 08/11/23 15:24:48.936
STEP: getting the test event 08/11/23 15:24:48.946
STEP: deleting the test event 08/11/23 15:24:48.95
STEP: listing events in all namespaces 08/11/23 15:24:48.959
STEP: listing events in test namespace 08/11/23 15:24:48.964
[AfterEach] [sig-instrumentation] Events API
  test/e2e/framework/framework.go:187
Aug 11 15:24:48.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "events-9965" for this suite. 08/11/23 15:24:48.972
{"msg":"PASSED [sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]","completed":294,"skipped":5524,"failed":0}
------------------------------
â€¢ [0.106 seconds]
[sig-instrumentation] Events API
test/e2e/instrumentation/common/framework.go:23
  should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
  test/e2e/instrumentation/events.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:24:48.874
    Aug 11 15:24:48.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename events 08/11/23 15:24:48.875
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:48.892
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:48.895
    [BeforeEach] [sig-instrumentation] Events API
      test/e2e/instrumentation/events.go:84
    [It] should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
      test/e2e/instrumentation/events.go:98
    STEP: creating a test event 08/11/23 15:24:48.898
    STEP: listing events in all namespaces 08/11/23 15:24:48.905
    STEP: listing events in test namespace 08/11/23 15:24:48.909
    STEP: listing events with field selection filtering on source 08/11/23 15:24:48.912
    STEP: listing events with field selection filtering on reportingController 08/11/23 15:24:48.915
    STEP: getting the test event 08/11/23 15:24:48.918
    STEP: patching the test event 08/11/23 15:24:48.923
    STEP: getting the test event 08/11/23 15:24:48.933
    STEP: updating the test event 08/11/23 15:24:48.936
    STEP: getting the test event 08/11/23 15:24:48.946
    STEP: deleting the test event 08/11/23 15:24:48.95
    STEP: listing events in all namespaces 08/11/23 15:24:48.959
    STEP: listing events in test namespace 08/11/23 15:24:48.964
    [AfterEach] [sig-instrumentation] Events API
      test/e2e/framework/framework.go:187
    Aug 11 15:24:48.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "events-9965" for this suite. 08/11/23 15:24:48.972
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:24:48.982
Aug 11 15:24:48.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename statefulset 08/11/23 15:24:48.983
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:48.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:49.003
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-8343 08/11/23 15:24:49.006
[It] should validate Statefulset Status endpoints [Conformance]
  test/e2e/apps/statefulset.go:975
STEP: Creating statefulset ss in namespace statefulset-8343 08/11/23 15:24:49.019
Aug 11 15:24:49.034: INFO: Found 0 stateful pods, waiting for 1
Aug 11 15:24:59.039: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Patch Statefulset to include a label 08/11/23 15:24:59.045
STEP: Getting /status 08/11/23 15:24:59.051
Aug 11 15:24:59.055: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
STEP: updating the StatefulSet Status 08/11/23 15:24:59.055
Aug 11 15:24:59.064: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the statefulset status to be updated 08/11/23 15:24:59.064
Aug 11 15:24:59.066: INFO: Observed &StatefulSet event: ADDED
Aug 11 15:24:59.066: INFO: Found Statefulset ss in namespace statefulset-8343 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 15:24:59.066: INFO: Statefulset ss has an updated status
STEP: patching the Statefulset Status 08/11/23 15:24:59.066
Aug 11 15:24:59.066: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 11 15:24:59.073: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Statefulset status to be patched 08/11/23 15:24:59.073
Aug 11 15:24:59.075: INFO: Observed &StatefulSet event: ADDED
Aug 11 15:24:59.075: INFO: Observed Statefulset ss in namespace statefulset-8343 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 15:24:59.075: INFO: Observed &StatefulSet event: MODIFIED
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 11 15:24:59.075: INFO: Deleting all statefulset in ns statefulset-8343
Aug 11 15:24:59.078: INFO: Scaling statefulset ss to 0
Aug 11 15:25:09.097: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:25:09.100: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 11 15:25:09.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-8343" for this suite. 08/11/23 15:25:09.121
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]","completed":295,"skipped":5562,"failed":0}
------------------------------
â€¢ [SLOW TEST] [20.146 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should validate Statefulset Status endpoints [Conformance]
    test/e2e/apps/statefulset.go:975

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:24:48.982
    Aug 11 15:24:48.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename statefulset 08/11/23 15:24:48.983
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:24:48.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:24:49.003
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-8343 08/11/23 15:24:49.006
    [It] should validate Statefulset Status endpoints [Conformance]
      test/e2e/apps/statefulset.go:975
    STEP: Creating statefulset ss in namespace statefulset-8343 08/11/23 15:24:49.019
    Aug 11 15:24:49.034: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 15:24:59.039: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Patch Statefulset to include a label 08/11/23 15:24:59.045
    STEP: Getting /status 08/11/23 15:24:59.051
    Aug 11 15:24:59.055: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
    STEP: updating the StatefulSet Status 08/11/23 15:24:59.055
    Aug 11 15:24:59.064: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the statefulset status to be updated 08/11/23 15:24:59.064
    Aug 11 15:24:59.066: INFO: Observed &StatefulSet event: ADDED
    Aug 11 15:24:59.066: INFO: Found Statefulset ss in namespace statefulset-8343 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 15:24:59.066: INFO: Statefulset ss has an updated status
    STEP: patching the Statefulset Status 08/11/23 15:24:59.066
    Aug 11 15:24:59.066: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 11 15:24:59.073: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Statefulset status to be patched 08/11/23 15:24:59.073
    Aug 11 15:24:59.075: INFO: Observed &StatefulSet event: ADDED
    Aug 11 15:24:59.075: INFO: Observed Statefulset ss in namespace statefulset-8343 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 15:24:59.075: INFO: Observed &StatefulSet event: MODIFIED
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 11 15:24:59.075: INFO: Deleting all statefulset in ns statefulset-8343
    Aug 11 15:24:59.078: INFO: Scaling statefulset ss to 0
    Aug 11 15:25:09.097: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:25:09.100: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 11 15:25:09.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-8343" for this suite. 08/11/23 15:25:09.121
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:09.13
Aug 11 15:25:09.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:25:09.131
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:09.147
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:09.151
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185
STEP: fetching services 08/11/23 15:25:09.153
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:25:09.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-8460" for this suite. 08/11/23 15:25:09.162
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should find a service from listing all namespaces [Conformance]","completed":296,"skipped":5586,"failed":0}
------------------------------
â€¢ [0.039 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should find a service from listing all namespaces [Conformance]
  test/e2e/network/service.go:3185

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:09.13
    Aug 11 15:25:09.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:25:09.131
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:09.147
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:09.151
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should find a service from listing all namespaces [Conformance]
      test/e2e/network/service.go:3185
    STEP: fetching services 08/11/23 15:25:09.153
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:25:09.158: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-8460" for this suite. 08/11/23 15:25:09.162
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] Job
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
[BeforeEach] [sig-apps] Job
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:09.169
Aug 11 15:25:09.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename job 08/11/23 15:25:09.17
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:09.186
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:09.189
[It] should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335
STEP: Creating a job 08/11/23 15:25:09.193
STEP: Ensuring active pods == parallelism 08/11/23 15:25:09.198
STEP: Orphaning one of the Job's Pods 08/11/23 15:25:11.204
Aug 11 15:25:11.722: INFO: Successfully updated pod "adopt-release-2fzcz"
STEP: Checking that the Job readopts the Pod 08/11/23 15:25:11.722
Aug 11 15:25:11.722: INFO: Waiting up to 15m0s for pod "adopt-release-2fzcz" in namespace "job-1305" to be "adopted"
Aug 11 15:25:11.725: INFO: Pod "adopt-release-2fzcz": Phase="Running", Reason="", readiness=true. Elapsed: 3.048625ms
Aug 11 15:25:13.731: INFO: Pod "adopt-release-2fzcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.008841142s
Aug 11 15:25:13.731: INFO: Pod "adopt-release-2fzcz" satisfied condition "adopted"
STEP: Removing the labels from the Job's Pod 08/11/23 15:25:13.731
Aug 11 15:25:14.244: INFO: Successfully updated pod "adopt-release-2fzcz"
STEP: Checking that the Job releases the Pod 08/11/23 15:25:14.244
Aug 11 15:25:14.244: INFO: Waiting up to 15m0s for pod "adopt-release-2fzcz" in namespace "job-1305" to be "released"
Aug 11 15:25:14.247: INFO: Pod "adopt-release-2fzcz": Phase="Running", Reason="", readiness=true. Elapsed: 3.52438ms
Aug 11 15:25:16.252: INFO: Pod "adopt-release-2fzcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.008623355s
Aug 11 15:25:16.252: INFO: Pod "adopt-release-2fzcz" satisfied condition "released"
[AfterEach] [sig-apps] Job
  test/e2e/framework/framework.go:187
Aug 11 15:25:16.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "job-1305" for this suite. 08/11/23 15:25:16.257
{"msg":"PASSED [sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]","completed":297,"skipped":5586,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.096 seconds]
[sig-apps] Job
test/e2e/apps/framework.go:23
  should adopt matching orphans and release non-matching pods [Conformance]
  test/e2e/apps/job.go:335

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Job
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:09.169
    Aug 11 15:25:09.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename job 08/11/23 15:25:09.17
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:09.186
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:09.189
    [It] should adopt matching orphans and release non-matching pods [Conformance]
      test/e2e/apps/job.go:335
    STEP: Creating a job 08/11/23 15:25:09.193
    STEP: Ensuring active pods == parallelism 08/11/23 15:25:09.198
    STEP: Orphaning one of the Job's Pods 08/11/23 15:25:11.204
    Aug 11 15:25:11.722: INFO: Successfully updated pod "adopt-release-2fzcz"
    STEP: Checking that the Job readopts the Pod 08/11/23 15:25:11.722
    Aug 11 15:25:11.722: INFO: Waiting up to 15m0s for pod "adopt-release-2fzcz" in namespace "job-1305" to be "adopted"
    Aug 11 15:25:11.725: INFO: Pod "adopt-release-2fzcz": Phase="Running", Reason="", readiness=true. Elapsed: 3.048625ms
    Aug 11 15:25:13.731: INFO: Pod "adopt-release-2fzcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.008841142s
    Aug 11 15:25:13.731: INFO: Pod "adopt-release-2fzcz" satisfied condition "adopted"
    STEP: Removing the labels from the Job's Pod 08/11/23 15:25:13.731
    Aug 11 15:25:14.244: INFO: Successfully updated pod "adopt-release-2fzcz"
    STEP: Checking that the Job releases the Pod 08/11/23 15:25:14.244
    Aug 11 15:25:14.244: INFO: Waiting up to 15m0s for pod "adopt-release-2fzcz" in namespace "job-1305" to be "released"
    Aug 11 15:25:14.247: INFO: Pod "adopt-release-2fzcz": Phase="Running", Reason="", readiness=true. Elapsed: 3.52438ms
    Aug 11 15:25:16.252: INFO: Pod "adopt-release-2fzcz": Phase="Running", Reason="", readiness=true. Elapsed: 2.008623355s
    Aug 11 15:25:16.252: INFO: Pod "adopt-release-2fzcz" satisfied condition "released"
    [AfterEach] [sig-apps] Job
      test/e2e/framework/framework.go:187
    Aug 11 15:25:16.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "job-1305" for this suite. 08/11/23 15:25:16.257
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:16.268
Aug 11 15:25:16.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 15:25:16.268
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:16.285
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:16.287
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161
STEP: Creating the pod 08/11/23 15:25:16.29
Aug 11 15:25:16.300: INFO: Waiting up to 5m0s for pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd" in namespace "downward-api-5893" to be "running and ready"
Aug 11 15:25:16.306: INFO: Pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.781849ms
Aug 11 15:25:16.306: INFO: The phase of Pod annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:25:18.311: INFO: Pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010758091s
Aug 11 15:25:18.311: INFO: The phase of Pod annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd is Running (Ready = true)
Aug 11 15:25:18.311: INFO: Pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd" satisfied condition "running and ready"
Aug 11 15:25:18.837: INFO: Successfully updated pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 15:25:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-5893" for this suite. 08/11/23 15:25:20.86
{"msg":"PASSED [sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]","completed":298,"skipped":5646,"failed":0}
------------------------------
â€¢ [4.600 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update annotations on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:161

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:16.268
    Aug 11 15:25:16.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:25:16.268
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:16.285
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:16.287
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update annotations on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:161
    STEP: Creating the pod 08/11/23 15:25:16.29
    Aug 11 15:25:16.300: INFO: Waiting up to 5m0s for pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd" in namespace "downward-api-5893" to be "running and ready"
    Aug 11 15:25:16.306: INFO: Pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd": Phase="Pending", Reason="", readiness=false. Elapsed: 5.781849ms
    Aug 11 15:25:16.306: INFO: The phase of Pod annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:25:18.311: INFO: Pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd": Phase="Running", Reason="", readiness=true. Elapsed: 2.010758091s
    Aug 11 15:25:18.311: INFO: The phase of Pod annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd is Running (Ready = true)
    Aug 11 15:25:18.311: INFO: Pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd" satisfied condition "running and ready"
    Aug 11 15:25:18.837: INFO: Successfully updated pod "annotationupdate9bb02f06-8932-4679-9330-d3723054e9dd"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 15:25:20.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-5893" for this suite. 08/11/23 15:25:20.86
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:20.87
Aug 11 15:25:20.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:25:20.871
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:20.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:20.891
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443
STEP: creating a service externalname-service with the type=ExternalName in namespace services-5111 08/11/23 15:25:20.894
STEP: changing the ExternalName service to type=NodePort 08/11/23 15:25:20.901
STEP: creating replication controller externalname-service in namespace services-5111 08/11/23 15:25:20.927
I0811 15:25:20.937045      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5111, replica count: 2
I0811 15:25:23.989055      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 15:25:23.989: INFO: Creating new exec pod
Aug 11 15:25:23.999: INFO: Waiting up to 5m0s for pod "execpodwpjkm" in namespace "services-5111" to be "running"
Aug 11 15:25:24.007: INFO: Pod "execpodwpjkm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.657209ms
Aug 11 15:25:26.013: INFO: Pod "execpodwpjkm": Phase="Running", Reason="", readiness=true. Elapsed: 2.01393489s
Aug 11 15:25:26.013: INFO: Pod "execpodwpjkm" satisfied condition "running"
Aug 11 15:25:27.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5111 exec execpodwpjkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
Aug 11 15:25:27.172: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
Aug 11 15:25:27.172: INFO: stdout: "externalname-service-l8d99"
Aug 11 15:25:27.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5111 exec execpodwpjkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.254.74 80'
Aug 11 15:25:27.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.254.74 80\nConnection to 10.99.254.74 80 port [tcp/http] succeeded!\n"
Aug 11 15:25:27.309: INFO: stdout: "externalname-service-l8d99"
Aug 11 15:25:27.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5111 exec execpodwpjkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.2 30207'
Aug 11 15:25:27.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.2 30207\nConnection to 192.168.178.2 30207 port [tcp/*] succeeded!\n"
Aug 11 15:25:27.455: INFO: stdout: "externalname-service-k5zj2"
Aug 11 15:25:27.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5111 exec execpodwpjkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.3 30207'
Aug 11 15:25:27.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.3 30207\nConnection to 192.168.178.3 30207 port [tcp/*] succeeded!\n"
Aug 11 15:25:27.596: INFO: stdout: "externalname-service-l8d99"
Aug 11 15:25:27.596: INFO: Cleaning up the ExternalName to NodePort test service
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:25:27.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5111" for this suite. 08/11/23 15:25:27.631
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]","completed":299,"skipped":5672,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.770 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to change the type from ExternalName to NodePort [Conformance]
  test/e2e/network/service.go:1443

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:20.87
    Aug 11 15:25:20.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:25:20.871
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:20.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:20.891
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to change the type from ExternalName to NodePort [Conformance]
      test/e2e/network/service.go:1443
    STEP: creating a service externalname-service with the type=ExternalName in namespace services-5111 08/11/23 15:25:20.894
    STEP: changing the ExternalName service to type=NodePort 08/11/23 15:25:20.901
    STEP: creating replication controller externalname-service in namespace services-5111 08/11/23 15:25:20.927
    I0811 15:25:20.937045      22 runners.go:193] Created replication controller with name: externalname-service, namespace: services-5111, replica count: 2
    I0811 15:25:23.989055      22 runners.go:193] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 15:25:23.989: INFO: Creating new exec pod
    Aug 11 15:25:23.999: INFO: Waiting up to 5m0s for pod "execpodwpjkm" in namespace "services-5111" to be "running"
    Aug 11 15:25:24.007: INFO: Pod "execpodwpjkm": Phase="Pending", Reason="", readiness=false. Elapsed: 8.657209ms
    Aug 11 15:25:26.013: INFO: Pod "execpodwpjkm": Phase="Running", Reason="", readiness=true. Elapsed: 2.01393489s
    Aug 11 15:25:26.013: INFO: Pod "execpodwpjkm" satisfied condition "running"
    Aug 11 15:25:27.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5111 exec execpodwpjkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
    Aug 11 15:25:27.172: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
    Aug 11 15:25:27.172: INFO: stdout: "externalname-service-l8d99"
    Aug 11 15:25:27.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5111 exec execpodwpjkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.254.74 80'
    Aug 11 15:25:27.309: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.254.74 80\nConnection to 10.99.254.74 80 port [tcp/http] succeeded!\n"
    Aug 11 15:25:27.309: INFO: stdout: "externalname-service-l8d99"
    Aug 11 15:25:27.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5111 exec execpodwpjkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.2 30207'
    Aug 11 15:25:27.455: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.2 30207\nConnection to 192.168.178.2 30207 port [tcp/*] succeeded!\n"
    Aug 11 15:25:27.455: INFO: stdout: "externalname-service-k5zj2"
    Aug 11 15:25:27.455: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-5111 exec execpodwpjkm -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.178.3 30207'
    Aug 11 15:25:27.596: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.178.3 30207\nConnection to 192.168.178.3 30207 port [tcp/*] succeeded!\n"
    Aug 11 15:25:27.596: INFO: stdout: "externalname-service-l8d99"
    Aug 11 15:25:27.596: INFO: Cleaning up the ExternalName to NodePort test service
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:25:27.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5111" for this suite. 08/11/23 15:25:27.631
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
[sig-network] Services
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:27.64
Aug 11 15:25:27.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:25:27.642
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:27.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:27.662
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210
STEP: creating an Endpoint 08/11/23 15:25:27.669
STEP: waiting for available Endpoint 08/11/23 15:25:27.676
STEP: listing all Endpoints 08/11/23 15:25:27.677
STEP: updating the Endpoint 08/11/23 15:25:27.681
STEP: fetching the Endpoint 08/11/23 15:25:27.689
STEP: patching the Endpoint 08/11/23 15:25:27.692
STEP: fetching the Endpoint 08/11/23 15:25:27.701
STEP: deleting the Endpoint by Collection 08/11/23 15:25:27.704
STEP: waiting for Endpoint deletion 08/11/23 15:25:27.713
STEP: fetching the Endpoint 08/11/23 15:25:27.715
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:25:27.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-5828" for this suite. 08/11/23 15:25:27.722
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should test the lifecycle of an Endpoint [Conformance]","completed":300,"skipped":5672,"failed":0}
------------------------------
â€¢ [0.089 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should test the lifecycle of an Endpoint [Conformance]
  test/e2e/network/service.go:3210

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:27.64
    Aug 11 15:25:27.641: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:25:27.642
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:27.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:27.662
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should test the lifecycle of an Endpoint [Conformance]
      test/e2e/network/service.go:3210
    STEP: creating an Endpoint 08/11/23 15:25:27.669
    STEP: waiting for available Endpoint 08/11/23 15:25:27.676
    STEP: listing all Endpoints 08/11/23 15:25:27.677
    STEP: updating the Endpoint 08/11/23 15:25:27.681
    STEP: fetching the Endpoint 08/11/23 15:25:27.689
    STEP: patching the Endpoint 08/11/23 15:25:27.692
    STEP: fetching the Endpoint 08/11/23 15:25:27.701
    STEP: deleting the Endpoint by Collection 08/11/23 15:25:27.704
    STEP: waiting for Endpoint deletion 08/11/23 15:25:27.713
    STEP: fetching the Endpoint 08/11/23 15:25:27.715
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:25:27.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-5828" for this suite. 08/11/23 15:25:27.722
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSlice
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:27.73
Aug 11 15:25:27.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename endpointslice 08/11/23 15:25:27.731
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:27.746
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:27.749
[BeforeEach] [sig-network] EndpointSlice
  test/e2e/network/endpointslice.go:51
[It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101
[AfterEach] [sig-network] EndpointSlice
  test/e2e/framework/framework.go:187
Aug 11 15:25:27.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "endpointslice-6899" for this suite. 08/11/23 15:25:27.81
{"msg":"PASSED [sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]","completed":301,"skipped":5679,"failed":0}
------------------------------
â€¢ [0.087 seconds]
[sig-network] EndpointSlice
test/e2e/network/common/framework.go:23
  should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
  test/e2e/network/endpointslice.go:101

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:27.73
    Aug 11 15:25:27.730: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename endpointslice 08/11/23 15:25:27.731
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:27.746
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:27.749
    [BeforeEach] [sig-network] EndpointSlice
      test/e2e/network/endpointslice.go:51
    [It] should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
      test/e2e/network/endpointslice.go:101
    [AfterEach] [sig-network] EndpointSlice
      test/e2e/framework/framework.go:187
    Aug 11 15:25:27.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "endpointslice-6899" for this suite. 08/11/23 15:25:27.81
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Containers
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
[BeforeEach] [sig-node] Containers
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:27.817
Aug 11 15:25:27.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename containers 08/11/23 15:25:27.818
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:27.834
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:27.837
[It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86
STEP: Creating a pod to test override all 08/11/23 15:25:27.84
Aug 11 15:25:27.852: INFO: Waiting up to 5m0s for pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4" in namespace "containers-9240" to be "Succeeded or Failed"
Aug 11 15:25:27.855: INFO: Pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.706385ms
Aug 11 15:25:29.862: INFO: Pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010289257s
Aug 11 15:25:31.859: INFO: Pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00758159s
STEP: Saw pod success 08/11/23 15:25:31.859
Aug 11 15:25:31.859: INFO: Pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4" satisfied condition "Succeeded or Failed"
Aug 11 15:25:31.862: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 15:25:31.871
Aug 11 15:25:31.886: INFO: Waiting for pod client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4 to disappear
Aug 11 15:25:31.889: INFO: Pod client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4 no longer exists
[AfterEach] [sig-node] Containers
  test/e2e/framework/framework.go:187
Aug 11 15:25:31.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "containers-9240" for this suite. 08/11/23 15:25:31.894
{"msg":"PASSED [sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]","completed":302,"skipped":5684,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-node] Containers
test/e2e/common/node/framework.go:23
  should be able to override the image's default command and arguments [NodeConformance] [Conformance]
  test/e2e/common/node/containers.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Containers
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:27.817
    Aug 11 15:25:27.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename containers 08/11/23 15:25:27.818
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:27.834
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:27.837
    [It] should be able to override the image's default command and arguments [NodeConformance] [Conformance]
      test/e2e/common/node/containers.go:86
    STEP: Creating a pod to test override all 08/11/23 15:25:27.84
    Aug 11 15:25:27.852: INFO: Waiting up to 5m0s for pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4" in namespace "containers-9240" to be "Succeeded or Failed"
    Aug 11 15:25:27.855: INFO: Pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 3.706385ms
    Aug 11 15:25:29.862: INFO: Pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010289257s
    Aug 11 15:25:31.859: INFO: Pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.00758159s
    STEP: Saw pod success 08/11/23 15:25:31.859
    Aug 11 15:25:31.859: INFO: Pod "client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4" satisfied condition "Succeeded or Failed"
    Aug 11 15:25:31.862: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 15:25:31.871
    Aug 11 15:25:31.886: INFO: Waiting for pod client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4 to disappear
    Aug 11 15:25:31.889: INFO: Pod client-containers-8705ed83-953c-4ce8-b0a4-4715a75f5ae4 no longer exists
    [AfterEach] [sig-node] Containers
      test/e2e/framework/framework.go:187
    Aug 11 15:25:31.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "containers-9240" for this suite. 08/11/23 15:25:31.894
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
[BeforeEach] [sig-network] HostPort
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:31.903
Aug 11 15:25:31.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename hostport 08/11/23 15:25:31.904
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:31.923
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:31.927
[BeforeEach] [sig-network] HostPort
  test/e2e/network/hostport.go:49
[It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63
STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/11/23 15:25:31.935
Aug 11 15:25:31.945: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8773" to be "running and ready"
Aug 11 15:25:31.948: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.314994ms
Aug 11 15:25:31.948: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:25:33.953: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008082878s
Aug 11 15:25:33.953: INFO: The phase of Pod pod1 is Running (Ready = true)
Aug 11 15:25:33.953: INFO: Pod "pod1" satisfied condition "running and ready"
STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.178.2 on the node which pod1 resides and expect scheduled 08/11/23 15:25:33.953
Aug 11 15:25:33.960: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8773" to be "running and ready"
Aug 11 15:25:33.964: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.392316ms
Aug 11 15:25:33.964: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:25:35.968: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007626834s
Aug 11 15:25:35.968: INFO: The phase of Pod pod2 is Running (Ready = true)
Aug 11 15:25:35.968: INFO: Pod "pod2" satisfied condition "running and ready"
STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.178.2 but use UDP protocol on the node which pod2 resides 08/11/23 15:25:35.968
Aug 11 15:25:35.974: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8773" to be "running and ready"
Aug 11 15:25:35.977: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997193ms
Aug 11 15:25:35.977: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:25:37.981: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007197291s
Aug 11 15:25:37.981: INFO: The phase of Pod pod3 is Running (Ready = true)
Aug 11 15:25:37.981: INFO: Pod "pod3" satisfied condition "running and ready"
Aug 11 15:25:37.987: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8773" to be "running and ready"
Aug 11 15:25:37.991: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133708ms
Aug 11 15:25:37.991: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:25:39.996: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008747149s
Aug 11 15:25:39.996: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
Aug 11 15:25:39.996: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/11/23 15:25:39.999
Aug 11 15:25:39.999: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.178.2 http://127.0.0.1:54323/hostname] Namespace:hostport-8773 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:25:39.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 15:25:40.000: INFO: ExecWithOptions: Clientset creation
Aug 11 15:25:40.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8773/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.178.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.178.2, port: 54323 08/11/23 15:25:40.078
Aug 11 15:25:40.079: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.178.2:54323/hostname] Namespace:hostport-8773 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:25:40.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 15:25:40.079: INFO: ExecWithOptions: Clientset creation
Aug 11 15:25:40.079: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8773/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.178.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.178.2, port: 54323 UDP 08/11/23 15:25:40.151
Aug 11 15:25:40.151: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.178.2 54323] Namespace:hostport-8773 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:25:40.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 15:25:40.152: INFO: ExecWithOptions: Clientset creation
Aug 11 15:25:40.152: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8773/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.178.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
[AfterEach] [sig-network] HostPort
  test/e2e/framework/framework.go:187
Aug 11 15:25:45.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "hostport-8773" for this suite. 08/11/23 15:25:45.243
{"msg":"PASSED [sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]","completed":303,"skipped":5703,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.349 seconds]
[sig-network] HostPort
test/e2e/network/common/framework.go:23
  validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
  test/e2e/network/hostport.go:63

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] HostPort
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:31.903
    Aug 11 15:25:31.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename hostport 08/11/23 15:25:31.904
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:31.923
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:31.927
    [BeforeEach] [sig-network] HostPort
      test/e2e/network/hostport.go:49
    [It] validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
      test/e2e/network/hostport.go:63
    STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled 08/11/23 15:25:31.935
    Aug 11 15:25:31.945: INFO: Waiting up to 5m0s for pod "pod1" in namespace "hostport-8773" to be "running and ready"
    Aug 11 15:25:31.948: INFO: Pod "pod1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.314994ms
    Aug 11 15:25:31.948: INFO: The phase of Pod pod1 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:25:33.953: INFO: Pod "pod1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008082878s
    Aug 11 15:25:33.953: INFO: The phase of Pod pod1 is Running (Ready = true)
    Aug 11 15:25:33.953: INFO: Pod "pod1" satisfied condition "running and ready"
    STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.178.2 on the node which pod1 resides and expect scheduled 08/11/23 15:25:33.953
    Aug 11 15:25:33.960: INFO: Waiting up to 5m0s for pod "pod2" in namespace "hostport-8773" to be "running and ready"
    Aug 11 15:25:33.964: INFO: Pod "pod2": Phase="Pending", Reason="", readiness=false. Elapsed: 3.392316ms
    Aug 11 15:25:33.964: INFO: The phase of Pod pod2 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:25:35.968: INFO: Pod "pod2": Phase="Running", Reason="", readiness=true. Elapsed: 2.007626834s
    Aug 11 15:25:35.968: INFO: The phase of Pod pod2 is Running (Ready = true)
    Aug 11 15:25:35.968: INFO: Pod "pod2" satisfied condition "running and ready"
    STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.178.2 but use UDP protocol on the node which pod2 resides 08/11/23 15:25:35.968
    Aug 11 15:25:35.974: INFO: Waiting up to 5m0s for pod "pod3" in namespace "hostport-8773" to be "running and ready"
    Aug 11 15:25:35.977: INFO: Pod "pod3": Phase="Pending", Reason="", readiness=false. Elapsed: 2.997193ms
    Aug 11 15:25:35.977: INFO: The phase of Pod pod3 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:25:37.981: INFO: Pod "pod3": Phase="Running", Reason="", readiness=true. Elapsed: 2.007197291s
    Aug 11 15:25:37.981: INFO: The phase of Pod pod3 is Running (Ready = true)
    Aug 11 15:25:37.981: INFO: Pod "pod3" satisfied condition "running and ready"
    Aug 11 15:25:37.987: INFO: Waiting up to 5m0s for pod "e2e-host-exec" in namespace "hostport-8773" to be "running and ready"
    Aug 11 15:25:37.991: INFO: Pod "e2e-host-exec": Phase="Pending", Reason="", readiness=false. Elapsed: 3.133708ms
    Aug 11 15:25:37.991: INFO: The phase of Pod e2e-host-exec is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:25:39.996: INFO: Pod "e2e-host-exec": Phase="Running", Reason="", readiness=true. Elapsed: 2.008747149s
    Aug 11 15:25:39.996: INFO: The phase of Pod e2e-host-exec is Running (Ready = true)
    Aug 11 15:25:39.996: INFO: Pod "e2e-host-exec" satisfied condition "running and ready"
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 08/11/23 15:25:39.999
    Aug 11 15:25:39.999: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.178.2 http://127.0.0.1:54323/hostname] Namespace:hostport-8773 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:25:39.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 15:25:40.000: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:25:40.000: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8773/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.178.2+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.178.2, port: 54323 08/11/23 15:25:40.078
    Aug 11 15:25:40.079: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.178.2:54323/hostname] Namespace:hostport-8773 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:25:40.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 15:25:40.079: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:25:40.079: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8773/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.178.2%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.178.2, port: 54323 UDP 08/11/23 15:25:40.151
    Aug 11 15:25:40.151: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.178.2 54323] Namespace:hostport-8773 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:25:40.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 15:25:40.152: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:25:40.152: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-8773/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.178.2+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
    [AfterEach] [sig-network] HostPort
      test/e2e/framework/framework.go:187
    Aug 11 15:25:45.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "hostport-8773" for this suite. 08/11/23 15:25:45.243
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:45.253
Aug 11 15:25:45.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-pred 08/11/23 15:25:45.254
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:45.269
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:45.272
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 11 15:25:45.275: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 11 15:25:45.283: INFO: Waiting for terminating namespaces to be deleted...
Aug 11 15:25:45.286: INFO: 
Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-0lc3 before test
Aug 11 15:25:45.294: INFO: e2e-host-exec from hostport-8773 started at 2023-08-11 15:25:37 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container e2e-host-exec ready: true, restart count 0
Aug 11 15:25:45.294: INFO: pod1 from hostport-8773 started at 2023-08-11 15:25:31 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container agnhost ready: true, restart count 0
Aug 11 15:25:45.294: INFO: pod2 from hostport-8773 started at 2023-08-11 15:25:33 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container agnhost ready: true, restart count 0
Aug 11 15:25:45.294: INFO: pod3 from hostport-8773 started at 2023-08-11 15:25:35 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container agnhost ready: true, restart count 0
Aug 11 15:25:45.294: INFO: adopt-release-2fzcz from job-1305 started at 2023-08-11 15:25:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container c ready: true, restart count 0
Aug 11 15:25:45.294: INFO: adopt-release-6gjsw from job-1305 started at 2023-08-11 15:25:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container c ready: true, restart count 0
Aug 11 15:25:45.294: INFO: adopt-release-tdzkf from job-1305 started at 2023-08-11 15:25:15 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container c ready: true, restart count 0
Aug 11 15:25:45.294: INFO: cilium-6s7tr from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 15:25:45.294: INFO: cilium-operator-5bfff4c47c-92tfw from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 11 15:25:45.294: INFO: csi-gce-pd-node-dsdbs from kube-system started at 2023-08-11 13:55:05 +0000 UTC (2 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 15:25:45.294: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 15:25:45.294: INFO: gcp-guest-agent-dmz5f from kube-system started at 2023-08-11 14:41:37 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 15:25:45.294: INFO: konnectivity-agent-6lwgs from kube-system started at 2023-08-11 14:41:38 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 15:25:45.294: INFO: kube-proxy-vbf6p from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 15:25:45.294: INFO: verification-service-ttpg4 from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 15:25:45.294: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 15:25:45.294: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:25:45.294: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 15:25:45.294: INFO: 
Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-wzlp before test
Aug 11 15:25:45.303: INFO: cilium-dcbmm from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 15:25:45.303: INFO: coredns-6c49cf4575-56cmb from kube-system started at 2023-08-11 14:41:36 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container coredns ready: true, restart count 0
Aug 11 15:25:45.303: INFO: csi-gce-pd-node-tnkjk from kube-system started at 2023-08-11 13:55:09 +0000 UTC (2 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 15:25:45.303: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 15:25:45.303: INFO: gcp-guest-agent-7fj57 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 15:25:45.303: INFO: konnectivity-agent-bwlfc from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 15:25:45.303: INFO: kube-proxy-9wx2l from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 15:25:45.303: INFO: verification-service-xmjlz from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 15:25:45.303: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:01:59 +0000 UTC (1 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 11 15:25:45.303: INFO: sonobuoy-e2e-job-2008a9ff359d4340 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container e2e ready: true, restart count 0
Aug 11 15:25:45.303: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:25:45.303: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 15:25:45.303: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:25:45.303: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438
STEP: Trying to schedule Pod with nonempty NodeSelector. 08/11/23 15:25:45.303
STEP: Considering event: 
Type = [Warning], Name = [restricted-pod.177a5d9e788683d8], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 08/11/23 15:25:45.341
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Aug 11 15:25:46.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-5058" for this suite. 08/11/23 15:25:46.344
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]","completed":304,"skipped":5717,"failed":0}
------------------------------
â€¢ [1.099 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if not matching  [Conformance]
  test/e2e/scheduling/predicates.go:438

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:45.253
    Aug 11 15:25:45.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-pred 08/11/23 15:25:45.254
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:45.269
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:45.272
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Aug 11 15:25:45.275: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 11 15:25:45.283: INFO: Waiting for terminating namespaces to be deleted...
    Aug 11 15:25:45.286: INFO: 
    Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-0lc3 before test
    Aug 11 15:25:45.294: INFO: e2e-host-exec from hostport-8773 started at 2023-08-11 15:25:37 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container e2e-host-exec ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: pod1 from hostport-8773 started at 2023-08-11 15:25:31 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container agnhost ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: pod2 from hostport-8773 started at 2023-08-11 15:25:33 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container agnhost ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: pod3 from hostport-8773 started at 2023-08-11 15:25:35 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container agnhost ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: adopt-release-2fzcz from job-1305 started at 2023-08-11 15:25:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container c ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: adopt-release-6gjsw from job-1305 started at 2023-08-11 15:25:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container c ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: adopt-release-tdzkf from job-1305 started at 2023-08-11 15:25:15 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container c ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: cilium-6s7tr from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 15:25:45.294: INFO: cilium-operator-5bfff4c47c-92tfw from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: csi-gce-pd-node-dsdbs from kube-system started at 2023-08-11 13:55:05 +0000 UTC (2 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: gcp-guest-agent-dmz5f from kube-system started at 2023-08-11 14:41:37 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: konnectivity-agent-6lwgs from kube-system started at 2023-08-11 14:41:38 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: kube-proxy-vbf6p from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: verification-service-ttpg4 from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 15:25:45.294: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 15:25:45.294: INFO: 
    Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-wzlp before test
    Aug 11 15:25:45.303: INFO: cilium-dcbmm from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 15:25:45.303: INFO: coredns-6c49cf4575-56cmb from kube-system started at 2023-08-11 14:41:36 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container coredns ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: csi-gce-pd-node-tnkjk from kube-system started at 2023-08-11 13:55:09 +0000 UTC (2 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: gcp-guest-agent-7fj57 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: konnectivity-agent-bwlfc from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: kube-proxy-9wx2l from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: verification-service-xmjlz from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:01:59 +0000 UTC (1 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: sonobuoy-e2e-job-2008a9ff359d4340 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container e2e ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 15:25:45.303: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:25:45.303: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if not matching  [Conformance]
      test/e2e/scheduling/predicates.go:438
    STEP: Trying to schedule Pod with nonempty NodeSelector. 08/11/23 15:25:45.303
    STEP: Considering event: 
    Type = [Warning], Name = [restricted-pod.177a5d9e788683d8], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 5 Preemption is not helpful for scheduling.] 08/11/23 15:25:45.341
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 15:25:46.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-5058" for this suite. 08/11/23 15:25:46.344
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial]
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:25:46.352
Aug 11 15:25:46.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-preemption 08/11/23 15:25:46.353
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:46.371
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:46.374
[BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:92
Aug 11 15:25:46.392: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 15:26:46.436: INFO: Waiting for terminating namespaces to be deleted...
[It] validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218
STEP: Create pods that use 4/5 of node resources. 08/11/23 15:26:46.439
Aug 11 15:26:46.476: INFO: Created pod: pod0-0-sched-preemption-low-priority
Aug 11 15:26:46.487: INFO: Created pod: pod0-1-sched-preemption-medium-priority
Aug 11 15:26:46.505: INFO: Created pod: pod1-0-sched-preemption-medium-priority
Aug 11 15:26:46.513: INFO: Created pod: pod1-1-sched-preemption-medium-priority
STEP: Wait for pods to be scheduled. 08/11/23 15:26:46.513
Aug 11 15:26:46.513: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5314" to be "running"
Aug 11 15:26:46.520: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585555ms
Aug 11 15:26:48.525: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011934589s
Aug 11 15:26:50.525: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.012316767s
Aug 11 15:26:50.525: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
Aug 11 15:26:50.525: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5314" to be "running"
Aug 11 15:26:50.529: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.23256ms
Aug 11 15:26:50.529: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
Aug 11 15:26:50.529: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5314" to be "running"
Aug 11 15:26:50.532: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.924141ms
Aug 11 15:26:50.532: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
Aug 11 15:26:50.532: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5314" to be "running"
Aug 11 15:26:50.535: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.86469ms
Aug 11 15:26:50.535: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
STEP: Run a critical pod that use same resources as that of a lower priority pod 08/11/23 15:26:50.535
Aug 11 15:26:50.546: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
Aug 11 15:26:50.549: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.177919ms
Aug 11 15:26:52.555: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008833831s
Aug 11 15:26:54.553: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007339942s
Aug 11 15:26:54.553: INFO: Pod "critical-pod" satisfied condition "running"
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/framework/framework.go:187
Aug 11 15:26:54.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-preemption-5314" for this suite. 08/11/23 15:26:54.598
[AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
  test/e2e/scheduling/preemption.go:80
{"msg":"PASSED [sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]","completed":305,"skipped":5724,"failed":0}
------------------------------
â€¢ [SLOW TEST] [68.290 seconds]
[sig-scheduling] SchedulerPreemption [Serial]
test/e2e/scheduling/framework.go:40
  validates lower priority pod preemption by critical pod [Conformance]
  test/e2e/scheduling/preemption.go:218

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:25:46.352
    Aug 11 15:25:46.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-preemption 08/11/23 15:25:46.353
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:25:46.371
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:25:46.374
    [BeforeEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:92
    Aug 11 15:25:46.392: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 15:26:46.436: INFO: Waiting for terminating namespaces to be deleted...
    [It] validates lower priority pod preemption by critical pod [Conformance]
      test/e2e/scheduling/preemption.go:218
    STEP: Create pods that use 4/5 of node resources. 08/11/23 15:26:46.439
    Aug 11 15:26:46.476: INFO: Created pod: pod0-0-sched-preemption-low-priority
    Aug 11 15:26:46.487: INFO: Created pod: pod0-1-sched-preemption-medium-priority
    Aug 11 15:26:46.505: INFO: Created pod: pod1-0-sched-preemption-medium-priority
    Aug 11 15:26:46.513: INFO: Created pod: pod1-1-sched-preemption-medium-priority
    STEP: Wait for pods to be scheduled. 08/11/23 15:26:46.513
    Aug 11 15:26:46.513: INFO: Waiting up to 5m0s for pod "pod0-0-sched-preemption-low-priority" in namespace "sched-preemption-5314" to be "running"
    Aug 11 15:26:46.520: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 6.585555ms
    Aug 11 15:26:48.525: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011934589s
    Aug 11 15:26:50.525: INFO: Pod "pod0-0-sched-preemption-low-priority": Phase="Running", Reason="", readiness=true. Elapsed: 4.012316767s
    Aug 11 15:26:50.525: INFO: Pod "pod0-0-sched-preemption-low-priority" satisfied condition "running"
    Aug 11 15:26:50.525: INFO: Waiting up to 5m0s for pod "pod0-1-sched-preemption-medium-priority" in namespace "sched-preemption-5314" to be "running"
    Aug 11 15:26:50.529: INFO: Pod "pod0-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 3.23256ms
    Aug 11 15:26:50.529: INFO: Pod "pod0-1-sched-preemption-medium-priority" satisfied condition "running"
    Aug 11 15:26:50.529: INFO: Waiting up to 5m0s for pod "pod1-0-sched-preemption-medium-priority" in namespace "sched-preemption-5314" to be "running"
    Aug 11 15:26:50.532: INFO: Pod "pod1-0-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.924141ms
    Aug 11 15:26:50.532: INFO: Pod "pod1-0-sched-preemption-medium-priority" satisfied condition "running"
    Aug 11 15:26:50.532: INFO: Waiting up to 5m0s for pod "pod1-1-sched-preemption-medium-priority" in namespace "sched-preemption-5314" to be "running"
    Aug 11 15:26:50.535: INFO: Pod "pod1-1-sched-preemption-medium-priority": Phase="Running", Reason="", readiness=true. Elapsed: 2.86469ms
    Aug 11 15:26:50.535: INFO: Pod "pod1-1-sched-preemption-medium-priority" satisfied condition "running"
    STEP: Run a critical pod that use same resources as that of a lower priority pod 08/11/23 15:26:50.535
    Aug 11 15:26:50.546: INFO: Waiting up to 2m0s for pod "critical-pod" in namespace "kube-system" to be "running"
    Aug 11 15:26:50.549: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 3.177919ms
    Aug 11 15:26:52.555: INFO: Pod "critical-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008833831s
    Aug 11 15:26:54.553: INFO: Pod "critical-pod": Phase="Running", Reason="", readiness=true. Elapsed: 4.007339942s
    Aug 11 15:26:54.553: INFO: Pod "critical-pod" satisfied condition "running"
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 15:26:54.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-preemption-5314" for this suite. 08/11/23 15:26:54.598
    [AfterEach] [sig-scheduling] SchedulerPreemption [Serial]
      test/e2e/scheduling/preemption.go:80
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation
  should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
[BeforeEach] [sig-node] Security Context
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:26:54.643
Aug 11 15:26:54.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename security-context-test 08/11/23 15:26:54.644
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:26:54.66
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:26:54.664
[BeforeEach] [sig-node] Security Context
  test/e2e/common/node/security_context.go:49
[It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/node/security_context.go:608
Aug 11 15:26:54.676: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6" in namespace "security-context-test-494" to be "Succeeded or Failed"
Aug 11 15:26:54.682: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.711828ms
Aug 11 15:26:56.687: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011249167s
Aug 11 15:26:58.689: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012631736s
Aug 11 15:27:00.689: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01284992s
Aug 11 15:27:00.689: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6" satisfied condition "Succeeded or Failed"
[AfterEach] [sig-node] Security Context
  test/e2e/framework/framework.go:187
Aug 11 15:27:00.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "security-context-test-494" for this suite. 08/11/23 15:27:00.708
{"msg":"PASSED [sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]","completed":306,"skipped":5727,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.072 seconds]
[sig-node] Security Context
test/e2e/common/node/framework.go:23
  when creating containers with AllowPrivilegeEscalation
  test/e2e/common/node/security_context.go:554
    should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/node/security_context.go:608

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Security Context
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:26:54.643
    Aug 11 15:26:54.643: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename security-context-test 08/11/23 15:26:54.644
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:26:54.66
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:26:54.664
    [BeforeEach] [sig-node] Security Context
      test/e2e/common/node/security_context.go:49
    [It] should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/node/security_context.go:608
    Aug 11 15:26:54.676: INFO: Waiting up to 5m0s for pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6" in namespace "security-context-test-494" to be "Succeeded or Failed"
    Aug 11 15:26:54.682: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6": Phase="Pending", Reason="", readiness=false. Elapsed: 5.711828ms
    Aug 11 15:26:56.687: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011249167s
    Aug 11 15:26:58.689: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6": Phase="Pending", Reason="", readiness=false. Elapsed: 4.012631736s
    Aug 11 15:27:00.689: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6": Phase="Succeeded", Reason="", readiness=false. Elapsed: 6.01284992s
    Aug 11 15:27:00.689: INFO: Pod "alpine-nnp-false-36e4f28f-8205-426a-be5e-d49c4aebebf6" satisfied condition "Succeeded or Failed"
    [AfterEach] [sig-node] Security Context
      test/e2e/framework/framework.go:187
    Aug 11 15:27:00.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "security-context-test-494" for this suite. 08/11/23 15:27:00.708
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
[BeforeEach] [sig-node] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:00.716
Aug 11 15:27:00.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 15:27:00.717
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:00.736
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:00.739
[It] should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139
STEP: Creating projection with secret that has name secret-emptykey-test-88719f37-da80-4f1c-8c3a-2fff2adfb217 08/11/23 15:27:00.742
[AfterEach] [sig-node] Secrets
  test/e2e/framework/framework.go:187
Aug 11 15:27:00.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-8591" for this suite. 08/11/23 15:27:00.748
{"msg":"PASSED [sig-node] Secrets should fail to create secret due to empty secret key [Conformance]","completed":307,"skipped":5752,"failed":0}
------------------------------
â€¢ [0.038 seconds]
[sig-node] Secrets
test/e2e/common/node/framework.go:23
  should fail to create secret due to empty secret key [Conformance]
  test/e2e/common/node/secrets.go:139

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:00.716
    Aug 11 15:27:00.716: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 15:27:00.717
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:00.736
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:00.739
    [It] should fail to create secret due to empty secret key [Conformance]
      test/e2e/common/node/secrets.go:139
    STEP: Creating projection with secret that has name secret-emptykey-test-88719f37-da80-4f1c-8c3a-2fff2adfb217 08/11/23 15:27:00.742
    [AfterEach] [sig-node] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 15:27:00.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-8591" for this suite. 08/11/23 15:27:00.748
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook
  should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
[BeforeEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:00.755
Aug 11 15:27:00.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 15:27:00.756
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:00.771
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:00.775
[BeforeEach] when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:55
STEP: create the container to handle the HTTPGet hook request. 08/11/23 15:27:00.783
Aug 11 15:27:00.790: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4907" to be "running and ready"
Aug 11 15:27:00.797: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368858ms
Aug 11 15:27:00.797: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:27:02.802: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011322388s
Aug 11 15:27:02.802: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
Aug 11 15:27:02.802: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
[It] should execute prestop http hook properly [NodeConformance] [Conformance]
  test/e2e/common/node/lifecycle_hook.go:152
STEP: create the pod with lifecycle hook 08/11/23 15:27:02.805
Aug 11 15:27:02.812: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4907" to be "running and ready"
Aug 11 15:27:02.817: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.870482ms
Aug 11 15:27:02.817: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:27:04.822: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00972s
Aug 11 15:27:04.822: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
Aug 11 15:27:04.822: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
STEP: delete the pod with lifecycle hook 08/11/23 15:27:04.828
Aug 11 15:27:04.835: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 11 15:27:04.839: INFO: Pod pod-with-prestop-http-hook still exists
Aug 11 15:27:06.839: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 11 15:27:06.844: INFO: Pod pod-with-prestop-http-hook still exists
Aug 11 15:27:08.841: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
Aug 11 15:27:08.845: INFO: Pod pod-with-prestop-http-hook no longer exists
STEP: check prestop hook 08/11/23 15:27:08.845
[AfterEach] [sig-node] Container Lifecycle Hook
  test/e2e/framework/framework.go:187
Aug 11 15:27:08.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-lifecycle-hook-4907" for this suite. 08/11/23 15:27:08.861
{"msg":"PASSED [sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]","completed":308,"skipped":5773,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.112 seconds]
[sig-node] Container Lifecycle Hook
test/e2e/common/node/framework.go:23
  when create a pod with lifecycle hook
  test/e2e/common/node/lifecycle_hook.go:46
    should execute prestop http hook properly [NodeConformance] [Conformance]
    test/e2e/common/node/lifecycle_hook.go:152

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:00.755
    Aug 11 15:27:00.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-lifecycle-hook 08/11/23 15:27:00.756
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:00.771
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:00.775
    [BeforeEach] when create a pod with lifecycle hook
      test/e2e/common/node/lifecycle_hook.go:55
    STEP: create the container to handle the HTTPGet hook request. 08/11/23 15:27:00.783
    Aug 11 15:27:00.790: INFO: Waiting up to 5m0s for pod "pod-handle-http-request" in namespace "container-lifecycle-hook-4907" to be "running and ready"
    Aug 11 15:27:00.797: INFO: Pod "pod-handle-http-request": Phase="Pending", Reason="", readiness=false. Elapsed: 6.368858ms
    Aug 11 15:27:00.797: INFO: The phase of Pod pod-handle-http-request is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:27:02.802: INFO: Pod "pod-handle-http-request": Phase="Running", Reason="", readiness=true. Elapsed: 2.011322388s
    Aug 11 15:27:02.802: INFO: The phase of Pod pod-handle-http-request is Running (Ready = true)
    Aug 11 15:27:02.802: INFO: Pod "pod-handle-http-request" satisfied condition "running and ready"
    [It] should execute prestop http hook properly [NodeConformance] [Conformance]
      test/e2e/common/node/lifecycle_hook.go:152
    STEP: create the pod with lifecycle hook 08/11/23 15:27:02.805
    Aug 11 15:27:02.812: INFO: Waiting up to 5m0s for pod "pod-with-prestop-http-hook" in namespace "container-lifecycle-hook-4907" to be "running and ready"
    Aug 11 15:27:02.817: INFO: Pod "pod-with-prestop-http-hook": Phase="Pending", Reason="", readiness=false. Elapsed: 4.870482ms
    Aug 11 15:27:02.817: INFO: The phase of Pod pod-with-prestop-http-hook is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:27:04.822: INFO: Pod "pod-with-prestop-http-hook": Phase="Running", Reason="", readiness=true. Elapsed: 2.00972s
    Aug 11 15:27:04.822: INFO: The phase of Pod pod-with-prestop-http-hook is Running (Ready = true)
    Aug 11 15:27:04.822: INFO: Pod "pod-with-prestop-http-hook" satisfied condition "running and ready"
    STEP: delete the pod with lifecycle hook 08/11/23 15:27:04.828
    Aug 11 15:27:04.835: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 11 15:27:04.839: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 11 15:27:06.839: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 11 15:27:06.844: INFO: Pod pod-with-prestop-http-hook still exists
    Aug 11 15:27:08.841: INFO: Waiting for pod pod-with-prestop-http-hook to disappear
    Aug 11 15:27:08.845: INFO: Pod pod-with-prestop-http-hook no longer exists
    STEP: check prestop hook 08/11/23 15:27:08.845
    [AfterEach] [sig-node] Container Lifecycle Hook
      test/e2e/framework/framework.go:187
    Aug 11 15:27:08.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-lifecycle-hook-4907" for this suite. 08/11/23 15:27:08.861
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:08.869
Aug 11 15:27:08.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:27:08.87
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:08.888
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:08.891
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620
STEP: creating a collection of services 08/11/23 15:27:08.893
Aug 11 15:27:08.894: INFO: Creating e2e-svc-a-bx22f
Aug 11 15:27:08.909: INFO: Creating e2e-svc-b-4zvfj
Aug 11 15:27:08.926: INFO: Creating e2e-svc-c-c2g82
STEP: deleting service collection 08/11/23 15:27:08.948
Aug 11 15:27:08.996: INFO: Collection of services has been deleted
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:27:08.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4845" for this suite. 08/11/23 15:27:09.001
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should delete a collection of services [Conformance]","completed":309,"skipped":5801,"failed":0}
------------------------------
â€¢ [0.140 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should delete a collection of services [Conformance]
  test/e2e/network/service.go:3620

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:08.869
    Aug 11 15:27:08.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:27:08.87
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:08.888
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:08.891
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should delete a collection of services [Conformance]
      test/e2e/network/service.go:3620
    STEP: creating a collection of services 08/11/23 15:27:08.893
    Aug 11 15:27:08.894: INFO: Creating e2e-svc-a-bx22f
    Aug 11 15:27:08.909: INFO: Creating e2e-svc-b-4zvfj
    Aug 11 15:27:08.926: INFO: Creating e2e-svc-c-c2g82
    STEP: deleting service collection 08/11/23 15:27:08.948
    Aug 11 15:27:08.996: INFO: Collection of services has been deleted
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:27:08.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4845" for this suite. 08/11/23 15:27:09.001
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
[BeforeEach] [sig-network] DNS
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:09.012
Aug 11 15:27:09.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename dns 08/11/23 15:27:09.012
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:09.031
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:09.035
[It] should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117
STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6855.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6855.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
 08/11/23 15:27:09.038
STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6855.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6855.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
 08/11/23 15:27:09.038
STEP: creating a pod to probe /etc/hosts 08/11/23 15:27:09.038
STEP: submitting the pod to kubernetes 08/11/23 15:27:09.038
Aug 11 15:27:09.049: INFO: Waiting up to 15m0s for pod "dns-test-a794be00-c12c-449b-8cde-475b7517b9b6" in namespace "dns-6855" to be "running"
Aug 11 15:27:09.055: INFO: Pod "dns-test-a794be00-c12c-449b-8cde-475b7517b9b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.13285ms
Aug 11 15:27:11.060: INFO: Pod "dns-test-a794be00-c12c-449b-8cde-475b7517b9b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011101611s
Aug 11 15:27:11.060: INFO: Pod "dns-test-a794be00-c12c-449b-8cde-475b7517b9b6" satisfied condition "running"
STEP: retrieving the pod 08/11/23 15:27:11.06
STEP: looking for the results for each expected name from probers 08/11/23 15:27:11.063
Aug 11 15:27:11.093: INFO: DNS probes using dns-6855/dns-test-a794be00-c12c-449b-8cde-475b7517b9b6 succeeded

STEP: deleting the pod 08/11/23 15:27:11.093
[AfterEach] [sig-network] DNS
  test/e2e/framework/framework.go:187
Aug 11 15:27:11.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "dns-6855" for this suite. 08/11/23 15:27:11.113
{"msg":"PASSED [sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]","completed":310,"skipped":5830,"failed":0}
------------------------------
â€¢ [2.108 seconds]
[sig-network] DNS
test/e2e/network/common/framework.go:23
  should provide /etc/hosts entries for the cluster [Conformance]
  test/e2e/network/dns.go:117

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] DNS
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:09.012
    Aug 11 15:27:09.012: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename dns 08/11/23 15:27:09.012
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:09.031
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:09.035
    [It] should provide /etc/hosts entries for the cluster [Conformance]
      test/e2e/network/dns.go:117
    STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6855.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-6855.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
     08/11/23 15:27:09.038
    STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-6855.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-6855.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
     08/11/23 15:27:09.038
    STEP: creating a pod to probe /etc/hosts 08/11/23 15:27:09.038
    STEP: submitting the pod to kubernetes 08/11/23 15:27:09.038
    Aug 11 15:27:09.049: INFO: Waiting up to 15m0s for pod "dns-test-a794be00-c12c-449b-8cde-475b7517b9b6" in namespace "dns-6855" to be "running"
    Aug 11 15:27:09.055: INFO: Pod "dns-test-a794be00-c12c-449b-8cde-475b7517b9b6": Phase="Pending", Reason="", readiness=false. Elapsed: 6.13285ms
    Aug 11 15:27:11.060: INFO: Pod "dns-test-a794be00-c12c-449b-8cde-475b7517b9b6": Phase="Running", Reason="", readiness=true. Elapsed: 2.011101611s
    Aug 11 15:27:11.060: INFO: Pod "dns-test-a794be00-c12c-449b-8cde-475b7517b9b6" satisfied condition "running"
    STEP: retrieving the pod 08/11/23 15:27:11.06
    STEP: looking for the results for each expected name from probers 08/11/23 15:27:11.063
    Aug 11 15:27:11.093: INFO: DNS probes using dns-6855/dns-test-a794be00-c12c-449b-8cde-475b7517b9b6 succeeded

    STEP: deleting the pod 08/11/23 15:27:11.093
    [AfterEach] [sig-network] DNS
      test/e2e/framework/framework.go:187
    Aug 11 15:27:11.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "dns-6855" for this suite. 08/11/23 15:27:11.113
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-network] Networking Granular Checks: Pods
  should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
[BeforeEach] [sig-network] Networking
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:11.121
Aug 11 15:27:11.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pod-network-test 08/11/23 15:27:11.122
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:11.138
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:11.141
[It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/network/networking.go:105
STEP: Performing setup for networking test in namespace pod-network-test-1161 08/11/23 15:27:11.144
STEP: creating a selector 08/11/23 15:27:11.144
STEP: Creating the service pods in kubernetes 08/11/23 15:27:11.144
Aug 11 15:27:11.144: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
Aug 11 15:27:11.168: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1161" to be "running and ready"
Aug 11 15:27:11.172: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043176ms
Aug 11 15:27:11.172: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:27:13.178: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009783591s
Aug 11 15:27:13.178: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:27:15.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008951273s
Aug 11 15:27:15.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:27:17.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009030883s
Aug 11 15:27:17.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:27:19.178: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010052339s
Aug 11 15:27:19.178: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:27:21.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008740995s
Aug 11 15:27:21.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
Aug 11 15:27:23.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009432143s
Aug 11 15:27:23.177: INFO: The phase of Pod netserver-0 is Running (Ready = true)
Aug 11 15:27:23.177: INFO: Pod "netserver-0" satisfied condition "running and ready"
Aug 11 15:27:23.181: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1161" to be "running and ready"
Aug 11 15:27:23.184: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.294022ms
Aug 11 15:27:23.184: INFO: The phase of Pod netserver-1 is Running (Ready = true)
Aug 11 15:27:23.184: INFO: Pod "netserver-1" satisfied condition "running and ready"
STEP: Creating test pods 08/11/23 15:27:23.188
Aug 11 15:27:23.203: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1161" to be "running"
Aug 11 15:27:23.209: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.612405ms
Aug 11 15:27:25.213: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010367718s
Aug 11 15:27:25.213: INFO: Pod "test-container-pod" satisfied condition "running"
Aug 11 15:27:25.216: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1161" to be "running"
Aug 11 15:27:25.220: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.935043ms
Aug 11 15:27:25.220: INFO: Pod "host-test-container-pod" satisfied condition "running"
Aug 11 15:27:25.224: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
Aug 11 15:27:25.224: INFO: Going to poll 10.10.0.215 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Aug 11 15:27:25.227: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.0.215:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1161 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:27:25.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 15:27:25.227: INFO: ExecWithOptions: Clientset creation
Aug 11 15:27:25.228: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1161/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.0.215%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 15:27:25.306: INFO: Found all 1 expected endpoints: [netserver-0]
Aug 11 15:27:25.306: INFO: Going to poll 10.10.1.146 on port 8083 at least 0 times, with a maximum of 34 tries before failing
Aug 11 15:27:25.310: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.1.146:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1161 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
Aug 11 15:27:25.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 15:27:25.311: INFO: ExecWithOptions: Clientset creation
Aug 11 15:27:25.311: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1161/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.1.146%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
Aug 11 15:27:25.390: INFO: Found all 1 expected endpoints: [netserver-1]
[AfterEach] [sig-network] Networking
  test/e2e/framework/framework.go:187
Aug 11 15:27:25.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pod-network-test-1161" for this suite. 08/11/23 15:27:25.397
{"msg":"PASSED [sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]","completed":311,"skipped":5834,"failed":0}
------------------------------
â€¢ [SLOW TEST] [14.283 seconds]
[sig-network] Networking
test/e2e/common/network/framework.go:23
  Granular Checks: Pods
  test/e2e/common/network/networking.go:32
    should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
    test/e2e/common/network/networking.go:105

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Networking
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:11.121
    Aug 11 15:27:11.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pod-network-test 08/11/23 15:27:11.122
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:11.138
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:11.141
    [It] should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/network/networking.go:105
    STEP: Performing setup for networking test in namespace pod-network-test-1161 08/11/23 15:27:11.144
    STEP: creating a selector 08/11/23 15:27:11.144
    STEP: Creating the service pods in kubernetes 08/11/23 15:27:11.144
    Aug 11 15:27:11.144: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
    Aug 11 15:27:11.168: INFO: Waiting up to 5m0s for pod "netserver-0" in namespace "pod-network-test-1161" to be "running and ready"
    Aug 11 15:27:11.172: INFO: Pod "netserver-0": Phase="Pending", Reason="", readiness=false. Elapsed: 4.043176ms
    Aug 11 15:27:11.172: INFO: The phase of Pod netserver-0 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:27:13.178: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 2.009783591s
    Aug 11 15:27:13.178: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:27:15.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 4.008951273s
    Aug 11 15:27:15.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:27:17.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 6.009030883s
    Aug 11 15:27:17.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:27:19.178: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 8.010052339s
    Aug 11 15:27:19.178: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:27:21.176: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=false. Elapsed: 10.008740995s
    Aug 11 15:27:21.177: INFO: The phase of Pod netserver-0 is Running (Ready = false)
    Aug 11 15:27:23.177: INFO: Pod "netserver-0": Phase="Running", Reason="", readiness=true. Elapsed: 12.009432143s
    Aug 11 15:27:23.177: INFO: The phase of Pod netserver-0 is Running (Ready = true)
    Aug 11 15:27:23.177: INFO: Pod "netserver-0" satisfied condition "running and ready"
    Aug 11 15:27:23.181: INFO: Waiting up to 5m0s for pod "netserver-1" in namespace "pod-network-test-1161" to be "running and ready"
    Aug 11 15:27:23.184: INFO: Pod "netserver-1": Phase="Running", Reason="", readiness=true. Elapsed: 3.294022ms
    Aug 11 15:27:23.184: INFO: The phase of Pod netserver-1 is Running (Ready = true)
    Aug 11 15:27:23.184: INFO: Pod "netserver-1" satisfied condition "running and ready"
    STEP: Creating test pods 08/11/23 15:27:23.188
    Aug 11 15:27:23.203: INFO: Waiting up to 5m0s for pod "test-container-pod" in namespace "pod-network-test-1161" to be "running"
    Aug 11 15:27:23.209: INFO: Pod "test-container-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 5.612405ms
    Aug 11 15:27:25.213: INFO: Pod "test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.010367718s
    Aug 11 15:27:25.213: INFO: Pod "test-container-pod" satisfied condition "running"
    Aug 11 15:27:25.216: INFO: Waiting up to 5m0s for pod "host-test-container-pod" in namespace "pod-network-test-1161" to be "running"
    Aug 11 15:27:25.220: INFO: Pod "host-test-container-pod": Phase="Running", Reason="", readiness=true. Elapsed: 3.935043ms
    Aug 11 15:27:25.220: INFO: Pod "host-test-container-pod" satisfied condition "running"
    Aug 11 15:27:25.224: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
    Aug 11 15:27:25.224: INFO: Going to poll 10.10.0.215 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Aug 11 15:27:25.227: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.0.215:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1161 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:27:25.227: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 15:27:25.227: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:27:25.228: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1161/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.0.215%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 15:27:25.306: INFO: Found all 1 expected endpoints: [netserver-0]
    Aug 11 15:27:25.306: INFO: Going to poll 10.10.1.146 on port 8083 at least 0 times, with a maximum of 34 tries before failing
    Aug 11 15:27:25.310: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.10.1.146:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1161 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
    Aug 11 15:27:25.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 15:27:25.311: INFO: ExecWithOptions: Clientset creation
    Aug 11 15:27:25.311: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1161/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.10.1.146%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
    Aug 11 15:27:25.390: INFO: Found all 1 expected endpoints: [netserver-1]
    [AfterEach] [sig-network] Networking
      test/e2e/framework/framework.go:187
    Aug 11 15:27:25.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pod-network-test-1161" for this suite. 08/11/23 15:27:25.397
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
[BeforeEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:25.405
Aug 11 15:27:25.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename gc 08/11/23 15:27:25.406
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:25.421
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:25.423
[It] should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491
STEP: create the deployment 08/11/23 15:27:25.426
STEP: Wait for the Deployment to create new ReplicaSet 08/11/23 15:27:25.432
STEP: delete the deployment 08/11/23 15:27:25.941
STEP: wait for all rs to be garbage collected 08/11/23 15:27:25.948
STEP: expected 0 rs, got 1 rs 08/11/23 15:27:25.954
STEP: expected 0 pods, got 2 pods 08/11/23 15:27:25.958
STEP: Gathering metrics 08/11/23 15:27:26.469
Aug 11 15:27:26.516: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
Aug 11 15:27:26.521: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 4.753288ms
Aug 11 15:27:26.521: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
Aug 11 15:27:26.521: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
Aug 11 15:27:26.588: INFO: For apiserver_request_total:
For apiserver_request_latency_seconds:
For apiserver_init_events_total:
For garbage_collector_attempt_to_delete_queue_latency:
For garbage_collector_attempt_to_delete_work_duration:
For garbage_collector_attempt_to_orphan_queue_latency:
For garbage_collector_attempt_to_orphan_work_duration:
For garbage_collector_dirty_processing_latency_microseconds:
For garbage_collector_event_processing_latency_microseconds:
For garbage_collector_graph_changes_queue_latency:
For garbage_collector_graph_changes_work_duration:
For garbage_collector_orphan_processing_latency_microseconds:
For namespace_queue_latency:
For namespace_queue_latency_sum:
For namespace_queue_latency_count:
For namespace_retries:
For namespace_work_duration:
For namespace_work_duration_sum:
For namespace_work_duration_count:
For function_duration_seconds:
For errors_total:
For evicted_pods_total:

[AfterEach] [sig-api-machinery] Garbage collector
  test/e2e/framework/framework.go:187
Aug 11 15:27:26.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "gc-9515" for this suite. 08/11/23 15:27:26.592
{"msg":"PASSED [sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]","completed":312,"skipped":5855,"failed":0}
------------------------------
â€¢ [1.194 seconds]
[sig-api-machinery] Garbage collector
test/e2e/apimachinery/framework.go:23
  should delete RS created by deployment when not orphaning [Conformance]
  test/e2e/apimachinery/garbage_collector.go:491

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:25.405
    Aug 11 15:27:25.405: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename gc 08/11/23 15:27:25.406
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:25.421
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:25.423
    [It] should delete RS created by deployment when not orphaning [Conformance]
      test/e2e/apimachinery/garbage_collector.go:491
    STEP: create the deployment 08/11/23 15:27:25.426
    STEP: Wait for the Deployment to create new ReplicaSet 08/11/23 15:27:25.432
    STEP: delete the deployment 08/11/23 15:27:25.941
    STEP: wait for all rs to be garbage collected 08/11/23 15:27:25.948
    STEP: expected 0 rs, got 1 rs 08/11/23 15:27:25.954
    STEP: expected 0 pods, got 2 pods 08/11/23 15:27:25.958
    STEP: Gathering metrics 08/11/23 15:27:26.469
    Aug 11 15:27:26.516: INFO: Waiting up to 5m0s for pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" in namespace "kube-system" to be "running and ready"
    Aug 11 15:27:26.521: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw": Phase="Running", Reason="", readiness=true. Elapsed: 4.753288ms
    Aug 11 15:27:26.521: INFO: The phase of Pod kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw is Running (Ready = true)
    Aug 11 15:27:26.521: INFO: Pod "kube-controller-manager-constell-d93e7e1d-control-plane-6ef148f4-d3pw" satisfied condition "running and ready"
    Aug 11 15:27:26.588: INFO: For apiserver_request_total:
    For apiserver_request_latency_seconds:
    For apiserver_init_events_total:
    For garbage_collector_attempt_to_delete_queue_latency:
    For garbage_collector_attempt_to_delete_work_duration:
    For garbage_collector_attempt_to_orphan_queue_latency:
    For garbage_collector_attempt_to_orphan_work_duration:
    For garbage_collector_dirty_processing_latency_microseconds:
    For garbage_collector_event_processing_latency_microseconds:
    For garbage_collector_graph_changes_queue_latency:
    For garbage_collector_graph_changes_work_duration:
    For garbage_collector_orphan_processing_latency_microseconds:
    For namespace_queue_latency:
    For namespace_queue_latency_sum:
    For namespace_queue_latency_count:
    For namespace_retries:
    For namespace_work_duration:
    For namespace_work_duration_sum:
    For namespace_work_duration_count:
    For function_duration_seconds:
    For errors_total:
    For evicted_pods_total:

    [AfterEach] [sig-api-machinery] Garbage collector
      test/e2e/framework/framework.go:187
    Aug 11 15:27:26.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "gc-9515" for this suite. 08/11/23 15:27:26.592
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:26.602
Aug 11 15:27:26.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 15:27:26.603
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:26.62
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:26.623
[It] should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44
STEP: Creating configMap configmap-73/configmap-test-e6aa22eb-2adb-47c3-97e9-bbbd8af06080 08/11/23 15:27:26.626
STEP: Creating a pod to test consume configMaps 08/11/23 15:27:26.631
Aug 11 15:27:26.641: INFO: Waiting up to 5m0s for pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95" in namespace "configmap-73" to be "Succeeded or Failed"
Aug 11 15:27:26.645: INFO: Pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032256ms
Aug 11 15:27:28.650: INFO: Pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009101492s
Aug 11 15:27:30.651: INFO: Pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009814029s
STEP: Saw pod success 08/11/23 15:27:30.651
Aug 11 15:27:30.651: INFO: Pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95" satisfied condition "Succeeded or Failed"
Aug 11 15:27:30.656: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95 container env-test: <nil>
STEP: delete the pod 08/11/23 15:27:30.669
Aug 11 15:27:30.686: INFO: Waiting for pod pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95 to disappear
Aug 11 15:27:30.689: INFO: Pod pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 15:27:30.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-73" for this suite. 08/11/23 15:27:30.694
{"msg":"PASSED [sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]","completed":313,"skipped":5885,"failed":0}
------------------------------
â€¢ [4.101 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via environment variable [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:44

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:26.602
    Aug 11 15:27:26.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 15:27:26.603
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:26.62
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:26.623
    [It] should be consumable via environment variable [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:44
    STEP: Creating configMap configmap-73/configmap-test-e6aa22eb-2adb-47c3-97e9-bbbd8af06080 08/11/23 15:27:26.626
    STEP: Creating a pod to test consume configMaps 08/11/23 15:27:26.631
    Aug 11 15:27:26.641: INFO: Waiting up to 5m0s for pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95" in namespace "configmap-73" to be "Succeeded or Failed"
    Aug 11 15:27:26.645: INFO: Pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95": Phase="Pending", Reason="", readiness=false. Elapsed: 4.032256ms
    Aug 11 15:27:28.650: INFO: Pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009101492s
    Aug 11 15:27:30.651: INFO: Pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009814029s
    STEP: Saw pod success 08/11/23 15:27:30.651
    Aug 11 15:27:30.651: INFO: Pod "pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95" satisfied condition "Succeeded or Failed"
    Aug 11 15:27:30.656: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95 container env-test: <nil>
    STEP: delete the pod 08/11/23 15:27:30.669
    Aug 11 15:27:30.686: INFO: Waiting for pod pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95 to disappear
    Aug 11 15:27:30.689: INFO: Pod pod-configmaps-8dec158b-5bf8-4c97-b029-cf3c34e8db95 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 15:27:30.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-73" for this suite. 08/11/23 15:27:30.694
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:30.705
Aug 11 15:27:30.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename namespaces 08/11/23 15:27:30.706
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:30.721
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:30.724
[It] should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242
STEP: Creating a test namespace 08/11/23 15:27:30.727
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:30.743
STEP: Creating a pod in the namespace 08/11/23 15:27:30.747
STEP: Waiting for the pod to have running status 08/11/23 15:27:30.756
Aug 11 15:27:30.756: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1973" to be "running"
Aug 11 15:27:30.760: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125258ms
Aug 11 15:27:32.765: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009209442s
Aug 11 15:27:32.765: INFO: Pod "test-pod" satisfied condition "running"
STEP: Deleting the namespace 08/11/23 15:27:32.765
STEP: Waiting for the namespace to be removed. 08/11/23 15:27:32.775
STEP: Recreating the namespace 08/11/23 15:27:43.78
STEP: Verifying there are no pods in the namespace 08/11/23 15:27:43.796
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Aug 11 15:27:43.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-9241" for this suite. 08/11/23 15:27:43.804
STEP: Destroying namespace "nsdeletetest-1973" for this suite. 08/11/23 15:27:43.812
Aug 11 15:27:43.815: INFO: Namespace nsdeletetest-1973 was already deleted
STEP: Destroying namespace "nsdeletetest-2170" for this suite. 08/11/23 15:27:43.815
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]","completed":314,"skipped":5887,"failed":0}
------------------------------
â€¢ [SLOW TEST] [13.117 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should ensure that all pods are removed when a namespace is deleted [Conformance]
  test/e2e/apimachinery/namespace.go:242

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:30.705
    Aug 11 15:27:30.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename namespaces 08/11/23 15:27:30.706
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:30.721
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:30.724
    [It] should ensure that all pods are removed when a namespace is deleted [Conformance]
      test/e2e/apimachinery/namespace.go:242
    STEP: Creating a test namespace 08/11/23 15:27:30.727
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:30.743
    STEP: Creating a pod in the namespace 08/11/23 15:27:30.747
    STEP: Waiting for the pod to have running status 08/11/23 15:27:30.756
    Aug 11 15:27:30.756: INFO: Waiting up to 5m0s for pod "test-pod" in namespace "nsdeletetest-1973" to be "running"
    Aug 11 15:27:30.760: INFO: Pod "test-pod": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125258ms
    Aug 11 15:27:32.765: INFO: Pod "test-pod": Phase="Running", Reason="", readiness=true. Elapsed: 2.009209442s
    Aug 11 15:27:32.765: INFO: Pod "test-pod" satisfied condition "running"
    STEP: Deleting the namespace 08/11/23 15:27:32.765
    STEP: Waiting for the namespace to be removed. 08/11/23 15:27:32.775
    STEP: Recreating the namespace 08/11/23 15:27:43.78
    STEP: Verifying there are no pods in the namespace 08/11/23 15:27:43.796
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 15:27:43.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-9241" for this suite. 08/11/23 15:27:43.804
    STEP: Destroying namespace "nsdeletetest-1973" for this suite. 08/11/23 15:27:43.812
    Aug 11 15:27:43.815: INFO: Namespace nsdeletetest-1973 was already deleted
    STEP: Destroying namespace "nsdeletetest-2170" for this suite. 08/11/23 15:27:43.815
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
[BeforeEach] [sig-node] Downward API
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:43.823
Aug 11 15:27:43.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 15:27:43.824
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:43.839
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:43.842
[It] should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266
STEP: Creating a pod to test downward api env vars 08/11/23 15:27:43.845
Aug 11 15:27:43.854: INFO: Waiting up to 5m0s for pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63" in namespace "downward-api-2516" to be "Succeeded or Failed"
Aug 11 15:27:43.859: INFO: Pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.624295ms
Aug 11 15:27:45.864: INFO: Pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010428401s
Aug 11 15:27:47.865: INFO: Pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011212272s
STEP: Saw pod success 08/11/23 15:27:47.865
Aug 11 15:27:47.865: INFO: Pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63" satisfied condition "Succeeded or Failed"
Aug 11 15:27:47.868: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-0600a096-dc18-44b3-b724-bb9982863c63 container dapi-container: <nil>
STEP: delete the pod 08/11/23 15:27:47.878
Aug 11 15:27:47.893: INFO: Waiting for pod downward-api-0600a096-dc18-44b3-b724-bb9982863c63 to disappear
Aug 11 15:27:47.896: INFO: Pod downward-api-0600a096-dc18-44b3-b724-bb9982863c63 no longer exists
[AfterEach] [sig-node] Downward API
  test/e2e/framework/framework.go:187
Aug 11 15:27:47.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-2516" for this suite. 08/11/23 15:27:47.901
{"msg":"PASSED [sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]","completed":315,"skipped":5914,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-node] Downward API
test/e2e/common/node/framework.go:23
  should provide pod UID as env vars [NodeConformance] [Conformance]
  test/e2e/common/node/downwardapi.go:266

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Downward API
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:43.823
    Aug 11 15:27:43.823: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:27:43.824
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:43.839
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:43.842
    [It] should provide pod UID as env vars [NodeConformance] [Conformance]
      test/e2e/common/node/downwardapi.go:266
    STEP: Creating a pod to test downward api env vars 08/11/23 15:27:43.845
    Aug 11 15:27:43.854: INFO: Waiting up to 5m0s for pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63" in namespace "downward-api-2516" to be "Succeeded or Failed"
    Aug 11 15:27:43.859: INFO: Pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63": Phase="Pending", Reason="", readiness=false. Elapsed: 5.624295ms
    Aug 11 15:27:45.864: INFO: Pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010428401s
    Aug 11 15:27:47.865: INFO: Pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011212272s
    STEP: Saw pod success 08/11/23 15:27:47.865
    Aug 11 15:27:47.865: INFO: Pod "downward-api-0600a096-dc18-44b3-b724-bb9982863c63" satisfied condition "Succeeded or Failed"
    Aug 11 15:27:47.868: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downward-api-0600a096-dc18-44b3-b724-bb9982863c63 container dapi-container: <nil>
    STEP: delete the pod 08/11/23 15:27:47.878
    Aug 11 15:27:47.893: INFO: Waiting for pod downward-api-0600a096-dc18-44b3-b724-bb9982863c63 to disappear
    Aug 11 15:27:47.896: INFO: Pod downward-api-0600a096-dc18-44b3-b724-bb9982863c63 no longer exists
    [AfterEach] [sig-node] Downward API
      test/e2e/framework/framework.go:187
    Aug 11 15:27:47.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-2516" for this suite. 08/11/23 15:27:47.901
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:47.909
Aug 11 15:27:47.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename svcaccounts 08/11/23 15:27:47.911
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:47.926
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:47.929
[It] should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158
Aug 11 15:27:47.951: INFO: created pod pod-service-account-defaultsa
Aug 11 15:27:47.951: INFO: pod pod-service-account-defaultsa service account token volume mount: true
Aug 11 15:27:47.958: INFO: created pod pod-service-account-mountsa
Aug 11 15:27:47.958: INFO: pod pod-service-account-mountsa service account token volume mount: true
Aug 11 15:27:47.968: INFO: created pod pod-service-account-nomountsa
Aug 11 15:27:47.968: INFO: pod pod-service-account-nomountsa service account token volume mount: false
Aug 11 15:27:47.981: INFO: created pod pod-service-account-defaultsa-mountspec
Aug 11 15:27:47.981: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
Aug 11 15:27:47.988: INFO: created pod pod-service-account-mountsa-mountspec
Aug 11 15:27:47.988: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
Aug 11 15:27:47.995: INFO: created pod pod-service-account-nomountsa-mountspec
Aug 11 15:27:47.995: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
Aug 11 15:27:48.006: INFO: created pod pod-service-account-defaultsa-nomountspec
Aug 11 15:27:48.006: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
Aug 11 15:27:48.018: INFO: created pod pod-service-account-mountsa-nomountspec
Aug 11 15:27:48.018: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
Aug 11 15:27:48.029: INFO: created pod pod-service-account-nomountsa-nomountspec
Aug 11 15:27:48.029: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 11 15:27:48.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-6001" for this suite. 08/11/23 15:27:48.04
{"msg":"PASSED [sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]","completed":316,"skipped":5916,"failed":0}
------------------------------
â€¢ [0.139 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  should allow opting out of API token automount  [Conformance]
  test/e2e/auth/service_accounts.go:158

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:47.909
    Aug 11 15:27:47.909: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 15:27:47.911
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:47.926
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:47.929
    [It] should allow opting out of API token automount  [Conformance]
      test/e2e/auth/service_accounts.go:158
    Aug 11 15:27:47.951: INFO: created pod pod-service-account-defaultsa
    Aug 11 15:27:47.951: INFO: pod pod-service-account-defaultsa service account token volume mount: true
    Aug 11 15:27:47.958: INFO: created pod pod-service-account-mountsa
    Aug 11 15:27:47.958: INFO: pod pod-service-account-mountsa service account token volume mount: true
    Aug 11 15:27:47.968: INFO: created pod pod-service-account-nomountsa
    Aug 11 15:27:47.968: INFO: pod pod-service-account-nomountsa service account token volume mount: false
    Aug 11 15:27:47.981: INFO: created pod pod-service-account-defaultsa-mountspec
    Aug 11 15:27:47.981: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
    Aug 11 15:27:47.988: INFO: created pod pod-service-account-mountsa-mountspec
    Aug 11 15:27:47.988: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
    Aug 11 15:27:47.995: INFO: created pod pod-service-account-nomountsa-mountspec
    Aug 11 15:27:47.995: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
    Aug 11 15:27:48.006: INFO: created pod pod-service-account-defaultsa-nomountspec
    Aug 11 15:27:48.006: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
    Aug 11 15:27:48.018: INFO: created pod pod-service-account-mountsa-nomountspec
    Aug 11 15:27:48.018: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
    Aug 11 15:27:48.029: INFO: created pod pod-service-account-nomountsa-nomountspec
    Aug 11 15:27:48.029: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 11 15:27:48.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-6001" for this suite. 08/11/23 15:27:48.04
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:48.05
Aug 11 15:27:48.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 15:27:48.051
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:48.067
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:48.074
[It] should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 11 15:27:48.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-7407" for this suite. 08/11/23 15:27:48.156
{"msg":"PASSED [sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]","completed":317,"skipped":5933,"failed":0}
------------------------------
â€¢ [0.117 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be immutable if `immutable` field is set [Conformance]
  test/e2e/common/storage/secrets_volume.go:385

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:48.05
    Aug 11 15:27:48.050: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 15:27:48.051
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:48.067
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:48.074
    [It] should be immutable if `immutable` field is set [Conformance]
      test/e2e/common/storage/secrets_volume.go:385
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 15:27:48.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-7407" for this suite. 08/11/23 15:27:48.156
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:37
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:48.168
Aug 11 15:27:48.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sysctl 08/11/23 15:27:48.169
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:48.184
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:48.187
[BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/common/node/sysctl.go:67
[It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123
STEP: Creating a pod with one valid and two invalid sysctls 08/11/23 15:27:48.191
[AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
  test/e2e/framework/framework.go:187
Aug 11 15:27:48.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sysctl-3907" for this suite. 08/11/23 15:27:48.201
{"msg":"PASSED [sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]","completed":318,"skipped":5933,"failed":0}
------------------------------
â€¢ [0.040 seconds]
[sig-node] Sysctls [LinuxOnly] [NodeConformance]
test/e2e/common/node/framework.go:23
  should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
  test/e2e/common/node/sysctl.go:123

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:37
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:48.168
    Aug 11 15:27:48.168: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sysctl 08/11/23 15:27:48.169
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:48.184
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:48.187
    [BeforeEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/common/node/sysctl.go:67
    [It] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
      test/e2e/common/node/sysctl.go:123
    STEP: Creating a pod with one valid and two invalid sysctls 08/11/23 15:27:48.191
    [AfterEach] [sig-node] Sysctls [LinuxOnly] [NodeConformance]
      test/e2e/framework/framework.go:187
    Aug 11 15:27:48.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sysctl-3907" for this suite. 08/11/23 15:27:48.201
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
[BeforeEach] [sig-node] Probing container
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:27:48.212
Aug 11 15:27:48.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-probe 08/11/23 15:27:48.214
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:48.23
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:48.233
[BeforeEach] [sig-node] Probing container
  test/e2e/common/node/container_probe.go:59
[It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165
STEP: Creating pod liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78 in namespace container-probe-1135 08/11/23 15:27:48.236
Aug 11 15:27:48.244: INFO: Waiting up to 5m0s for pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78" in namespace "container-probe-1135" to be "not pending"
Aug 11 15:27:48.251: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78": Phase="Pending", Reason="", readiness=false. Elapsed: 6.345578ms
Aug 11 15:27:50.256: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011789504s
Aug 11 15:27:52.256: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01176845s
Aug 11 15:27:54.255: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78": Phase="Running", Reason="", readiness=true. Elapsed: 6.01088851s
Aug 11 15:27:54.255: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78" satisfied condition "not pending"
Aug 11 15:27:54.255: INFO: Started pod liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78 in namespace container-probe-1135
STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:27:54.255
Aug 11 15:27:54.259: INFO: Initial restart count of pod liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78 is 0
Aug 11 15:28:10.305: INFO: Restart count of pod container-probe-1135/liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78 is now 1 (16.04586422s elapsed)
STEP: deleting the pod 08/11/23 15:28:10.305
[AfterEach] [sig-node] Probing container
  test/e2e/framework/framework.go:187
Aug 11 15:28:10.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-probe-1135" for this suite. 08/11/23 15:28:10.329
{"msg":"PASSED [sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]","completed":319,"skipped":5965,"failed":0}
------------------------------
â€¢ [SLOW TEST] [22.125 seconds]
[sig-node] Probing container
test/e2e/common/node/framework.go:23
  should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
  test/e2e/common/node/container_probe.go:165

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Probing container
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:27:48.212
    Aug 11 15:27:48.213: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-probe 08/11/23 15:27:48.214
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:27:48.23
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:27:48.233
    [BeforeEach] [sig-node] Probing container
      test/e2e/common/node/container_probe.go:59
    [It] should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
      test/e2e/common/node/container_probe.go:165
    STEP: Creating pod liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78 in namespace container-probe-1135 08/11/23 15:27:48.236
    Aug 11 15:27:48.244: INFO: Waiting up to 5m0s for pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78" in namespace "container-probe-1135" to be "not pending"
    Aug 11 15:27:48.251: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78": Phase="Pending", Reason="", readiness=false. Elapsed: 6.345578ms
    Aug 11 15:27:50.256: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011789504s
    Aug 11 15:27:52.256: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78": Phase="Pending", Reason="", readiness=false. Elapsed: 4.01176845s
    Aug 11 15:27:54.255: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78": Phase="Running", Reason="", readiness=true. Elapsed: 6.01088851s
    Aug 11 15:27:54.255: INFO: Pod "liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78" satisfied condition "not pending"
    Aug 11 15:27:54.255: INFO: Started pod liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78 in namespace container-probe-1135
    STEP: checking the pod's current state and verifying that restartCount is present 08/11/23 15:27:54.255
    Aug 11 15:27:54.259: INFO: Initial restart count of pod liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78 is 0
    Aug 11 15:28:10.305: INFO: Restart count of pod container-probe-1135/liveness-53b8e49a-6bdf-422e-8691-0a5bf91afb78 is now 1 (16.04586422s elapsed)
    STEP: deleting the pod 08/11/23 15:28:10.305
    [AfterEach] [sig-node] Probing container
      test/e2e/framework/framework.go:187
    Aug 11 15:28:10.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-probe-1135" for this suite. 08/11/23 15:28:10.329
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
[BeforeEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:28:10.338
Aug 11 15:28:10.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename var-expansion 08/11/23 15:28:10.339
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:10.357
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:10.36
[It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151
Aug 11 15:28:10.373: INFO: Waiting up to 2m0s for pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d" in namespace "var-expansion-4269" to be "container 0 failed with reason CreateContainerConfigError"
Aug 11 15:28:10.377: INFO: Pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.613993ms
Aug 11 15:28:12.382: INFO: Pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008865522s
Aug 11 15:28:12.382: INFO: Pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
Aug 11 15:28:12.382: INFO: Deleting pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d" in namespace "var-expansion-4269"
Aug 11 15:28:12.391: INFO: Wait up to 5m0s for pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d" to be fully deleted
[AfterEach] [sig-node] Variable Expansion
  test/e2e/framework/framework.go:187
Aug 11 15:28:14.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "var-expansion-4269" for this suite. 08/11/23 15:28:14.405
{"msg":"PASSED [sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]","completed":320,"skipped":5974,"failed":0}
------------------------------
â€¢ [4.073 seconds]
[sig-node] Variable Expansion
test/e2e/common/node/framework.go:23
  should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
  test/e2e/common/node/expansion.go:151

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:28:10.338
    Aug 11 15:28:10.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename var-expansion 08/11/23 15:28:10.339
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:10.357
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:10.36
    [It] should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
      test/e2e/common/node/expansion.go:151
    Aug 11 15:28:10.373: INFO: Waiting up to 2m0s for pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d" in namespace "var-expansion-4269" to be "container 0 failed with reason CreateContainerConfigError"
    Aug 11 15:28:10.377: INFO: Pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 3.613993ms
    Aug 11 15:28:12.382: INFO: Pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008865522s
    Aug 11 15:28:12.382: INFO: Pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d" satisfied condition "container 0 failed with reason CreateContainerConfigError"
    Aug 11 15:28:12.382: INFO: Deleting pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d" in namespace "var-expansion-4269"
    Aug 11 15:28:12.391: INFO: Wait up to 5m0s for pod "var-expansion-764d3e23-7d25-4962-a4d7-9bbf32c48a2d" to be fully deleted
    [AfterEach] [sig-node] Variable Expansion
      test/e2e/framework/framework.go:187
    Aug 11 15:28:14.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "var-expansion-4269" for this suite. 08/11/23 15:28:14.405
  << End Captured GinkgoWriter Output
------------------------------
SSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:28:14.412
Aug 11 15:28:14.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename taint-multiple-pods 08/11/23 15:28:14.413
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:14.428
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:14.432
[BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/node/taints.go:348
Aug 11 15:28:14.434: INFO: Waiting up to 1m0s for all nodes to be ready
Aug 11 15:29:14.479: INFO: Waiting for terminating namespaces to be deleted...
[It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420
Aug 11 15:29:14.483: INFO: Starting informer...
STEP: Starting pods... 08/11/23 15:29:14.483
Aug 11 15:29:14.702: INFO: Pod1 is running on constell-d93e7e1d-worker-d314547c-0lc3. Tainting Node
Aug 11 15:29:14.914: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7699" to be "running"
Aug 11 15:29:14.918: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.791088ms
Aug 11 15:29:16.923: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008556992s
Aug 11 15:29:16.923: INFO: Pod "taint-eviction-b1" satisfied condition "running"
Aug 11 15:29:16.923: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7699" to be "running"
Aug 11 15:29:16.926: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.126208ms
Aug 11 15:29:16.926: INFO: Pod "taint-eviction-b2" satisfied condition "running"
Aug 11 15:29:16.926: INFO: Pod2 is running on constell-d93e7e1d-worker-d314547c-0lc3. Tainting Node
STEP: Trying to apply a taint on the Node 08/11/23 15:29:16.926
STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 15:29:16.938
STEP: Waiting for Pod1 and Pod2 to be deleted 08/11/23 15:29:16.942
Aug 11 15:29:23.388: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
Aug 11 15:29:42.426: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 15:29:42.44
[AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
  test/e2e/framework/framework.go:187
Aug 11 15:29:42.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "taint-multiple-pods-7699" for this suite. 08/11/23 15:29:42.448
{"msg":"PASSED [sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]","completed":321,"skipped":5978,"failed":0}
------------------------------
â€¢ [SLOW TEST] [88.056 seconds]
[sig-node] NoExecuteTaintManager Multiple Pods [Serial]
test/e2e/node/framework.go:23
  evicts pods with minTolerationSeconds [Disruptive] [Conformance]
  test/e2e/node/taints.go:420

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:28:14.412
    Aug 11 15:28:14.412: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename taint-multiple-pods 08/11/23 15:28:14.413
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:28:14.428
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:28:14.432
    [BeforeEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/node/taints.go:348
    Aug 11 15:28:14.434: INFO: Waiting up to 1m0s for all nodes to be ready
    Aug 11 15:29:14.479: INFO: Waiting for terminating namespaces to be deleted...
    [It] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
      test/e2e/node/taints.go:420
    Aug 11 15:29:14.483: INFO: Starting informer...
    STEP: Starting pods... 08/11/23 15:29:14.483
    Aug 11 15:29:14.702: INFO: Pod1 is running on constell-d93e7e1d-worker-d314547c-0lc3. Tainting Node
    Aug 11 15:29:14.914: INFO: Waiting up to 5m0s for pod "taint-eviction-b1" in namespace "taint-multiple-pods-7699" to be "running"
    Aug 11 15:29:14.918: INFO: Pod "taint-eviction-b1": Phase="Pending", Reason="", readiness=false. Elapsed: 3.791088ms
    Aug 11 15:29:16.923: INFO: Pod "taint-eviction-b1": Phase="Running", Reason="", readiness=true. Elapsed: 2.008556992s
    Aug 11 15:29:16.923: INFO: Pod "taint-eviction-b1" satisfied condition "running"
    Aug 11 15:29:16.923: INFO: Waiting up to 5m0s for pod "taint-eviction-b2" in namespace "taint-multiple-pods-7699" to be "running"
    Aug 11 15:29:16.926: INFO: Pod "taint-eviction-b2": Phase="Running", Reason="", readiness=true. Elapsed: 3.126208ms
    Aug 11 15:29:16.926: INFO: Pod "taint-eviction-b2" satisfied condition "running"
    Aug 11 15:29:16.926: INFO: Pod2 is running on constell-d93e7e1d-worker-d314547c-0lc3. Tainting Node
    STEP: Trying to apply a taint on the Node 08/11/23 15:29:16.926
    STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 15:29:16.938
    STEP: Waiting for Pod1 and Pod2 to be deleted 08/11/23 15:29:16.942
    Aug 11 15:29:23.388: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
    Aug 11 15:29:42.426: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
    STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute 08/11/23 15:29:42.44
    [AfterEach] [sig-node] NoExecuteTaintManager Multiple Pods [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 15:29:42.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "taint-multiple-pods-7699" for this suite. 08/11/23 15:29:42.448
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version
  should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:29:42.469
Aug 11 15:29:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 15:29:42.47
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:42.51
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:42.513
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should check is all data is printed  [Conformance]
  test/e2e/kubectl/kubectl.go:1683
Aug 11 15:29:42.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7432 version'
Aug 11 15:29:42.563: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
Aug 11 15:29:42.563: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.11\", GitCommit:\"8cfcba0b15c343a8dc48567a74c29ec4844e0b9e\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:57:26Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.11\", GitCommit:\"8cfcba0b15c343a8dc48567a74c29ec4844e0b9e\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:49:38Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 15:29:42.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-7432" for this suite. 08/11/23 15:29:42.568
{"msg":"PASSED [sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]","completed":322,"skipped":6015,"failed":0}
------------------------------
â€¢ [0.108 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl version
  test/e2e/kubectl/kubectl.go:1677
    should check is all data is printed  [Conformance]
    test/e2e/kubectl/kubectl.go:1683

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:29:42.469
    Aug 11 15:29:42.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:29:42.47
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:42.51
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:42.513
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should check is all data is printed  [Conformance]
      test/e2e/kubectl/kubectl.go:1683
    Aug 11 15:29:42.516: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-7432 version'
    Aug 11 15:29:42.563: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
    Aug 11 15:29:42.563: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.11\", GitCommit:\"8cfcba0b15c343a8dc48567a74c29ec4844e0b9e\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:57:26Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v4.5.7\nServer Version: version.Info{Major:\"1\", Minor:\"25\", GitVersion:\"v1.25.11\", GitCommit:\"8cfcba0b15c343a8dc48567a74c29ec4844e0b9e\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:49:38Z\", GoVersion:\"go1.19.10\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 15:29:42.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-7432" for this suite. 08/11/23 15:29:42.568
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected secret
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
[BeforeEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:29:42.578
Aug 11 15:29:42.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 15:29:42.579
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:42.594
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:42.596
[It] should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45
STEP: Creating projection with secret that has name projected-secret-test-cd805a67-91ce-4d41-9bc9-497035cbe81e 08/11/23 15:29:42.6
STEP: Creating a pod to test consume secrets 08/11/23 15:29:42.606
Aug 11 15:29:42.617: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a" in namespace "projected-4095" to be "Succeeded or Failed"
Aug 11 15:29:42.621: INFO: Pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.535611ms
Aug 11 15:29:44.627: INFO: Pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010040098s
Aug 11 15:29:46.626: INFO: Pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009342004s
STEP: Saw pod success 08/11/23 15:29:46.626
Aug 11 15:29:46.627: INFO: Pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a" satisfied condition "Succeeded or Failed"
Aug 11 15:29:46.630: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a container projected-secret-volume-test: <nil>
STEP: delete the pod 08/11/23 15:29:46.651
Aug 11 15:29:46.666: INFO: Waiting for pod pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a to disappear
Aug 11 15:29:46.670: INFO: Pod pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a no longer exists
[AfterEach] [sig-storage] Projected secret
  test/e2e/framework/framework.go:187
Aug 11 15:29:46.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-4095" for this suite. 08/11/23 15:29:46.675
{"msg":"PASSED [sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]","completed":323,"skipped":6026,"failed":0}
------------------------------
â€¢ [4.105 seconds]
[sig-storage] Projected secret
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_secret.go:45

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:29:42.578
    Aug 11 15:29:42.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 15:29:42.579
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:42.594
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:42.596
    [It] should be consumable from pods in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_secret.go:45
    STEP: Creating projection with secret that has name projected-secret-test-cd805a67-91ce-4d41-9bc9-497035cbe81e 08/11/23 15:29:42.6
    STEP: Creating a pod to test consume secrets 08/11/23 15:29:42.606
    Aug 11 15:29:42.617: INFO: Waiting up to 5m0s for pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a" in namespace "projected-4095" to be "Succeeded or Failed"
    Aug 11 15:29:42.621: INFO: Pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a": Phase="Pending", Reason="", readiness=false. Elapsed: 4.535611ms
    Aug 11 15:29:44.627: INFO: Pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010040098s
    Aug 11 15:29:46.626: INFO: Pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009342004s
    STEP: Saw pod success 08/11/23 15:29:46.626
    Aug 11 15:29:46.627: INFO: Pod "pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a" satisfied condition "Succeeded or Failed"
    Aug 11 15:29:46.630: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a container projected-secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 15:29:46.651
    Aug 11 15:29:46.666: INFO: Waiting for pod pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a to disappear
    Aug 11 15:29:46.670: INFO: Pod pod-projected-secrets-2f2f8ea0-f6ff-4a56-a17e-f8836cab6e2a no longer exists
    [AfterEach] [sig-storage] Projected secret
      test/e2e/framework/framework.go:187
    Aug 11 15:29:46.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-4095" for this suite. 08/11/23 15:29:46.675
  << End Captured GinkgoWriter Output
------------------------------
SSS
------------------------------
[sig-node] Pods
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
[BeforeEach] [sig-node] Pods
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:29:46.683
Aug 11 15:29:46.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename pods 08/11/23 15:29:46.684
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:46.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:46.703
[BeforeEach] [sig-node] Pods
  test/e2e/common/node/pods.go:193
[It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617
Aug 11 15:29:46.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: creating the pod 08/11/23 15:29:46.706
STEP: submitting the pod to kubernetes 08/11/23 15:29:46.706
Aug 11 15:29:46.718: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814" in namespace "pods-4996" to be "running and ready"
Aug 11 15:29:46.722: INFO: Pod "pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125858ms
Aug 11 15:29:46.722: INFO: The phase of Pod pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:29:48.728: INFO: Pod "pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814": Phase="Running", Reason="", readiness=true. Elapsed: 2.00976223s
Aug 11 15:29:48.728: INFO: The phase of Pod pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814 is Running (Ready = true)
Aug 11 15:29:48.728: INFO: Pod "pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814" satisfied condition "running and ready"
[AfterEach] [sig-node] Pods
  test/e2e/framework/framework.go:187
Aug 11 15:29:48.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "pods-4996" for this suite. 08/11/23 15:29:48.75
{"msg":"PASSED [sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]","completed":324,"skipped":6029,"failed":0}
------------------------------
â€¢ [2.074 seconds]
[sig-node] Pods
test/e2e/common/node/framework.go:23
  should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
  test/e2e/common/node/pods.go:617

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Pods
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:29:46.683
    Aug 11 15:29:46.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename pods 08/11/23 15:29:46.684
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:46.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:46.703
    [BeforeEach] [sig-node] Pods
      test/e2e/common/node/pods.go:193
    [It] should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
      test/e2e/common/node/pods.go:617
    Aug 11 15:29:46.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: creating the pod 08/11/23 15:29:46.706
    STEP: submitting the pod to kubernetes 08/11/23 15:29:46.706
    Aug 11 15:29:46.718: INFO: Waiting up to 5m0s for pod "pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814" in namespace "pods-4996" to be "running and ready"
    Aug 11 15:29:46.722: INFO: Pod "pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814": Phase="Pending", Reason="", readiness=false. Elapsed: 4.125858ms
    Aug 11 15:29:46.722: INFO: The phase of Pod pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:29:48.728: INFO: Pod "pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814": Phase="Running", Reason="", readiness=true. Elapsed: 2.00976223s
    Aug 11 15:29:48.728: INFO: The phase of Pod pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814 is Running (Ready = true)
    Aug 11 15:29:48.728: INFO: Pod "pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814" satisfied condition "running and ready"
    [AfterEach] [sig-node] Pods
      test/e2e/framework/framework.go:187
    Aug 11 15:29:48.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "pods-4996" for this suite. 08/11/23 15:29:48.75
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition
  creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
[BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:29:48.758
Aug 11 15:29:48.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 15:29:48.759
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:48.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:48.779
[It] creating/deleting custom resource definition objects works  [Conformance]
  test/e2e/apimachinery/custom_resource_definition.go:58
Aug 11 15:29:48.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
[AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:29:49.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "custom-resource-definition-9277" for this suite. 08/11/23 15:29:49.816
{"msg":"PASSED [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]","completed":325,"skipped":6038,"failed":0}
------------------------------
â€¢ [1.066 seconds]
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  Simple CustomResourceDefinition
  test/e2e/apimachinery/custom_resource_definition.go:50
    creating/deleting custom resource definition objects works  [Conformance]
    test/e2e/apimachinery/custom_resource_definition.go:58

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:29:48.758
    Aug 11 15:29:48.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename custom-resource-definition 08/11/23 15:29:48.759
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:48.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:48.779
    [It] creating/deleting custom resource definition objects works  [Conformance]
      test/e2e/apimachinery/custom_resource_definition.go:58
    Aug 11 15:29:48.782: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    [AfterEach] [sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:29:49.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "custom-resource-definition-9277" for this suite. 08/11/23 15:29:49.816
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-storage] ConfigMap
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
[BeforeEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:29:49.824
Aug 11 15:29:49.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 15:29:49.825
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:49.84
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:49.843
[It] optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239
STEP: Creating configMap with name cm-test-opt-del-95cf7016-9978-41f1-9b35-14b82c8fcbd8 08/11/23 15:29:49.851
STEP: Creating configMap with name cm-test-opt-upd-0e1b7a84-ca08-4db9-bf74-4b1b95d6a579 08/11/23 15:29:49.856
STEP: Creating the pod 08/11/23 15:29:49.862
Aug 11 15:29:49.872: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5" in namespace "configmap-1641" to be "running and ready"
Aug 11 15:29:49.879: INFO: Pod "pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328347ms
Aug 11 15:29:49.879: INFO: The phase of Pod pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:29:51.884: INFO: Pod "pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011423742s
Aug 11 15:29:51.884: INFO: The phase of Pod pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5 is Running (Ready = true)
Aug 11 15:29:51.884: INFO: Pod "pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5" satisfied condition "running and ready"
STEP: Deleting configmap cm-test-opt-del-95cf7016-9978-41f1-9b35-14b82c8fcbd8 08/11/23 15:29:51.912
STEP: Updating configmap cm-test-opt-upd-0e1b7a84-ca08-4db9-bf74-4b1b95d6a579 08/11/23 15:29:51.918
STEP: Creating configMap with name cm-test-opt-create-e970c72f-2ffb-4eb4-9088-87758400f7b9 08/11/23 15:29:51.923
STEP: waiting to observe update in volume 08/11/23 15:29:51.928
[AfterEach] [sig-storage] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 15:29:53.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-1641" for this suite. 08/11/23 15:29:53.974
{"msg":"PASSED [sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]","completed":326,"skipped":6040,"failed":0}
------------------------------
â€¢ [4.157 seconds]
[sig-storage] ConfigMap
test/e2e/common/storage/framework.go:23
  optional updates should be reflected in volume [NodeConformance] [Conformance]
  test/e2e/common/storage/configmap_volume.go:239

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:29:49.824
    Aug 11 15:29:49.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 15:29:49.825
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:49.84
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:49.843
    [It] optional updates should be reflected in volume [NodeConformance] [Conformance]
      test/e2e/common/storage/configmap_volume.go:239
    STEP: Creating configMap with name cm-test-opt-del-95cf7016-9978-41f1-9b35-14b82c8fcbd8 08/11/23 15:29:49.851
    STEP: Creating configMap with name cm-test-opt-upd-0e1b7a84-ca08-4db9-bf74-4b1b95d6a579 08/11/23 15:29:49.856
    STEP: Creating the pod 08/11/23 15:29:49.862
    Aug 11 15:29:49.872: INFO: Waiting up to 5m0s for pod "pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5" in namespace "configmap-1641" to be "running and ready"
    Aug 11 15:29:49.879: INFO: Pod "pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5": Phase="Pending", Reason="", readiness=false. Elapsed: 6.328347ms
    Aug 11 15:29:49.879: INFO: The phase of Pod pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:29:51.884: INFO: Pod "pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5": Phase="Running", Reason="", readiness=true. Elapsed: 2.011423742s
    Aug 11 15:29:51.884: INFO: The phase of Pod pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5 is Running (Ready = true)
    Aug 11 15:29:51.884: INFO: Pod "pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5" satisfied condition "running and ready"
    STEP: Deleting configmap cm-test-opt-del-95cf7016-9978-41f1-9b35-14b82c8fcbd8 08/11/23 15:29:51.912
    STEP: Updating configmap cm-test-opt-upd-0e1b7a84-ca08-4db9-bf74-4b1b95d6a579 08/11/23 15:29:51.918
    STEP: Creating configMap with name cm-test-opt-create-e970c72f-2ffb-4eb4-9088-87758400f7b9 08/11/23 15:29:51.923
    STEP: waiting to observe update in volume 08/11/23 15:29:51.928
    [AfterEach] [sig-storage] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 15:29:53.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-1641" for this suite. 08/11/23 15:29:53.974
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:29:53.982
Aug 11 15:29:53.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-pred 08/11/23 15:29:53.983
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:53.999
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:54.006
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 11 15:29:54.009: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 11 15:29:54.020: INFO: Waiting for terminating namespaces to be deleted...
Aug 11 15:29:54.024: INFO: 
Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-0lc3 before test
Aug 11 15:29:54.041: INFO: pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5 from configmap-1641 started at 2023-08-11 15:29:49 +0000 UTC (3 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container createcm-volume-test ready: true, restart count 0
Aug 11 15:29:54.041: INFO: 	Container delcm-volume-test ready: true, restart count 0
Aug 11 15:29:54.041: INFO: 	Container updcm-volume-test ready: true, restart count 0
Aug 11 15:29:54.041: INFO: cilium-6s7tr from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 15:29:54.041: INFO: cilium-operator-5bfff4c47c-92tfw from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 11 15:29:54.041: INFO: csi-gce-pd-node-dsdbs from kube-system started at 2023-08-11 13:55:05 +0000 UTC (2 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 15:29:54.041: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 15:29:54.041: INFO: gcp-guest-agent-rzv9l from kube-system started at 2023-08-11 15:29:42 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 15:29:54.041: INFO: konnectivity-agent-6nzsl from kube-system started at 2023-08-11 15:29:42 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 15:29:54.041: INFO: kube-proxy-vbf6p from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 15:29:54.041: INFO: verification-service-ttpg4 from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 15:29:54.041: INFO: pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814 from pods-4996 started at 2023-08-11 15:29:46 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container main ready: true, restart count 0
Aug 11 15:29:54.041: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 15:29:54.041: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:29:54.041: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 15:29:54.041: INFO: 
Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-wzlp before test
Aug 11 15:29:54.059: INFO: cilium-dcbmm from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.059: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 15:29:54.059: INFO: coredns-6c49cf4575-56cmb from kube-system started at 2023-08-11 14:41:36 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.059: INFO: 	Container coredns ready: true, restart count 0
Aug 11 15:29:54.059: INFO: csi-gce-pd-node-tnkjk from kube-system started at 2023-08-11 13:55:09 +0000 UTC (2 container statuses recorded)
Aug 11 15:29:54.059: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 15:29:54.059: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 15:29:54.059: INFO: gcp-guest-agent-7fj57 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.059: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 15:29:54.059: INFO: konnectivity-agent-bwlfc from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.059: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 15:29:54.059: INFO: kube-proxy-9wx2l from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.059: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 15:29:54.059: INFO: verification-service-xmjlz from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.059: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 15:29:54.060: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:01:59 +0000 UTC (1 container statuses recorded)
Aug 11 15:29:54.060: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 11 15:29:54.060: INFO: sonobuoy-e2e-job-2008a9ff359d4340 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 15:29:54.060: INFO: 	Container e2e ready: true, restart count 0
Aug 11 15:29:54.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:29:54.060: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 15:29:54.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:29:54.060: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461
STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 15:29:54.06
Aug 11 15:29:54.077: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3691" to be "running"
Aug 11 15:29:54.084: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346347ms
Aug 11 15:29:56.089: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011445123s
Aug 11 15:29:56.089: INFO: Pod "without-label" satisfied condition "running"
STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 15:29:56.092
STEP: Trying to apply a random label on the found node. 08/11/23 15:29:56.106
STEP: verifying the node has the label kubernetes.io/e2e-4760cc13-b198-49d1-ba05-9247ba777f29 42 08/11/23 15:29:56.117
STEP: Trying to relaunch the pod, now with labels. 08/11/23 15:29:56.12
Aug 11 15:29:56.130: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3691" to be "not pending"
Aug 11 15:29:56.134: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002754ms
Aug 11 15:29:58.140: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.00978746s
Aug 11 15:29:58.140: INFO: Pod "with-labels" satisfied condition "not pending"
STEP: removing the label kubernetes.io/e2e-4760cc13-b198-49d1-ba05-9247ba777f29 off the node constell-d93e7e1d-worker-d314547c-wzlp 08/11/23 15:29:58.143
STEP: verifying the node doesn't have the label kubernetes.io/e2e-4760cc13-b198-49d1-ba05-9247ba777f29 08/11/23 15:29:58.157
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Aug 11 15:29:58.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-3691" for this suite. 08/11/23 15:29:58.167
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]","completed":327,"skipped":6070,"failed":0}
------------------------------
â€¢ [4.192 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates that NodeSelector is respected if matching  [Conformance]
  test/e2e/scheduling/predicates.go:461

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:29:53.982
    Aug 11 15:29:53.982: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-pred 08/11/23 15:29:53.983
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:53.999
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:54.006
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Aug 11 15:29:54.009: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 11 15:29:54.020: INFO: Waiting for terminating namespaces to be deleted...
    Aug 11 15:29:54.024: INFO: 
    Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-0lc3 before test
    Aug 11 15:29:54.041: INFO: pod-configmaps-8f80f23e-bfb4-423f-abc5-13f79e940dd5 from configmap-1641 started at 2023-08-11 15:29:49 +0000 UTC (3 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container createcm-volume-test ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: 	Container delcm-volume-test ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: 	Container updcm-volume-test ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: cilium-6s7tr from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 15:29:54.041: INFO: cilium-operator-5bfff4c47c-92tfw from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: csi-gce-pd-node-dsdbs from kube-system started at 2023-08-11 13:55:05 +0000 UTC (2 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: gcp-guest-agent-rzv9l from kube-system started at 2023-08-11 15:29:42 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: konnectivity-agent-6nzsl from kube-system started at 2023-08-11 15:29:42 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: kube-proxy-vbf6p from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: verification-service-ttpg4 from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: pod-logs-websocket-5d54fd2a-2054-4ca6-851b-de2ff52bf814 from pods-4996 started at 2023-08-11 15:29:46 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container main ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 15:29:54.041: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 15:29:54.041: INFO: 
    Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-wzlp before test
    Aug 11 15:29:54.059: INFO: cilium-dcbmm from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.059: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 15:29:54.059: INFO: coredns-6c49cf4575-56cmb from kube-system started at 2023-08-11 14:41:36 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.059: INFO: 	Container coredns ready: true, restart count 0
    Aug 11 15:29:54.059: INFO: csi-gce-pd-node-tnkjk from kube-system started at 2023-08-11 13:55:09 +0000 UTC (2 container statuses recorded)
    Aug 11 15:29:54.059: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 15:29:54.059: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 15:29:54.059: INFO: gcp-guest-agent-7fj57 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.059: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 15:29:54.059: INFO: konnectivity-agent-bwlfc from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.059: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 15:29:54.059: INFO: kube-proxy-9wx2l from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.059: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 15:29:54.059: INFO: verification-service-xmjlz from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.059: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 15:29:54.060: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:01:59 +0000 UTC (1 container statuses recorded)
    Aug 11 15:29:54.060: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 11 15:29:54.060: INFO: sonobuoy-e2e-job-2008a9ff359d4340 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 15:29:54.060: INFO: 	Container e2e ready: true, restart count 0
    Aug 11 15:29:54.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:29:54.060: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 15:29:54.060: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:29:54.060: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates that NodeSelector is respected if matching  [Conformance]
      test/e2e/scheduling/predicates.go:461
    STEP: Trying to launch a pod without a label to get a node which can launch it. 08/11/23 15:29:54.06
    Aug 11 15:29:54.077: INFO: Waiting up to 1m0s for pod "without-label" in namespace "sched-pred-3691" to be "running"
    Aug 11 15:29:54.084: INFO: Pod "without-label": Phase="Pending", Reason="", readiness=false. Elapsed: 6.346347ms
    Aug 11 15:29:56.089: INFO: Pod "without-label": Phase="Running", Reason="", readiness=true. Elapsed: 2.011445123s
    Aug 11 15:29:56.089: INFO: Pod "without-label" satisfied condition "running"
    STEP: Explicitly delete pod here to free the resource it takes. 08/11/23 15:29:56.092
    STEP: Trying to apply a random label on the found node. 08/11/23 15:29:56.106
    STEP: verifying the node has the label kubernetes.io/e2e-4760cc13-b198-49d1-ba05-9247ba777f29 42 08/11/23 15:29:56.117
    STEP: Trying to relaunch the pod, now with labels. 08/11/23 15:29:56.12
    Aug 11 15:29:56.130: INFO: Waiting up to 5m0s for pod "with-labels" in namespace "sched-pred-3691" to be "not pending"
    Aug 11 15:29:56.134: INFO: Pod "with-labels": Phase="Pending", Reason="", readiness=false. Elapsed: 4.002754ms
    Aug 11 15:29:58.140: INFO: Pod "with-labels": Phase="Running", Reason="", readiness=true. Elapsed: 2.00978746s
    Aug 11 15:29:58.140: INFO: Pod "with-labels" satisfied condition "not pending"
    STEP: removing the label kubernetes.io/e2e-4760cc13-b198-49d1-ba05-9247ba777f29 off the node constell-d93e7e1d-worker-d314547c-wzlp 08/11/23 15:29:58.143
    STEP: verifying the node doesn't have the label kubernetes.io/e2e-4760cc13-b198-49d1-ba05-9247ba777f29 08/11/23 15:29:58.157
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 15:29:58.161: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-3691" for this suite. 08/11/23 15:29:58.167
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
[BeforeEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:29:58.174
Aug 11 15:29:58.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename svcaccounts 08/11/23 15:29:58.176
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:58.19
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:58.193
[It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528
Aug 11 15:29:58.212: INFO: created pod
Aug 11 15:29:58.212: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7606" to be "Succeeded or Failed"
Aug 11 15:29:58.218: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.849552ms
Aug 11 15:30:00.223: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010421581s
Aug 11 15:30:02.224: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011300786s
STEP: Saw pod success 08/11/23 15:30:02.224
Aug 11 15:30:02.224: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
Aug 11 15:30:32.225: INFO: polling logs
Aug 11 15:30:32.252: INFO: Pod logs: 
I0811 15:29:59.044152       1 log.go:195] OK: Got token
I0811 15:29:59.044195       1 log.go:195] validating with in-cluster discovery
I0811 15:29:59.044519       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
I0811 15:29:59.044553       1 log.go:195] Full, not-validated claims: 
openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7606:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691768398, NotBefore:1691767798, IssuedAt:1691767798, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7606", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"aace1017-1c0c-4bea-b90c-37f334eacfd0"}}}
I0811 15:29:59.055890       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
I0811 15:29:59.060462       1 log.go:195] OK: Validated signature on JWT
I0811 15:29:59.060574       1 log.go:195] OK: Got valid claims from token!
I0811 15:29:59.060610       1 log.go:195] Full, validated claims: 
&openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7606:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691768398, NotBefore:1691767798, IssuedAt:1691767798, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7606", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"aace1017-1c0c-4bea-b90c-37f334eacfd0"}}}

Aug 11 15:30:32.252: INFO: completed pod
[AfterEach] [sig-auth] ServiceAccounts
  test/e2e/framework/framework.go:187
Aug 11 15:30:32.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svcaccounts-7606" for this suite. 08/11/23 15:30:32.264
{"msg":"PASSED [sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]","completed":328,"skipped":6079,"failed":0}
------------------------------
â€¢ [SLOW TEST] [34.096 seconds]
[sig-auth] ServiceAccounts
test/e2e/auth/framework.go:23
  ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
  test/e2e/auth/service_accounts.go:528

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:29:58.174
    Aug 11 15:29:58.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename svcaccounts 08/11/23 15:29:58.176
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:29:58.19
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:29:58.193
    [It] ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
      test/e2e/auth/service_accounts.go:528
    Aug 11 15:29:58.212: INFO: created pod
    Aug 11 15:29:58.212: INFO: Waiting up to 5m0s for pod "oidc-discovery-validator" in namespace "svcaccounts-7606" to be "Succeeded or Failed"
    Aug 11 15:29:58.218: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 5.849552ms
    Aug 11 15:30:00.223: INFO: Pod "oidc-discovery-validator": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010421581s
    Aug 11 15:30:02.224: INFO: Pod "oidc-discovery-validator": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011300786s
    STEP: Saw pod success 08/11/23 15:30:02.224
    Aug 11 15:30:02.224: INFO: Pod "oidc-discovery-validator" satisfied condition "Succeeded or Failed"
    Aug 11 15:30:32.225: INFO: polling logs
    Aug 11 15:30:32.252: INFO: Pod logs: 
    I0811 15:29:59.044152       1 log.go:195] OK: Got token
    I0811 15:29:59.044195       1 log.go:195] validating with in-cluster discovery
    I0811 15:29:59.044519       1 log.go:195] OK: got issuer https://kubernetes.default.svc.cluster.local
    I0811 15:29:59.044553       1 log.go:195] Full, not-validated claims: 
    openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7606:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691768398, NotBefore:1691767798, IssuedAt:1691767798, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7606", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"aace1017-1c0c-4bea-b90c-37f334eacfd0"}}}
    I0811 15:29:59.055890       1 log.go:195] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
    I0811 15:29:59.060462       1 log.go:195] OK: Validated signature on JWT
    I0811 15:29:59.060574       1 log.go:195] OK: Got valid claims from token!
    I0811 15:29:59.060610       1 log.go:195] Full, validated claims: 
    &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7606:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691768398, NotBefore:1691767798, IssuedAt:1691767798, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7606", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"aace1017-1c0c-4bea-b90c-37f334eacfd0"}}}

    Aug 11 15:30:32.252: INFO: completed pod
    [AfterEach] [sig-auth] ServiceAccounts
      test/e2e/framework/framework.go:187
    Aug 11 15:30:32.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svcaccounts-7606" for this suite. 08/11/23 15:30:32.264
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial]
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
[BeforeEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:30:32.271
Aug 11 15:30:32.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename namespaces 08/11/23 15:30:32.272
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:32.29
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:32.294
[It] should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267
STEP: creating a Namespace 08/11/23 15:30:32.297
STEP: patching the Namespace 08/11/23 15:30:32.314
STEP: get the Namespace and ensuring it has the label 08/11/23 15:30:32.322
[AfterEach] [sig-api-machinery] Namespaces [Serial]
  test/e2e/framework/framework.go:187
Aug 11 15:30:32.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "namespaces-940" for this suite. 08/11/23 15:30:32.33
STEP: Destroying namespace "nspatchtest-d127edc5-e73f-47f3-9ce8-b2e6b66466f8-5919" for this suite. 08/11/23 15:30:32.337
{"msg":"PASSED [sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]","completed":329,"skipped":6081,"failed":0}
------------------------------
â€¢ [0.072 seconds]
[sig-api-machinery] Namespaces [Serial]
test/e2e/apimachinery/framework.go:23
  should patch a Namespace [Conformance]
  test/e2e/apimachinery/namespace.go:267

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:30:32.271
    Aug 11 15:30:32.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename namespaces 08/11/23 15:30:32.272
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:32.29
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:32.294
    [It] should patch a Namespace [Conformance]
      test/e2e/apimachinery/namespace.go:267
    STEP: creating a Namespace 08/11/23 15:30:32.297
    STEP: patching the Namespace 08/11/23 15:30:32.314
    STEP: get the Namespace and ensuring it has the label 08/11/23 15:30:32.322
    [AfterEach] [sig-api-machinery] Namespaces [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 15:30:32.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "namespaces-940" for this suite. 08/11/23 15:30:32.33
    STEP: Destroying namespace "nspatchtest-d127edc5-e73f-47f3-9ce8-b2e6b66466f8-5919" for this suite. 08/11/23 15:30:32.337
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:30:32.345
Aug 11 15:30:32.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename statefulset 08/11/23 15:30:32.346
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:32.363
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:32.366
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-9442 08/11/23 15:30:32.368
[It] should perform rolling updates and roll backs of template modifications [Conformance]
  test/e2e/apps/statefulset.go:304
STEP: Creating a new StatefulSet 08/11/23 15:30:32.374
Aug 11 15:30:32.387: INFO: Found 0 stateful pods, waiting for 3
Aug 11 15:30:42.393: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:30:42.393: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:30:42.393: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:30:42.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-9442 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:30:42.572: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:30:42.572: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:30:42.572: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 08/11/23 15:30:52.588
Aug 11 15:30:52.609: INFO: Updating stateful set ss2
STEP: Creating a new revision 08/11/23 15:30:52.609
STEP: Updating Pods in reverse ordinal order 08/11/23 15:31:02.63
Aug 11 15:31:02.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-9442 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 15:31:02.776: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 15:31:02.776: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 15:31:02.776: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

STEP: Rolling back to a previous revision 08/11/23 15:31:12.799
Aug 11 15:31:12.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-9442 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:31:12.945: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:31:12.945: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:31:12.945: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 15:31:22.984: INFO: Updating stateful set ss2
STEP: Rolling back update in reverse ordinal order 08/11/23 15:31:33.005
Aug 11 15:31:33.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-9442 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 15:31:33.152: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 15:31:33.152: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 15:31:33.152: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 11 15:31:43.175: INFO: Deleting all statefulset in ns statefulset-9442
Aug 11 15:31:43.178: INFO: Scaling statefulset ss2 to 0
Aug 11 15:31:53.194: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:31:53.197: INFO: Deleting statefulset ss2
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 11 15:31:53.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-9442" for this suite. 08/11/23 15:31:53.217
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]","completed":330,"skipped":6115,"failed":0}
------------------------------
â€¢ [SLOW TEST] [80.878 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    should perform rolling updates and roll backs of template modifications [Conformance]
    test/e2e/apps/statefulset.go:304

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:30:32.345
    Aug 11 15:30:32.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename statefulset 08/11/23 15:30:32.346
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:30:32.363
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:30:32.366
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-9442 08/11/23 15:30:32.368
    [It] should perform rolling updates and roll backs of template modifications [Conformance]
      test/e2e/apps/statefulset.go:304
    STEP: Creating a new StatefulSet 08/11/23 15:30:32.374
    Aug 11 15:30:32.387: INFO: Found 0 stateful pods, waiting for 3
    Aug 11 15:30:42.393: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:30:42.393: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:30:42.393: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:30:42.403: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-9442 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:30:42.572: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:30:42.572: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:30:42.572: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-2 to registry.k8s.io/e2e-test-images/httpd:2.4.39-2 08/11/23 15:30:52.588
    Aug 11 15:30:52.609: INFO: Updating stateful set ss2
    STEP: Creating a new revision 08/11/23 15:30:52.609
    STEP: Updating Pods in reverse ordinal order 08/11/23 15:31:02.63
    Aug 11 15:31:02.633: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-9442 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 15:31:02.776: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 15:31:02.776: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 15:31:02.776: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    STEP: Rolling back to a previous revision 08/11/23 15:31:12.799
    Aug 11 15:31:12.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-9442 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:31:12.945: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:31:12.945: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:31:12.945: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 15:31:22.984: INFO: Updating stateful set ss2
    STEP: Rolling back update in reverse ordinal order 08/11/23 15:31:33.005
    Aug 11 15:31:33.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-9442 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 15:31:33.152: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 15:31:33.152: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 15:31:33.152: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 11 15:31:43.175: INFO: Deleting all statefulset in ns statefulset-9442
    Aug 11 15:31:43.178: INFO: Scaling statefulset ss2 to 0
    Aug 11 15:31:53.194: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:31:53.197: INFO: Deleting statefulset ss2
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 11 15:31:53.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-9442" for this suite. 08/11/23 15:31:53.217
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial]
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:31:53.225
Aug 11 15:31:53.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename sched-pred 08/11/23 15:31:53.226
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:53.242
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:53.245
[BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:92
Aug 11 15:31:53.248: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
Aug 11 15:31:53.257: INFO: Waiting for terminating namespaces to be deleted...
Aug 11 15:31:53.263: INFO: 
Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-0lc3 before test
Aug 11 15:31:53.271: INFO: cilium-6s7tr from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.271: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 15:31:53.271: INFO: cilium-operator-5bfff4c47c-92tfw from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.271: INFO: 	Container cilium-operator ready: true, restart count 0
Aug 11 15:31:53.271: INFO: csi-gce-pd-node-dsdbs from kube-system started at 2023-08-11 13:55:05 +0000 UTC (2 container statuses recorded)
Aug 11 15:31:53.271: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 15:31:53.271: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 15:31:53.271: INFO: gcp-guest-agent-rzv9l from kube-system started at 2023-08-11 15:29:42 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.271: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 15:31:53.271: INFO: konnectivity-agent-6nzsl from kube-system started at 2023-08-11 15:29:42 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.271: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 15:31:53.271: INFO: kube-proxy-vbf6p from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.271: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 15:31:53.271: INFO: verification-service-ttpg4 from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.271: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 15:31:53.271: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 15:31:53.271: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:31:53.271: INFO: 	Container systemd-logs ready: true, restart count 0
Aug 11 15:31:53.271: INFO: 
Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-wzlp before test
Aug 11 15:31:53.281: INFO: cilium-dcbmm from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container cilium-agent ready: true, restart count 1
Aug 11 15:31:53.281: INFO: coredns-6c49cf4575-56cmb from kube-system started at 2023-08-11 14:41:36 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container coredns ready: true, restart count 0
Aug 11 15:31:53.281: INFO: csi-gce-pd-node-tnkjk from kube-system started at 2023-08-11 13:55:09 +0000 UTC (2 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container csi-driver-registrar ready: true, restart count 0
Aug 11 15:31:53.281: INFO: 	Container gce-pd-driver ready: true, restart count 0
Aug 11 15:31:53.281: INFO: gcp-guest-agent-7fj57 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container gcp-guest-agent ready: true, restart count 0
Aug 11 15:31:53.281: INFO: konnectivity-agent-bwlfc from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container konnectivity-agent ready: true, restart count 0
Aug 11 15:31:53.281: INFO: kube-proxy-9wx2l from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container kube-proxy ready: true, restart count 0
Aug 11 15:31:53.281: INFO: verification-service-xmjlz from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container verification-service ready: true, restart count 0
Aug 11 15:31:53.281: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:01:59 +0000 UTC (1 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container kube-sonobuoy ready: true, restart count 0
Aug 11 15:31:53.281: INFO: sonobuoy-e2e-job-2008a9ff359d4340 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container e2e ready: true, restart count 0
Aug 11 15:31:53.281: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:31:53.281: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
Aug 11 15:31:53.281: INFO: 	Container sonobuoy-worker ready: true, restart count 0
Aug 11 15:31:53.281: INFO: 	Container systemd-logs ready: true, restart count 0
[It] validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326
STEP: verifying the node has the label node constell-d93e7e1d-worker-d314547c-0lc3 08/11/23 15:31:53.308
STEP: verifying the node has the label node constell-d93e7e1d-worker-d314547c-wzlp 08/11/23 15:31:53.323
Aug 11 15:31:53.348: INFO: Pod cilium-6s7tr requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
Aug 11 15:31:53.348: INFO: Pod cilium-dcbmm requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.348: INFO: Pod cilium-operator-5bfff4c47c-92tfw requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
Aug 11 15:31:53.348: INFO: Pod coredns-6c49cf4575-56cmb requesting resource cpu=100m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.349: INFO: Pod csi-gce-pd-node-dsdbs requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
Aug 11 15:31:53.349: INFO: Pod csi-gce-pd-node-tnkjk requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.349: INFO: Pod gcp-guest-agent-7fj57 requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.349: INFO: Pod gcp-guest-agent-rzv9l requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
Aug 11 15:31:53.349: INFO: Pod konnectivity-agent-6nzsl requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
Aug 11 15:31:53.349: INFO: Pod konnectivity-agent-bwlfc requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.349: INFO: Pod kube-proxy-9wx2l requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.350: INFO: Pod kube-proxy-vbf6p requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
Aug 11 15:31:53.350: INFO: Pod verification-service-ttpg4 requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
Aug 11 15:31:53.350: INFO: Pod verification-service-xmjlz requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.350: INFO: Pod sonobuoy requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.350: INFO: Pod sonobuoy-e2e-job-2008a9ff359d4340 requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.350: INFO: Pod sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
Aug 11 15:31:53.350: INFO: Pod sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
STEP: Starting Pods to consume most of the cluster CPU. 08/11/23 15:31:53.35
Aug 11 15:31:53.351: INFO: Creating a pod which consumes cpu=2800m on Node constell-d93e7e1d-worker-d314547c-0lc3
Aug 11 15:31:53.361: INFO: Creating a pod which consumes cpu=2730m on Node constell-d93e7e1d-worker-d314547c-wzlp
Aug 11 15:31:53.371: INFO: Waiting up to 5m0s for pod "filler-pod-6bace636-5920-455a-8adb-76569a904f7f" in namespace "sched-pred-4858" to be "running"
Aug 11 15:31:53.375: INFO: Pod "filler-pod-6bace636-5920-455a-8adb-76569a904f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.802759ms
Aug 11 15:31:55.380: INFO: Pod "filler-pod-6bace636-5920-455a-8adb-76569a904f7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009030758s
Aug 11 15:31:55.380: INFO: Pod "filler-pod-6bace636-5920-455a-8adb-76569a904f7f" satisfied condition "running"
Aug 11 15:31:55.380: INFO: Waiting up to 5m0s for pod "filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468" in namespace "sched-pred-4858" to be "running"
Aug 11 15:31:55.384: INFO: Pod "filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468": Phase="Running", Reason="", readiness=true. Elapsed: 3.336584ms
Aug 11 15:31:55.384: INFO: Pod "filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468" satisfied condition "running"
STEP: Creating another pod that requires unavailable amount of CPU. 08/11/23 15:31:55.384
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6bace636-5920-455a-8adb-76569a904f7f.177a5df42896ac9f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4858/filler-pod-6bace636-5920-455a-8adb-76569a904f7f to constell-d93e7e1d-worker-d314547c-0lc3] 08/11/23 15:31:55.388
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6bace636-5920-455a-8adb-76569a904f7f.177a5df44b044e46], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 08/11/23 15:31:55.388
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6bace636-5920-455a-8adb-76569a904f7f.177a5df44c7ccf52], Reason = [Created], Message = [Created container filler-pod-6bace636-5920-455a-8adb-76569a904f7f] 08/11/23 15:31:55.388
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-6bace636-5920-455a-8adb-76569a904f7f.177a5df4522eebb9], Reason = [Started], Message = [Started container filler-pod-6bace636-5920-455a-8adb-76569a904f7f] 08/11/23 15:31:55.388
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468.177a5df4290da993], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4858/filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468 to constell-d93e7e1d-worker-d314547c-wzlp] 08/11/23 15:31:55.388
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468.177a5df4565d09b7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 08/11/23 15:31:55.388
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468.177a5df4584f9c82], Reason = [Created], Message = [Created container filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468] 08/11/23 15:31:55.388
STEP: Considering event: 
Type = [Normal], Name = [filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468.177a5df45e6c75a5], Reason = [Started], Message = [Started container filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468] 08/11/23 15:31:55.388
STEP: Considering event: 
Type = [Warning], Name = [additional-pod.177a5df4a19305dd], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 08/11/23 15:31:55.403
STEP: removing the label node off the node constell-d93e7e1d-worker-d314547c-0lc3 08/11/23 15:31:56.401
STEP: verifying the node doesn't have the label node 08/11/23 15:31:56.415
STEP: removing the label node off the node constell-d93e7e1d-worker-d314547c-wzlp 08/11/23 15:31:56.419
STEP: verifying the node doesn't have the label node 08/11/23 15:31:56.444
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/framework/framework.go:187
Aug 11 15:31:56.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "sched-pred-4858" for this suite. 08/11/23 15:31:56.462
[AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
  test/e2e/scheduling/predicates.go:83
{"msg":"PASSED [sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]","completed":331,"skipped":6138,"failed":0}
------------------------------
â€¢ [3.255 seconds]
[sig-scheduling] SchedulerPredicates [Serial]
test/e2e/scheduling/framework.go:40
  validates resource limits of pods that are allowed to run  [Conformance]
  test/e2e/scheduling/predicates.go:326

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:31:53.225
    Aug 11 15:31:53.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename sched-pred 08/11/23 15:31:53.226
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:53.242
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:53.245
    [BeforeEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:92
    Aug 11 15:31:53.248: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
    Aug 11 15:31:53.257: INFO: Waiting for terminating namespaces to be deleted...
    Aug 11 15:31:53.263: INFO: 
    Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-0lc3 before test
    Aug 11 15:31:53.271: INFO: cilium-6s7tr from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.271: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 15:31:53.271: INFO: cilium-operator-5bfff4c47c-92tfw from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.271: INFO: 	Container cilium-operator ready: true, restart count 0
    Aug 11 15:31:53.271: INFO: csi-gce-pd-node-dsdbs from kube-system started at 2023-08-11 13:55:05 +0000 UTC (2 container statuses recorded)
    Aug 11 15:31:53.271: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 15:31:53.271: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 15:31:53.271: INFO: gcp-guest-agent-rzv9l from kube-system started at 2023-08-11 15:29:42 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.271: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 15:31:53.271: INFO: konnectivity-agent-6nzsl from kube-system started at 2023-08-11 15:29:42 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.271: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 15:31:53.271: INFO: kube-proxy-vbf6p from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.271: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 15:31:53.271: INFO: verification-service-ttpg4 from kube-system started at 2023-08-11 13:55:05 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.271: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 15:31:53.271: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 15:31:53.271: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:31:53.271: INFO: 	Container systemd-logs ready: true, restart count 0
    Aug 11 15:31:53.271: INFO: 
    Logging pods the apiserver thinks is on node constell-d93e7e1d-worker-d314547c-wzlp before test
    Aug 11 15:31:53.281: INFO: cilium-dcbmm from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container cilium-agent ready: true, restart count 1
    Aug 11 15:31:53.281: INFO: coredns-6c49cf4575-56cmb from kube-system started at 2023-08-11 14:41:36 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container coredns ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: csi-gce-pd-node-tnkjk from kube-system started at 2023-08-11 13:55:09 +0000 UTC (2 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container csi-driver-registrar ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: 	Container gce-pd-driver ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: gcp-guest-agent-7fj57 from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container gcp-guest-agent ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: konnectivity-agent-bwlfc from kube-system started at 2023-08-11 13:55:34 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container konnectivity-agent ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: kube-proxy-9wx2l from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container kube-proxy ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: verification-service-xmjlz from kube-system started at 2023-08-11 13:55:09 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container verification-service ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: sonobuoy from sonobuoy started at 2023-08-11 14:01:59 +0000 UTC (1 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container kube-sonobuoy ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: sonobuoy-e2e-job-2008a9ff359d4340 from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container e2e ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll from sonobuoy started at 2023-08-11 14:02:03 +0000 UTC (2 container statuses recorded)
    Aug 11 15:31:53.281: INFO: 	Container sonobuoy-worker ready: true, restart count 0
    Aug 11 15:31:53.281: INFO: 	Container systemd-logs ready: true, restart count 0
    [It] validates resource limits of pods that are allowed to run  [Conformance]
      test/e2e/scheduling/predicates.go:326
    STEP: verifying the node has the label node constell-d93e7e1d-worker-d314547c-0lc3 08/11/23 15:31:53.308
    STEP: verifying the node has the label node constell-d93e7e1d-worker-d314547c-wzlp 08/11/23 15:31:53.323
    Aug 11 15:31:53.348: INFO: Pod cilium-6s7tr requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
    Aug 11 15:31:53.348: INFO: Pod cilium-dcbmm requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.348: INFO: Pod cilium-operator-5bfff4c47c-92tfw requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
    Aug 11 15:31:53.348: INFO: Pod coredns-6c49cf4575-56cmb requesting resource cpu=100m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.349: INFO: Pod csi-gce-pd-node-dsdbs requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
    Aug 11 15:31:53.349: INFO: Pod csi-gce-pd-node-tnkjk requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.349: INFO: Pod gcp-guest-agent-7fj57 requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.349: INFO: Pod gcp-guest-agent-rzv9l requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
    Aug 11 15:31:53.349: INFO: Pod konnectivity-agent-6nzsl requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
    Aug 11 15:31:53.349: INFO: Pod konnectivity-agent-bwlfc requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.349: INFO: Pod kube-proxy-9wx2l requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.350: INFO: Pod kube-proxy-vbf6p requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
    Aug 11 15:31:53.350: INFO: Pod verification-service-ttpg4 requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
    Aug 11 15:31:53.350: INFO: Pod verification-service-xmjlz requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.350: INFO: Pod sonobuoy requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.350: INFO: Pod sonobuoy-e2e-job-2008a9ff359d4340 requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.350: INFO: Pod sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-66454 requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-0lc3
    Aug 11 15:31:53.350: INFO: Pod sonobuoy-systemd-logs-daemon-set-e60d6dc7beac4e1b-zntll requesting resource cpu=0m on Node constell-d93e7e1d-worker-d314547c-wzlp
    STEP: Starting Pods to consume most of the cluster CPU. 08/11/23 15:31:53.35
    Aug 11 15:31:53.351: INFO: Creating a pod which consumes cpu=2800m on Node constell-d93e7e1d-worker-d314547c-0lc3
    Aug 11 15:31:53.361: INFO: Creating a pod which consumes cpu=2730m on Node constell-d93e7e1d-worker-d314547c-wzlp
    Aug 11 15:31:53.371: INFO: Waiting up to 5m0s for pod "filler-pod-6bace636-5920-455a-8adb-76569a904f7f" in namespace "sched-pred-4858" to be "running"
    Aug 11 15:31:53.375: INFO: Pod "filler-pod-6bace636-5920-455a-8adb-76569a904f7f": Phase="Pending", Reason="", readiness=false. Elapsed: 3.802759ms
    Aug 11 15:31:55.380: INFO: Pod "filler-pod-6bace636-5920-455a-8adb-76569a904f7f": Phase="Running", Reason="", readiness=true. Elapsed: 2.009030758s
    Aug 11 15:31:55.380: INFO: Pod "filler-pod-6bace636-5920-455a-8adb-76569a904f7f" satisfied condition "running"
    Aug 11 15:31:55.380: INFO: Waiting up to 5m0s for pod "filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468" in namespace "sched-pred-4858" to be "running"
    Aug 11 15:31:55.384: INFO: Pod "filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468": Phase="Running", Reason="", readiness=true. Elapsed: 3.336584ms
    Aug 11 15:31:55.384: INFO: Pod "filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468" satisfied condition "running"
    STEP: Creating another pod that requires unavailable amount of CPU. 08/11/23 15:31:55.384
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6bace636-5920-455a-8adb-76569a904f7f.177a5df42896ac9f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4858/filler-pod-6bace636-5920-455a-8adb-76569a904f7f to constell-d93e7e1d-worker-d314547c-0lc3] 08/11/23 15:31:55.388
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6bace636-5920-455a-8adb-76569a904f7f.177a5df44b044e46], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 08/11/23 15:31:55.388
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6bace636-5920-455a-8adb-76569a904f7f.177a5df44c7ccf52], Reason = [Created], Message = [Created container filler-pod-6bace636-5920-455a-8adb-76569a904f7f] 08/11/23 15:31:55.388
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-6bace636-5920-455a-8adb-76569a904f7f.177a5df4522eebb9], Reason = [Started], Message = [Started container filler-pod-6bace636-5920-455a-8adb-76569a904f7f] 08/11/23 15:31:55.388
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468.177a5df4290da993], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4858/filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468 to constell-d93e7e1d-worker-d314547c-wzlp] 08/11/23 15:31:55.388
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468.177a5df4565d09b7], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.8" already present on machine] 08/11/23 15:31:55.388
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468.177a5df4584f9c82], Reason = [Created], Message = [Created container filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468] 08/11/23 15:31:55.388
    STEP: Considering event: 
    Type = [Normal], Name = [filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468.177a5df45e6c75a5], Reason = [Started], Message = [Started container filler-pod-c6238aba-ae37-4bfc-bc73-eb2b8be91468] 08/11/23 15:31:55.388
    STEP: Considering event: 
    Type = [Warning], Name = [additional-pod.177a5df4a19305dd], Reason = [FailedScheduling], Message = [0/5 nodes are available: 2 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/5 nodes are available: 2 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling.] 08/11/23 15:31:55.403
    STEP: removing the label node off the node constell-d93e7e1d-worker-d314547c-0lc3 08/11/23 15:31:56.401
    STEP: verifying the node doesn't have the label node 08/11/23 15:31:56.415
    STEP: removing the label node off the node constell-d93e7e1d-worker-d314547c-wzlp 08/11/23 15:31:56.419
    STEP: verifying the node doesn't have the label node 08/11/23 15:31:56.444
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 15:31:56.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "sched-pred-4858" for this suite. 08/11/23 15:31:56.462
    [AfterEach] [sig-scheduling] SchedulerPredicates [Serial]
      test/e2e/scheduling/predicates.go:83
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:31:56.492
Aug 11 15:31:56.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename deployment 08/11/23 15:31:56.493
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:56.511
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:56.515
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479
STEP: creating a Deployment 08/11/23 15:31:56.522
Aug 11 15:31:56.522: INFO: Creating simple deployment test-deployment-q44pr
Aug 11 15:31:56.534: INFO: new replicaset for deployment "test-deployment-q44pr" is yet to be created
STEP: Getting /status 08/11/23 15:31:58.551
Aug 11 15:31:58.555: INFO: Deployment test-deployment-q44pr has Conditions: [{Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}]
STEP: updating Deployment Status 08/11/23 15:31:58.555
Aug 11 15:31:58.565: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 31, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 31, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 31, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 31, 56, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-q44pr-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
STEP: watching for the Deployment status to be updated 08/11/23 15:31:58.565
Aug 11 15:31:58.568: INFO: Observed &Deployment event: ADDED
Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q44pr-777898ffcc"}
Aug 11 15:31:58.568: INFO: Observed &Deployment event: MODIFIED
Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q44pr-777898ffcc"}
Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 11 15:31:58.568: INFO: Observed &Deployment event: MODIFIED
Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q44pr-777898ffcc" is progressing.}
Aug 11 15:31:58.569: INFO: Observed &Deployment event: MODIFIED
Aug 11 15:31:58.569: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 11 15:31:58.569: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}
Aug 11 15:31:58.569: INFO: Observed &Deployment event: MODIFIED
Aug 11 15:31:58.569: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 11 15:31:58.569: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}
Aug 11 15:31:58.569: INFO: Found Deployment test-deployment-q44pr in namespace deployment-3196 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 15:31:58.569: INFO: Deployment test-deployment-q44pr has an updated status
STEP: patching the Statefulset Status 08/11/23 15:31:58.569
Aug 11 15:31:58.569: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
Aug 11 15:31:58.577: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
STEP: watching for the Deployment status to be patched 08/11/23 15:31:58.577
Aug 11 15:31:58.579: INFO: Observed &Deployment event: ADDED
Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q44pr-777898ffcc"}
Aug 11 15:31:58.579: INFO: Observed &Deployment event: MODIFIED
Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q44pr-777898ffcc"}
Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 11 15:31:58.579: INFO: Observed &Deployment event: MODIFIED
Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q44pr-777898ffcc" is progressing.}
Aug 11 15:31:58.580: INFO: Observed &Deployment event: MODIFIED
Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}
Aug 11 15:31:58.580: INFO: Observed &Deployment event: MODIFIED
Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}
Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
Aug 11 15:31:58.580: INFO: Observed &Deployment event: MODIFIED
Aug 11 15:31:58.580: INFO: Found deployment test-deployment-q44pr in namespace deployment-3196 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
Aug 11 15:31:58.580: INFO: Deployment test-deployment-q44pr has a patched status
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 15:31:58.586: INFO: Deployment "test-deployment-q44pr":
&Deployment{ObjectMeta:{test-deployment-q44pr  deployment-3196  a893db6f-aa0d-426f-9898-1ea90a12a109 56386 1 2023-08-11 15:31:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-11 15:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-11 15:31:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:31:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003efae08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-q44pr-777898ffcc",LastUpdateTime:2023-08-11 15:31:58 +0000 UTC,LastTransitionTime:2023-08-11 15:31:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 11 15:31:58.594: INFO: New ReplicaSet "test-deployment-q44pr-777898ffcc" of Deployment "test-deployment-q44pr":
&ReplicaSet{ObjectMeta:{test-deployment-q44pr-777898ffcc  deployment-3196  4fdb22f3-acb6-4dd4-9f10-5e17e761df55 56319 1 2023-08-11 15:31:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-q44pr a893db6f-aa0d-426f-9898-1ea90a12a109 0xc003efb200 0xc003efb201}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a893db6f-aa0d-426f-9898-1ea90a12a109\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:31:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003efb2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:31:58.598: INFO: Pod "test-deployment-q44pr-777898ffcc-gt826" is available:
&Pod{ObjectMeta:{test-deployment-q44pr-777898ffcc-gt826 test-deployment-q44pr-777898ffcc- deployment-3196  ff8c4dd6-065c-4788-bf1f-3ddf3e214255 56318 0 2023-08-11 15:31:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-q44pr-777898ffcc 4fdb22f3-acb6-4dd4-9f10-5e17e761df55 0xc003efb660 0xc003efb661}] [] [{kube-controller-manager Update v1 2023-08-11 15:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4fdb22f3-acb6-4dd4-9f10-5e17e761df55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:31:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rw7mn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rw7mn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:31:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:31:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.22,StartTime:2023-08-11 15:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:31:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8529228e29495e1b5cd7e6163c78773fce03779ab235d392fc61a199bd3e1f4d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 11 15:31:58.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-3196" for this suite. 08/11/23 15:31:58.602
{"msg":"PASSED [sig-apps] Deployment should validate Deployment Status endpoints [Conformance]","completed":332,"skipped":6173,"failed":0}
------------------------------
â€¢ [2.118 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  should validate Deployment Status endpoints [Conformance]
  test/e2e/apps/deployment.go:479

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:31:56.492
    Aug 11 15:31:56.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename deployment 08/11/23 15:31:56.493
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:56.511
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:56.515
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] should validate Deployment Status endpoints [Conformance]
      test/e2e/apps/deployment.go:479
    STEP: creating a Deployment 08/11/23 15:31:56.522
    Aug 11 15:31:56.522: INFO: Creating simple deployment test-deployment-q44pr
    Aug 11 15:31:56.534: INFO: new replicaset for deployment "test-deployment-q44pr" is yet to be created
    STEP: Getting /status 08/11/23 15:31:58.551
    Aug 11 15:31:58.555: INFO: Deployment test-deployment-q44pr has Conditions: [{Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}]
    STEP: updating Deployment Status 08/11/23 15:31:58.555
    Aug 11 15:31:58.565: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 31, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 31, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 11, 15, 31, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 11, 15, 31, 56, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-q44pr-777898ffcc\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
    STEP: watching for the Deployment status to be updated 08/11/23 15:31:58.565
    Aug 11 15:31:58.568: INFO: Observed &Deployment event: ADDED
    Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q44pr-777898ffcc"}
    Aug 11 15:31:58.568: INFO: Observed &Deployment event: MODIFIED
    Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q44pr-777898ffcc"}
    Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 11 15:31:58.568: INFO: Observed &Deployment event: MODIFIED
    Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 11 15:31:58.568: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q44pr-777898ffcc" is progressing.}
    Aug 11 15:31:58.569: INFO: Observed &Deployment event: MODIFIED
    Aug 11 15:31:58.569: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 11 15:31:58.569: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}
    Aug 11 15:31:58.569: INFO: Observed &Deployment event: MODIFIED
    Aug 11 15:31:58.569: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 11 15:31:58.569: INFO: Observed Deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}
    Aug 11 15:31:58.569: INFO: Found Deployment test-deployment-q44pr in namespace deployment-3196 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 15:31:58.569: INFO: Deployment test-deployment-q44pr has an updated status
    STEP: patching the Statefulset Status 08/11/23 15:31:58.569
    Aug 11 15:31:58.569: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
    Aug 11 15:31:58.577: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
    STEP: watching for the Deployment status to be patched 08/11/23 15:31:58.577
    Aug 11 15:31:58.579: INFO: Observed &Deployment event: ADDED
    Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q44pr-777898ffcc"}
    Aug 11 15:31:58.579: INFO: Observed &Deployment event: MODIFIED
    Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-q44pr-777898ffcc"}
    Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 11 15:31:58.579: INFO: Observed &Deployment event: MODIFIED
    Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
    Aug 11 15:31:58.579: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:56 +0000 UTC 2023-08-11 15:31:56 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-q44pr-777898ffcc" is progressing.}
    Aug 11 15:31:58.580: INFO: Observed &Deployment event: MODIFIED
    Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}
    Aug 11 15:31:58.580: INFO: Observed &Deployment event: MODIFIED
    Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:57 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
    Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-11 15:31:57 +0000 UTC 2023-08-11 15:31:56 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-q44pr-777898ffcc" has successfully progressed.}
    Aug 11 15:31:58.580: INFO: Observed deployment test-deployment-q44pr in namespace deployment-3196 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
    Aug 11 15:31:58.580: INFO: Observed &Deployment event: MODIFIED
    Aug 11 15:31:58.580: INFO: Found deployment test-deployment-q44pr in namespace deployment-3196 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
    Aug 11 15:31:58.580: INFO: Deployment test-deployment-q44pr has a patched status
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 15:31:58.586: INFO: Deployment "test-deployment-q44pr":
    &Deployment{ObjectMeta:{test-deployment-q44pr  deployment-3196  a893db6f-aa0d-426f-9898-1ea90a12a109 56386 1 2023-08-11 15:31:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-11 15:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-11 15:31:58 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-11 15:31:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003efae08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-q44pr-777898ffcc",LastUpdateTime:2023-08-11 15:31:58 +0000 UTC,LastTransitionTime:2023-08-11 15:31:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 11 15:31:58.594: INFO: New ReplicaSet "test-deployment-q44pr-777898ffcc" of Deployment "test-deployment-q44pr":
    &ReplicaSet{ObjectMeta:{test-deployment-q44pr-777898ffcc  deployment-3196  4fdb22f3-acb6-4dd4-9f10-5e17e761df55 56319 1 2023-08-11 15:31:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-q44pr a893db6f-aa0d-426f-9898-1ea90a12a109 0xc003efb200 0xc003efb201}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a893db6f-aa0d-426f-9898-1ea90a12a109\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:31:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 777898ffcc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-2 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003efb2a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:31:58.598: INFO: Pod "test-deployment-q44pr-777898ffcc-gt826" is available:
    &Pod{ObjectMeta:{test-deployment-q44pr-777898ffcc-gt826 test-deployment-q44pr-777898ffcc- deployment-3196  ff8c4dd6-065c-4788-bf1f-3ddf3e214255 56318 0 2023-08-11 15:31:56 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:777898ffcc] map[] [{apps/v1 ReplicaSet test-deployment-q44pr-777898ffcc 4fdb22f3-acb6-4dd4-9f10-5e17e761df55 0xc003efb660 0xc003efb661}] [] [{kube-controller-manager Update v1 2023-08-11 15:31:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4fdb22f3-acb6-4dd4-9f10-5e17e761df55\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:31:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.22\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rw7mn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rw7mn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:31:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:31:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:31:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:31:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.22,StartTime:2023-08-11 15:31:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:31:57 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-2,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:1b9d1b2f36cb2dbee1960e82a9344aeb11bd4c4c03abf5e1853e0559c23855e3,ContainerID:containerd://8529228e29495e1b5cd7e6163c78773fce03779ab235d392fc61a199bd3e1f4d,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.22,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 11 15:31:58.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-3196" for this suite. 08/11/23 15:31:58.602
  << End Captured GinkgoWriter Output
------------------------------
SS
------------------------------
[sig-apps] Deployment
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
[BeforeEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:31:58.611
Aug 11 15:31:58.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename deployment 08/11/23 15:31:58.612
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:58.626
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:58.629
[BeforeEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:91
[It] deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122
Aug 11 15:31:58.643: INFO: Pod name cleanup-pod: Found 0 pods out of 1
Aug 11 15:32:03.652: INFO: Pod name cleanup-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 15:32:03.652
Aug 11 15:32:03.652: INFO: Creating deployment test-cleanup-deployment
STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/11/23 15:32:03.662
[AfterEach] [sig-apps] Deployment
  test/e2e/apps/deployment.go:84
Aug 11 15:32:05.691: INFO: Deployment "test-cleanup-deployment":
&Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8669  af1a01c3-ef45-4ddd-8bfe-89a51b5335fe 56527 1 2023-08-11 15:32:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-11 15:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:32:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034a4618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 15:32:03 +0000 UTC,LastTransitionTime:2023-08-11 15:32:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-08-11 15:32:04 +0000 UTC,LastTransitionTime:2023-08-11 15:32:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

Aug 11 15:32:05.695: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
&ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-8669  81654747-cd0b-4aac-b519-1b9b2ffbd086 56515 1 2023-08-11 15:32:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment af1a01c3-ef45-4ddd-8bfe-89a51b5335fe 0xc0034a4d37 0xc0034a4d38}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af1a01c3-ef45-4ddd-8bfe-89a51b5335fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:32:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034a4f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
Aug 11 15:32:05.699: INFO: Pod "test-cleanup-deployment-69cb9c5497-2bgnw" is available:
&Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-2bgnw test-cleanup-deployment-69cb9c5497- deployment-8669  21567e36-9287-4675-b551-e582bad99573 56514 0 2023-08-11 15:32:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 81654747-cd0b-4aac-b519-1b9b2ffbd086 0xc0034a5977 0xc0034a5978}] [] [{kube-controller-manager Update v1 2023-08-11 15:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81654747-cd0b-4aac-b519-1b9b2ffbd086\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:32:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pb2mh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pb2mh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:32:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:32:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:32:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:32:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.84,StartTime:2023-08-11 15:32:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:32:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://1610745958f870ea35e6f28f893606054a51c829a5477d0209af122ef423a2c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
[AfterEach] [sig-apps] Deployment
  test/e2e/framework/framework.go:187
Aug 11 15:32:05.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "deployment-8669" for this suite. 08/11/23 15:32:05.703
{"msg":"PASSED [sig-apps] Deployment deployment should delete old replica sets [Conformance]","completed":333,"skipped":6175,"failed":0}
------------------------------
â€¢ [SLOW TEST] [7.100 seconds]
[sig-apps] Deployment
test/e2e/apps/framework.go:23
  deployment should delete old replica sets [Conformance]
  test/e2e/apps/deployment.go:122

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:31:58.611
    Aug 11 15:31:58.611: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename deployment 08/11/23 15:31:58.612
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:31:58.626
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:31:58.629
    [BeforeEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:91
    [It] deployment should delete old replica sets [Conformance]
      test/e2e/apps/deployment.go:122
    Aug 11 15:31:58.643: INFO: Pod name cleanup-pod: Found 0 pods out of 1
    Aug 11 15:32:03.652: INFO: Pod name cleanup-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 15:32:03.652
    Aug 11 15:32:03.652: INFO: Creating deployment test-cleanup-deployment
    STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up 08/11/23 15:32:03.662
    [AfterEach] [sig-apps] Deployment
      test/e2e/apps/deployment.go:84
    Aug 11 15:32:05.691: INFO: Deployment "test-cleanup-deployment":
    &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-8669  af1a01c3-ef45-4ddd-8bfe-89a51b5335fe 56527 1 2023-08-11 15:32:03 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-11 15:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:32:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034a4618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-11 15:32:03 +0000 UTC,LastTransitionTime:2023-08-11 15:32:03 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-69cb9c5497" has successfully progressed.,LastUpdateTime:2023-08-11 15:32:04 +0000 UTC,LastTransitionTime:2023-08-11 15:32:03 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

    Aug 11 15:32:05.695: INFO: New ReplicaSet "test-cleanup-deployment-69cb9c5497" of Deployment "test-cleanup-deployment":
    &ReplicaSet{ObjectMeta:{test-cleanup-deployment-69cb9c5497  deployment-8669  81654747-cd0b-4aac-b519-1b9b2ffbd086 56515 1 2023-08-11 15:32:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment af1a01c3-ef45-4ddd-8bfe-89a51b5335fe 0xc0034a4d37 0xc0034a4d38}] [] [{kube-controller-manager Update apps/v1 2023-08-11 15:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af1a01c3-ef45-4ddd-8bfe-89a51b5335fe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-11 15:32:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 69cb9c5497,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.40 [] []  [] [] [] {map[] map[]} [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0034a4f58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil>}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
    Aug 11 15:32:05.699: INFO: Pod "test-cleanup-deployment-69cb9c5497-2bgnw" is available:
    &Pod{ObjectMeta:{test-cleanup-deployment-69cb9c5497-2bgnw test-cleanup-deployment-69cb9c5497- deployment-8669  21567e36-9287-4675-b551-e582bad99573 56514 0 2023-08-11 15:32:03 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:69cb9c5497] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-69cb9c5497 81654747-cd0b-4aac-b519-1b9b2ffbd086 0xc0034a5977 0xc0034a5978}] [] [{kube-controller-manager Update v1 2023-08-11 15:32:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"81654747-cd0b-4aac-b519-1b9b2ffbd086\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-11 15:32:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.10.0.84\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pb2mh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pb2mh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:constell-d93e7e1d-worker-d314547c-0lc3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:32:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:32:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:32:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-11 15:32:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.178.2,PodIP:10.10.0.84,StartTime:2023-08-11 15:32:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-11 15:32:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.40,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:af7e3857d87770ddb40f5ea4f89b5a2709504ab1ee31f9ea4ab5823c045f2146,ContainerID:containerd://1610745958f870ea35e6f28f893606054a51c829a5477d0209af122ef423a2c7,Started:*true,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.10.0.84,},},EphemeralContainerStatuses:[]ContainerStatus{},},}
    [AfterEach] [sig-apps] Deployment
      test/e2e/framework/framework.go:187
    Aug 11 15:32:05.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "deployment-8669" for this suite. 08/11/23 15:32:05.703
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:32:05.712
Aug 11 15:32:05.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:32:05.712
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:05.729
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:05.731
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179
STEP: creating service in namespace services-154 08/11/23 15:32:05.734
STEP: creating service affinity-clusterip-transition in namespace services-154 08/11/23 15:32:05.735
STEP: creating replication controller affinity-clusterip-transition in namespace services-154 08/11/23 15:32:05.751
I0811 15:32:05.764813      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-154, replica count: 3
I0811 15:32:08.816472      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 15:32:08.824: INFO: Creating new exec pod
Aug 11 15:32:08.835: INFO: Waiting up to 5m0s for pod "execpod-affinity42vhd" in namespace "services-154" to be "running"
Aug 11 15:32:08.839: INFO: Pod "execpod-affinity42vhd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.906782ms
Aug 11 15:32:10.845: INFO: Pod "execpod-affinity42vhd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009819302s
Aug 11 15:32:10.845: INFO: Pod "execpod-affinity42vhd" satisfied condition "running"
Aug 11 15:32:11.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-154 exec execpod-affinity42vhd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
Aug 11 15:32:11.985: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
Aug 11 15:32:11.985: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:32:11.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-154 exec execpod-affinity42vhd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.118.54 80'
Aug 11 15:32:12.126: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.118.54 80\nConnection to 10.108.118.54 80 port [tcp/http] succeeded!\n"
Aug 11 15:32:12.126: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
Aug 11 15:32:12.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-154 exec execpod-affinity42vhd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.118.54:80/ ; done'
Aug 11 15:32:12.324: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n"
Aug 11 15:32:12.324: INFO: stdout: "\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-7dwbp\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc"
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-7dwbp
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
Aug 11 15:32:12.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-154 exec execpod-affinity42vhd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.118.54:80/ ; done'
Aug 11 15:32:12.528: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n"
Aug 11 15:32:12.528: INFO: stdout: "\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b"
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
Aug 11 15:32:12.528: INFO: Cleaning up the exec pod
STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-154, will wait for the garbage collector to delete the pods 08/11/23 15:32:12.542
Aug 11 15:32:12.604: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.772552ms
Aug 11 15:32:12.705: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.809787ms
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:32:15.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-154" for this suite. 08/11/23 15:32:15.836
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]","completed":334,"skipped":6184,"failed":0}
------------------------------
â€¢ [SLOW TEST] [10.134 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
  test/e2e/network/service.go:2179

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:32:05.712
    Aug 11 15:32:05.712: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:32:05.712
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:05.729
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:05.731
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
      test/e2e/network/service.go:2179
    STEP: creating service in namespace services-154 08/11/23 15:32:05.734
    STEP: creating service affinity-clusterip-transition in namespace services-154 08/11/23 15:32:05.735
    STEP: creating replication controller affinity-clusterip-transition in namespace services-154 08/11/23 15:32:05.751
    I0811 15:32:05.764813      22 runners.go:193] Created replication controller with name: affinity-clusterip-transition, namespace: services-154, replica count: 3
    I0811 15:32:08.816472      22 runners.go:193] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 15:32:08.824: INFO: Creating new exec pod
    Aug 11 15:32:08.835: INFO: Waiting up to 5m0s for pod "execpod-affinity42vhd" in namespace "services-154" to be "running"
    Aug 11 15:32:08.839: INFO: Pod "execpod-affinity42vhd": Phase="Pending", Reason="", readiness=false. Elapsed: 3.906782ms
    Aug 11 15:32:10.845: INFO: Pod "execpod-affinity42vhd": Phase="Running", Reason="", readiness=true. Elapsed: 2.009819302s
    Aug 11 15:32:10.845: INFO: Pod "execpod-affinity42vhd" satisfied condition "running"
    Aug 11 15:32:11.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-154 exec execpod-affinity42vhd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
    Aug 11 15:32:11.985: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
    Aug 11 15:32:11.985: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:32:11.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-154 exec execpod-affinity42vhd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.118.54 80'
    Aug 11 15:32:12.126: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.118.54 80\nConnection to 10.108.118.54 80 port [tcp/http] succeeded!\n"
    Aug 11 15:32:12.126: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
    Aug 11 15:32:12.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-154 exec execpod-affinity42vhd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.118.54:80/ ; done'
    Aug 11 15:32:12.324: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n"
    Aug 11 15:32:12.324: INFO: stdout: "\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-7dwbp\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-8gkdc\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-8gkdc"
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-7dwbp
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.324: INFO: Received response from host: affinity-clusterip-transition-8gkdc
    Aug 11 15:32:12.334: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=services-154 exec execpod-affinity42vhd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.108.118.54:80/ ; done'
    Aug 11 15:32:12.528: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.108.118.54:80/\n"
    Aug 11 15:32:12.528: INFO: stdout: "\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b\naffinity-clusterip-transition-2wk4b"
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Received response from host: affinity-clusterip-transition-2wk4b
    Aug 11 15:32:12.528: INFO: Cleaning up the exec pod
    STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-154, will wait for the garbage collector to delete the pods 08/11/23 15:32:12.542
    Aug 11 15:32:12.604: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.772552ms
    Aug 11 15:32:12.705: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.809787ms
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:32:15.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-154" for this suite. 08/11/23 15:32:15.836
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:32:15.846
Aug 11 15:32:15.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:32:15.847
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:15.864
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:15.867
[It] works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275
STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/11/23 15:32:15.87
Aug 11 15:32:15.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 15:32:19.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:32:31.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-5573" for this suite. 08/11/23 15:32:31.568
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]","completed":335,"skipped":6191,"failed":0}
------------------------------
â€¢ [SLOW TEST] [15.729 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of different groups [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:275

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:32:15.846
    Aug 11 15:32:15.846: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:32:15.847
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:15.864
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:15.867
    [It] works for multiple CRDs of different groups [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:275
    STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation 08/11/23 15:32:15.87
    Aug 11 15:32:15.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 15:32:19.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:32:31.560: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-5573" for this suite. 08/11/23 15:32:31.568
  << End Captured GinkgoWriter Output
------------------------------
[sig-node] ConfigMap
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
[BeforeEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:32:31.575
Aug 11 15:32:31.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename configmap 08/11/23 15:32:31.576
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:31.591
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:31.594
[It] should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92
STEP: Creating configMap configmap-4812/configmap-test-46b4cb75-c988-4dce-a36c-63ed4869b91e 08/11/23 15:32:31.597
STEP: Creating a pod to test consume configMaps 08/11/23 15:32:31.601
Aug 11 15:32:31.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2" in namespace "configmap-4812" to be "Succeeded or Failed"
Aug 11 15:32:31.615: INFO: Pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.376246ms
Aug 11 15:32:33.621: INFO: Pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010067331s
Aug 11 15:32:35.621: INFO: Pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009673234s
STEP: Saw pod success 08/11/23 15:32:35.621
Aug 11 15:32:35.621: INFO: Pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2" satisfied condition "Succeeded or Failed"
Aug 11 15:32:35.624: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2 container env-test: <nil>
STEP: delete the pod 08/11/23 15:32:35.645
Aug 11 15:32:35.659: INFO: Waiting for pod pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2 to disappear
Aug 11 15:32:35.662: INFO: Pod pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2 no longer exists
[AfterEach] [sig-node] ConfigMap
  test/e2e/framework/framework.go:187
Aug 11 15:32:35.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "configmap-4812" for this suite. 08/11/23 15:32:35.665
{"msg":"PASSED [sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]","completed":336,"skipped":6191,"failed":0}
------------------------------
â€¢ [4.096 seconds]
[sig-node] ConfigMap
test/e2e/common/node/framework.go:23
  should be consumable via the environment [NodeConformance] [Conformance]
  test/e2e/common/node/configmap.go:92

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:32:31.575
    Aug 11 15:32:31.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename configmap 08/11/23 15:32:31.576
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:31.591
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:31.594
    [It] should be consumable via the environment [NodeConformance] [Conformance]
      test/e2e/common/node/configmap.go:92
    STEP: Creating configMap configmap-4812/configmap-test-46b4cb75-c988-4dce-a36c-63ed4869b91e 08/11/23 15:32:31.597
    STEP: Creating a pod to test consume configMaps 08/11/23 15:32:31.601
    Aug 11 15:32:31.611: INFO: Waiting up to 5m0s for pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2" in namespace "configmap-4812" to be "Succeeded or Failed"
    Aug 11 15:32:31.615: INFO: Pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2": Phase="Pending", Reason="", readiness=false. Elapsed: 4.376246ms
    Aug 11 15:32:33.621: INFO: Pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010067331s
    Aug 11 15:32:35.621: INFO: Pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009673234s
    STEP: Saw pod success 08/11/23 15:32:35.621
    Aug 11 15:32:35.621: INFO: Pod "pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2" satisfied condition "Succeeded or Failed"
    Aug 11 15:32:35.624: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2 container env-test: <nil>
    STEP: delete the pod 08/11/23 15:32:35.645
    Aug 11 15:32:35.659: INFO: Waiting for pod pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2 to disappear
    Aug 11 15:32:35.662: INFO: Pod pod-configmaps-3cc2e6f0-7193-4495-8eb3-ac86b2b62db2 no longer exists
    [AfterEach] [sig-node] ConfigMap
      test/e2e/framework/framework.go:187
    Aug 11 15:32:35.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "configmap-4812" for this suite. 08/11/23 15:32:35.665
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:32:35.672
Aug 11 15:32:35.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 15:32:35.673
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:35.691
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:35.693
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 15:32:35.707
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:32:35.975
STEP: Deploying the webhook pod 08/11/23 15:32:35.983
STEP: Wait for the deployment to be ready 08/11/23 15:32:35.995
Aug 11 15:32:36.001: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:32:38.014
STEP: Verifying the service has paired with the endpoint 08/11/23 15:32:38.029
Aug 11 15:32:39.029: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251
STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/11/23 15:32:39.034
STEP: create a configmap that should be updated by the webhook 08/11/23 15:32:39.056
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:32:39.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-1962" for this suite. 08/11/23 15:32:39.092
STEP: Destroying namespace "webhook-1962-markers" for this suite. 08/11/23 15:32:39.097
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]","completed":337,"skipped":6205,"failed":0}
------------------------------
â€¢ [3.483 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate configmap [Conformance]
  test/e2e/apimachinery/webhook.go:251

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:32:35.672
    Aug 11 15:32:35.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 15:32:35.673
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:35.691
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:35.693
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 15:32:35.707
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:32:35.975
    STEP: Deploying the webhook pod 08/11/23 15:32:35.983
    STEP: Wait for the deployment to be ready 08/11/23 15:32:35.995
    Aug 11 15:32:36.001: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:32:38.014
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:32:38.029
    Aug 11 15:32:39.029: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate configmap [Conformance]
      test/e2e/apimachinery/webhook.go:251
    STEP: Registering the mutating configmap webhook via the AdmissionRegistration API 08/11/23 15:32:39.034
    STEP: create a configmap that should be updated by the webhook 08/11/23 15:32:39.056
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:32:39.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-1962" for this suite. 08/11/23 15:32:39.092
    STEP: Destroying namespace "webhook-1962-markers" for this suite. 08/11/23 15:32:39.097
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
[sig-apps] ReplicaSet
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:32:39.156
Aug 11 15:32:39.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replicaset 08/11/23 15:32:39.157
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:39.173
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:39.175
[It] should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131
STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/11/23 15:32:39.178
Aug 11 15:32:39.186: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3929" to be "running and ready"
Aug 11 15:32:39.190: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.644745ms
Aug 11 15:32:39.191: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:32:41.195: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008727059s
Aug 11 15:32:41.195: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
Aug 11 15:32:41.195: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
STEP: When a replicaset with a matching selector is created 08/11/23 15:32:41.197
STEP: Then the orphan pod is adopted 08/11/23 15:32:41.202
STEP: When the matched label of one of its pods change 08/11/23 15:32:42.21
Aug 11 15:32:42.213: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
STEP: Then the pod is released 08/11/23 15:32:42.222
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 11 15:32:43.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-3929" for this suite. 08/11/23 15:32:43.233
{"msg":"PASSED [sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]","completed":338,"skipped":6205,"failed":0}
------------------------------
â€¢ [4.085 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  should adopt matching pods on creation and release no longer matching pods [Conformance]
  test/e2e/apps/replica_set.go:131

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:32:39.156
    Aug 11 15:32:39.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replicaset 08/11/23 15:32:39.157
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:39.173
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:39.175
    [It] should adopt matching pods on creation and release no longer matching pods [Conformance]
      test/e2e/apps/replica_set.go:131
    STEP: Given a Pod with a 'name' label pod-adoption-release is created 08/11/23 15:32:39.178
    Aug 11 15:32:39.186: INFO: Waiting up to 5m0s for pod "pod-adoption-release" in namespace "replicaset-3929" to be "running and ready"
    Aug 11 15:32:39.190: INFO: Pod "pod-adoption-release": Phase="Pending", Reason="", readiness=false. Elapsed: 4.644745ms
    Aug 11 15:32:39.191: INFO: The phase of Pod pod-adoption-release is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:32:41.195: INFO: Pod "pod-adoption-release": Phase="Running", Reason="", readiness=true. Elapsed: 2.008727059s
    Aug 11 15:32:41.195: INFO: The phase of Pod pod-adoption-release is Running (Ready = true)
    Aug 11 15:32:41.195: INFO: Pod "pod-adoption-release" satisfied condition "running and ready"
    STEP: When a replicaset with a matching selector is created 08/11/23 15:32:41.197
    STEP: Then the orphan pod is adopted 08/11/23 15:32:41.202
    STEP: When the matched label of one of its pods change 08/11/23 15:32:42.21
    Aug 11 15:32:42.213: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
    STEP: Then the pod is released 08/11/23 15:32:42.222
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 11 15:32:43.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-3929" for this suite. 08/11/23 15:32:43.233
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod
  should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:32:43.242
Aug 11 15:32:43.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:32:43.243
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:43.259
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:43.261
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[It] should print the output to logs [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:52
Aug 11 15:32:43.272: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e" in namespace "kubelet-test-3325" to be "running and ready"
Aug 11 15:32:43.276: INFO: Pod "busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037385ms
Aug 11 15:32:43.276: INFO: The phase of Pod busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:32:45.283: INFO: Pod "busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010360479s
Aug 11 15:32:45.283: INFO: The phase of Pod busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e is Running (Ready = true)
Aug 11 15:32:45.283: INFO: Pod "busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e" satisfied condition "running and ready"
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 11 15:32:45.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-3325" for this suite. 08/11/23 15:32:45.301
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]","completed":339,"skipped":6225,"failed":0}
------------------------------
â€¢ [2.066 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command in a pod
  test/e2e/common/node/kubelet.go:44
    should print the output to logs [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:52

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:32:43.242
    Aug 11 15:32:43.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:32:43.243
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:43.259
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:43.261
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [It] should print the output to logs [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:52
    Aug 11 15:32:43.272: INFO: Waiting up to 5m0s for pod "busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e" in namespace "kubelet-test-3325" to be "running and ready"
    Aug 11 15:32:43.276: INFO: Pod "busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e": Phase="Pending", Reason="", readiness=false. Elapsed: 4.037385ms
    Aug 11 15:32:43.276: INFO: The phase of Pod busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:32:45.283: INFO: Pod "busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e": Phase="Running", Reason="", readiness=true. Elapsed: 2.010360479s
    Aug 11 15:32:45.283: INFO: The phase of Pod busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e is Running (Ready = true)
    Aug 11 15:32:45.283: INFO: Pod "busybox-scheduling-a067676d-2cd4-4b6c-93be-34af32b7253e" satisfied condition "running and ready"
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 11 15:32:45.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-3325" for this suite. 08/11/23 15:32:45.301
  << End Captured GinkgoWriter Output
------------------------------
[sig-storage] Projected configMap
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:32:45.308
Aug 11 15:32:45.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 15:32:45.31
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:45.328
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:45.33
[It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374
STEP: Creating configMap with name projected-configmap-test-volume-e51b439c-56d0-4a54-86c1-c587fb154674 08/11/23 15:32:45.333
STEP: Creating a pod to test consume configMaps 08/11/23 15:32:45.338
Aug 11 15:32:45.348: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8" in namespace "projected-5089" to be "Succeeded or Failed"
Aug 11 15:32:45.354: INFO: Pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.289435ms
Aug 11 15:32:47.359: INFO: Pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010073031s
Aug 11 15:32:49.359: INFO: Pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010159889s
STEP: Saw pod success 08/11/23 15:32:49.359
Aug 11 15:32:49.359: INFO: Pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8" satisfied condition "Succeeded or Failed"
Aug 11 15:32:49.362: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8 container projected-configmap-volume-test: <nil>
STEP: delete the pod 08/11/23 15:32:49.37
Aug 11 15:32:49.384: INFO: Waiting for pod pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8 to disappear
Aug 11 15:32:49.387: INFO: Pod pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 11 15:32:49.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5089" for this suite. 08/11/23 15:32:49.391
{"msg":"PASSED [sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]","completed":340,"skipped":6225,"failed":0}
------------------------------
â€¢ [4.089 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:374

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:32:45.308
    Aug 11 15:32:45.309: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 15:32:45.31
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:45.328
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:45.33
    [It] should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:374
    STEP: Creating configMap with name projected-configmap-test-volume-e51b439c-56d0-4a54-86c1-c587fb154674 08/11/23 15:32:45.333
    STEP: Creating a pod to test consume configMaps 08/11/23 15:32:45.338
    Aug 11 15:32:45.348: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8" in namespace "projected-5089" to be "Succeeded or Failed"
    Aug 11 15:32:45.354: INFO: Pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 5.289435ms
    Aug 11 15:32:47.359: INFO: Pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8": Phase="Pending", Reason="", readiness=false. Elapsed: 2.010073031s
    Aug 11 15:32:49.359: INFO: Pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010159889s
    STEP: Saw pod success 08/11/23 15:32:49.359
    Aug 11 15:32:49.359: INFO: Pod "pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8" satisfied condition "Succeeded or Failed"
    Aug 11 15:32:49.362: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8 container projected-configmap-volume-test: <nil>
    STEP: delete the pod 08/11/23 15:32:49.37
    Aug 11 15:32:49.384: INFO: Waiting for pod pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8 to disappear
    Aug 11 15:32:49.387: INFO: Pod pod-projected-configmaps-5d690e08-907c-48d2-8f0b-7a267749a8f8 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 11 15:32:49.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5089" for this suite. 08/11/23 15:32:49.391
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:32:49.399
Aug 11 15:32:49.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename webhook 08/11/23 15:32:49.4
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:49.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:49.416
[BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:89
STEP: Setting up server cert 08/11/23 15:32:49.431
STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:32:49.626
STEP: Deploying the webhook pod 08/11/23 15:32:49.633
STEP: Wait for the deployment to be ready 08/11/23 15:32:49.646
Aug 11 15:32:49.656: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
STEP: Deploying the webhook service 08/11/23 15:32:51.668
STEP: Verifying the service has paired with the endpoint 08/11/23 15:32:51.684
Aug 11 15:32:52.685: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
[It] should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290
Aug 11 15:32:52.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6199-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 15:32:53.199
STEP: Creating a custom resource that should be mutated by the webhook 08/11/23 15:32:53.226
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:32:55.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "webhook-4907" for this suite. 08/11/23 15:32:55.796
STEP: Destroying namespace "webhook-4907-markers" for this suite. 08/11/23 15:32:55.803
[AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
  test/e2e/apimachinery/webhook.go:104
{"msg":"PASSED [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]","completed":341,"skipped":6256,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.454 seconds]
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  should mutate custom resource [Conformance]
  test/e2e/apimachinery/webhook.go:290

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:32:49.399
    Aug 11 15:32:49.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename webhook 08/11/23 15:32:49.4
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:49.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:49.416
    [BeforeEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:89
    STEP: Setting up server cert 08/11/23 15:32:49.431
    STEP: Create role binding to let webhook read extension-apiserver-authentication 08/11/23 15:32:49.626
    STEP: Deploying the webhook pod 08/11/23 15:32:49.633
    STEP: Wait for the deployment to be ready 08/11/23 15:32:49.646
    Aug 11 15:32:49.656: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
    STEP: Deploying the webhook service 08/11/23 15:32:51.668
    STEP: Verifying the service has paired with the endpoint 08/11/23 15:32:51.684
    Aug 11 15:32:52.685: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
    [It] should mutate custom resource [Conformance]
      test/e2e/apimachinery/webhook.go:290
    Aug 11 15:32:52.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6199-crds.webhook.example.com via the AdmissionRegistration API 08/11/23 15:32:53.199
    STEP: Creating a custom resource that should be mutated by the webhook 08/11/23 15:32:53.226
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:32:55.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "webhook-4907" for this suite. 08/11/23 15:32:55.796
    STEP: Destroying namespace "webhook-4907-markers" for this suite. 08/11/23 15:32:55.803
    [AfterEach] [sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin]
      test/e2e/apimachinery/webhook.go:104
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
[BeforeEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:32:55.857
Aug 11 15:32:55.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename svc-latency 08/11/23 15:32:55.858
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:55.883
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:55.887
[It] should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59
Aug 11 15:32:55.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: creating replication controller svc-latency-rc in namespace svc-latency-1601 08/11/23 15:32:55.89
I0811 15:32:55.898594      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1601, replica count: 1
I0811 15:32:56.949643      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
Aug 11 15:32:57.065: INFO: Created: latency-svc-w7mw9
Aug 11 15:32:57.071: INFO: Got endpoints: latency-svc-w7mw9 [21.627423ms]
Aug 11 15:32:57.091: INFO: Created: latency-svc-s6szq
Aug 11 15:32:57.101: INFO: Got endpoints: latency-svc-s6szq [29.419815ms]
Aug 11 15:32:57.105: INFO: Created: latency-svc-skdzc
Aug 11 15:32:57.120: INFO: Got endpoints: latency-svc-skdzc [47.611962ms]
Aug 11 15:32:57.124: INFO: Created: latency-svc-ph6sg
Aug 11 15:32:57.134: INFO: Got endpoints: latency-svc-ph6sg [62.303179ms]
Aug 11 15:32:57.138: INFO: Created: latency-svc-bdkqz
Aug 11 15:32:57.144: INFO: Got endpoints: latency-svc-bdkqz [72.060502ms]
Aug 11 15:32:57.154: INFO: Created: latency-svc-dhql8
Aug 11 15:32:57.160: INFO: Got endpoints: latency-svc-dhql8 [88.746292ms]
Aug 11 15:32:57.164: INFO: Created: latency-svc-cbllv
Aug 11 15:32:57.173: INFO: Got endpoints: latency-svc-cbllv [100.990073ms]
Aug 11 15:32:57.178: INFO: Created: latency-svc-blkns
Aug 11 15:32:57.183: INFO: Got endpoints: latency-svc-blkns [110.736626ms]
Aug 11 15:32:57.191: INFO: Created: latency-svc-rd48z
Aug 11 15:32:57.205: INFO: Got endpoints: latency-svc-rd48z [133.000558ms]
Aug 11 15:32:57.214: INFO: Created: latency-svc-hz6vh
Aug 11 15:32:57.234: INFO: Got endpoints: latency-svc-hz6vh [162.666432ms]
Aug 11 15:32:57.238: INFO: Created: latency-svc-dpwgw
Aug 11 15:32:57.246: INFO: Got endpoints: latency-svc-dpwgw [173.451507ms]
Aug 11 15:32:57.254: INFO: Created: latency-svc-bcnzg
Aug 11 15:32:57.259: INFO: Got endpoints: latency-svc-bcnzg [186.327298ms]
Aug 11 15:32:57.268: INFO: Created: latency-svc-gd85g
Aug 11 15:32:57.279: INFO: Got endpoints: latency-svc-gd85g [206.797696ms]
Aug 11 15:32:57.284: INFO: Created: latency-svc-9cfl4
Aug 11 15:32:57.292: INFO: Got endpoints: latency-svc-9cfl4 [220.069168ms]
Aug 11 15:32:57.295: INFO: Created: latency-svc-h6dkw
Aug 11 15:32:57.304: INFO: Got endpoints: latency-svc-h6dkw [232.193246ms]
Aug 11 15:32:57.309: INFO: Created: latency-svc-gxrqx
Aug 11 15:32:57.318: INFO: Got endpoints: latency-svc-gxrqx [245.711266ms]
Aug 11 15:32:57.321: INFO: Created: latency-svc-5gts5
Aug 11 15:32:57.333: INFO: Got endpoints: latency-svc-5gts5 [231.904096ms]
Aug 11 15:32:57.347: INFO: Created: latency-svc-v7q6j
Aug 11 15:32:57.347: INFO: Got endpoints: latency-svc-v7q6j [227.421618ms]
Aug 11 15:32:57.359: INFO: Created: latency-svc-869q9
Aug 11 15:32:57.369: INFO: Got endpoints: latency-svc-869q9 [234.612801ms]
Aug 11 15:32:57.374: INFO: Created: latency-svc-w9k7v
Aug 11 15:32:57.384: INFO: Got endpoints: latency-svc-w9k7v [239.502354ms]
Aug 11 15:32:57.388: INFO: Created: latency-svc-ggz7b
Aug 11 15:32:57.397: INFO: Got endpoints: latency-svc-ggz7b [236.306683ms]
Aug 11 15:32:57.401: INFO: Created: latency-svc-s7hgm
Aug 11 15:32:57.410: INFO: Got endpoints: latency-svc-s7hgm [236.918993ms]
Aug 11 15:32:57.414: INFO: Created: latency-svc-nklqk
Aug 11 15:32:57.422: INFO: Got endpoints: latency-svc-nklqk [239.125871ms]
Aug 11 15:32:57.427: INFO: Created: latency-svc-x5mdg
Aug 11 15:32:57.442: INFO: Created: latency-svc-4k8ns
Aug 11 15:32:57.459: INFO: Got endpoints: latency-svc-4k8ns [224.280779ms]
Aug 11 15:32:57.459: INFO: Got endpoints: latency-svc-x5mdg [253.793418ms]
Aug 11 15:32:57.463: INFO: Created: latency-svc-tmhhh
Aug 11 15:32:57.472: INFO: Got endpoints: latency-svc-tmhhh [226.321163ms]
Aug 11 15:32:57.476: INFO: Created: latency-svc-l7zz7
Aug 11 15:32:57.486: INFO: Got endpoints: latency-svc-l7zz7 [227.108138ms]
Aug 11 15:32:57.494: INFO: Created: latency-svc-7mn7g
Aug 11 15:32:57.507: INFO: Got endpoints: latency-svc-7mn7g [227.761658ms]
Aug 11 15:32:57.509: INFO: Created: latency-svc-nz8p6
Aug 11 15:32:57.518: INFO: Got endpoints: latency-svc-nz8p6 [226.296412ms]
Aug 11 15:32:57.521: INFO: Created: latency-svc-xtfgg
Aug 11 15:32:57.530: INFO: Got endpoints: latency-svc-xtfgg [225.513228ms]
Aug 11 15:32:57.535: INFO: Created: latency-svc-gm8wf
Aug 11 15:32:57.543: INFO: Got endpoints: latency-svc-gm8wf [225.105885ms]
Aug 11 15:32:57.548: INFO: Created: latency-svc-9r5qg
Aug 11 15:32:57.569: INFO: Got endpoints: latency-svc-9r5qg [235.141537ms]
Aug 11 15:32:57.572: INFO: Created: latency-svc-nxmm6
Aug 11 15:32:57.580: INFO: Got endpoints: latency-svc-nxmm6 [233.423164ms]
Aug 11 15:32:57.584: INFO: Created: latency-svc-k8xw5
Aug 11 15:32:57.593: INFO: Got endpoints: latency-svc-k8xw5 [224.483305ms]
Aug 11 15:32:57.598: INFO: Created: latency-svc-fknjx
Aug 11 15:32:57.605: INFO: Got endpoints: latency-svc-fknjx [221.245895ms]
Aug 11 15:32:57.609: INFO: Created: latency-svc-cdg7g
Aug 11 15:32:57.629: INFO: Got endpoints: latency-svc-cdg7g [231.800734ms]
Aug 11 15:32:57.651: INFO: Created: latency-svc-pkhl8
Aug 11 15:32:57.653: INFO: Got endpoints: latency-svc-pkhl8 [242.63958ms]
Aug 11 15:32:57.664: INFO: Created: latency-svc-hpmdc
Aug 11 15:32:57.680: INFO: Got endpoints: latency-svc-hpmdc [257.519973ms]
Aug 11 15:32:57.683: INFO: Created: latency-svc-t779g
Aug 11 15:32:57.693: INFO: Got endpoints: latency-svc-t779g [234.58833ms]
Aug 11 15:32:57.697: INFO: Created: latency-svc-nc2kf
Aug 11 15:32:57.706: INFO: Got endpoints: latency-svc-nc2kf [247.414689ms]
Aug 11 15:32:57.709: INFO: Created: latency-svc-8rxhk
Aug 11 15:32:57.716: INFO: Got endpoints: latency-svc-8rxhk [244.271211ms]
Aug 11 15:32:57.721: INFO: Created: latency-svc-htfmm
Aug 11 15:32:57.730: INFO: Got endpoints: latency-svc-htfmm [244.637432ms]
Aug 11 15:32:57.734: INFO: Created: latency-svc-gxbqp
Aug 11 15:32:57.750: INFO: Got endpoints: latency-svc-gxbqp [243.053744ms]
Aug 11 15:32:57.753: INFO: Created: latency-svc-xx6lx
Aug 11 15:32:57.763: INFO: Created: latency-svc-pghjp
Aug 11 15:32:57.775: INFO: Got endpoints: latency-svc-xx6lx [256.959936ms]
Aug 11 15:32:57.779: INFO: Created: latency-svc-7k56z
Aug 11 15:32:57.801: INFO: Created: latency-svc-k7jsg
Aug 11 15:32:57.814: INFO: Created: latency-svc-k7ctp
Aug 11 15:32:57.824: INFO: Got endpoints: latency-svc-pghjp [294.129653ms]
Aug 11 15:32:57.829: INFO: Created: latency-svc-8xr2z
Aug 11 15:32:57.845: INFO: Created: latency-svc-mhl5h
Aug 11 15:32:57.855: INFO: Created: latency-svc-b2x8f
Aug 11 15:32:57.866: INFO: Created: latency-svc-wvj9m
Aug 11 15:32:57.874: INFO: Got endpoints: latency-svc-7k56z [331.740973ms]
Aug 11 15:32:57.879: INFO: Created: latency-svc-b94bn
Aug 11 15:32:57.890: INFO: Created: latency-svc-rfrxd
Aug 11 15:32:57.910: INFO: Created: latency-svc-m66t6
Aug 11 15:32:57.924: INFO: Got endpoints: latency-svc-k7jsg [355.800602ms]
Aug 11 15:32:57.925: INFO: Created: latency-svc-42pwd
Aug 11 15:32:57.935: INFO: Created: latency-svc-7698f
Aug 11 15:32:57.947: INFO: Created: latency-svc-gcdr4
Aug 11 15:32:57.959: INFO: Created: latency-svc-t79kp
Aug 11 15:32:57.970: INFO: Created: latency-svc-b6m55
Aug 11 15:32:57.973: INFO: Got endpoints: latency-svc-k7ctp [392.264587ms]
Aug 11 15:32:57.982: INFO: Created: latency-svc-gkqrj
Aug 11 15:32:57.991: INFO: Created: latency-svc-w98mv
Aug 11 15:32:58.004: INFO: Created: latency-svc-p4ngr
Aug 11 15:32:58.022: INFO: Got endpoints: latency-svc-8xr2z [429.083152ms]
Aug 11 15:32:58.037: INFO: Created: latency-svc-dh2m7
Aug 11 15:32:58.072: INFO: Got endpoints: latency-svc-mhl5h [467.25703ms]
Aug 11 15:32:58.090: INFO: Created: latency-svc-2zpwn
Aug 11 15:32:58.122: INFO: Got endpoints: latency-svc-b2x8f [493.62552ms]
Aug 11 15:32:58.146: INFO: Created: latency-svc-wmxx5
Aug 11 15:32:58.172: INFO: Got endpoints: latency-svc-wvj9m [519.061154ms]
Aug 11 15:32:58.190: INFO: Created: latency-svc-dt9dk
Aug 11 15:32:58.223: INFO: Got endpoints: latency-svc-b94bn [542.929055ms]
Aug 11 15:32:58.246: INFO: Created: latency-svc-v52gj
Aug 11 15:32:58.273: INFO: Got endpoints: latency-svc-rfrxd [579.0726ms]
Aug 11 15:32:58.288: INFO: Created: latency-svc-bqqmq
Aug 11 15:32:58.321: INFO: Got endpoints: latency-svc-m66t6 [614.991167ms]
Aug 11 15:32:58.337: INFO: Created: latency-svc-pqls2
Aug 11 15:32:58.372: INFO: Got endpoints: latency-svc-42pwd [655.986114ms]
Aug 11 15:32:58.388: INFO: Created: latency-svc-xkx7q
Aug 11 15:32:58.423: INFO: Got endpoints: latency-svc-7698f [692.21233ms]
Aug 11 15:32:58.439: INFO: Created: latency-svc-sdrdv
Aug 11 15:32:58.472: INFO: Got endpoints: latency-svc-gcdr4 [721.492191ms]
Aug 11 15:32:58.487: INFO: Created: latency-svc-m786m
Aug 11 15:32:58.522: INFO: Got endpoints: latency-svc-t79kp [746.908494ms]
Aug 11 15:32:58.538: INFO: Created: latency-svc-tf289
Aug 11 15:32:58.573: INFO: Got endpoints: latency-svc-b6m55 [749.271405ms]
Aug 11 15:32:58.590: INFO: Created: latency-svc-2h9lk
Aug 11 15:32:58.621: INFO: Got endpoints: latency-svc-gkqrj [746.138548ms]
Aug 11 15:32:58.635: INFO: Created: latency-svc-fqmrd
Aug 11 15:32:58.678: INFO: Got endpoints: latency-svc-w98mv [753.835108ms]
Aug 11 15:32:58.693: INFO: Created: latency-svc-lzskn
Aug 11 15:32:58.722: INFO: Got endpoints: latency-svc-p4ngr [748.958386ms]
Aug 11 15:32:58.738: INFO: Created: latency-svc-slvk8
Aug 11 15:32:58.772: INFO: Got endpoints: latency-svc-dh2m7 [749.793163ms]
Aug 11 15:32:58.791: INFO: Created: latency-svc-8vkst
Aug 11 15:32:58.821: INFO: Got endpoints: latency-svc-2zpwn [748.230164ms]
Aug 11 15:32:58.837: INFO: Created: latency-svc-lxfq4
Aug 11 15:32:58.871: INFO: Got endpoints: latency-svc-wmxx5 [748.651807ms]
Aug 11 15:32:58.887: INFO: Created: latency-svc-85hsk
Aug 11 15:32:58.920: INFO: Got endpoints: latency-svc-dt9dk [748.424778ms]
Aug 11 15:32:58.935: INFO: Created: latency-svc-s87gg
Aug 11 15:32:58.971: INFO: Got endpoints: latency-svc-v52gj [747.78648ms]
Aug 11 15:32:58.985: INFO: Created: latency-svc-pnxsx
Aug 11 15:32:59.020: INFO: Got endpoints: latency-svc-bqqmq [747.311185ms]
Aug 11 15:32:59.036: INFO: Created: latency-svc-zck6x
Aug 11 15:32:59.072: INFO: Got endpoints: latency-svc-pqls2 [750.531895ms]
Aug 11 15:32:59.087: INFO: Created: latency-svc-vr5lm
Aug 11 15:32:59.121: INFO: Got endpoints: latency-svc-xkx7q [748.658827ms]
Aug 11 15:32:59.140: INFO: Created: latency-svc-mrxj6
Aug 11 15:32:59.172: INFO: Got endpoints: latency-svc-sdrdv [748.524754ms]
Aug 11 15:32:59.187: INFO: Created: latency-svc-9k8mv
Aug 11 15:32:59.222: INFO: Got endpoints: latency-svc-m786m [750.364128ms]
Aug 11 15:32:59.237: INFO: Created: latency-svc-pggzj
Aug 11 15:32:59.273: INFO: Got endpoints: latency-svc-tf289 [750.919404ms]
Aug 11 15:32:59.288: INFO: Created: latency-svc-6dw2q
Aug 11 15:32:59.322: INFO: Got endpoints: latency-svc-2h9lk [748.660157ms]
Aug 11 15:32:59.342: INFO: Created: latency-svc-r8qqb
Aug 11 15:32:59.373: INFO: Got endpoints: latency-svc-fqmrd [752.25532ms]
Aug 11 15:32:59.388: INFO: Created: latency-svc-lpkn5
Aug 11 15:32:59.422: INFO: Got endpoints: latency-svc-lzskn [743.881848ms]
Aug 11 15:32:59.443: INFO: Created: latency-svc-4ltks
Aug 11 15:32:59.472: INFO: Got endpoints: latency-svc-slvk8 [750.417292ms]
Aug 11 15:32:59.487: INFO: Created: latency-svc-pc2vv
Aug 11 15:32:59.524: INFO: Got endpoints: latency-svc-8vkst [751.375483ms]
Aug 11 15:32:59.549: INFO: Created: latency-svc-fzwks
Aug 11 15:32:59.573: INFO: Got endpoints: latency-svc-lxfq4 [751.869547ms]
Aug 11 15:32:59.590: INFO: Created: latency-svc-mcc5g
Aug 11 15:32:59.621: INFO: Got endpoints: latency-svc-85hsk [749.867396ms]
Aug 11 15:32:59.638: INFO: Created: latency-svc-bb4fb
Aug 11 15:32:59.673: INFO: Got endpoints: latency-svc-s87gg [752.515337ms]
Aug 11 15:32:59.688: INFO: Created: latency-svc-ht9hv
Aug 11 15:32:59.724: INFO: Got endpoints: latency-svc-pnxsx [752.967261ms]
Aug 11 15:32:59.738: INFO: Created: latency-svc-k2lnm
Aug 11 15:32:59.773: INFO: Got endpoints: latency-svc-zck6x [752.688992ms]
Aug 11 15:32:59.787: INFO: Created: latency-svc-xn2cj
Aug 11 15:32:59.823: INFO: Got endpoints: latency-svc-vr5lm [750.593598ms]
Aug 11 15:32:59.840: INFO: Created: latency-svc-6qrsj
Aug 11 15:32:59.875: INFO: Got endpoints: latency-svc-mrxj6 [754.090795ms]
Aug 11 15:32:59.893: INFO: Created: latency-svc-7bslq
Aug 11 15:32:59.922: INFO: Got endpoints: latency-svc-9k8mv [750.145412ms]
Aug 11 15:32:59.939: INFO: Created: latency-svc-tftn5
Aug 11 15:32:59.973: INFO: Got endpoints: latency-svc-pggzj [750.566398ms]
Aug 11 15:32:59.994: INFO: Created: latency-svc-qn729
Aug 11 15:33:00.020: INFO: Got endpoints: latency-svc-6dw2q [746.79803ms]
Aug 11 15:33:00.035: INFO: Created: latency-svc-q59vr
Aug 11 15:33:00.072: INFO: Got endpoints: latency-svc-r8qqb [750.235786ms]
Aug 11 15:33:00.096: INFO: Created: latency-svc-cbwm5
Aug 11 15:33:00.124: INFO: Got endpoints: latency-svc-lpkn5 [750.67888ms]
Aug 11 15:33:00.140: INFO: Created: latency-svc-x9c5x
Aug 11 15:33:00.173: INFO: Got endpoints: latency-svc-4ltks [750.803543ms]
Aug 11 15:33:00.204: INFO: Created: latency-svc-btkz2
Aug 11 15:33:00.222: INFO: Got endpoints: latency-svc-pc2vv [749.988378ms]
Aug 11 15:33:00.240: INFO: Created: latency-svc-sjn8b
Aug 11 15:33:00.272: INFO: Got endpoints: latency-svc-fzwks [748.522831ms]
Aug 11 15:33:00.290: INFO: Created: latency-svc-7kn4k
Aug 11 15:33:00.322: INFO: Got endpoints: latency-svc-mcc5g [749.198324ms]
Aug 11 15:33:00.338: INFO: Created: latency-svc-66k2w
Aug 11 15:33:00.371: INFO: Got endpoints: latency-svc-bb4fb [750.309547ms]
Aug 11 15:33:00.387: INFO: Created: latency-svc-xrw4p
Aug 11 15:33:00.421: INFO: Got endpoints: latency-svc-ht9hv [748.175212ms]
Aug 11 15:33:00.436: INFO: Created: latency-svc-4svdg
Aug 11 15:33:00.471: INFO: Got endpoints: latency-svc-k2lnm [747.218382ms]
Aug 11 15:33:00.487: INFO: Created: latency-svc-kdh45
Aug 11 15:33:00.529: INFO: Got endpoints: latency-svc-xn2cj [756.590285ms]
Aug 11 15:33:00.547: INFO: Created: latency-svc-t82ms
Aug 11 15:33:00.571: INFO: Got endpoints: latency-svc-6qrsj [748.003777ms]
Aug 11 15:33:00.587: INFO: Created: latency-svc-xph5q
Aug 11 15:33:00.622: INFO: Got endpoints: latency-svc-7bslq [746.208272ms]
Aug 11 15:33:00.644: INFO: Created: latency-svc-hzngq
Aug 11 15:33:00.673: INFO: Got endpoints: latency-svc-tftn5 [751.036211ms]
Aug 11 15:33:00.690: INFO: Created: latency-svc-gjb7z
Aug 11 15:33:00.724: INFO: Got endpoints: latency-svc-qn729 [750.715091ms]
Aug 11 15:33:00.739: INFO: Created: latency-svc-cskbv
Aug 11 15:33:00.772: INFO: Got endpoints: latency-svc-q59vr [751.938739ms]
Aug 11 15:33:00.791: INFO: Created: latency-svc-k5bvt
Aug 11 15:33:00.826: INFO: Got endpoints: latency-svc-cbwm5 [753.295981ms]
Aug 11 15:33:00.843: INFO: Created: latency-svc-55sfc
Aug 11 15:33:00.871: INFO: Got endpoints: latency-svc-x9c5x [747.080618ms]
Aug 11 15:33:00.888: INFO: Created: latency-svc-b54bm
Aug 11 15:33:00.923: INFO: Got endpoints: latency-svc-btkz2 [749.533755ms]
Aug 11 15:33:00.942: INFO: Created: latency-svc-rwvhw
Aug 11 15:33:00.971: INFO: Got endpoints: latency-svc-sjn8b [748.882036ms]
Aug 11 15:33:00.989: INFO: Created: latency-svc-7674s
Aug 11 15:33:01.021: INFO: Got endpoints: latency-svc-7kn4k [748.937489ms]
Aug 11 15:33:01.038: INFO: Created: latency-svc-65kb6
Aug 11 15:33:01.075: INFO: Got endpoints: latency-svc-66k2w [753.468287ms]
Aug 11 15:33:01.094: INFO: Created: latency-svc-mn7n4
Aug 11 15:33:01.121: INFO: Got endpoints: latency-svc-xrw4p [749.71972ms]
Aug 11 15:33:01.137: INFO: Created: latency-svc-xdwck
Aug 11 15:33:01.173: INFO: Got endpoints: latency-svc-4svdg [751.32659ms]
Aug 11 15:33:01.195: INFO: Created: latency-svc-z95vd
Aug 11 15:33:01.222: INFO: Got endpoints: latency-svc-kdh45 [750.907727ms]
Aug 11 15:33:01.240: INFO: Created: latency-svc-hpwqs
Aug 11 15:33:01.273: INFO: Got endpoints: latency-svc-t82ms [743.552928ms]
Aug 11 15:33:01.295: INFO: Created: latency-svc-44s2l
Aug 11 15:33:01.322: INFO: Got endpoints: latency-svc-xph5q [751.542469ms]
Aug 11 15:33:01.341: INFO: Created: latency-svc-lvkhs
Aug 11 15:33:01.373: INFO: Got endpoints: latency-svc-hzngq [751.01692ms]
Aug 11 15:33:01.389: INFO: Created: latency-svc-tj2kb
Aug 11 15:33:01.422: INFO: Got endpoints: latency-svc-gjb7z [749.306118ms]
Aug 11 15:33:01.438: INFO: Created: latency-svc-989qg
Aug 11 15:33:01.474: INFO: Got endpoints: latency-svc-cskbv [750.087614ms]
Aug 11 15:33:01.493: INFO: Created: latency-svc-rwrlc
Aug 11 15:33:01.522: INFO: Got endpoints: latency-svc-k5bvt [750.423023ms]
Aug 11 15:33:01.540: INFO: Created: latency-svc-4js2v
Aug 11 15:33:01.573: INFO: Got endpoints: latency-svc-55sfc [746.917543ms]
Aug 11 15:33:01.590: INFO: Created: latency-svc-29bhv
Aug 11 15:33:01.623: INFO: Got endpoints: latency-svc-b54bm [752.408994ms]
Aug 11 15:33:01.638: INFO: Created: latency-svc-vxj6k
Aug 11 15:33:01.672: INFO: Got endpoints: latency-svc-rwvhw [749.088931ms]
Aug 11 15:33:01.688: INFO: Created: latency-svc-7fb98
Aug 11 15:33:01.730: INFO: Got endpoints: latency-svc-7674s [758.502013ms]
Aug 11 15:33:01.747: INFO: Created: latency-svc-hp5gk
Aug 11 15:33:01.772: INFO: Got endpoints: latency-svc-65kb6 [750.409709ms]
Aug 11 15:33:01.787: INFO: Created: latency-svc-gkrfj
Aug 11 15:33:01.824: INFO: Got endpoints: latency-svc-mn7n4 [748.248435ms]
Aug 11 15:33:01.844: INFO: Created: latency-svc-l5ff5
Aug 11 15:33:01.873: INFO: Got endpoints: latency-svc-xdwck [751.591849ms]
Aug 11 15:33:01.915: INFO: Created: latency-svc-nmsz5
Aug 11 15:33:01.925: INFO: Got endpoints: latency-svc-z95vd [751.96537ms]
Aug 11 15:33:01.941: INFO: Created: latency-svc-nqthc
Aug 11 15:33:01.972: INFO: Got endpoints: latency-svc-hpwqs [750.429162ms]
Aug 11 15:33:01.988: INFO: Created: latency-svc-ckfdp
Aug 11 15:33:02.021: INFO: Got endpoints: latency-svc-44s2l [747.758388ms]
Aug 11 15:33:02.036: INFO: Created: latency-svc-tvm86
Aug 11 15:33:02.072: INFO: Got endpoints: latency-svc-lvkhs [749.855222ms]
Aug 11 15:33:02.087: INFO: Created: latency-svc-djdtv
Aug 11 15:33:02.121: INFO: Got endpoints: latency-svc-tj2kb [748.219264ms]
Aug 11 15:33:02.136: INFO: Created: latency-svc-hvcn4
Aug 11 15:33:02.171: INFO: Got endpoints: latency-svc-989qg [748.879794ms]
Aug 11 15:33:02.186: INFO: Created: latency-svc-mnsnd
Aug 11 15:33:02.222: INFO: Got endpoints: latency-svc-rwrlc [748.10443ms]
Aug 11 15:33:02.242: INFO: Created: latency-svc-bmjr7
Aug 11 15:33:02.277: INFO: Got endpoints: latency-svc-4js2v [754.385375ms]
Aug 11 15:33:02.294: INFO: Created: latency-svc-stdd4
Aug 11 15:33:02.324: INFO: Got endpoints: latency-svc-29bhv [751.33619ms]
Aug 11 15:33:02.339: INFO: Created: latency-svc-f6xhr
Aug 11 15:33:02.372: INFO: Got endpoints: latency-svc-vxj6k [748.319275ms]
Aug 11 15:33:02.391: INFO: Created: latency-svc-6c4mh
Aug 11 15:33:02.421: INFO: Got endpoints: latency-svc-7fb98 [749.190863ms]
Aug 11 15:33:02.439: INFO: Created: latency-svc-2z9sb
Aug 11 15:33:02.474: INFO: Got endpoints: latency-svc-hp5gk [743.703533ms]
Aug 11 15:33:02.497: INFO: Created: latency-svc-l994c
Aug 11 15:33:02.522: INFO: Got endpoints: latency-svc-gkrfj [750.639449ms]
Aug 11 15:33:02.538: INFO: Created: latency-svc-kxwbd
Aug 11 15:33:02.571: INFO: Got endpoints: latency-svc-l5ff5 [747.50284ms]
Aug 11 15:33:02.586: INFO: Created: latency-svc-smxn9
Aug 11 15:33:02.623: INFO: Got endpoints: latency-svc-nmsz5 [749.821403ms]
Aug 11 15:33:02.639: INFO: Created: latency-svc-8jcbv
Aug 11 15:33:02.672: INFO: Got endpoints: latency-svc-nqthc [747.079409ms]
Aug 11 15:33:02.690: INFO: Created: latency-svc-klwmj
Aug 11 15:33:02.723: INFO: Got endpoints: latency-svc-ckfdp [750.544426ms]
Aug 11 15:33:02.740: INFO: Created: latency-svc-rdckh
Aug 11 15:33:02.771: INFO: Got endpoints: latency-svc-tvm86 [749.704582ms]
Aug 11 15:33:02.785: INFO: Created: latency-svc-cbgr7
Aug 11 15:33:02.825: INFO: Got endpoints: latency-svc-djdtv [752.604021ms]
Aug 11 15:33:02.843: INFO: Created: latency-svc-qfljp
Aug 11 15:33:02.872: INFO: Got endpoints: latency-svc-hvcn4 [750.706181ms]
Aug 11 15:33:02.888: INFO: Created: latency-svc-v2r7p
Aug 11 15:33:02.931: INFO: Got endpoints: latency-svc-mnsnd [759.542515ms]
Aug 11 15:33:02.946: INFO: Created: latency-svc-lcbw5
Aug 11 15:33:02.971: INFO: Got endpoints: latency-svc-bmjr7 [749.323209ms]
Aug 11 15:33:02.991: INFO: Created: latency-svc-t95qb
Aug 11 15:33:03.022: INFO: Got endpoints: latency-svc-stdd4 [744.611542ms]
Aug 11 15:33:03.042: INFO: Created: latency-svc-qdrcj
Aug 11 15:33:03.073: INFO: Got endpoints: latency-svc-f6xhr [749.424252ms]
Aug 11 15:33:03.090: INFO: Created: latency-svc-q8rvr
Aug 11 15:33:03.122: INFO: Got endpoints: latency-svc-6c4mh [750.015772ms]
Aug 11 15:33:03.138: INFO: Created: latency-svc-z9nnv
Aug 11 15:33:03.172: INFO: Got endpoints: latency-svc-2z9sb [750.550478ms]
Aug 11 15:33:03.187: INFO: Created: latency-svc-wnt88
Aug 11 15:33:03.222: INFO: Got endpoints: latency-svc-l994c [747.744518ms]
Aug 11 15:33:03.239: INFO: Created: latency-svc-nsfc6
Aug 11 15:33:03.273: INFO: Got endpoints: latency-svc-kxwbd [750.057371ms]
Aug 11 15:33:03.288: INFO: Created: latency-svc-d5s9d
Aug 11 15:33:03.323: INFO: Got endpoints: latency-svc-smxn9 [751.250567ms]
Aug 11 15:33:03.340: INFO: Created: latency-svc-2vwhp
Aug 11 15:33:03.373: INFO: Got endpoints: latency-svc-8jcbv [750.6499ms]
Aug 11 15:33:03.390: INFO: Created: latency-svc-wx65l
Aug 11 15:33:03.422: INFO: Got endpoints: latency-svc-klwmj [749.912335ms]
Aug 11 15:33:03.438: INFO: Created: latency-svc-45zpf
Aug 11 15:33:03.476: INFO: Got endpoints: latency-svc-rdckh [752.807406ms]
Aug 11 15:33:03.492: INFO: Created: latency-svc-khxmb
Aug 11 15:33:03.520: INFO: Got endpoints: latency-svc-cbgr7 [749.830733ms]
Aug 11 15:33:03.535: INFO: Created: latency-svc-mgvs2
Aug 11 15:33:03.571: INFO: Got endpoints: latency-svc-qfljp [745.757087ms]
Aug 11 15:33:03.588: INFO: Created: latency-svc-fp757
Aug 11 15:33:03.621: INFO: Got endpoints: latency-svc-v2r7p [749.284707ms]
Aug 11 15:33:03.639: INFO: Created: latency-svc-92bml
Aug 11 15:33:03.672: INFO: Got endpoints: latency-svc-lcbw5 [740.99117ms]
Aug 11 15:33:03.696: INFO: Created: latency-svc-vcv9m
Aug 11 15:33:03.722: INFO: Got endpoints: latency-svc-t95qb [751.071361ms]
Aug 11 15:33:03.743: INFO: Created: latency-svc-tgt86
Aug 11 15:33:03.773: INFO: Got endpoints: latency-svc-qdrcj [751.349451ms]
Aug 11 15:33:03.790: INFO: Created: latency-svc-b4wg7
Aug 11 15:33:03.822: INFO: Got endpoints: latency-svc-q8rvr [748.09532ms]
Aug 11 15:33:03.838: INFO: Created: latency-svc-hb2tm
Aug 11 15:33:03.872: INFO: Got endpoints: latency-svc-z9nnv [749.73417ms]
Aug 11 15:33:03.889: INFO: Created: latency-svc-9886n
Aug 11 15:33:03.922: INFO: Got endpoints: latency-svc-wnt88 [749.77464ms]
Aug 11 15:33:03.937: INFO: Created: latency-svc-xxtwj
Aug 11 15:33:03.972: INFO: Got endpoints: latency-svc-nsfc6 [750.645989ms]
Aug 11 15:33:03.989: INFO: Created: latency-svc-gh6sl
Aug 11 15:33:04.023: INFO: Got endpoints: latency-svc-d5s9d [750.403202ms]
Aug 11 15:33:04.039: INFO: Created: latency-svc-p6lpq
Aug 11 15:33:04.073: INFO: Got endpoints: latency-svc-2vwhp [750.436525ms]
Aug 11 15:33:04.088: INFO: Created: latency-svc-qbggk
Aug 11 15:33:04.120: INFO: Got endpoints: latency-svc-wx65l [746.743517ms]
Aug 11 15:33:04.143: INFO: Created: latency-svc-l2hjf
Aug 11 15:33:04.172: INFO: Got endpoints: latency-svc-45zpf [749.828174ms]
Aug 11 15:33:04.189: INFO: Created: latency-svc-p66qx
Aug 11 15:33:04.222: INFO: Got endpoints: latency-svc-khxmb [746.030775ms]
Aug 11 15:33:04.243: INFO: Created: latency-svc-zx999
Aug 11 15:33:04.273: INFO: Got endpoints: latency-svc-mgvs2 [752.091994ms]
Aug 11 15:33:04.291: INFO: Created: latency-svc-zjszt
Aug 11 15:33:04.322: INFO: Got endpoints: latency-svc-fp757 [751.229977ms]
Aug 11 15:33:04.339: INFO: Created: latency-svc-dlqt9
Aug 11 15:33:04.374: INFO: Got endpoints: latency-svc-92bml [753.139676ms]
Aug 11 15:33:04.391: INFO: Created: latency-svc-4mxpp
Aug 11 15:33:04.421: INFO: Got endpoints: latency-svc-vcv9m [748.927825ms]
Aug 11 15:33:04.437: INFO: Created: latency-svc-k5kct
Aug 11 15:33:04.470: INFO: Got endpoints: latency-svc-tgt86 [747.892415ms]
Aug 11 15:33:04.488: INFO: Created: latency-svc-cx5px
Aug 11 15:33:04.523: INFO: Got endpoints: latency-svc-b4wg7 [749.760882ms]
Aug 11 15:33:04.540: INFO: Created: latency-svc-vsdp9
Aug 11 15:33:04.572: INFO: Got endpoints: latency-svc-hb2tm [750.272808ms]
Aug 11 15:33:04.588: INFO: Created: latency-svc-rr9lc
Aug 11 15:33:04.622: INFO: Got endpoints: latency-svc-9886n [750.579496ms]
Aug 11 15:33:04.638: INFO: Created: latency-svc-lsgqd
Aug 11 15:33:04.676: INFO: Got endpoints: latency-svc-xxtwj [754.131998ms]
Aug 11 15:33:04.694: INFO: Created: latency-svc-pc4zs
Aug 11 15:33:04.721: INFO: Got endpoints: latency-svc-gh6sl [748.917056ms]
Aug 11 15:33:04.738: INFO: Created: latency-svc-l5cl6
Aug 11 15:33:04.772: INFO: Got endpoints: latency-svc-p6lpq [748.806681ms]
Aug 11 15:33:04.795: INFO: Created: latency-svc-j97t8
Aug 11 15:33:04.820: INFO: Got endpoints: latency-svc-qbggk [747.197ms]
Aug 11 15:33:04.836: INFO: Created: latency-svc-mjnln
Aug 11 15:33:04.871: INFO: Got endpoints: latency-svc-l2hjf [751.152604ms]
Aug 11 15:33:04.896: INFO: Created: latency-svc-2kbb8
Aug 11 15:33:04.920: INFO: Got endpoints: latency-svc-p66qx [748.697088ms]
Aug 11 15:33:04.973: INFO: Got endpoints: latency-svc-zx999 [750.653999ms]
Aug 11 15:33:05.022: INFO: Got endpoints: latency-svc-zjszt [749.166693ms]
Aug 11 15:33:05.070: INFO: Got endpoints: latency-svc-dlqt9 [747.860002ms]
Aug 11 15:33:05.121: INFO: Got endpoints: latency-svc-4mxpp [746.269253ms]
Aug 11 15:33:05.171: INFO: Got endpoints: latency-svc-k5kct [749.919187ms]
Aug 11 15:33:05.222: INFO: Got endpoints: latency-svc-cx5px [751.496454ms]
Aug 11 15:33:05.272: INFO: Got endpoints: latency-svc-vsdp9 [749.563485ms]
Aug 11 15:33:05.323: INFO: Got endpoints: latency-svc-rr9lc [750.864535ms]
Aug 11 15:33:05.372: INFO: Got endpoints: latency-svc-lsgqd [749.765931ms]
Aug 11 15:33:05.421: INFO: Got endpoints: latency-svc-pc4zs [745.23672ms]
Aug 11 15:33:05.474: INFO: Got endpoints: latency-svc-l5cl6 [752.990702ms]
Aug 11 15:33:05.522: INFO: Got endpoints: latency-svc-j97t8 [750.365003ms]
Aug 11 15:33:05.572: INFO: Got endpoints: latency-svc-mjnln [751.694532ms]
Aug 11 15:33:05.623: INFO: Got endpoints: latency-svc-2kbb8 [751.265038ms]
Aug 11 15:33:05.623: INFO: Latencies: [29.419815ms 47.611962ms 62.303179ms 72.060502ms 88.746292ms 100.990073ms 110.736626ms 133.000558ms 162.666432ms 173.451507ms 186.327298ms 206.797696ms 220.069168ms 221.245895ms 224.280779ms 224.483305ms 225.105885ms 225.513228ms 226.296412ms 226.321163ms 227.108138ms 227.421618ms 227.761658ms 231.800734ms 231.904096ms 232.193246ms 233.423164ms 234.58833ms 234.612801ms 235.141537ms 236.306683ms 236.918993ms 239.125871ms 239.502354ms 242.63958ms 243.053744ms 244.271211ms 244.637432ms 245.711266ms 247.414689ms 253.793418ms 256.959936ms 257.519973ms 294.129653ms 331.740973ms 355.800602ms 392.264587ms 429.083152ms 467.25703ms 493.62552ms 519.061154ms 542.929055ms 579.0726ms 614.991167ms 655.986114ms 692.21233ms 721.492191ms 740.99117ms 743.552928ms 743.703533ms 743.881848ms 744.611542ms 745.23672ms 745.757087ms 746.030775ms 746.138548ms 746.208272ms 746.269253ms 746.743517ms 746.79803ms 746.908494ms 746.917543ms 747.079409ms 747.080618ms 747.197ms 747.218382ms 747.311185ms 747.50284ms 747.744518ms 747.758388ms 747.78648ms 747.860002ms 747.892415ms 748.003777ms 748.09532ms 748.10443ms 748.175212ms 748.219264ms 748.230164ms 748.248435ms 748.319275ms 748.424778ms 748.522831ms 748.524754ms 748.651807ms 748.658827ms 748.660157ms 748.697088ms 748.806681ms 748.879794ms 748.882036ms 748.917056ms 748.927825ms 748.937489ms 748.958386ms 749.088931ms 749.166693ms 749.190863ms 749.198324ms 749.271405ms 749.284707ms 749.306118ms 749.323209ms 749.424252ms 749.533755ms 749.563485ms 749.704582ms 749.71972ms 749.73417ms 749.760882ms 749.765931ms 749.77464ms 749.793163ms 749.821403ms 749.828174ms 749.830733ms 749.855222ms 749.867396ms 749.912335ms 749.919187ms 749.988378ms 750.015772ms 750.057371ms 750.087614ms 750.145412ms 750.235786ms 750.272808ms 750.309547ms 750.364128ms 750.365003ms 750.403202ms 750.409709ms 750.417292ms 750.423023ms 750.429162ms 750.436525ms 750.531895ms 750.544426ms 750.550478ms 750.566398ms 750.579496ms 750.593598ms 750.639449ms 750.645989ms 750.6499ms 750.653999ms 750.67888ms 750.706181ms 750.715091ms 750.803543ms 750.864535ms 750.907727ms 750.919404ms 751.01692ms 751.036211ms 751.071361ms 751.152604ms 751.229977ms 751.250567ms 751.265038ms 751.32659ms 751.33619ms 751.349451ms 751.375483ms 751.496454ms 751.542469ms 751.591849ms 751.694532ms 751.869547ms 751.938739ms 751.96537ms 752.091994ms 752.25532ms 752.408994ms 752.515337ms 752.604021ms 752.688992ms 752.807406ms 752.967261ms 752.990702ms 753.139676ms 753.295981ms 753.468287ms 753.835108ms 754.090795ms 754.131998ms 754.385375ms 756.590285ms 758.502013ms 759.542515ms]
Aug 11 15:33:05.623: INFO: 50 %ile: 748.882036ms
Aug 11 15:33:05.623: INFO: 90 %ile: 751.96537ms
Aug 11 15:33:05.623: INFO: 99 %ile: 758.502013ms
Aug 11 15:33:05.623: INFO: Total sample count: 200
[AfterEach] [sig-network] Service endpoints latency
  test/e2e/framework/framework.go:187
Aug 11 15:33:05.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "svc-latency-1601" for this suite. 08/11/23 15:33:05.63
{"msg":"PASSED [sig-network] Service endpoints latency should not be very high  [Conformance]","completed":342,"skipped":6288,"failed":0}
------------------------------
â€¢ [SLOW TEST] [9.781 seconds]
[sig-network] Service endpoints latency
test/e2e/network/common/framework.go:23
  should not be very high  [Conformance]
  test/e2e/network/service_latency.go:59

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:32:55.857
    Aug 11 15:32:55.857: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename svc-latency 08/11/23 15:32:55.858
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:32:55.883
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:32:55.887
    [It] should not be very high  [Conformance]
      test/e2e/network/service_latency.go:59
    Aug 11 15:32:55.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: creating replication controller svc-latency-rc in namespace svc-latency-1601 08/11/23 15:32:55.89
    I0811 15:32:55.898594      22 runners.go:193] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1601, replica count: 1
    I0811 15:32:56.949643      22 runners.go:193] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
    Aug 11 15:32:57.065: INFO: Created: latency-svc-w7mw9
    Aug 11 15:32:57.071: INFO: Got endpoints: latency-svc-w7mw9 [21.627423ms]
    Aug 11 15:32:57.091: INFO: Created: latency-svc-s6szq
    Aug 11 15:32:57.101: INFO: Got endpoints: latency-svc-s6szq [29.419815ms]
    Aug 11 15:32:57.105: INFO: Created: latency-svc-skdzc
    Aug 11 15:32:57.120: INFO: Got endpoints: latency-svc-skdzc [47.611962ms]
    Aug 11 15:32:57.124: INFO: Created: latency-svc-ph6sg
    Aug 11 15:32:57.134: INFO: Got endpoints: latency-svc-ph6sg [62.303179ms]
    Aug 11 15:32:57.138: INFO: Created: latency-svc-bdkqz
    Aug 11 15:32:57.144: INFO: Got endpoints: latency-svc-bdkqz [72.060502ms]
    Aug 11 15:32:57.154: INFO: Created: latency-svc-dhql8
    Aug 11 15:32:57.160: INFO: Got endpoints: latency-svc-dhql8 [88.746292ms]
    Aug 11 15:32:57.164: INFO: Created: latency-svc-cbllv
    Aug 11 15:32:57.173: INFO: Got endpoints: latency-svc-cbllv [100.990073ms]
    Aug 11 15:32:57.178: INFO: Created: latency-svc-blkns
    Aug 11 15:32:57.183: INFO: Got endpoints: latency-svc-blkns [110.736626ms]
    Aug 11 15:32:57.191: INFO: Created: latency-svc-rd48z
    Aug 11 15:32:57.205: INFO: Got endpoints: latency-svc-rd48z [133.000558ms]
    Aug 11 15:32:57.214: INFO: Created: latency-svc-hz6vh
    Aug 11 15:32:57.234: INFO: Got endpoints: latency-svc-hz6vh [162.666432ms]
    Aug 11 15:32:57.238: INFO: Created: latency-svc-dpwgw
    Aug 11 15:32:57.246: INFO: Got endpoints: latency-svc-dpwgw [173.451507ms]
    Aug 11 15:32:57.254: INFO: Created: latency-svc-bcnzg
    Aug 11 15:32:57.259: INFO: Got endpoints: latency-svc-bcnzg [186.327298ms]
    Aug 11 15:32:57.268: INFO: Created: latency-svc-gd85g
    Aug 11 15:32:57.279: INFO: Got endpoints: latency-svc-gd85g [206.797696ms]
    Aug 11 15:32:57.284: INFO: Created: latency-svc-9cfl4
    Aug 11 15:32:57.292: INFO: Got endpoints: latency-svc-9cfl4 [220.069168ms]
    Aug 11 15:32:57.295: INFO: Created: latency-svc-h6dkw
    Aug 11 15:32:57.304: INFO: Got endpoints: latency-svc-h6dkw [232.193246ms]
    Aug 11 15:32:57.309: INFO: Created: latency-svc-gxrqx
    Aug 11 15:32:57.318: INFO: Got endpoints: latency-svc-gxrqx [245.711266ms]
    Aug 11 15:32:57.321: INFO: Created: latency-svc-5gts5
    Aug 11 15:32:57.333: INFO: Got endpoints: latency-svc-5gts5 [231.904096ms]
    Aug 11 15:32:57.347: INFO: Created: latency-svc-v7q6j
    Aug 11 15:32:57.347: INFO: Got endpoints: latency-svc-v7q6j [227.421618ms]
    Aug 11 15:32:57.359: INFO: Created: latency-svc-869q9
    Aug 11 15:32:57.369: INFO: Got endpoints: latency-svc-869q9 [234.612801ms]
    Aug 11 15:32:57.374: INFO: Created: latency-svc-w9k7v
    Aug 11 15:32:57.384: INFO: Got endpoints: latency-svc-w9k7v [239.502354ms]
    Aug 11 15:32:57.388: INFO: Created: latency-svc-ggz7b
    Aug 11 15:32:57.397: INFO: Got endpoints: latency-svc-ggz7b [236.306683ms]
    Aug 11 15:32:57.401: INFO: Created: latency-svc-s7hgm
    Aug 11 15:32:57.410: INFO: Got endpoints: latency-svc-s7hgm [236.918993ms]
    Aug 11 15:32:57.414: INFO: Created: latency-svc-nklqk
    Aug 11 15:32:57.422: INFO: Got endpoints: latency-svc-nklqk [239.125871ms]
    Aug 11 15:32:57.427: INFO: Created: latency-svc-x5mdg
    Aug 11 15:32:57.442: INFO: Created: latency-svc-4k8ns
    Aug 11 15:32:57.459: INFO: Got endpoints: latency-svc-4k8ns [224.280779ms]
    Aug 11 15:32:57.459: INFO: Got endpoints: latency-svc-x5mdg [253.793418ms]
    Aug 11 15:32:57.463: INFO: Created: latency-svc-tmhhh
    Aug 11 15:32:57.472: INFO: Got endpoints: latency-svc-tmhhh [226.321163ms]
    Aug 11 15:32:57.476: INFO: Created: latency-svc-l7zz7
    Aug 11 15:32:57.486: INFO: Got endpoints: latency-svc-l7zz7 [227.108138ms]
    Aug 11 15:32:57.494: INFO: Created: latency-svc-7mn7g
    Aug 11 15:32:57.507: INFO: Got endpoints: latency-svc-7mn7g [227.761658ms]
    Aug 11 15:32:57.509: INFO: Created: latency-svc-nz8p6
    Aug 11 15:32:57.518: INFO: Got endpoints: latency-svc-nz8p6 [226.296412ms]
    Aug 11 15:32:57.521: INFO: Created: latency-svc-xtfgg
    Aug 11 15:32:57.530: INFO: Got endpoints: latency-svc-xtfgg [225.513228ms]
    Aug 11 15:32:57.535: INFO: Created: latency-svc-gm8wf
    Aug 11 15:32:57.543: INFO: Got endpoints: latency-svc-gm8wf [225.105885ms]
    Aug 11 15:32:57.548: INFO: Created: latency-svc-9r5qg
    Aug 11 15:32:57.569: INFO: Got endpoints: latency-svc-9r5qg [235.141537ms]
    Aug 11 15:32:57.572: INFO: Created: latency-svc-nxmm6
    Aug 11 15:32:57.580: INFO: Got endpoints: latency-svc-nxmm6 [233.423164ms]
    Aug 11 15:32:57.584: INFO: Created: latency-svc-k8xw5
    Aug 11 15:32:57.593: INFO: Got endpoints: latency-svc-k8xw5 [224.483305ms]
    Aug 11 15:32:57.598: INFO: Created: latency-svc-fknjx
    Aug 11 15:32:57.605: INFO: Got endpoints: latency-svc-fknjx [221.245895ms]
    Aug 11 15:32:57.609: INFO: Created: latency-svc-cdg7g
    Aug 11 15:32:57.629: INFO: Got endpoints: latency-svc-cdg7g [231.800734ms]
    Aug 11 15:32:57.651: INFO: Created: latency-svc-pkhl8
    Aug 11 15:32:57.653: INFO: Got endpoints: latency-svc-pkhl8 [242.63958ms]
    Aug 11 15:32:57.664: INFO: Created: latency-svc-hpmdc
    Aug 11 15:32:57.680: INFO: Got endpoints: latency-svc-hpmdc [257.519973ms]
    Aug 11 15:32:57.683: INFO: Created: latency-svc-t779g
    Aug 11 15:32:57.693: INFO: Got endpoints: latency-svc-t779g [234.58833ms]
    Aug 11 15:32:57.697: INFO: Created: latency-svc-nc2kf
    Aug 11 15:32:57.706: INFO: Got endpoints: latency-svc-nc2kf [247.414689ms]
    Aug 11 15:32:57.709: INFO: Created: latency-svc-8rxhk
    Aug 11 15:32:57.716: INFO: Got endpoints: latency-svc-8rxhk [244.271211ms]
    Aug 11 15:32:57.721: INFO: Created: latency-svc-htfmm
    Aug 11 15:32:57.730: INFO: Got endpoints: latency-svc-htfmm [244.637432ms]
    Aug 11 15:32:57.734: INFO: Created: latency-svc-gxbqp
    Aug 11 15:32:57.750: INFO: Got endpoints: latency-svc-gxbqp [243.053744ms]
    Aug 11 15:32:57.753: INFO: Created: latency-svc-xx6lx
    Aug 11 15:32:57.763: INFO: Created: latency-svc-pghjp
    Aug 11 15:32:57.775: INFO: Got endpoints: latency-svc-xx6lx [256.959936ms]
    Aug 11 15:32:57.779: INFO: Created: latency-svc-7k56z
    Aug 11 15:32:57.801: INFO: Created: latency-svc-k7jsg
    Aug 11 15:32:57.814: INFO: Created: latency-svc-k7ctp
    Aug 11 15:32:57.824: INFO: Got endpoints: latency-svc-pghjp [294.129653ms]
    Aug 11 15:32:57.829: INFO: Created: latency-svc-8xr2z
    Aug 11 15:32:57.845: INFO: Created: latency-svc-mhl5h
    Aug 11 15:32:57.855: INFO: Created: latency-svc-b2x8f
    Aug 11 15:32:57.866: INFO: Created: latency-svc-wvj9m
    Aug 11 15:32:57.874: INFO: Got endpoints: latency-svc-7k56z [331.740973ms]
    Aug 11 15:32:57.879: INFO: Created: latency-svc-b94bn
    Aug 11 15:32:57.890: INFO: Created: latency-svc-rfrxd
    Aug 11 15:32:57.910: INFO: Created: latency-svc-m66t6
    Aug 11 15:32:57.924: INFO: Got endpoints: latency-svc-k7jsg [355.800602ms]
    Aug 11 15:32:57.925: INFO: Created: latency-svc-42pwd
    Aug 11 15:32:57.935: INFO: Created: latency-svc-7698f
    Aug 11 15:32:57.947: INFO: Created: latency-svc-gcdr4
    Aug 11 15:32:57.959: INFO: Created: latency-svc-t79kp
    Aug 11 15:32:57.970: INFO: Created: latency-svc-b6m55
    Aug 11 15:32:57.973: INFO: Got endpoints: latency-svc-k7ctp [392.264587ms]
    Aug 11 15:32:57.982: INFO: Created: latency-svc-gkqrj
    Aug 11 15:32:57.991: INFO: Created: latency-svc-w98mv
    Aug 11 15:32:58.004: INFO: Created: latency-svc-p4ngr
    Aug 11 15:32:58.022: INFO: Got endpoints: latency-svc-8xr2z [429.083152ms]
    Aug 11 15:32:58.037: INFO: Created: latency-svc-dh2m7
    Aug 11 15:32:58.072: INFO: Got endpoints: latency-svc-mhl5h [467.25703ms]
    Aug 11 15:32:58.090: INFO: Created: latency-svc-2zpwn
    Aug 11 15:32:58.122: INFO: Got endpoints: latency-svc-b2x8f [493.62552ms]
    Aug 11 15:32:58.146: INFO: Created: latency-svc-wmxx5
    Aug 11 15:32:58.172: INFO: Got endpoints: latency-svc-wvj9m [519.061154ms]
    Aug 11 15:32:58.190: INFO: Created: latency-svc-dt9dk
    Aug 11 15:32:58.223: INFO: Got endpoints: latency-svc-b94bn [542.929055ms]
    Aug 11 15:32:58.246: INFO: Created: latency-svc-v52gj
    Aug 11 15:32:58.273: INFO: Got endpoints: latency-svc-rfrxd [579.0726ms]
    Aug 11 15:32:58.288: INFO: Created: latency-svc-bqqmq
    Aug 11 15:32:58.321: INFO: Got endpoints: latency-svc-m66t6 [614.991167ms]
    Aug 11 15:32:58.337: INFO: Created: latency-svc-pqls2
    Aug 11 15:32:58.372: INFO: Got endpoints: latency-svc-42pwd [655.986114ms]
    Aug 11 15:32:58.388: INFO: Created: latency-svc-xkx7q
    Aug 11 15:32:58.423: INFO: Got endpoints: latency-svc-7698f [692.21233ms]
    Aug 11 15:32:58.439: INFO: Created: latency-svc-sdrdv
    Aug 11 15:32:58.472: INFO: Got endpoints: latency-svc-gcdr4 [721.492191ms]
    Aug 11 15:32:58.487: INFO: Created: latency-svc-m786m
    Aug 11 15:32:58.522: INFO: Got endpoints: latency-svc-t79kp [746.908494ms]
    Aug 11 15:32:58.538: INFO: Created: latency-svc-tf289
    Aug 11 15:32:58.573: INFO: Got endpoints: latency-svc-b6m55 [749.271405ms]
    Aug 11 15:32:58.590: INFO: Created: latency-svc-2h9lk
    Aug 11 15:32:58.621: INFO: Got endpoints: latency-svc-gkqrj [746.138548ms]
    Aug 11 15:32:58.635: INFO: Created: latency-svc-fqmrd
    Aug 11 15:32:58.678: INFO: Got endpoints: latency-svc-w98mv [753.835108ms]
    Aug 11 15:32:58.693: INFO: Created: latency-svc-lzskn
    Aug 11 15:32:58.722: INFO: Got endpoints: latency-svc-p4ngr [748.958386ms]
    Aug 11 15:32:58.738: INFO: Created: latency-svc-slvk8
    Aug 11 15:32:58.772: INFO: Got endpoints: latency-svc-dh2m7 [749.793163ms]
    Aug 11 15:32:58.791: INFO: Created: latency-svc-8vkst
    Aug 11 15:32:58.821: INFO: Got endpoints: latency-svc-2zpwn [748.230164ms]
    Aug 11 15:32:58.837: INFO: Created: latency-svc-lxfq4
    Aug 11 15:32:58.871: INFO: Got endpoints: latency-svc-wmxx5 [748.651807ms]
    Aug 11 15:32:58.887: INFO: Created: latency-svc-85hsk
    Aug 11 15:32:58.920: INFO: Got endpoints: latency-svc-dt9dk [748.424778ms]
    Aug 11 15:32:58.935: INFO: Created: latency-svc-s87gg
    Aug 11 15:32:58.971: INFO: Got endpoints: latency-svc-v52gj [747.78648ms]
    Aug 11 15:32:58.985: INFO: Created: latency-svc-pnxsx
    Aug 11 15:32:59.020: INFO: Got endpoints: latency-svc-bqqmq [747.311185ms]
    Aug 11 15:32:59.036: INFO: Created: latency-svc-zck6x
    Aug 11 15:32:59.072: INFO: Got endpoints: latency-svc-pqls2 [750.531895ms]
    Aug 11 15:32:59.087: INFO: Created: latency-svc-vr5lm
    Aug 11 15:32:59.121: INFO: Got endpoints: latency-svc-xkx7q [748.658827ms]
    Aug 11 15:32:59.140: INFO: Created: latency-svc-mrxj6
    Aug 11 15:32:59.172: INFO: Got endpoints: latency-svc-sdrdv [748.524754ms]
    Aug 11 15:32:59.187: INFO: Created: latency-svc-9k8mv
    Aug 11 15:32:59.222: INFO: Got endpoints: latency-svc-m786m [750.364128ms]
    Aug 11 15:32:59.237: INFO: Created: latency-svc-pggzj
    Aug 11 15:32:59.273: INFO: Got endpoints: latency-svc-tf289 [750.919404ms]
    Aug 11 15:32:59.288: INFO: Created: latency-svc-6dw2q
    Aug 11 15:32:59.322: INFO: Got endpoints: latency-svc-2h9lk [748.660157ms]
    Aug 11 15:32:59.342: INFO: Created: latency-svc-r8qqb
    Aug 11 15:32:59.373: INFO: Got endpoints: latency-svc-fqmrd [752.25532ms]
    Aug 11 15:32:59.388: INFO: Created: latency-svc-lpkn5
    Aug 11 15:32:59.422: INFO: Got endpoints: latency-svc-lzskn [743.881848ms]
    Aug 11 15:32:59.443: INFO: Created: latency-svc-4ltks
    Aug 11 15:32:59.472: INFO: Got endpoints: latency-svc-slvk8 [750.417292ms]
    Aug 11 15:32:59.487: INFO: Created: latency-svc-pc2vv
    Aug 11 15:32:59.524: INFO: Got endpoints: latency-svc-8vkst [751.375483ms]
    Aug 11 15:32:59.549: INFO: Created: latency-svc-fzwks
    Aug 11 15:32:59.573: INFO: Got endpoints: latency-svc-lxfq4 [751.869547ms]
    Aug 11 15:32:59.590: INFO: Created: latency-svc-mcc5g
    Aug 11 15:32:59.621: INFO: Got endpoints: latency-svc-85hsk [749.867396ms]
    Aug 11 15:32:59.638: INFO: Created: latency-svc-bb4fb
    Aug 11 15:32:59.673: INFO: Got endpoints: latency-svc-s87gg [752.515337ms]
    Aug 11 15:32:59.688: INFO: Created: latency-svc-ht9hv
    Aug 11 15:32:59.724: INFO: Got endpoints: latency-svc-pnxsx [752.967261ms]
    Aug 11 15:32:59.738: INFO: Created: latency-svc-k2lnm
    Aug 11 15:32:59.773: INFO: Got endpoints: latency-svc-zck6x [752.688992ms]
    Aug 11 15:32:59.787: INFO: Created: latency-svc-xn2cj
    Aug 11 15:32:59.823: INFO: Got endpoints: latency-svc-vr5lm [750.593598ms]
    Aug 11 15:32:59.840: INFO: Created: latency-svc-6qrsj
    Aug 11 15:32:59.875: INFO: Got endpoints: latency-svc-mrxj6 [754.090795ms]
    Aug 11 15:32:59.893: INFO: Created: latency-svc-7bslq
    Aug 11 15:32:59.922: INFO: Got endpoints: latency-svc-9k8mv [750.145412ms]
    Aug 11 15:32:59.939: INFO: Created: latency-svc-tftn5
    Aug 11 15:32:59.973: INFO: Got endpoints: latency-svc-pggzj [750.566398ms]
    Aug 11 15:32:59.994: INFO: Created: latency-svc-qn729
    Aug 11 15:33:00.020: INFO: Got endpoints: latency-svc-6dw2q [746.79803ms]
    Aug 11 15:33:00.035: INFO: Created: latency-svc-q59vr
    Aug 11 15:33:00.072: INFO: Got endpoints: latency-svc-r8qqb [750.235786ms]
    Aug 11 15:33:00.096: INFO: Created: latency-svc-cbwm5
    Aug 11 15:33:00.124: INFO: Got endpoints: latency-svc-lpkn5 [750.67888ms]
    Aug 11 15:33:00.140: INFO: Created: latency-svc-x9c5x
    Aug 11 15:33:00.173: INFO: Got endpoints: latency-svc-4ltks [750.803543ms]
    Aug 11 15:33:00.204: INFO: Created: latency-svc-btkz2
    Aug 11 15:33:00.222: INFO: Got endpoints: latency-svc-pc2vv [749.988378ms]
    Aug 11 15:33:00.240: INFO: Created: latency-svc-sjn8b
    Aug 11 15:33:00.272: INFO: Got endpoints: latency-svc-fzwks [748.522831ms]
    Aug 11 15:33:00.290: INFO: Created: latency-svc-7kn4k
    Aug 11 15:33:00.322: INFO: Got endpoints: latency-svc-mcc5g [749.198324ms]
    Aug 11 15:33:00.338: INFO: Created: latency-svc-66k2w
    Aug 11 15:33:00.371: INFO: Got endpoints: latency-svc-bb4fb [750.309547ms]
    Aug 11 15:33:00.387: INFO: Created: latency-svc-xrw4p
    Aug 11 15:33:00.421: INFO: Got endpoints: latency-svc-ht9hv [748.175212ms]
    Aug 11 15:33:00.436: INFO: Created: latency-svc-4svdg
    Aug 11 15:33:00.471: INFO: Got endpoints: latency-svc-k2lnm [747.218382ms]
    Aug 11 15:33:00.487: INFO: Created: latency-svc-kdh45
    Aug 11 15:33:00.529: INFO: Got endpoints: latency-svc-xn2cj [756.590285ms]
    Aug 11 15:33:00.547: INFO: Created: latency-svc-t82ms
    Aug 11 15:33:00.571: INFO: Got endpoints: latency-svc-6qrsj [748.003777ms]
    Aug 11 15:33:00.587: INFO: Created: latency-svc-xph5q
    Aug 11 15:33:00.622: INFO: Got endpoints: latency-svc-7bslq [746.208272ms]
    Aug 11 15:33:00.644: INFO: Created: latency-svc-hzngq
    Aug 11 15:33:00.673: INFO: Got endpoints: latency-svc-tftn5 [751.036211ms]
    Aug 11 15:33:00.690: INFO: Created: latency-svc-gjb7z
    Aug 11 15:33:00.724: INFO: Got endpoints: latency-svc-qn729 [750.715091ms]
    Aug 11 15:33:00.739: INFO: Created: latency-svc-cskbv
    Aug 11 15:33:00.772: INFO: Got endpoints: latency-svc-q59vr [751.938739ms]
    Aug 11 15:33:00.791: INFO: Created: latency-svc-k5bvt
    Aug 11 15:33:00.826: INFO: Got endpoints: latency-svc-cbwm5 [753.295981ms]
    Aug 11 15:33:00.843: INFO: Created: latency-svc-55sfc
    Aug 11 15:33:00.871: INFO: Got endpoints: latency-svc-x9c5x [747.080618ms]
    Aug 11 15:33:00.888: INFO: Created: latency-svc-b54bm
    Aug 11 15:33:00.923: INFO: Got endpoints: latency-svc-btkz2 [749.533755ms]
    Aug 11 15:33:00.942: INFO: Created: latency-svc-rwvhw
    Aug 11 15:33:00.971: INFO: Got endpoints: latency-svc-sjn8b [748.882036ms]
    Aug 11 15:33:00.989: INFO: Created: latency-svc-7674s
    Aug 11 15:33:01.021: INFO: Got endpoints: latency-svc-7kn4k [748.937489ms]
    Aug 11 15:33:01.038: INFO: Created: latency-svc-65kb6
    Aug 11 15:33:01.075: INFO: Got endpoints: latency-svc-66k2w [753.468287ms]
    Aug 11 15:33:01.094: INFO: Created: latency-svc-mn7n4
    Aug 11 15:33:01.121: INFO: Got endpoints: latency-svc-xrw4p [749.71972ms]
    Aug 11 15:33:01.137: INFO: Created: latency-svc-xdwck
    Aug 11 15:33:01.173: INFO: Got endpoints: latency-svc-4svdg [751.32659ms]
    Aug 11 15:33:01.195: INFO: Created: latency-svc-z95vd
    Aug 11 15:33:01.222: INFO: Got endpoints: latency-svc-kdh45 [750.907727ms]
    Aug 11 15:33:01.240: INFO: Created: latency-svc-hpwqs
    Aug 11 15:33:01.273: INFO: Got endpoints: latency-svc-t82ms [743.552928ms]
    Aug 11 15:33:01.295: INFO: Created: latency-svc-44s2l
    Aug 11 15:33:01.322: INFO: Got endpoints: latency-svc-xph5q [751.542469ms]
    Aug 11 15:33:01.341: INFO: Created: latency-svc-lvkhs
    Aug 11 15:33:01.373: INFO: Got endpoints: latency-svc-hzngq [751.01692ms]
    Aug 11 15:33:01.389: INFO: Created: latency-svc-tj2kb
    Aug 11 15:33:01.422: INFO: Got endpoints: latency-svc-gjb7z [749.306118ms]
    Aug 11 15:33:01.438: INFO: Created: latency-svc-989qg
    Aug 11 15:33:01.474: INFO: Got endpoints: latency-svc-cskbv [750.087614ms]
    Aug 11 15:33:01.493: INFO: Created: latency-svc-rwrlc
    Aug 11 15:33:01.522: INFO: Got endpoints: latency-svc-k5bvt [750.423023ms]
    Aug 11 15:33:01.540: INFO: Created: latency-svc-4js2v
    Aug 11 15:33:01.573: INFO: Got endpoints: latency-svc-55sfc [746.917543ms]
    Aug 11 15:33:01.590: INFO: Created: latency-svc-29bhv
    Aug 11 15:33:01.623: INFO: Got endpoints: latency-svc-b54bm [752.408994ms]
    Aug 11 15:33:01.638: INFO: Created: latency-svc-vxj6k
    Aug 11 15:33:01.672: INFO: Got endpoints: latency-svc-rwvhw [749.088931ms]
    Aug 11 15:33:01.688: INFO: Created: latency-svc-7fb98
    Aug 11 15:33:01.730: INFO: Got endpoints: latency-svc-7674s [758.502013ms]
    Aug 11 15:33:01.747: INFO: Created: latency-svc-hp5gk
    Aug 11 15:33:01.772: INFO: Got endpoints: latency-svc-65kb6 [750.409709ms]
    Aug 11 15:33:01.787: INFO: Created: latency-svc-gkrfj
    Aug 11 15:33:01.824: INFO: Got endpoints: latency-svc-mn7n4 [748.248435ms]
    Aug 11 15:33:01.844: INFO: Created: latency-svc-l5ff5
    Aug 11 15:33:01.873: INFO: Got endpoints: latency-svc-xdwck [751.591849ms]
    Aug 11 15:33:01.915: INFO: Created: latency-svc-nmsz5
    Aug 11 15:33:01.925: INFO: Got endpoints: latency-svc-z95vd [751.96537ms]
    Aug 11 15:33:01.941: INFO: Created: latency-svc-nqthc
    Aug 11 15:33:01.972: INFO: Got endpoints: latency-svc-hpwqs [750.429162ms]
    Aug 11 15:33:01.988: INFO: Created: latency-svc-ckfdp
    Aug 11 15:33:02.021: INFO: Got endpoints: latency-svc-44s2l [747.758388ms]
    Aug 11 15:33:02.036: INFO: Created: latency-svc-tvm86
    Aug 11 15:33:02.072: INFO: Got endpoints: latency-svc-lvkhs [749.855222ms]
    Aug 11 15:33:02.087: INFO: Created: latency-svc-djdtv
    Aug 11 15:33:02.121: INFO: Got endpoints: latency-svc-tj2kb [748.219264ms]
    Aug 11 15:33:02.136: INFO: Created: latency-svc-hvcn4
    Aug 11 15:33:02.171: INFO: Got endpoints: latency-svc-989qg [748.879794ms]
    Aug 11 15:33:02.186: INFO: Created: latency-svc-mnsnd
    Aug 11 15:33:02.222: INFO: Got endpoints: latency-svc-rwrlc [748.10443ms]
    Aug 11 15:33:02.242: INFO: Created: latency-svc-bmjr7
    Aug 11 15:33:02.277: INFO: Got endpoints: latency-svc-4js2v [754.385375ms]
    Aug 11 15:33:02.294: INFO: Created: latency-svc-stdd4
    Aug 11 15:33:02.324: INFO: Got endpoints: latency-svc-29bhv [751.33619ms]
    Aug 11 15:33:02.339: INFO: Created: latency-svc-f6xhr
    Aug 11 15:33:02.372: INFO: Got endpoints: latency-svc-vxj6k [748.319275ms]
    Aug 11 15:33:02.391: INFO: Created: latency-svc-6c4mh
    Aug 11 15:33:02.421: INFO: Got endpoints: latency-svc-7fb98 [749.190863ms]
    Aug 11 15:33:02.439: INFO: Created: latency-svc-2z9sb
    Aug 11 15:33:02.474: INFO: Got endpoints: latency-svc-hp5gk [743.703533ms]
    Aug 11 15:33:02.497: INFO: Created: latency-svc-l994c
    Aug 11 15:33:02.522: INFO: Got endpoints: latency-svc-gkrfj [750.639449ms]
    Aug 11 15:33:02.538: INFO: Created: latency-svc-kxwbd
    Aug 11 15:33:02.571: INFO: Got endpoints: latency-svc-l5ff5 [747.50284ms]
    Aug 11 15:33:02.586: INFO: Created: latency-svc-smxn9
    Aug 11 15:33:02.623: INFO: Got endpoints: latency-svc-nmsz5 [749.821403ms]
    Aug 11 15:33:02.639: INFO: Created: latency-svc-8jcbv
    Aug 11 15:33:02.672: INFO: Got endpoints: latency-svc-nqthc [747.079409ms]
    Aug 11 15:33:02.690: INFO: Created: latency-svc-klwmj
    Aug 11 15:33:02.723: INFO: Got endpoints: latency-svc-ckfdp [750.544426ms]
    Aug 11 15:33:02.740: INFO: Created: latency-svc-rdckh
    Aug 11 15:33:02.771: INFO: Got endpoints: latency-svc-tvm86 [749.704582ms]
    Aug 11 15:33:02.785: INFO: Created: latency-svc-cbgr7
    Aug 11 15:33:02.825: INFO: Got endpoints: latency-svc-djdtv [752.604021ms]
    Aug 11 15:33:02.843: INFO: Created: latency-svc-qfljp
    Aug 11 15:33:02.872: INFO: Got endpoints: latency-svc-hvcn4 [750.706181ms]
    Aug 11 15:33:02.888: INFO: Created: latency-svc-v2r7p
    Aug 11 15:33:02.931: INFO: Got endpoints: latency-svc-mnsnd [759.542515ms]
    Aug 11 15:33:02.946: INFO: Created: latency-svc-lcbw5
    Aug 11 15:33:02.971: INFO: Got endpoints: latency-svc-bmjr7 [749.323209ms]
    Aug 11 15:33:02.991: INFO: Created: latency-svc-t95qb
    Aug 11 15:33:03.022: INFO: Got endpoints: latency-svc-stdd4 [744.611542ms]
    Aug 11 15:33:03.042: INFO: Created: latency-svc-qdrcj
    Aug 11 15:33:03.073: INFO: Got endpoints: latency-svc-f6xhr [749.424252ms]
    Aug 11 15:33:03.090: INFO: Created: latency-svc-q8rvr
    Aug 11 15:33:03.122: INFO: Got endpoints: latency-svc-6c4mh [750.015772ms]
    Aug 11 15:33:03.138: INFO: Created: latency-svc-z9nnv
    Aug 11 15:33:03.172: INFO: Got endpoints: latency-svc-2z9sb [750.550478ms]
    Aug 11 15:33:03.187: INFO: Created: latency-svc-wnt88
    Aug 11 15:33:03.222: INFO: Got endpoints: latency-svc-l994c [747.744518ms]
    Aug 11 15:33:03.239: INFO: Created: latency-svc-nsfc6
    Aug 11 15:33:03.273: INFO: Got endpoints: latency-svc-kxwbd [750.057371ms]
    Aug 11 15:33:03.288: INFO: Created: latency-svc-d5s9d
    Aug 11 15:33:03.323: INFO: Got endpoints: latency-svc-smxn9 [751.250567ms]
    Aug 11 15:33:03.340: INFO: Created: latency-svc-2vwhp
    Aug 11 15:33:03.373: INFO: Got endpoints: latency-svc-8jcbv [750.6499ms]
    Aug 11 15:33:03.390: INFO: Created: latency-svc-wx65l
    Aug 11 15:33:03.422: INFO: Got endpoints: latency-svc-klwmj [749.912335ms]
    Aug 11 15:33:03.438: INFO: Created: latency-svc-45zpf
    Aug 11 15:33:03.476: INFO: Got endpoints: latency-svc-rdckh [752.807406ms]
    Aug 11 15:33:03.492: INFO: Created: latency-svc-khxmb
    Aug 11 15:33:03.520: INFO: Got endpoints: latency-svc-cbgr7 [749.830733ms]
    Aug 11 15:33:03.535: INFO: Created: latency-svc-mgvs2
    Aug 11 15:33:03.571: INFO: Got endpoints: latency-svc-qfljp [745.757087ms]
    Aug 11 15:33:03.588: INFO: Created: latency-svc-fp757
    Aug 11 15:33:03.621: INFO: Got endpoints: latency-svc-v2r7p [749.284707ms]
    Aug 11 15:33:03.639: INFO: Created: latency-svc-92bml
    Aug 11 15:33:03.672: INFO: Got endpoints: latency-svc-lcbw5 [740.99117ms]
    Aug 11 15:33:03.696: INFO: Created: latency-svc-vcv9m
    Aug 11 15:33:03.722: INFO: Got endpoints: latency-svc-t95qb [751.071361ms]
    Aug 11 15:33:03.743: INFO: Created: latency-svc-tgt86
    Aug 11 15:33:03.773: INFO: Got endpoints: latency-svc-qdrcj [751.349451ms]
    Aug 11 15:33:03.790: INFO: Created: latency-svc-b4wg7
    Aug 11 15:33:03.822: INFO: Got endpoints: latency-svc-q8rvr [748.09532ms]
    Aug 11 15:33:03.838: INFO: Created: latency-svc-hb2tm
    Aug 11 15:33:03.872: INFO: Got endpoints: latency-svc-z9nnv [749.73417ms]
    Aug 11 15:33:03.889: INFO: Created: latency-svc-9886n
    Aug 11 15:33:03.922: INFO: Got endpoints: latency-svc-wnt88 [749.77464ms]
    Aug 11 15:33:03.937: INFO: Created: latency-svc-xxtwj
    Aug 11 15:33:03.972: INFO: Got endpoints: latency-svc-nsfc6 [750.645989ms]
    Aug 11 15:33:03.989: INFO: Created: latency-svc-gh6sl
    Aug 11 15:33:04.023: INFO: Got endpoints: latency-svc-d5s9d [750.403202ms]
    Aug 11 15:33:04.039: INFO: Created: latency-svc-p6lpq
    Aug 11 15:33:04.073: INFO: Got endpoints: latency-svc-2vwhp [750.436525ms]
    Aug 11 15:33:04.088: INFO: Created: latency-svc-qbggk
    Aug 11 15:33:04.120: INFO: Got endpoints: latency-svc-wx65l [746.743517ms]
    Aug 11 15:33:04.143: INFO: Created: latency-svc-l2hjf
    Aug 11 15:33:04.172: INFO: Got endpoints: latency-svc-45zpf [749.828174ms]
    Aug 11 15:33:04.189: INFO: Created: latency-svc-p66qx
    Aug 11 15:33:04.222: INFO: Got endpoints: latency-svc-khxmb [746.030775ms]
    Aug 11 15:33:04.243: INFO: Created: latency-svc-zx999
    Aug 11 15:33:04.273: INFO: Got endpoints: latency-svc-mgvs2 [752.091994ms]
    Aug 11 15:33:04.291: INFO: Created: latency-svc-zjszt
    Aug 11 15:33:04.322: INFO: Got endpoints: latency-svc-fp757 [751.229977ms]
    Aug 11 15:33:04.339: INFO: Created: latency-svc-dlqt9
    Aug 11 15:33:04.374: INFO: Got endpoints: latency-svc-92bml [753.139676ms]
    Aug 11 15:33:04.391: INFO: Created: latency-svc-4mxpp
    Aug 11 15:33:04.421: INFO: Got endpoints: latency-svc-vcv9m [748.927825ms]
    Aug 11 15:33:04.437: INFO: Created: latency-svc-k5kct
    Aug 11 15:33:04.470: INFO: Got endpoints: latency-svc-tgt86 [747.892415ms]
    Aug 11 15:33:04.488: INFO: Created: latency-svc-cx5px
    Aug 11 15:33:04.523: INFO: Got endpoints: latency-svc-b4wg7 [749.760882ms]
    Aug 11 15:33:04.540: INFO: Created: latency-svc-vsdp9
    Aug 11 15:33:04.572: INFO: Got endpoints: latency-svc-hb2tm [750.272808ms]
    Aug 11 15:33:04.588: INFO: Created: latency-svc-rr9lc
    Aug 11 15:33:04.622: INFO: Got endpoints: latency-svc-9886n [750.579496ms]
    Aug 11 15:33:04.638: INFO: Created: latency-svc-lsgqd
    Aug 11 15:33:04.676: INFO: Got endpoints: latency-svc-xxtwj [754.131998ms]
    Aug 11 15:33:04.694: INFO: Created: latency-svc-pc4zs
    Aug 11 15:33:04.721: INFO: Got endpoints: latency-svc-gh6sl [748.917056ms]
    Aug 11 15:33:04.738: INFO: Created: latency-svc-l5cl6
    Aug 11 15:33:04.772: INFO: Got endpoints: latency-svc-p6lpq [748.806681ms]
    Aug 11 15:33:04.795: INFO: Created: latency-svc-j97t8
    Aug 11 15:33:04.820: INFO: Got endpoints: latency-svc-qbggk [747.197ms]
    Aug 11 15:33:04.836: INFO: Created: latency-svc-mjnln
    Aug 11 15:33:04.871: INFO: Got endpoints: latency-svc-l2hjf [751.152604ms]
    Aug 11 15:33:04.896: INFO: Created: latency-svc-2kbb8
    Aug 11 15:33:04.920: INFO: Got endpoints: latency-svc-p66qx [748.697088ms]
    Aug 11 15:33:04.973: INFO: Got endpoints: latency-svc-zx999 [750.653999ms]
    Aug 11 15:33:05.022: INFO: Got endpoints: latency-svc-zjszt [749.166693ms]
    Aug 11 15:33:05.070: INFO: Got endpoints: latency-svc-dlqt9 [747.860002ms]
    Aug 11 15:33:05.121: INFO: Got endpoints: latency-svc-4mxpp [746.269253ms]
    Aug 11 15:33:05.171: INFO: Got endpoints: latency-svc-k5kct [749.919187ms]
    Aug 11 15:33:05.222: INFO: Got endpoints: latency-svc-cx5px [751.496454ms]
    Aug 11 15:33:05.272: INFO: Got endpoints: latency-svc-vsdp9 [749.563485ms]
    Aug 11 15:33:05.323: INFO: Got endpoints: latency-svc-rr9lc [750.864535ms]
    Aug 11 15:33:05.372: INFO: Got endpoints: latency-svc-lsgqd [749.765931ms]
    Aug 11 15:33:05.421: INFO: Got endpoints: latency-svc-pc4zs [745.23672ms]
    Aug 11 15:33:05.474: INFO: Got endpoints: latency-svc-l5cl6 [752.990702ms]
    Aug 11 15:33:05.522: INFO: Got endpoints: latency-svc-j97t8 [750.365003ms]
    Aug 11 15:33:05.572: INFO: Got endpoints: latency-svc-mjnln [751.694532ms]
    Aug 11 15:33:05.623: INFO: Got endpoints: latency-svc-2kbb8 [751.265038ms]
    Aug 11 15:33:05.623: INFO: Latencies: [29.419815ms 47.611962ms 62.303179ms 72.060502ms 88.746292ms 100.990073ms 110.736626ms 133.000558ms 162.666432ms 173.451507ms 186.327298ms 206.797696ms 220.069168ms 221.245895ms 224.280779ms 224.483305ms 225.105885ms 225.513228ms 226.296412ms 226.321163ms 227.108138ms 227.421618ms 227.761658ms 231.800734ms 231.904096ms 232.193246ms 233.423164ms 234.58833ms 234.612801ms 235.141537ms 236.306683ms 236.918993ms 239.125871ms 239.502354ms 242.63958ms 243.053744ms 244.271211ms 244.637432ms 245.711266ms 247.414689ms 253.793418ms 256.959936ms 257.519973ms 294.129653ms 331.740973ms 355.800602ms 392.264587ms 429.083152ms 467.25703ms 493.62552ms 519.061154ms 542.929055ms 579.0726ms 614.991167ms 655.986114ms 692.21233ms 721.492191ms 740.99117ms 743.552928ms 743.703533ms 743.881848ms 744.611542ms 745.23672ms 745.757087ms 746.030775ms 746.138548ms 746.208272ms 746.269253ms 746.743517ms 746.79803ms 746.908494ms 746.917543ms 747.079409ms 747.080618ms 747.197ms 747.218382ms 747.311185ms 747.50284ms 747.744518ms 747.758388ms 747.78648ms 747.860002ms 747.892415ms 748.003777ms 748.09532ms 748.10443ms 748.175212ms 748.219264ms 748.230164ms 748.248435ms 748.319275ms 748.424778ms 748.522831ms 748.524754ms 748.651807ms 748.658827ms 748.660157ms 748.697088ms 748.806681ms 748.879794ms 748.882036ms 748.917056ms 748.927825ms 748.937489ms 748.958386ms 749.088931ms 749.166693ms 749.190863ms 749.198324ms 749.271405ms 749.284707ms 749.306118ms 749.323209ms 749.424252ms 749.533755ms 749.563485ms 749.704582ms 749.71972ms 749.73417ms 749.760882ms 749.765931ms 749.77464ms 749.793163ms 749.821403ms 749.828174ms 749.830733ms 749.855222ms 749.867396ms 749.912335ms 749.919187ms 749.988378ms 750.015772ms 750.057371ms 750.087614ms 750.145412ms 750.235786ms 750.272808ms 750.309547ms 750.364128ms 750.365003ms 750.403202ms 750.409709ms 750.417292ms 750.423023ms 750.429162ms 750.436525ms 750.531895ms 750.544426ms 750.550478ms 750.566398ms 750.579496ms 750.593598ms 750.639449ms 750.645989ms 750.6499ms 750.653999ms 750.67888ms 750.706181ms 750.715091ms 750.803543ms 750.864535ms 750.907727ms 750.919404ms 751.01692ms 751.036211ms 751.071361ms 751.152604ms 751.229977ms 751.250567ms 751.265038ms 751.32659ms 751.33619ms 751.349451ms 751.375483ms 751.496454ms 751.542469ms 751.591849ms 751.694532ms 751.869547ms 751.938739ms 751.96537ms 752.091994ms 752.25532ms 752.408994ms 752.515337ms 752.604021ms 752.688992ms 752.807406ms 752.967261ms 752.990702ms 753.139676ms 753.295981ms 753.468287ms 753.835108ms 754.090795ms 754.131998ms 754.385375ms 756.590285ms 758.502013ms 759.542515ms]
    Aug 11 15:33:05.623: INFO: 50 %ile: 748.882036ms
    Aug 11 15:33:05.623: INFO: 90 %ile: 751.96537ms
    Aug 11 15:33:05.623: INFO: 99 %ile: 758.502013ms
    Aug 11 15:33:05.623: INFO: Total sample count: 200
    [AfterEach] [sig-network] Service endpoints latency
      test/e2e/framework/framework.go:187
    Aug 11 15:33:05.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "svc-latency-1601" for this suite. 08/11/23 15:33:05.63
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:05.639
Aug 11 15:33:05.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 15:33:05.64
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:05.659
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:05.662
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129
STEP: Creating the pod 08/11/23 15:33:05.665
Aug 11 15:33:05.675: INFO: Waiting up to 5m0s for pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856" in namespace "downward-api-8177" to be "running and ready"
Aug 11 15:33:05.681: INFO: Pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856": Phase="Pending", Reason="", readiness=false. Elapsed: 5.540852ms
Aug 11 15:33:05.681: INFO: The phase of Pod labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:33:07.686: INFO: Pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856": Phase="Running", Reason="", readiness=true. Elapsed: 2.011067162s
Aug 11 15:33:07.686: INFO: The phase of Pod labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856 is Running (Ready = true)
Aug 11 15:33:07.686: INFO: Pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856" satisfied condition "running and ready"
Aug 11 15:33:08.212: INFO: Successfully updated pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856"
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 15:33:12.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8177" for this suite. 08/11/23 15:33:12.247
{"msg":"PASSED [sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]","completed":343,"skipped":6299,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.621 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should update labels on modification [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:129

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:05.639
    Aug 11 15:33:05.639: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:33:05.64
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:05.659
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:05.662
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should update labels on modification [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:129
    STEP: Creating the pod 08/11/23 15:33:05.665
    Aug 11 15:33:05.675: INFO: Waiting up to 5m0s for pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856" in namespace "downward-api-8177" to be "running and ready"
    Aug 11 15:33:05.681: INFO: Pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856": Phase="Pending", Reason="", readiness=false. Elapsed: 5.540852ms
    Aug 11 15:33:05.681: INFO: The phase of Pod labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:33:07.686: INFO: Pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856": Phase="Running", Reason="", readiness=true. Elapsed: 2.011067162s
    Aug 11 15:33:07.686: INFO: The phase of Pod labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856 is Running (Ready = true)
    Aug 11 15:33:07.686: INFO: Pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856" satisfied condition "running and ready"
    Aug 11 15:33:08.212: INFO: Successfully updated pod "labelsupdate4dec1002-ce38-436a-a11a-4a13eeb02856"
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 15:33:12.241: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8177" for this suite. 08/11/23 15:33:12.247
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces
  should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:12.26
Aug 11 15:33:12.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename disruption 08/11/23 15:33:12.262
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:12.291
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:12.294
[BeforeEach] [sig-apps] DisruptionController
  test/e2e/apps/disruption.go:71
[BeforeEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:12.298
Aug 11 15:33:12.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename disruption-2 08/11/23 15:33:12.299
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:12.319
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:12.322
[It] should list and delete a collection of PodDisruptionBudgets [Conformance]
  test/e2e/apps/disruption.go:86
STEP: Waiting for the pdb to be processed 08/11/23 15:33:12.33
STEP: Waiting for the pdb to be processed 08/11/23 15:33:14.356
STEP: Waiting for the pdb to be processed 08/11/23 15:33:14.371
STEP: listing a collection of PDBs across all namespaces 08/11/23 15:33:16.379
STEP: listing a collection of PDBs in namespace disruption-2171 08/11/23 15:33:16.382
STEP: deleting a collection of PDBs 08/11/23 15:33:16.384
STEP: Waiting for the PDB collection to be deleted 08/11/23 15:33:16.397
[AfterEach] Listing PodDisruptionBudgets for all namespaces
  test/e2e/framework/framework.go:187
Aug 11 15:33:16.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2-204" for this suite. 08/11/23 15:33:16.404
[AfterEach] [sig-apps] DisruptionController
  test/e2e/framework/framework.go:187
Aug 11 15:33:16.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "disruption-2171" for this suite. 08/11/23 15:33:16.413
{"msg":"PASSED [sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]","completed":344,"skipped":6300,"failed":0}
------------------------------
â€¢ [4.159 seconds]
[sig-apps] DisruptionController
test/e2e/apps/framework.go:23
  Listing PodDisruptionBudgets for all namespaces
  test/e2e/apps/disruption.go:77
    should list and delete a collection of PodDisruptionBudgets [Conformance]
    test/e2e/apps/disruption.go:86

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:12.26
    Aug 11 15:33:12.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename disruption 08/11/23 15:33:12.262
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:12.291
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:12.294
    [BeforeEach] [sig-apps] DisruptionController
      test/e2e/apps/disruption.go:71
    [BeforeEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:12.298
    Aug 11 15:33:12.298: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename disruption-2 08/11/23 15:33:12.299
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:12.319
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:12.322
    [It] should list and delete a collection of PodDisruptionBudgets [Conformance]
      test/e2e/apps/disruption.go:86
    STEP: Waiting for the pdb to be processed 08/11/23 15:33:12.33
    STEP: Waiting for the pdb to be processed 08/11/23 15:33:14.356
    STEP: Waiting for the pdb to be processed 08/11/23 15:33:14.371
    STEP: listing a collection of PDBs across all namespaces 08/11/23 15:33:16.379
    STEP: listing a collection of PDBs in namespace disruption-2171 08/11/23 15:33:16.382
    STEP: deleting a collection of PDBs 08/11/23 15:33:16.384
    STEP: Waiting for the PDB collection to be deleted 08/11/23 15:33:16.397
    [AfterEach] Listing PodDisruptionBudgets for all namespaces
      test/e2e/framework/framework.go:187
    Aug 11 15:33:16.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2-204" for this suite. 08/11/23 15:33:16.404
    [AfterEach] [sig-apps] DisruptionController
      test/e2e/framework/framework.go:187
    Aug 11 15:33:16.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "disruption-2171" for this suite. 08/11/23 15:33:16.413
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial]
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:16.42
Aug 11 15:33:16.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename daemonsets 08/11/23 15:33:16.421
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:16.433
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:16.436
[BeforeEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:145
[It] should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193
Aug 11 15:33:16.454: INFO: Creating daemon "daemon-set" with a node selector
STEP: Initially, daemon pods should not be running on any nodes. 08/11/23 15:33:16.459
Aug 11 15:33:16.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:33:16.464: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Change node label to blue, check that daemon pod is launched. 08/11/23 15:33:16.464
Aug 11 15:33:16.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:33:16.484: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 15:33:17.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:33:17.490: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 15:33:18.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 15:33:18.489: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
STEP: Update the node label to green, and wait for daemons to be unscheduled 08/11/23 15:33:18.492
Aug 11 15:33:18.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 15:33:18.508: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
Aug 11 15:33:19.512: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:33:19.512: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/11/23 15:33:19.512
Aug 11 15:33:19.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:33:19.524: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 15:33:20.529: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:33:20.529: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 15:33:21.529: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:33:21.529: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
Aug 11 15:33:22.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
Aug 11 15:33:22.530: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/apps/daemon_set.go:110
STEP: Deleting DaemonSet "daemon-set" 08/11/23 15:33:22.55
STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3793, will wait for the garbage collector to delete the pods 08/11/23 15:33:22.55
Aug 11 15:33:22.614: INFO: Deleting DaemonSet.extensions daemon-set took: 6.743719ms
Aug 11 15:33:22.714: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.758355ms
Aug 11 15:33:25.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
Aug 11 15:33:25.019: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
Aug 11 15:33:25.022: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59236"},"items":null}

Aug 11 15:33:25.025: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59236"},"items":null}

[AfterEach] [sig-apps] Daemon set [Serial]
  test/e2e/framework/framework.go:187
Aug 11 15:33:25.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "daemonsets-3793" for this suite. 08/11/23 15:33:25.054
{"msg":"PASSED [sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]","completed":345,"skipped":6310,"failed":0}
------------------------------
â€¢ [SLOW TEST] [8.642 seconds]
[sig-apps] Daemon set [Serial]
test/e2e/apps/framework.go:23
  should run and stop complex daemon [Conformance]
  test/e2e/apps/daemon_set.go:193

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:16.42
    Aug 11 15:33:16.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename daemonsets 08/11/23 15:33:16.421
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:16.433
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:16.436
    [BeforeEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:145
    [It] should run and stop complex daemon [Conformance]
      test/e2e/apps/daemon_set.go:193
    Aug 11 15:33:16.454: INFO: Creating daemon "daemon-set" with a node selector
    STEP: Initially, daemon pods should not be running on any nodes. 08/11/23 15:33:16.459
    Aug 11 15:33:16.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:33:16.464: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Change node label to blue, check that daemon pod is launched. 08/11/23 15:33:16.464
    Aug 11 15:33:16.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:33:16.484: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 15:33:17.490: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:33:17.490: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 15:33:18.489: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 15:33:18.489: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    STEP: Update the node label to green, and wait for daemons to be unscheduled 08/11/23 15:33:18.492
    Aug 11 15:33:18.508: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 15:33:18.508: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
    Aug 11 15:33:19.512: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:33:19.512: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate 08/11/23 15:33:19.512
    Aug 11 15:33:19.524: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:33:19.524: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 15:33:20.529: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:33:20.529: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 15:33:21.529: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:33:21.529: INFO: Node constell-d93e7e1d-worker-d314547c-0lc3 is running 0 daemon pod, expected 1
    Aug 11 15:33:22.530: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
    Aug 11 15:33:22.530: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/apps/daemon_set.go:110
    STEP: Deleting DaemonSet "daemon-set" 08/11/23 15:33:22.55
    STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3793, will wait for the garbage collector to delete the pods 08/11/23 15:33:22.55
    Aug 11 15:33:22.614: INFO: Deleting DaemonSet.extensions daemon-set took: 6.743719ms
    Aug 11 15:33:22.714: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.758355ms
    Aug 11 15:33:25.019: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
    Aug 11 15:33:25.019: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
    Aug 11 15:33:25.022: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"59236"},"items":null}

    Aug 11 15:33:25.025: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"59236"},"items":null}

    [AfterEach] [sig-apps] Daemon set [Serial]
      test/e2e/framework/framework.go:187
    Aug 11 15:33:25.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "daemonsets-3793" for this suite. 08/11/23 15:33:25.054
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
[BeforeEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:25.063
Aug 11 15:33:25.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename secrets 08/11/23 15:33:25.064
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:25.08
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:25.082
[It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88
STEP: Creating secret with name secret-test-map-cc586f3b-8946-4e07-a8de-394e5eecb6c9 08/11/23 15:33:25.085
STEP: Creating a pod to test consume secrets 08/11/23 15:33:25.09
Aug 11 15:33:25.101: INFO: Waiting up to 5m0s for pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089" in namespace "secrets-9739" to be "Succeeded or Failed"
Aug 11 15:33:25.107: INFO: Pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089": Phase="Pending", Reason="", readiness=false. Elapsed: 5.905004ms
Aug 11 15:33:27.112: INFO: Pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011066362s
Aug 11 15:33:29.113: INFO: Pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011842023s
STEP: Saw pod success 08/11/23 15:33:29.113
Aug 11 15:33:29.113: INFO: Pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089" satisfied condition "Succeeded or Failed"
Aug 11 15:33:29.117: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089 container secret-volume-test: <nil>
STEP: delete the pod 08/11/23 15:33:29.127
Aug 11 15:33:29.139: INFO: Waiting for pod pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089 to disappear
Aug 11 15:33:29.142: INFO: Pod pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089 no longer exists
[AfterEach] [sig-storage] Secrets
  test/e2e/framework/framework.go:187
Aug 11 15:33:29.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "secrets-9739" for this suite. 08/11/23 15:33:29.146
{"msg":"PASSED [sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":346,"skipped":6328,"failed":0}
------------------------------
â€¢ [4.091 seconds]
[sig-storage] Secrets
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/secrets_volume.go:88

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:25.063
    Aug 11 15:33:25.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename secrets 08/11/23 15:33:25.064
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:25.08
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:25.082
    [It] should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/secrets_volume.go:88
    STEP: Creating secret with name secret-test-map-cc586f3b-8946-4e07-a8de-394e5eecb6c9 08/11/23 15:33:25.085
    STEP: Creating a pod to test consume secrets 08/11/23 15:33:25.09
    Aug 11 15:33:25.101: INFO: Waiting up to 5m0s for pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089" in namespace "secrets-9739" to be "Succeeded or Failed"
    Aug 11 15:33:25.107: INFO: Pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089": Phase="Pending", Reason="", readiness=false. Elapsed: 5.905004ms
    Aug 11 15:33:27.112: INFO: Pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089": Phase="Pending", Reason="", readiness=false. Elapsed: 2.011066362s
    Aug 11 15:33:29.113: INFO: Pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.011842023s
    STEP: Saw pod success 08/11/23 15:33:29.113
    Aug 11 15:33:29.113: INFO: Pod "pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089" satisfied condition "Succeeded or Failed"
    Aug 11 15:33:29.117: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089 container secret-volume-test: <nil>
    STEP: delete the pod 08/11/23 15:33:29.127
    Aug 11 15:33:29.139: INFO: Waiting for pod pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089 to disappear
    Aug 11 15:33:29.142: INFO: Pod pod-secrets-24970138-7c85-4c46-9e93-e23cb596e089 no longer exists
    [AfterEach] [sig-storage] Secrets
      test/e2e/framework/framework.go:187
    Aug 11 15:33:29.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "secrets-9739" for this suite. 08/11/23 15:33:29.146
  << End Captured GinkgoWriter Output
------------------------------
[sig-api-machinery] server version
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
[BeforeEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:29.154
Aug 11 15:33:29.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename server-version 08/11/23 15:33:29.156
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:29.174
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:29.177
[It] should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39
STEP: Request ServerVersion 08/11/23 15:33:29.18
STEP: Confirm major version 08/11/23 15:33:29.182
Aug 11 15:33:29.182: INFO: Major version: 1
STEP: Confirm minor version 08/11/23 15:33:29.182
Aug 11 15:33:29.182: INFO: cleanMinorVersion: 25
Aug 11 15:33:29.182: INFO: Minor version: 25
[AfterEach] [sig-api-machinery] server version
  test/e2e/framework/framework.go:187
Aug 11 15:33:29.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "server-version-8657" for this suite. 08/11/23 15:33:29.186
{"msg":"PASSED [sig-api-machinery] server version should find the server version [Conformance]","completed":347,"skipped":6328,"failed":0}
------------------------------
â€¢ [0.038 seconds]
[sig-api-machinery] server version
test/e2e/apimachinery/framework.go:23
  should find the server version [Conformance]
  test/e2e/apimachinery/server_version.go:39

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:29.154
    Aug 11 15:33:29.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename server-version 08/11/23 15:33:29.156
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:29.174
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:29.177
    [It] should find the server version [Conformance]
      test/e2e/apimachinery/server_version.go:39
    STEP: Request ServerVersion 08/11/23 15:33:29.18
    STEP: Confirm major version 08/11/23 15:33:29.182
    Aug 11 15:33:29.182: INFO: Major version: 1
    STEP: Confirm minor version 08/11/23 15:33:29.182
    Aug 11 15:33:29.182: INFO: cleanMinorVersion: 25
    Aug 11 15:33:29.182: INFO: Minor version: 25
    [AfterEach] [sig-api-machinery] server version
      test/e2e/framework/framework.go:187
    Aug 11 15:33:29.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "server-version-8657" for this suite. 08/11/23 15:33:29.186
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
[BeforeEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:29.195
Aug 11 15:33:29.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename emptydir-wrapper 08/11/23 15:33:29.196
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:29.21
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:29.213
[It] should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67
Aug 11 15:33:29.233: INFO: Waiting up to 5m0s for pod "pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543" in namespace "emptydir-wrapper-7185" to be "running and ready"
Aug 11 15:33:29.239: INFO: Pod "pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543": Phase="Pending", Reason="", readiness=false. Elapsed: 5.637155ms
Aug 11 15:33:29.239: INFO: The phase of Pod pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543 is Pending, waiting for it to be Running (with Ready = true)
Aug 11 15:33:31.244: INFO: Pod "pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543": Phase="Running", Reason="", readiness=true. Elapsed: 2.010434682s
Aug 11 15:33:31.244: INFO: The phase of Pod pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543 is Running (Ready = true)
Aug 11 15:33:31.244: INFO: Pod "pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543" satisfied condition "running and ready"
STEP: Cleaning up the secret 08/11/23 15:33:31.247
STEP: Cleaning up the configmap 08/11/23 15:33:31.254
STEP: Cleaning up the pod 08/11/23 15:33:31.26
[AfterEach] [sig-storage] EmptyDir wrapper volumes
  test/e2e/framework/framework.go:187
Aug 11 15:33:31.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "emptydir-wrapper-7185" for this suite. 08/11/23 15:33:31.275
{"msg":"PASSED [sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]","completed":348,"skipped":6381,"failed":0}
------------------------------
â€¢ [2.087 seconds]
[sig-storage] EmptyDir wrapper volumes
test/e2e/storage/utils/framework.go:23
  should not conflict [Conformance]
  test/e2e/storage/empty_dir_wrapper.go:67

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:29.195
    Aug 11 15:33:29.195: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename emptydir-wrapper 08/11/23 15:33:29.196
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:29.21
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:29.213
    [It] should not conflict [Conformance]
      test/e2e/storage/empty_dir_wrapper.go:67
    Aug 11 15:33:29.233: INFO: Waiting up to 5m0s for pod "pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543" in namespace "emptydir-wrapper-7185" to be "running and ready"
    Aug 11 15:33:29.239: INFO: Pod "pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543": Phase="Pending", Reason="", readiness=false. Elapsed: 5.637155ms
    Aug 11 15:33:29.239: INFO: The phase of Pod pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543 is Pending, waiting for it to be Running (with Ready = true)
    Aug 11 15:33:31.244: INFO: Pod "pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543": Phase="Running", Reason="", readiness=true. Elapsed: 2.010434682s
    Aug 11 15:33:31.244: INFO: The phase of Pod pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543 is Running (Ready = true)
    Aug 11 15:33:31.244: INFO: Pod "pod-secrets-344dfae8-3e0f-459e-8a06-101f263ba543" satisfied condition "running and ready"
    STEP: Cleaning up the secret 08/11/23 15:33:31.247
    STEP: Cleaning up the configmap 08/11/23 15:33:31.254
    STEP: Cleaning up the pod 08/11/23 15:33:31.26
    [AfterEach] [sig-storage] EmptyDir wrapper volumes
      test/e2e/framework/framework.go:187
    Aug 11 15:33:31.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "emptydir-wrapper-7185" for this suite. 08/11/23 15:33:31.275
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:31.283
Aug 11 15:33:31.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 15:33:31.284
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:31.298
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:31.301
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:33:31.304
Aug 11 15:33:31.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19" in namespace "downward-api-1881" to be "Succeeded or Failed"
Aug 11 15:33:31.317: INFO: Pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.284234ms
Aug 11 15:33:33.321: INFO: Pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008854363s
Aug 11 15:33:35.321: INFO: Pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008791277s
STEP: Saw pod success 08/11/23 15:33:35.321
Aug 11 15:33:35.321: INFO: Pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19" satisfied condition "Succeeded or Failed"
Aug 11 15:33:35.324: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19 container client-container: <nil>
STEP: delete the pod 08/11/23 15:33:35.334
Aug 11 15:33:35.349: INFO: Waiting for pod downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19 to disappear
Aug 11 15:33:35.352: INFO: Pod downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19 no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 15:33:35.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-1881" for this suite. 08/11/23 15:33:35.356
{"msg":"PASSED [sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]","completed":349,"skipped":6382,"failed":0}
------------------------------
â€¢ [4.081 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:260

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:31.283
    Aug 11 15:33:31.283: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:33:31.284
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:31.298
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:31.301
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:260
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:33:31.304
    Aug 11 15:33:31.312: INFO: Waiting up to 5m0s for pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19" in namespace "downward-api-1881" to be "Succeeded or Failed"
    Aug 11 15:33:31.317: INFO: Pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19": Phase="Pending", Reason="", readiness=false. Elapsed: 4.284234ms
    Aug 11 15:33:33.321: INFO: Pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008854363s
    Aug 11 15:33:35.321: INFO: Pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.008791277s
    STEP: Saw pod success 08/11/23 15:33:35.321
    Aug 11 15:33:35.321: INFO: Pod "downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19" satisfied condition "Succeeded or Failed"
    Aug 11 15:33:35.324: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19 container client-container: <nil>
    STEP: delete the pod 08/11/23 15:33:35.334
    Aug 11 15:33:35.349: INFO: Waiting for pod downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19 to disappear
    Aug 11 15:33:35.352: INFO: Pod downwardapi-volume-3e4f8a0e-8e2c-449b-a995-a8522ac17d19 no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 15:33:35.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-1881" for this suite. 08/11/23 15:33:35.356
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[BeforeEach] [sig-network] Services
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:35.364
Aug 11 15:33:35.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename services 08/11/23 15:33:35.365
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:35.38
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:35.383
[BeforeEach] [sig-network] Services
  test/e2e/network/service.go:758
[It] should provide secure master service  [Conformance]
  test/e2e/network/service.go:781
[AfterEach] [sig-network] Services
  test/e2e/framework/framework.go:187
Aug 11 15:33:35.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "services-4163" for this suite. 08/11/23 15:33:35.392
[AfterEach] [sig-network] Services
  test/e2e/network/service.go:762
{"msg":"PASSED [sig-network] Services should provide secure master service  [Conformance]","completed":350,"skipped":6393,"failed":0}
------------------------------
â€¢ [0.034 seconds]
[sig-network] Services
test/e2e/network/common/framework.go:23
  should provide secure master service  [Conformance]
  test/e2e/network/service.go:781

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-network] Services
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:35.364
    Aug 11 15:33:35.364: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename services 08/11/23 15:33:35.365
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:35.38
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:35.383
    [BeforeEach] [sig-network] Services
      test/e2e/network/service.go:758
    [It] should provide secure master service  [Conformance]
      test/e2e/network/service.go:781
    [AfterEach] [sig-network] Services
      test/e2e/framework/framework.go:187
    Aug 11 15:33:35.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "services-4163" for this suite. 08/11/23 15:33:35.392
    [AfterEach] [sig-network] Services
      test/e2e/network/service.go:762
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container
  should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
[BeforeEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:35.399
Aug 11 15:33:35.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename container-runtime 08/11/23 15:33:35.4
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:35.413
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:35.416
[It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
  test/e2e/common/node/runtime.go:194
STEP: create the container 08/11/23 15:33:35.419
STEP: wait for the container to reach Succeeded 08/11/23 15:33:35.426
STEP: get the container status 08/11/23 15:33:39.457
STEP: the container should be terminated 08/11/23 15:33:39.461
STEP: the termination message should be set 08/11/23 15:33:39.461
Aug 11 15:33:39.461: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
STEP: delete the container 08/11/23 15:33:39.461
[AfterEach] [sig-node] Container Runtime
  test/e2e/framework/framework.go:187
Aug 11 15:33:39.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "container-runtime-8743" for this suite. 08/11/23 15:33:39.484
{"msg":"PASSED [sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]","completed":351,"skipped":6411,"failed":0}
------------------------------
â€¢ [4.092 seconds]
[sig-node] Container Runtime
test/e2e/common/node/framework.go:23
  blackbox test
  test/e2e/common/node/runtime.go:43
    on terminated container
    test/e2e/common/node/runtime.go:136
      should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:35.399
    Aug 11 15:33:35.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename container-runtime 08/11/23 15:33:35.4
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:35.413
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:35.416
    [It] should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
      test/e2e/common/node/runtime.go:194
    STEP: create the container 08/11/23 15:33:35.419
    STEP: wait for the container to reach Succeeded 08/11/23 15:33:35.426
    STEP: get the container status 08/11/23 15:33:39.457
    STEP: the container should be terminated 08/11/23 15:33:39.461
    STEP: the termination message should be set 08/11/23 15:33:39.461
    Aug 11 15:33:39.461: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
    STEP: delete the container 08/11/23 15:33:39.461
    [AfterEach] [sig-node] Container Runtime
      test/e2e/framework/framework.go:187
    Aug 11 15:33:39.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "container-runtime-8743" for this suite. 08/11/23 15:33:39.484
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
[BeforeEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:39.492
Aug 11 15:33:39.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename cronjob 08/11/23 15:33:39.493
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:39.507
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:39.51
[It] should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319
STEP: Creating a cronjob 08/11/23 15:33:39.513
STEP: creating 08/11/23 15:33:39.513
STEP: getting 08/11/23 15:33:39.519
STEP: listing 08/11/23 15:33:39.522
STEP: watching 08/11/23 15:33:39.525
Aug 11 15:33:39.525: INFO: starting watch
STEP: cluster-wide listing 08/11/23 15:33:39.526
STEP: cluster-wide watching 08/11/23 15:33:39.529
Aug 11 15:33:39.530: INFO: starting watch
STEP: patching 08/11/23 15:33:39.531
STEP: updating 08/11/23 15:33:39.538
Aug 11 15:33:39.545: INFO: waiting for watch events with expected annotations
Aug 11 15:33:39.545: INFO: saw patched and updated annotations
STEP: patching /status 08/11/23 15:33:39.545
STEP: updating /status 08/11/23 15:33:39.552
STEP: get /status 08/11/23 15:33:39.559
STEP: deleting 08/11/23 15:33:39.563
STEP: deleting a collection 08/11/23 15:33:39.578
[AfterEach] [sig-apps] CronJob
  test/e2e/framework/framework.go:187
Aug 11 15:33:39.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "cronjob-8243" for this suite. 08/11/23 15:33:39.596
{"msg":"PASSED [sig-apps] CronJob should support CronJob API operations [Conformance]","completed":352,"skipped":6443,"failed":0}
------------------------------
â€¢ [0.110 seconds]
[sig-apps] CronJob
test/e2e/apps/framework.go:23
  should support CronJob API operations [Conformance]
  test/e2e/apps/cronjob.go:319

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:39.492
    Aug 11 15:33:39.492: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename cronjob 08/11/23 15:33:39.493
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:39.507
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:39.51
    [It] should support CronJob API operations [Conformance]
      test/e2e/apps/cronjob.go:319
    STEP: Creating a cronjob 08/11/23 15:33:39.513
    STEP: creating 08/11/23 15:33:39.513
    STEP: getting 08/11/23 15:33:39.519
    STEP: listing 08/11/23 15:33:39.522
    STEP: watching 08/11/23 15:33:39.525
    Aug 11 15:33:39.525: INFO: starting watch
    STEP: cluster-wide listing 08/11/23 15:33:39.526
    STEP: cluster-wide watching 08/11/23 15:33:39.529
    Aug 11 15:33:39.530: INFO: starting watch
    STEP: patching 08/11/23 15:33:39.531
    STEP: updating 08/11/23 15:33:39.538
    Aug 11 15:33:39.545: INFO: waiting for watch events with expected annotations
    Aug 11 15:33:39.545: INFO: saw patched and updated annotations
    STEP: patching /status 08/11/23 15:33:39.545
    STEP: updating /status 08/11/23 15:33:39.552
    STEP: get /status 08/11/23 15:33:39.559
    STEP: deleting 08/11/23 15:33:39.563
    STEP: deleting a collection 08/11/23 15:33:39.578
    [AfterEach] [sig-apps] CronJob
      test/e2e/framework/framework.go:187
    Aug 11 15:33:39.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "cronjob-8243" for this suite. 08/11/23 15:33:39.596
  << End Captured GinkgoWriter Output
------------------------------
S
------------------------------
[sig-storage] Downward API volume
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:39.603
Aug 11 15:33:39.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename downward-api 08/11/23 15:33:39.604
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:39.618
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:39.621
[BeforeEach] [sig-storage] Downward API volume
  test/e2e/common/storage/downwardapi_volume.go:43
[It] should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:33:39.624
Aug 11 15:33:39.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f" in namespace "downward-api-8914" to be "Succeeded or Failed"
Aug 11 15:33:39.638: INFO: Pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.454109ms
Aug 11 15:33:41.642: INFO: Pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009644456s
Aug 11 15:33:43.643: INFO: Pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010784358s
STEP: Saw pod success 08/11/23 15:33:43.643
Aug 11 15:33:43.643: INFO: Pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f" satisfied condition "Succeeded or Failed"
Aug 11 15:33:43.646: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f container client-container: <nil>
STEP: delete the pod 08/11/23 15:33:43.656
Aug 11 15:33:43.667: INFO: Waiting for pod downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f to disappear
Aug 11 15:33:43.671: INFO: Pod downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f no longer exists
[AfterEach] [sig-storage] Downward API volume
  test/e2e/framework/framework.go:187
Aug 11 15:33:43.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "downward-api-8914" for this suite. 08/11/23 15:33:43.675
{"msg":"PASSED [sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]","completed":353,"skipped":6444,"failed":0}
------------------------------
â€¢ [4.078 seconds]
[sig-storage] Downward API volume
test/e2e/common/storage/framework.go:23
  should provide container's memory request [NodeConformance] [Conformance]
  test/e2e/common/storage/downwardapi_volume.go:234

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:39.603
    Aug 11 15:33:39.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename downward-api 08/11/23 15:33:39.604
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:39.618
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:39.621
    [BeforeEach] [sig-storage] Downward API volume
      test/e2e/common/storage/downwardapi_volume.go:43
    [It] should provide container's memory request [NodeConformance] [Conformance]
      test/e2e/common/storage/downwardapi_volume.go:234
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:33:39.624
    Aug 11 15:33:39.632: INFO: Waiting up to 5m0s for pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f" in namespace "downward-api-8914" to be "Succeeded or Failed"
    Aug 11 15:33:39.638: INFO: Pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f": Phase="Pending", Reason="", readiness=false. Elapsed: 5.454109ms
    Aug 11 15:33:41.642: INFO: Pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f": Phase="Pending", Reason="", readiness=false. Elapsed: 2.009644456s
    Aug 11 15:33:43.643: INFO: Pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.010784358s
    STEP: Saw pod success 08/11/23 15:33:43.643
    Aug 11 15:33:43.643: INFO: Pod "downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f" satisfied condition "Succeeded or Failed"
    Aug 11 15:33:43.646: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f container client-container: <nil>
    STEP: delete the pod 08/11/23 15:33:43.656
    Aug 11 15:33:43.667: INFO: Waiting for pod downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f to disappear
    Aug 11 15:33:43.671: INFO: Pod downwardapi-volume-a8d30163-6a33-44d0-a6a5-20fae018325f no longer exists
    [AfterEach] [sig-storage] Downward API volume
      test/e2e/framework/framework.go:187
    Aug 11 15:33:43.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "downward-api-8914" for this suite. 08/11/23 15:33:43.675
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:43.684
Aug 11 15:33:43.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 15:33:43.684
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:43.7
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:43.702
[BeforeEach] [sig-storage] Projected downwardAPI
  test/e2e/common/storage/projected_downwardapi.go:43
[It] should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220
STEP: Creating a pod to test downward API volume plugin 08/11/23 15:33:43.705
Aug 11 15:33:43.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34" in namespace "projected-5043" to be "Succeeded or Failed"
Aug 11 15:33:43.715: INFO: Pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.776936ms
Aug 11 15:33:45.720: INFO: Pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007369115s
Aug 11 15:33:47.719: INFO: Pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006904667s
STEP: Saw pod success 08/11/23 15:33:47.719
Aug 11 15:33:47.720: INFO: Pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34" satisfied condition "Succeeded or Failed"
Aug 11 15:33:47.723: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34 container client-container: <nil>
STEP: delete the pod 08/11/23 15:33:47.731
Aug 11 15:33:47.742: INFO: Waiting for pod downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34 to disappear
Aug 11 15:33:47.745: INFO: Pod downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34 no longer exists
[AfterEach] [sig-storage] Projected downwardAPI
  test/e2e/framework/framework.go:187
Aug 11 15:33:47.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-5043" for this suite. 08/11/23 15:33:47.749
{"msg":"PASSED [sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]","completed":354,"skipped":6518,"failed":0}
------------------------------
â€¢ [4.074 seconds]
[sig-storage] Projected downwardAPI
test/e2e/common/storage/framework.go:23
  should provide container's cpu request [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_downwardapi.go:220

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:43.684
    Aug 11 15:33:43.684: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 15:33:43.684
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:43.7
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:43.702
    [BeforeEach] [sig-storage] Projected downwardAPI
      test/e2e/common/storage/projected_downwardapi.go:43
    [It] should provide container's cpu request [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_downwardapi.go:220
    STEP: Creating a pod to test downward API volume plugin 08/11/23 15:33:43.705
    Aug 11 15:33:43.713: INFO: Waiting up to 5m0s for pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34" in namespace "projected-5043" to be "Succeeded or Failed"
    Aug 11 15:33:43.715: INFO: Pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.776936ms
    Aug 11 15:33:45.720: INFO: Pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34": Phase="Pending", Reason="", readiness=false. Elapsed: 2.007369115s
    Aug 11 15:33:47.719: INFO: Pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.006904667s
    STEP: Saw pod success 08/11/23 15:33:47.719
    Aug 11 15:33:47.720: INFO: Pod "downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34" satisfied condition "Succeeded or Failed"
    Aug 11 15:33:47.723: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34 container client-container: <nil>
    STEP: delete the pod 08/11/23 15:33:47.731
    Aug 11 15:33:47.742: INFO: Waiting for pod downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34 to disappear
    Aug 11 15:33:47.745: INFO: Pod downwardapi-volume-e227da4a-2c9a-4661-81ea-7e219c8c4e34 no longer exists
    [AfterEach] [sig-storage] Projected downwardAPI
      test/e2e/framework/framework.go:187
    Aug 11 15:33:47.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-5043" for this suite. 08/11/23 15:33:47.749
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
[BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:33:47.76
Aug 11 15:33:47.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:33:47.761
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:47.775
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:47.778
[It] works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356
STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/11/23 15:33:47.78
Aug 11 15:33:47.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
Aug 11 15:33:51.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
[AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
  test/e2e/framework/framework.go:187
Aug 11 15:34:04.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "crd-publish-openapi-7402" for this suite. 08/11/23 15:34:04.904
{"msg":"PASSED [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]","completed":355,"skipped":6560,"failed":0}
------------------------------
â€¢ [SLOW TEST] [17.150 seconds]
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
test/e2e/apimachinery/framework.go:23
  works for multiple CRDs of same group and version but different kinds [Conformance]
  test/e2e/apimachinery/crd_publish_openapi.go:356

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:33:47.76
    Aug 11 15:33:47.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename crd-publish-openapi 08/11/23 15:33:47.761
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:33:47.775
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:33:47.778
    [It] works for multiple CRDs of same group and version but different kinds [Conformance]
      test/e2e/apimachinery/crd_publish_openapi.go:356
    STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation 08/11/23 15:33:47.78
    Aug 11 15:33:47.781: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    Aug 11 15:33:51.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    [AfterEach] [sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin]
      test/e2e/framework/framework.go:187
    Aug 11 15:34:04.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "crd-publish-openapi-7402" for this suite. 08/11/23 15:34:04.904
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
[BeforeEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:34:04.914
Aug 11 15:34:04.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename projected 08/11/23 15:34:04.914
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:04.93
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:04.933
[It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98
STEP: Creating configMap with name projected-configmap-test-volume-map-25b8f0f7-647f-42fc-8634-b929077bf85e 08/11/23 15:34:04.936
STEP: Creating a pod to test consume configMaps 08/11/23 15:34:04.941
Aug 11 15:34:04.949: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976" in namespace "projected-6620" to be "Succeeded or Failed"
Aug 11 15:34:04.953: INFO: Pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058486ms
Aug 11 15:34:06.958: INFO: Pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008624515s
Aug 11 15:34:08.959: INFO: Pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009750986s
STEP: Saw pod success 08/11/23 15:34:08.959
Aug 11 15:34:08.959: INFO: Pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976" satisfied condition "Succeeded or Failed"
Aug 11 15:34:08.962: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976 container agnhost-container: <nil>
STEP: delete the pod 08/11/23 15:34:08.973
Aug 11 15:34:08.983: INFO: Waiting for pod pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976 to disappear
Aug 11 15:34:08.986: INFO: Pod pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976 no longer exists
[AfterEach] [sig-storage] Projected configMap
  test/e2e/framework/framework.go:187
Aug 11 15:34:08.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "projected-6620" for this suite. 08/11/23 15:34:08.99
{"msg":"PASSED [sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]","completed":356,"skipped":6626,"failed":0}
------------------------------
â€¢ [4.082 seconds]
[sig-storage] Projected configMap
test/e2e/common/storage/framework.go:23
  should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
  test/e2e/common/storage/projected_configmap.go:98

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:34:04.914
    Aug 11 15:34:04.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename projected 08/11/23 15:34:04.914
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:04.93
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:04.933
    [It] should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
      test/e2e/common/storage/projected_configmap.go:98
    STEP: Creating configMap with name projected-configmap-test-volume-map-25b8f0f7-647f-42fc-8634-b929077bf85e 08/11/23 15:34:04.936
    STEP: Creating a pod to test consume configMaps 08/11/23 15:34:04.941
    Aug 11 15:34:04.949: INFO: Waiting up to 5m0s for pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976" in namespace "projected-6620" to be "Succeeded or Failed"
    Aug 11 15:34:04.953: INFO: Pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976": Phase="Pending", Reason="", readiness=false. Elapsed: 4.058486ms
    Aug 11 15:34:06.958: INFO: Pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976": Phase="Pending", Reason="", readiness=false. Elapsed: 2.008624515s
    Aug 11 15:34:08.959: INFO: Pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976": Phase="Succeeded", Reason="", readiness=false. Elapsed: 4.009750986s
    STEP: Saw pod success 08/11/23 15:34:08.959
    Aug 11 15:34:08.959: INFO: Pod "pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976" satisfied condition "Succeeded or Failed"
    Aug 11 15:34:08.962: INFO: Trying to get logs from node constell-d93e7e1d-worker-d314547c-0lc3 pod pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976 container agnhost-container: <nil>
    STEP: delete the pod 08/11/23 15:34:08.973
    Aug 11 15:34:08.983: INFO: Waiting for pod pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976 to disappear
    Aug 11 15:34:08.986: INFO: Pod pod-projected-configmaps-fc846287-01bf-43bd-8381-995a7edbe976 no longer exists
    [AfterEach] [sig-storage] Projected configMap
      test/e2e/framework/framework.go:187
    Aug 11 15:34:08.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "projected-6620" for this suite. 08/11/23 15:34:08.99
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose
  should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:34:08.997
Aug 11 15:34:08.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubectl 08/11/23 15:34:08.998
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:09.014
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:09.016
[BeforeEach] [sig-cli] Kubectl client
  test/e2e/kubectl/kubectl.go:272
[It] should create services for rc  [Conformance]
  test/e2e/kubectl/kubectl.go:1413
STEP: creating Agnhost RC 08/11/23 15:34:09.019
Aug 11 15:34:09.019: INFO: namespace kubectl-2138
Aug 11 15:34:09.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2138 create -f -'
Aug 11 15:34:09.538: INFO: stderr: ""
Aug 11 15:34:09.538: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
STEP: Waiting for Agnhost primary to start. 08/11/23 15:34:09.538
Aug 11 15:34:10.542: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 15:34:10.542: INFO: Found 0 / 1
Aug 11 15:34:11.543: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 15:34:11.543: INFO: Found 1 / 1
Aug 11 15:34:11.543: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
Aug 11 15:34:11.546: INFO: Selector matched 1 pods for map[app:agnhost]
Aug 11 15:34:11.547: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
Aug 11 15:34:11.547: INFO: wait on agnhost-primary startup in kubectl-2138 
Aug 11 15:34:11.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2138 logs agnhost-primary-xff5h agnhost-primary'
Aug 11 15:34:11.627: INFO: stderr: ""
Aug 11 15:34:11.627: INFO: stdout: "Paused\n"
STEP: exposing RC 08/11/23 15:34:11.627
Aug 11 15:34:11.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2138 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
Aug 11 15:34:11.704: INFO: stderr: ""
Aug 11 15:34:11.704: INFO: stdout: "service/rm2 exposed\n"
Aug 11 15:34:11.710: INFO: Service rm2 in namespace kubectl-2138 found.
STEP: exposing service 08/11/23 15:34:13.717
Aug 11 15:34:13.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2138 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
Aug 11 15:34:13.788: INFO: stderr: ""
Aug 11 15:34:13.788: INFO: stdout: "service/rm3 exposed\n"
Aug 11 15:34:13.793: INFO: Service rm3 in namespace kubectl-2138 found.
[AfterEach] [sig-cli] Kubectl client
  test/e2e/framework/framework.go:187
Aug 11 15:34:15.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubectl-2138" for this suite. 08/11/23 15:34:15.804
{"msg":"PASSED [sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]","completed":357,"skipped":6641,"failed":0}
------------------------------
â€¢ [SLOW TEST] [6.815 seconds]
[sig-cli] Kubectl client
test/e2e/kubectl/framework.go:23
  Kubectl expose
  test/e2e/kubectl/kubectl.go:1407
    should create services for rc  [Conformance]
    test/e2e/kubectl/kubectl.go:1413

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:34:08.997
    Aug 11 15:34:08.997: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubectl 08/11/23 15:34:08.998
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:09.014
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:09.016
    [BeforeEach] [sig-cli] Kubectl client
      test/e2e/kubectl/kubectl.go:272
    [It] should create services for rc  [Conformance]
      test/e2e/kubectl/kubectl.go:1413
    STEP: creating Agnhost RC 08/11/23 15:34:09.019
    Aug 11 15:34:09.019: INFO: namespace kubectl-2138
    Aug 11 15:34:09.019: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2138 create -f -'
    Aug 11 15:34:09.538: INFO: stderr: ""
    Aug 11 15:34:09.538: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
    STEP: Waiting for Agnhost primary to start. 08/11/23 15:34:09.538
    Aug 11 15:34:10.542: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 15:34:10.542: INFO: Found 0 / 1
    Aug 11 15:34:11.543: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 15:34:11.543: INFO: Found 1 / 1
    Aug 11 15:34:11.543: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
    Aug 11 15:34:11.546: INFO: Selector matched 1 pods for map[app:agnhost]
    Aug 11 15:34:11.547: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
    Aug 11 15:34:11.547: INFO: wait on agnhost-primary startup in kubectl-2138 
    Aug 11 15:34:11.547: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2138 logs agnhost-primary-xff5h agnhost-primary'
    Aug 11 15:34:11.627: INFO: stderr: ""
    Aug 11 15:34:11.627: INFO: stdout: "Paused\n"
    STEP: exposing RC 08/11/23 15:34:11.627
    Aug 11 15:34:11.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2138 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
    Aug 11 15:34:11.704: INFO: stderr: ""
    Aug 11 15:34:11.704: INFO: stdout: "service/rm2 exposed\n"
    Aug 11 15:34:11.710: INFO: Service rm2 in namespace kubectl-2138 found.
    STEP: exposing service 08/11/23 15:34:13.717
    Aug 11 15:34:13.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=kubectl-2138 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
    Aug 11 15:34:13.788: INFO: stderr: ""
    Aug 11 15:34:13.788: INFO: stdout: "service/rm3 exposed\n"
    Aug 11 15:34:13.793: INFO: Service rm3 in namespace kubectl-2138 found.
    [AfterEach] [sig-cli] Kubectl client
      test/e2e/framework/framework.go:187
    Aug 11 15:34:15.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubectl-2138" for this suite. 08/11/23 15:34:15.804
  << End Captured GinkgoWriter Output
------------------------------
SSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod
  should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[BeforeEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:34:15.812
Aug 11 15:34:15.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:34:15.813
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:15.827
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:15.83
[BeforeEach] [sig-node] Kubelet
  test/e2e/common/node/kubelet.go:41
[BeforeEach] when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:85
[It] should be possible to delete [NodeConformance] [Conformance]
  test/e2e/common/node/kubelet.go:135
[AfterEach] [sig-node] Kubelet
  test/e2e/framework/framework.go:187
Aug 11 15:34:15.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "kubelet-test-8315" for this suite. 08/11/23 15:34:15.854
{"msg":"PASSED [sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]","completed":358,"skipped":6646,"failed":0}
------------------------------
â€¢ [0.049 seconds]
[sig-node] Kubelet
test/e2e/common/node/framework.go:23
  when scheduling a busybox command that always fails in a pod
  test/e2e/common/node/kubelet.go:82
    should be possible to delete [NodeConformance] [Conformance]
    test/e2e/common/node/kubelet.go:135

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:34:15.812
    Aug 11 15:34:15.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename kubelet-test 08/11/23 15:34:15.813
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:15.827
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:15.83
    [BeforeEach] [sig-node] Kubelet
      test/e2e/common/node/kubelet.go:41
    [BeforeEach] when scheduling a busybox command that always fails in a pod
      test/e2e/common/node/kubelet.go:85
    [It] should be possible to delete [NodeConformance] [Conformance]
      test/e2e/common/node/kubelet.go:135
    [AfterEach] [sig-node] Kubelet
      test/e2e/framework/framework.go:187
    Aug 11 15:34:15.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "kubelet-test-8315" for this suite. 08/11/23 15:34:15.854
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic]
  Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:34:15.862
Aug 11 15:34:15.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename statefulset 08/11/23 15:34:15.863
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:15.875
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:15.878
[BeforeEach] [sig-apps] StatefulSet
  test/e2e/apps/statefulset.go:96
[BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:111
STEP: Creating service test in namespace statefulset-2271 08/11/23 15:34:15.881
[It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
  test/e2e/apps/statefulset.go:585
STEP: Initializing watcher for selector baz=blah,foo=bar 08/11/23 15:34:15.89
STEP: Creating stateful set ss in namespace statefulset-2271 08/11/23 15:34:15.893
STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2271 08/11/23 15:34:15.898
Aug 11 15:34:15.900: INFO: Found 0 stateful pods, waiting for 1
Aug 11 15:34:25.906: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/11/23 15:34:25.906
Aug 11 15:34:25.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:34:26.053: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:34:26.053: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:34:26.053: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 15:34:26.057: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
Aug 11 15:34:36.066: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 15:34:36.066: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:34:36.081: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999976s
Aug 11 15:34:37.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996583405s
Aug 11 15:34:38.090: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99250275s
Aug 11 15:34:39.095: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987730552s
Aug 11 15:34:40.100: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983030549s
Aug 11 15:34:41.104: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978233169s
Aug 11 15:34:42.109: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97399795s
Aug 11 15:34:43.113: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.969309746s
Aug 11 15:34:44.118: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.9648963s
Aug 11 15:34:45.122: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.557187ms
STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2271 08/11/23 15:34:46.122
Aug 11 15:34:46.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 15:34:46.259: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 15:34:46.259: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 15:34:46.259: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 15:34:46.263: INFO: Found 1 stateful pods, waiting for 3
Aug 11 15:34:56.271: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:34:56.271: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
Aug 11 15:34:56.271: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
STEP: Verifying that stateful set ss was scaled up in order 08/11/23 15:34:56.271
STEP: Scale down will halt with unhealthy stateful pod 08/11/23 15:34:56.271
Aug 11 15:34:56.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:34:56.414: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:34:56.414: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:34:56.414: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 15:34:56.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:34:56.562: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:34:56.563: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:34:56.563: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 15:34:56.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
Aug 11 15:34:56.698: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
Aug 11 15:34:56.698: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
Aug 11 15:34:56.698: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

Aug 11 15:34:56.698: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:34:56.702: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
Aug 11 15:35:06.711: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 15:35:06.711: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 15:35:06.711: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
Aug 11 15:35:06.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999919s
Aug 11 15:35:07.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996608146s
Aug 11 15:35:08.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99185808s
Aug 11 15:35:09.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986871455s
Aug 11 15:35:10.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982358275s
Aug 11 15:35:11.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977128347s
Aug 11 15:35:12.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973439504s
Aug 11 15:35:13.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969592244s
Aug 11 15:35:14.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964629403s
Aug 11 15:35:15.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.296118ms
STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2271 08/11/23 15:35:16.765
Aug 11 15:35:16.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 15:35:16.911: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 15:35:16.911: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 15:35:16.911: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 15:35:16.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 15:35:17.051: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 15:35:17.051: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 15:35:17.051: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 15:35:17.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
Aug 11 15:35:17.185: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
Aug 11 15:35:17.185: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
Aug 11 15:35:17.185: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

Aug 11 15:35:17.185: INFO: Scaling statefulset ss to 0
STEP: Verifying that stateful set ss was scaled down in reverse order 08/11/23 15:35:27.201
[AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:122
Aug 11 15:35:27.201: INFO: Deleting all statefulset in ns statefulset-2271
Aug 11 15:35:27.205: INFO: Scaling statefulset ss to 0
Aug 11 15:35:27.224: INFO: Waiting for statefulset status.replicas updated to 0
Aug 11 15:35:27.227: INFO: Deleting statefulset ss
[AfterEach] [sig-apps] StatefulSet
  test/e2e/framework/framework.go:187
Aug 11 15:35:27.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "statefulset-2271" for this suite. 08/11/23 15:35:27.244
{"msg":"PASSED [sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]","completed":359,"skipped":6655,"failed":0}
------------------------------
â€¢ [SLOW TEST] [71.391 seconds]
[sig-apps] StatefulSet
test/e2e/apps/framework.go:23
  Basic StatefulSet functionality [StatefulSetBasic]
  test/e2e/apps/statefulset.go:101
    Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
    test/e2e/apps/statefulset.go:585

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:34:15.862
    Aug 11 15:34:15.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename statefulset 08/11/23 15:34:15.863
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:34:15.875
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:34:15.878
    [BeforeEach] [sig-apps] StatefulSet
      test/e2e/apps/statefulset.go:96
    [BeforeEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:111
    STEP: Creating service test in namespace statefulset-2271 08/11/23 15:34:15.881
    [It] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
      test/e2e/apps/statefulset.go:585
    STEP: Initializing watcher for selector baz=blah,foo=bar 08/11/23 15:34:15.89
    STEP: Creating stateful set ss in namespace statefulset-2271 08/11/23 15:34:15.893
    STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2271 08/11/23 15:34:15.898
    Aug 11 15:34:15.900: INFO: Found 0 stateful pods, waiting for 1
    Aug 11 15:34:25.906: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod 08/11/23 15:34:25.906
    Aug 11 15:34:25.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:34:26.053: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:34:26.053: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:34:26.053: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 15:34:26.057: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
    Aug 11 15:34:36.066: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 15:34:36.066: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:34:36.081: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999976s
    Aug 11 15:34:37.086: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996583405s
    Aug 11 15:34:38.090: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99250275s
    Aug 11 15:34:39.095: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.987730552s
    Aug 11 15:34:40.100: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.983030549s
    Aug 11 15:34:41.104: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.978233169s
    Aug 11 15:34:42.109: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.97399795s
    Aug 11 15:34:43.113: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.969309746s
    Aug 11 15:34:44.118: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.9648963s
    Aug 11 15:34:45.122: INFO: Verifying statefulset ss doesn't scale past 1 for another 960.557187ms
    STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2271 08/11/23 15:34:46.122
    Aug 11 15:34:46.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 15:34:46.259: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 15:34:46.259: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 15:34:46.259: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 15:34:46.263: INFO: Found 1 stateful pods, waiting for 3
    Aug 11 15:34:56.271: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:34:56.271: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
    Aug 11 15:34:56.271: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
    STEP: Verifying that stateful set ss was scaled up in order 08/11/23 15:34:56.271
    STEP: Scale down will halt with unhealthy stateful pod 08/11/23 15:34:56.271
    Aug 11 15:34:56.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:34:56.414: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:34:56.414: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:34:56.414: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 15:34:56.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:34:56.562: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:34:56.563: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:34:56.563: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 15:34:56.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
    Aug 11 15:34:56.698: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
    Aug 11 15:34:56.698: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
    Aug 11 15:34:56.698: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

    Aug 11 15:34:56.698: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:34:56.702: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
    Aug 11 15:35:06.711: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 15:35:06.711: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 15:35:06.711: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
    Aug 11 15:35:06.724: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999919s
    Aug 11 15:35:07.729: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996608146s
    Aug 11 15:35:08.734: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99185808s
    Aug 11 15:35:09.738: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.986871455s
    Aug 11 15:35:10.743: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.982358275s
    Aug 11 15:35:11.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.977128347s
    Aug 11 15:35:12.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973439504s
    Aug 11 15:35:13.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969592244s
    Aug 11 15:35:14.760: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964629403s
    Aug 11 15:35:15.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 960.296118ms
    STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2271 08/11/23 15:35:16.765
    Aug 11 15:35:16.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 15:35:16.911: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 15:35:16.911: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 15:35:16.911: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 15:35:16.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 15:35:17.051: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 15:35:17.051: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 15:35:17.051: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 15:35:17.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2921299098 --namespace=statefulset-2271 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
    Aug 11 15:35:17.185: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
    Aug 11 15:35:17.185: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
    Aug 11 15:35:17.185: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

    Aug 11 15:35:17.185: INFO: Scaling statefulset ss to 0
    STEP: Verifying that stateful set ss was scaled down in reverse order 08/11/23 15:35:27.201
    [AfterEach] Basic StatefulSet functionality [StatefulSetBasic]
      test/e2e/apps/statefulset.go:122
    Aug 11 15:35:27.201: INFO: Deleting all statefulset in ns statefulset-2271
    Aug 11 15:35:27.205: INFO: Scaling statefulset ss to 0
    Aug 11 15:35:27.224: INFO: Waiting for statefulset status.replicas updated to 0
    Aug 11 15:35:27.227: INFO: Deleting statefulset ss
    [AfterEach] [sig-apps] StatefulSet
      test/e2e/framework/framework.go:187
    Aug 11 15:35:27.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "statefulset-2271" for this suite. 08/11/23 15:35:27.244
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
[BeforeEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:186
STEP: Creating a kubernetes client 08/11/23 15:35:27.255
Aug 11 15:35:27.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
STEP: Building a namespace api object, basename replicaset 08/11/23 15:35:27.256
STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:27.271
STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:27.274
[It] Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143
STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/11/23 15:35:27.277
Aug 11 15:35:27.287: INFO: Pod name sample-pod: Found 0 pods out of 1
Aug 11 15:35:32.292: INFO: Pod name sample-pod: Found 1 pods out of 1
STEP: ensuring each pod is running 08/11/23 15:35:32.292
STEP: getting scale subresource 08/11/23 15:35:32.292
STEP: updating a scale subresource 08/11/23 15:35:32.296
STEP: verifying the replicaset Spec.Replicas was modified 08/11/23 15:35:32.303
STEP: Patch a scale subresource 08/11/23 15:35:32.308
[AfterEach] [sig-apps] ReplicaSet
  test/e2e/framework/framework.go:187
Aug 11 15:35:32.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
STEP: Destroying namespace "replicaset-625" for this suite. 08/11/23 15:35:32.341
{"msg":"PASSED [sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]","completed":360,"skipped":6693,"failed":0}
------------------------------
â€¢ [SLOW TEST] [5.098 seconds]
[sig-apps] ReplicaSet
test/e2e/apps/framework.go:23
  Replicaset should have a working scale subresource [Conformance]
  test/e2e/apps/replica_set.go:143

  Begin Captured GinkgoWriter Output >>
    [BeforeEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:186
    STEP: Creating a kubernetes client 08/11/23 15:35:27.255
    Aug 11 15:35:27.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2921299098
    STEP: Building a namespace api object, basename replicaset 08/11/23 15:35:27.256
    STEP: Waiting for a default service account to be provisioned in namespace 08/11/23 15:35:27.271
    STEP: Waiting for kube-root-ca.crt to be provisioned in namespace 08/11/23 15:35:27.274
    [It] Replicaset should have a working scale subresource [Conformance]
      test/e2e/apps/replica_set.go:143
    STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota 08/11/23 15:35:27.277
    Aug 11 15:35:27.287: INFO: Pod name sample-pod: Found 0 pods out of 1
    Aug 11 15:35:32.292: INFO: Pod name sample-pod: Found 1 pods out of 1
    STEP: ensuring each pod is running 08/11/23 15:35:32.292
    STEP: getting scale subresource 08/11/23 15:35:32.292
    STEP: updating a scale subresource 08/11/23 15:35:32.296
    STEP: verifying the replicaset Spec.Replicas was modified 08/11/23 15:35:32.303
    STEP: Patch a scale subresource 08/11/23 15:35:32.308
    [AfterEach] [sig-apps] ReplicaSet
      test/e2e/framework/framework.go:187
    Aug 11 15:35:32.336: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
    STEP: Destroying namespace "replicaset-625" for this suite. 08/11/23 15:35:32.341
  << End Captured GinkgoWriter Output
------------------------------
SSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
{"msg":"Test Suite completed","completed":360,"skipped":6706,"failed":0}
Aug 11 15:35:32.355: INFO: Running AfterSuite actions on all nodes
Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
[SynchronizedAfterSuite] TOP-LEVEL
  test/e2e/e2e.go:87
Aug 11 15:35:32.355: INFO: Running AfterSuite actions on node 1
Aug 11 15:35:32.355: INFO: Skipping dumping logs from cluster
------------------------------
[SynchronizedAfterSuite] PASSED [0.000 seconds]
[SynchronizedAfterSuite] 
test/e2e/e2e.go:87

  Begin Captured GinkgoWriter Output >>
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Aug 11 15:35:32.355: INFO: Running AfterSuite actions on all nodes
    Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func20.2
    Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func10.2
    Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage.glob..func9.2
    Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func17.3
    Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func9.2
    Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func4.2
    Aug 11 15:35:32.355: INFO: Running Cleanup Action: k8s.io/kubernetes/test/e2e/storage/vsphere.glob..func1.3
    [SynchronizedAfterSuite] TOP-LEVEL
      test/e2e/e2e.go:87
    Aug 11 15:35:32.355: INFO: Running AfterSuite actions on node 1
    Aug 11 15:35:32.355: INFO: Skipping dumping logs from cluster
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146
[ReportAfterSuite] TOP-LEVEL
  test/e2e/e2e_test.go:146
------------------------------
[ReportAfterSuite] PASSED [0.000 seconds]
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:146

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/e2e_test.go:146
  << End Captured GinkgoWriter Output
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559
[ReportAfterSuite] TOP-LEVEL
  test/e2e/framework/test_context.go:559
------------------------------
[ReportAfterSuite] PASSED [0.046 seconds]
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:559

  Begin Captured GinkgoWriter Output >>
    [ReportAfterSuite] TOP-LEVEL
      test/e2e/framework/test_context.go:559
  << End Captured GinkgoWriter Output
------------------------------

Ran 360 of 7066 Specs in 5595.866 seconds
SUCCESS! -- 360 Passed | 0 Failed | 0 Pending | 6706 Skipped
PASS

Ginkgo ran 1 suite in 1h33m16.113907276s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.1.6[0m


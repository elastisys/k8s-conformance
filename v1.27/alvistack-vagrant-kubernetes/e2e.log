  I0824 11:39:13.337969      15 e2e.go:117] Starting e2e run "388bc1eb-53bb-4308-9fd0-f4aa3ff2edb9" on Ginkgo node 1
  Aug 24 11:39:13.408: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1692877152 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Aug 24 11:39:13.749: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:39:13.757: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Aug 24 11:39:13.843: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Aug 24 11:39:13.856: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
  Aug 24 11:39:13.856: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
  Aug 24 11:39:13.856: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Aug 24 11:39:13.856: INFO: e2e test version: v1.27.5
  Aug 24 11:39:13.861: INFO: kube-apiserver version: v1.27.5
  Aug 24 11:39:13.862: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:39:13.872: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.124 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 08/24/23 11:39:14.364
  Aug 24 11:39:14.364: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 11:39:14.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:39:14.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:39:14.413
  STEP: Creating the pod @ 08/24/23 11:39:14.42
  Aug 24 11:39:31.108: INFO: Successfully updated pod "labelsupdate400b93d0-e9b1-4b41-a944-5e0aa8912aa5"
  Aug 24 11:39:33.146: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1233" for this suite. @ 08/24/23 11:39:33.156
• [18.806 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 08/24/23 11:39:33.174
  Aug 24 11:39:33.174: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sysctl @ 08/24/23 11:39:33.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:39:33.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:39:33.239
  STEP: Creating a pod with one valid and two invalid sysctls @ 08/24/23 11:39:33.245
  Aug 24 11:39:33.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-2179" for this suite. @ 08/24/23 11:39:33.264
• [0.110 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 08/24/23 11:39:33.287
  Aug 24 11:39:33.287: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename init-container @ 08/24/23 11:39:33.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:39:33.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:39:33.321
  STEP: creating the pod @ 08/24/23 11:39:33.326
  Aug 24 11:39:33.326: INFO: PodSpec: initContainers in spec.initContainers
  Aug 24 11:40:13.278: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-77793094-464a-4151-a1f6-77a1843d041a", GenerateName:"", Namespace:"init-container-4665", SelfLink:"", UID:"94fac296-aab3-4837-84e1-5745b553ec9a", ResourceVersion:"3816", Generation:0, CreationTimestamp:time.Date(2023, time.August, 24, 11, 39, 33, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"326295412"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 39, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001688870), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 24, 11, 40, 13, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0016888a0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-59lsk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc000e1a660), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-59lsk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-59lsk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-59lsk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0019cd8b0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"quohp9aeph3i-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0001d6700), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0019cd940)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0019cd960)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0019cd968), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0019cd96c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc000dc58b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 39, 33, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 39, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 39, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 24, 11, 39, 33, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.15", PodIP:"10.233.66.199", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.199"}}, StartTime:time.Date(2023, time.August, 24, 11, 39, 33, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0001d6850)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0001d68c0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://4d91633836cc6150809851d888fe9a44bdc21a6829d595f922bd4f678825b247", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e1a6e0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc000e1a6c0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0019cd9e4), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Aug 24 11:40:13.281: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4665" for this suite. @ 08/24/23 11:40:13.296
• [40.028 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 08/24/23 11:40:13.319
  Aug 24 11:40:13.319: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 11:40:13.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:13.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:13.377
  STEP: Creating configMap configmap-8617/configmap-test-1f19415c-398d-487d-abe3-68e617ad4ba5 @ 08/24/23 11:40:13.384
  STEP: Creating a pod to test consume configMaps @ 08/24/23 11:40:13.397
  STEP: Saw pod success @ 08/24/23 11:40:17.455
  Aug 24 11:40:17.464: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-45b911e8-1ca3-4d9d-b1f3-d1464a5cd3da container env-test: <nil>
  STEP: delete the pod @ 08/24/23 11:40:17.486
  Aug 24 11:40:17.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8617" for this suite. @ 08/24/23 11:40:17.534
• [4.235 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 08/24/23 11:40:17.558
  Aug 24 11:40:17.558: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 11:40:17.562
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:17.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:17.612
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 08/24/23 11:40:17.618
  Aug 24 11:40:17.620: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:40:20.505: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:40:28.437: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9503" for this suite. @ 08/24/23 11:40:28.456
• [10.909 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 08/24/23 11:40:28.472
  Aug 24 11:40:28.472: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename server-version @ 08/24/23 11:40:28.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:28.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:28.505
  STEP: Request ServerVersion @ 08/24/23 11:40:28.51
  STEP: Confirm major version @ 08/24/23 11:40:28.512
  Aug 24 11:40:28.512: INFO: Major version: 1
  STEP: Confirm minor version @ 08/24/23 11:40:28.512
  Aug 24 11:40:28.512: INFO: cleanMinorVersion: 27
  Aug 24 11:40:28.512: INFO: Minor version: 27
  Aug 24 11:40:28.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-5210" for this suite. @ 08/24/23 11:40:28.524
• [0.075 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 08/24/23 11:40:28.548
  Aug 24 11:40:28.548: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 11:40:28.55
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:28.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:28.592
  STEP: creating a Namespace @ 08/24/23 11:40:28.596
  STEP: patching the Namespace @ 08/24/23 11:40:28.626
  STEP: get the Namespace and ensuring it has the label @ 08/24/23 11:40:28.641
  Aug 24 11:40:28.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3583" for this suite. @ 08/24/23 11:40:28.66
  STEP: Destroying namespace "nspatchtest-b92d346e-9974-4b74-9d43-3a269f144881-9429" for this suite. @ 08/24/23 11:40:28.673
• [0.138 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 08/24/23 11:40:28.689
  Aug 24 11:40:28.689: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 11:40:28.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:28.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:28.73
  Aug 24 11:40:28.737: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:40:35.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9766" for this suite. @ 08/24/23 11:40:35.381
• [6.709 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 08/24/23 11:40:35.419
  Aug 24 11:40:35.419: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 11:40:35.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:35.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:35.507
  STEP: Creating secret with name secret-test-09336312-daba-44e9-8200-7f301bb2f2de @ 08/24/23 11:40:35.568
  STEP: Creating a pod to test consume secrets @ 08/24/23 11:40:35.579
  STEP: Saw pod success @ 08/24/23 11:40:39.616
  Aug 24 11:40:39.621: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-secrets-dc47ae13-279c-4307-a287-b6ee71a5e286 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 11:40:39.646
  Aug 24 11:40:39.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2193" for this suite. @ 08/24/23 11:40:39.684
  STEP: Destroying namespace "secret-namespace-6364" for this suite. @ 08/24/23 11:40:39.695
• [4.291 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 08/24/23 11:40:39.711
  Aug 24 11:40:39.711: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 11:40:39.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:40:39.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:40:39.756
  STEP: Creating a suspended cronjob @ 08/24/23 11:40:39.763
  STEP: Ensuring no jobs are scheduled @ 08/24/23 11:40:39.779
  STEP: Ensuring no job exists by listing jobs explicitly @ 08/24/23 11:45:39.8
  STEP: Removing cronjob @ 08/24/23 11:45:39.809
  Aug 24 11:45:39.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7050" for this suite. @ 08/24/23 11:45:39.845
• [300.151 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 08/24/23 11:45:39.867
  Aug 24 11:45:39.867: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 11:45:39.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:45:39.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:45:39.912
  STEP: Setting up server cert @ 08/24/23 11:45:39.965
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 11:45:40.572
  STEP: Deploying the webhook pod @ 08/24/23 11:45:40.589
  STEP: Wait for the deployment to be ready @ 08/24/23 11:45:40.62
  Aug 24 11:45:40.639: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/24/23 11:45:42.662
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 11:45:42.682
  Aug 24 11:45:43.683: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/24/23 11:45:43.692
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/24/23 11:45:43.73
  STEP: Creating a dummy validating-webhook-configuration object @ 08/24/23 11:45:43.756
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 08/24/23 11:45:43.776
  STEP: Creating a dummy mutating-webhook-configuration object @ 08/24/23 11:45:43.793
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 08/24/23 11:45:43.81
  Aug 24 11:45:43.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4345" for this suite. @ 08/24/23 11:45:44.028
  STEP: Destroying namespace "webhook-markers-4154" for this suite. @ 08/24/23 11:45:44.048
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 08/24/23 11:45:44.065
  Aug 24 11:45:44.065: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename dns @ 08/24/23 11:45:44.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:45:44.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:45:44.123
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4172.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-4172.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 08/24/23 11:45:44.13
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-4172.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-4172.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 08/24/23 11:45:44.13
  STEP: creating a pod to probe /etc/hosts @ 08/24/23 11:45:44.13
  STEP: submitting the pod to kubernetes @ 08/24/23 11:45:44.13
  STEP: retrieving the pod @ 08/24/23 11:46:08.303
  STEP: looking for the results for each expected name from probers @ 08/24/23 11:46:08.31
  Aug 24 11:46:08.341: INFO: DNS probes using dns-4172/dns-test-381424c4-f266-456c-bdb5-398aa7a04bab succeeded

  Aug 24 11:46:08.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 11:46:08.351
  STEP: Destroying namespace "dns-4172" for this suite. @ 08/24/23 11:46:08.378
• [24.328 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 08/24/23 11:46:08.401
  Aug 24 11:46:08.401: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-preemption @ 08/24/23 11:46:08.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:46:08.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:46:08.441
  Aug 24 11:46:08.478: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug 24 11:47:08.529: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/24/23 11:47:08.535
  Aug 24 11:47:08.535: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/24/23 11:47:08.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:08.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:08.574
  STEP: Finding an available node @ 08/24/23 11:47:08.578
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/24/23 11:47:08.578
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/24/23 11:47:10.619
  Aug 24 11:47:10.652: INFO: found a healthy node: quohp9aeph3i-3
  Aug 24 11:47:16.879: INFO: pods created so far: [1 1 1]
  Aug 24 11:47:16.879: INFO: length of pods created so far: 3
  Aug 24 11:47:18.898: INFO: pods created so far: [2 2 1]
  Aug 24 11:47:25.901: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 11:47:25.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-4899" for this suite. @ 08/24/23 11:47:26.074
  STEP: Destroying namespace "sched-preemption-1175" for this suite. @ 08/24/23 11:47:26.085
• [77.696 seconds]
------------------------------
SS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 08/24/23 11:47:26.098
  Aug 24 11:47:26.098: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 11:47:26.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:26.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:26.138
  STEP: Creating a pod to test downward api env vars @ 08/24/23 11:47:26.143
  STEP: Saw pod success @ 08/24/23 11:47:30.192
  Aug 24 11:47:30.199: INFO: Trying to get logs from node quohp9aeph3i-3 pod downward-api-a4b37bb9-bce7-4b1d-b4fe-503c659523b9 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 11:47:30.232
  Aug 24 11:47:30.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9771" for this suite. @ 08/24/23 11:47:30.268
• [4.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 08/24/23 11:47:30.284
  Aug 24 11:47:30.284: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 11:47:30.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:30.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:30.32
  STEP: create the container @ 08/24/23 11:47:30.324
  W0824 11:47:30.342615      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/24/23 11:47:30.342
  STEP: get the container status @ 08/24/23 11:47:33.383
  STEP: the container should be terminated @ 08/24/23 11:47:33.391
  STEP: the termination message should be set @ 08/24/23 11:47:33.392
  Aug 24 11:47:33.392: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 08/24/23 11:47:33.392
  Aug 24 11:47:33.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9035" for this suite. @ 08/24/23 11:47:33.434
• [3.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 08/24/23 11:47:33.466
  Aug 24 11:47:33.466: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 11:47:33.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:33.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:33.508
  STEP: Creating configMap with name projected-configmap-test-volume-f1578829-9356-41b6-9d95-06a1d9317b8f @ 08/24/23 11:47:33.513
  STEP: Creating a pod to test consume configMaps @ 08/24/23 11:47:33.52
  STEP: Saw pod success @ 08/24/23 11:47:37.562
  Aug 24 11:47:37.570: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-configmaps-e79eb812-790a-4817-941c-91cadad9d425 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 11:47:37.583
  Aug 24 11:47:37.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8475" for this suite. @ 08/24/23 11:47:37.618
• [4.163 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 08/24/23 11:47:37.631
  Aug 24 11:47:37.631: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 11:47:37.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:37.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:37.673
  STEP: Creating replication controller my-hostname-basic-cedb7a98-6405-459f-9dd2-765b4300e841 @ 08/24/23 11:47:37.678
  Aug 24 11:47:37.703: INFO: Pod name my-hostname-basic-cedb7a98-6405-459f-9dd2-765b4300e841: Found 0 pods out of 1
  Aug 24 11:47:42.719: INFO: Pod name my-hostname-basic-cedb7a98-6405-459f-9dd2-765b4300e841: Found 1 pods out of 1
  Aug 24 11:47:42.719: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cedb7a98-6405-459f-9dd2-765b4300e841" are running
  Aug 24 11:47:42.728: INFO: Pod "my-hostname-basic-cedb7a98-6405-459f-9dd2-765b4300e841-wpf8c" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:47:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:47:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:47:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 11:47:37 +0000 UTC Reason: Message:}])
  Aug 24 11:47:42.729: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/24/23 11:47:42.729
  Aug 24 11:47:42.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1068" for this suite. @ 08/24/23 11:47:42.775
• [5.159 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 08/24/23 11:47:42.793
  Aug 24 11:47:42.793: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 11:47:42.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:42.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:42.84
  STEP: Creating projection with secret that has name projected-secret-test-38776c3c-8762-489e-b390-55954e7c817a @ 08/24/23 11:47:42.847
  STEP: Creating a pod to test consume secrets @ 08/24/23 11:47:42.858
  STEP: Saw pod success @ 08/24/23 11:47:46.91
  Aug 24 11:47:46.915: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-secrets-892121ec-4310-4a98-a0c2-487e8683d4a8 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 11:47:46.925
  Aug 24 11:47:46.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3630" for this suite. @ 08/24/23 11:47:46.953
• [4.174 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 08/24/23 11:47:46.968
  Aug 24 11:47:46.969: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 11:47:46.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:47.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:47.026
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 08/24/23 11:47:47.031
  STEP: When a replicaset with a matching selector is created @ 08/24/23 11:47:57.131
  STEP: Then the orphan pod is adopted @ 08/24/23 11:47:57.15
  STEP: When the matched label of one of its pods change @ 08/24/23 11:47:58.174
  Aug 24 11:47:58.186: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/24/23 11:47:58.214
  Aug 24 11:47:59.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3704" for this suite. @ 08/24/23 11:47:59.249
• [12.302 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 08/24/23 11:47:59.271
  Aug 24 11:47:59.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 11:47:59.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:59.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:59.322
  Aug 24 11:47:59.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9821" for this suite. @ 08/24/23 11:47:59.467
• [0.217 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 08/24/23 11:47:59.495
  Aug 24 11:47:59.495: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 11:47:59.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:47:59.54
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:47:59.546
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/24/23 11:47:59.551
  STEP: Saw pod success @ 08/24/23 11:48:03.617
  Aug 24 11:48:03.625: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-4657d95f-27b5-4ec7-b2be-096f7322306c container test-container: <nil>
  STEP: delete the pod @ 08/24/23 11:48:03.64
  Aug 24 11:48:03.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4377" for this suite. @ 08/24/23 11:48:03.683
• [4.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 08/24/23 11:48:03.702
  Aug 24 11:48:03.702: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 11:48:03.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:48:03.733
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:48:03.739
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-388 @ 08/24/23 11:48:03.746
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/24/23 11:48:03.767
  STEP: creating service externalsvc in namespace services-388 @ 08/24/23 11:48:03.768
  STEP: creating replication controller externalsvc in namespace services-388 @ 08/24/23 11:48:03.82
  I0824 11:48:03.841561      15 runners.go:194] Created replication controller with name: externalsvc, namespace: services-388, replica count: 2
  I0824 11:48:06.893778      15 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0824 11:48:09.894925      15 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0824 11:48:12.895776      15 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0824 11:48:15.896826      15 runners.go:194] externalsvc Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0824 11:48:18.897920      15 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 08/24/23 11:48:18.908
  Aug 24 11:48:18.951: INFO: Creating new exec pod
  Aug 24 11:48:21.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-388 exec execpods2wqr -- /bin/sh -x -c nslookup clusterip-service.services-388.svc.cluster.local'
  Aug 24 11:48:21.680: INFO: stderr: "+ nslookup clusterip-service.services-388.svc.cluster.local\n"
  Aug 24 11:48:21.680: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-388.svc.cluster.local\tcanonical name = externalsvc.services-388.svc.cluster.local.\nName:\texternalsvc.services-388.svc.cluster.local\nAddress: 10.233.44.109\n\n"
  Aug 24 11:48:21.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-388, will wait for the garbage collector to delete the pods @ 08/24/23 11:48:21.693
  Aug 24 11:48:21.764: INFO: Deleting ReplicationController externalsvc took: 14.878462ms
  Aug 24 11:48:21.880: INFO: Terminating ReplicationController externalsvc pods took: 115.934987ms
  Aug 24 11:48:23.624: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-388" for this suite. @ 08/24/23 11:48:23.649
• [19.972 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 08/24/23 11:48:23.675
  Aug 24 11:48:23.675: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 11:48:23.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:48:23.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:48:23.709
  Aug 24 11:48:23.715: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:48:24.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6242" for this suite. @ 08/24/23 11:48:24.801
• [1.140 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 08/24/23 11:48:24.821
  Aug 24 11:48:24.822: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 11:48:24.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:48:24.868
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:48:24.878
  STEP: Create a ReplicaSet @ 08/24/23 11:48:24.884
  STEP: Verify that the required pods have come up @ 08/24/23 11:48:24.898
  Aug 24 11:48:24.915: INFO: Pod name sample-pod: Found 0 pods out of 3
  Aug 24 11:48:29.927: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 08/24/23 11:48:29.927
  Aug 24 11:48:42.011: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 08/24/23 11:48:42.011
  STEP: DeleteCollection of the ReplicaSets @ 08/24/23 11:48:42.024
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 08/24/23 11:48:42.042
  Aug 24 11:48:42.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4900" for this suite. @ 08/24/23 11:48:42.084
• [17.310 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 08/24/23 11:48:42.133
  Aug 24 11:48:42.134: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 11:48:42.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:48:42.186
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:48:42.19
  Aug 24 11:48:42.232: INFO: created pod
  STEP: Saw pod success @ 08/24/23 11:48:46.258
  Aug 24 11:49:16.260: INFO: polling logs
  Aug 24 11:49:16.282: INFO: Pod logs: 
  I0824 11:48:43.364853       1 log.go:198] OK: Got token
  I0824 11:48:43.365376       1 log.go:198] validating with in-cluster discovery
  I0824 11:48:43.367218       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0824 11:48:43.367477       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8768:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692878322, NotBefore:1692877722, IssuedAt:1692877722, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8768", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"87250097-12dd-422e-97d7-fa02f98932ea"}}}
  I0824 11:48:43.394148       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0824 11:48:43.406954       1 log.go:198] OK: Validated signature on JWT
  I0824 11:48:43.407156       1 log.go:198] OK: Got valid claims from token!
  I0824 11:48:43.407206       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-8768:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1692878322, NotBefore:1692877722, IssuedAt:1692877722, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-8768", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"87250097-12dd-422e-97d7-fa02f98932ea"}}}

  Aug 24 11:49:16.289: INFO: completed pod
  Aug 24 11:49:16.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8768" for this suite. @ 08/24/23 11:49:16.321
• [34.203 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 08/24/23 11:49:16.337
  Aug 24 11:49:16.337: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 11:49:16.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:49:16.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:49:16.381
  STEP: Setting up server cert @ 08/24/23 11:49:16.425
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 11:49:17.206
  STEP: Deploying the webhook pod @ 08/24/23 11:49:17.228
  STEP: Wait for the deployment to be ready @ 08/24/23 11:49:17.258
  Aug 24 11:49:17.278: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/24/23 11:49:19.299
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 11:49:19.316
  Aug 24 11:49:20.316: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 08/24/23 11:49:20.328
  STEP: create a configmap that should be updated by the webhook @ 08/24/23 11:49:20.371
  Aug 24 11:49:20.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7261" for this suite. @ 08/24/23 11:49:20.53
  STEP: Destroying namespace "webhook-markers-8008" for this suite. @ 08/24/23 11:49:20.556
• [4.249 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 08/24/23 11:49:20.591
  Aug 24 11:49:20.591: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 11:49:20.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:49:20.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:49:20.628
  STEP: apply creating a deployment @ 08/24/23 11:49:20.635
  Aug 24 11:49:20.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2432" for this suite. @ 08/24/23 11:49:20.677
• [0.100 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 08/24/23 11:49:20.693
  Aug 24 11:49:20.693: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/24/23 11:49:20.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:49:20.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:49:20.732
  STEP: creating @ 08/24/23 11:49:20.738
  STEP: getting @ 08/24/23 11:49:20.795
  STEP: listing in namespace @ 08/24/23 11:49:20.806
  STEP: patching @ 08/24/23 11:49:20.812
  STEP: deleting @ 08/24/23 11:49:20.859
  Aug 24 11:49:20.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-4558" for this suite. @ 08/24/23 11:49:20.926
• [0.257 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 08/24/23 11:49:20.951
  Aug 24 11:49:20.951: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-watch @ 08/24/23 11:49:20.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:49:20.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:49:20.998
  Aug 24 11:49:21.005: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Creating first CR  @ 08/24/23 11:49:23.735
  Aug 24 11:49:23.747: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:49:23Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:49:23Z]] name:name1 resourceVersion:6084 uid:15d2c72c-fda7-4cb4-8e83-813399a45948] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 08/24/23 11:49:33.748
  Aug 24 11:49:33.759: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:49:33Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:49:33Z]] name:name2 resourceVersion:6130 uid:59d81ec6-2d17-44ae-a082-ba84c708c645] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 08/24/23 11:49:43.76
  Aug 24 11:49:43.780: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:49:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:49:43Z]] name:name1 resourceVersion:6153 uid:15d2c72c-fda7-4cb4-8e83-813399a45948] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 08/24/23 11:49:53.781
  Aug 24 11:49:53.796: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:49:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:49:53Z]] name:name2 resourceVersion:6175 uid:59d81ec6-2d17-44ae-a082-ba84c708c645] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 08/24/23 11:50:03.797
  Aug 24 11:50:03.813: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:49:23Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:49:43Z]] name:name1 resourceVersion:6197 uid:15d2c72c-fda7-4cb4-8e83-813399a45948] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 08/24/23 11:50:13.814
  Aug 24 11:50:13.829: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-24T11:49:33Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-24T11:49:53Z]] name:name2 resourceVersion:6220 uid:59d81ec6-2d17-44ae-a082-ba84c708c645] num:map[num1:9223372036854775807 num2:1000000]]}
  Aug 24 11:50:24.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-3876" for this suite. @ 08/24/23 11:50:24.373
• [63.434 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 08/24/23 11:50:24.387
  Aug 24 11:50:24.387: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pod-network-test @ 08/24/23 11:50:24.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:50:24.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:50:24.431
  STEP: Performing setup for networking test in namespace pod-network-test-8351 @ 08/24/23 11:50:24.437
  STEP: creating a selector @ 08/24/23 11:50:24.437
  STEP: Creating the service pods in kubernetes @ 08/24/23 11:50:24.438
  Aug 24 11:50:24.438: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 08/24/23 11:50:56.741
  Aug 24 11:50:58.793: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 24 11:50:58.793: INFO: Breadth first check of 10.233.65.71 on host 192.168.121.64...
  Aug 24 11:50:58.803: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.202:9080/dial?request=hostname&protocol=http&host=10.233.65.71&port=8083&tries=1'] Namespace:pod-network-test-8351 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:50:58.803: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:50:58.806: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:50:58.806: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8351/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.202%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.71%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 11:50:59.012: INFO: Waiting for responses: map[]
  Aug 24 11:50:59.013: INFO: reached 10.233.65.71 after 0/1 tries
  Aug 24 11:50:59.013: INFO: Breadth first check of 10.233.64.88 on host 192.168.121.19...
  Aug 24 11:50:59.023: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.202:9080/dial?request=hostname&protocol=http&host=10.233.64.88&port=8083&tries=1'] Namespace:pod-network-test-8351 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:50:59.024: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:50:59.026: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:50:59.026: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8351/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.202%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.88%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 11:50:59.189: INFO: Waiting for responses: map[]
  Aug 24 11:50:59.189: INFO: reached 10.233.64.88 after 0/1 tries
  Aug 24 11:50:59.190: INFO: Breadth first check of 10.233.66.201 on host 192.168.121.15...
  Aug 24 11:50:59.198: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.202:9080/dial?request=hostname&protocol=http&host=10.233.66.201&port=8083&tries=1'] Namespace:pod-network-test-8351 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:50:59.198: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:50:59.200: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:50:59.201: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-8351/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.202%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.201%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 11:50:59.327: INFO: Waiting for responses: map[]
  Aug 24 11:50:59.327: INFO: reached 10.233.66.201 after 0/1 tries
  Aug 24 11:50:59.327: INFO: Going to retry 0 out of 3 pods....
  Aug 24 11:50:59.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8351" for this suite. @ 08/24/23 11:50:59.338
• [34.967 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 08/24/23 11:50:59.363
  Aug 24 11:50:59.363: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 11:50:59.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:50:59.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:50:59.399
  STEP: set up a multi version CRD @ 08/24/23 11:50:59.404
  Aug 24 11:50:59.405: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: rename a version @ 08/24/23 11:51:04.241
  STEP: check the new version name is served @ 08/24/23 11:51:04.286
  STEP: check the old version name is removed @ 08/24/23 11:51:06.559
  STEP: check the other version is not changed @ 08/24/23 11:51:07.487
  Aug 24 11:51:11.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-225" for this suite. @ 08/24/23 11:51:11.134
• [11.787 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 08/24/23 11:51:11.158
  Aug 24 11:51:11.159: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 11:51:11.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:11.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:11.21
  Aug 24 11:51:13.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2004" for this suite. @ 08/24/23 11:51:13.294
• [2.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 08/24/23 11:51:13.307
  Aug 24 11:51:13.307: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 11:51:13.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:13.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:13.354
  STEP: creating secret secrets-9252/secret-test-31eaaac8-c482-466b-80f5-c8439e34a756 @ 08/24/23 11:51:13.36
  STEP: Creating a pod to test consume secrets @ 08/24/23 11:51:13.373
  STEP: Saw pod success @ 08/24/23 11:51:17.451
  Aug 24 11:51:17.457: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-d094fa91-9a2b-4b98-b6d3-662b204addde container env-test: <nil>
  STEP: delete the pod @ 08/24/23 11:51:17.477
  Aug 24 11:51:17.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9252" for this suite. @ 08/24/23 11:51:17.521
• [4.232 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 08/24/23 11:51:17.55
  Aug 24 11:51:17.550: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 11:51:17.552
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:17.585
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:17.59
  STEP: Setting up server cert @ 08/24/23 11:51:17.665
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 11:51:18.333
  STEP: Deploying the webhook pod @ 08/24/23 11:51:18.349
  STEP: Wait for the deployment to be ready @ 08/24/23 11:51:18.369
  Aug 24 11:51:18.380: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 08/24/23 11:51:20.411
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 11:51:20.429
  Aug 24 11:51:21.429: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 08/24/23 11:51:21.437
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 08/24/23 11:51:21.44
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 08/24/23 11:51:21.44
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 08/24/23 11:51:21.44
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 08/24/23 11:51:21.442
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/24/23 11:51:21.442
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/24/23 11:51:21.444
  Aug 24 11:51:21.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4106" for this suite. @ 08/24/23 11:51:21.576
  STEP: Destroying namespace "webhook-markers-8498" for this suite. @ 08/24/23 11:51:21.591
• [4.055 seconds]
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 08/24/23 11:51:21.605
  Aug 24 11:51:21.606: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 11:51:21.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:21.646
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:21.651
  Aug 24 11:51:21.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9092" for this suite. @ 08/24/23 11:51:21.67
• [0.077 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 08/24/23 11:51:21.682
  Aug 24 11:51:21.682: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-webhook @ 08/24/23 11:51:21.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:21.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:21.74
  STEP: Setting up server cert @ 08/24/23 11:51:21.745
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/24/23 11:51:22.623
  STEP: Deploying the custom resource conversion webhook pod @ 08/24/23 11:51:22.638
  STEP: Wait for the deployment to be ready @ 08/24/23 11:51:22.655
  Aug 24 11:51:22.669: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/24/23 11:51:24.689
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 11:51:24.716
  Aug 24 11:51:25.717: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug 24 11:51:25.728: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Creating a v1 custom resource @ 08/24/23 11:51:28.524
  STEP: v2 custom resource should be converted @ 08/24/23 11:51:28.534
  Aug 24 11:51:28.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6954" for this suite. @ 08/24/23 11:51:29.178
• [7.518 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 08/24/23 11:51:29.206
  Aug 24 11:51:29.206: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename endpointslice @ 08/24/23 11:51:29.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:29.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:29.295
  Aug 24 11:51:31.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5857" for this suite. @ 08/24/23 11:51:31.405
• [2.212 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 08/24/23 11:51:31.419
  Aug 24 11:51:31.419: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 11:51:31.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:31.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:31.456
  STEP: creating Agnhost RC @ 08/24/23 11:51:31.461
  Aug 24 11:51:31.461: INFO: namespace kubectl-4423
  Aug 24 11:51:31.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-4423 create -f -'
  Aug 24 11:51:32.308: INFO: stderr: ""
  Aug 24 11:51:32.308: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/24/23 11:51:32.308
  Aug 24 11:51:33.316: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 11:51:33.316: INFO: Found 0 / 1
  Aug 24 11:51:34.317: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 11:51:34.317: INFO: Found 1 / 1
  Aug 24 11:51:34.317: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug 24 11:51:34.322: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 11:51:34.322: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 24 11:51:34.322: INFO: wait on agnhost-primary startup in kubectl-4423 
  Aug 24 11:51:34.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-4423 logs agnhost-primary-s8xw7 agnhost-primary'
  Aug 24 11:51:34.519: INFO: stderr: ""
  Aug 24 11:51:34.520: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 08/24/23 11:51:34.52
  Aug 24 11:51:34.520: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-4423 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Aug 24 11:51:34.714: INFO: stderr: ""
  Aug 24 11:51:34.714: INFO: stdout: "service/rm2 exposed\n"
  Aug 24 11:51:34.730: INFO: Service rm2 in namespace kubectl-4423 found.
  STEP: exposing service @ 08/24/23 11:51:36.741
  Aug 24 11:51:36.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-4423 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Aug 24 11:51:36.949: INFO: stderr: ""
  Aug 24 11:51:36.949: INFO: stdout: "service/rm3 exposed\n"
  Aug 24 11:51:36.966: INFO: Service rm3 in namespace kubectl-4423 found.
  Aug 24 11:51:38.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4423" for this suite. @ 08/24/23 11:51:38.989
• [7.583 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 08/24/23 11:51:39.01
  Aug 24 11:51:39.010: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 11:51:39.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:39.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:39.05
  STEP: Creating secret with name secret-test-map-d683547d-057f-448a-98c6-0c66a5219789 @ 08/24/23 11:51:39.054
  STEP: Creating a pod to test consume secrets @ 08/24/23 11:51:39.061
  STEP: Saw pod success @ 08/24/23 11:51:43.114
  Aug 24 11:51:43.122: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-secrets-6dc676f1-1ca1-4a28-93f4-de88ebe3428a container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 11:51:43.14
  Aug 24 11:51:43.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3183" for this suite. @ 08/24/23 11:51:43.197
• [4.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 08/24/23 11:51:43.219
  Aug 24 11:51:43.219: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 11:51:43.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:43.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:43.26
  STEP: Creating a pod to test substitution in container's args @ 08/24/23 11:51:43.266
  STEP: Saw pod success @ 08/24/23 11:51:47.3
  Aug 24 11:51:47.308: INFO: Trying to get logs from node quohp9aeph3i-3 pod var-expansion-7386f0af-a5fb-4922-a6c3-b04fb469619f container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 11:51:47.324
  Aug 24 11:51:47.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4390" for this suite. @ 08/24/23 11:51:47.365
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 08/24/23 11:51:47.384
  Aug 24 11:51:47.384: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 11:51:47.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:47.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:47.423
  Aug 24 11:51:47.459: INFO: Pod name sample-pod: Found 0 pods out of 1
  Aug 24 11:51:52.467: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 11:51:52.468
  STEP: Scaling up "test-rs" replicaset  @ 08/24/23 11:51:52.468
  Aug 24 11:51:52.483: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 08/24/23 11:51:52.484
  W0824 11:51:52.501742      15 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 24 11:51:52.504: INFO: observed ReplicaSet test-rs in namespace replicaset-4707 with ReadyReplicas 1, AvailableReplicas 1
  Aug 24 11:51:52.579: INFO: observed ReplicaSet test-rs in namespace replicaset-4707 with ReadyReplicas 1, AvailableReplicas 1
  Aug 24 11:51:52.610: INFO: observed ReplicaSet test-rs in namespace replicaset-4707 with ReadyReplicas 1, AvailableReplicas 1
  Aug 24 11:51:52.636: INFO: observed ReplicaSet test-rs in namespace replicaset-4707 with ReadyReplicas 1, AvailableReplicas 1
  Aug 24 11:51:54.521: INFO: observed ReplicaSet test-rs in namespace replicaset-4707 with ReadyReplicas 2, AvailableReplicas 2
  Aug 24 11:51:55.075: INFO: observed Replicaset test-rs in namespace replicaset-4707 with ReadyReplicas 3 found true
  Aug 24 11:51:55.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4707" for this suite. @ 08/24/23 11:51:55.085
• [7.714 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 08/24/23 11:51:55.099
  Aug 24 11:51:55.099: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 11:51:55.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:51:55.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:51:55.141
  Aug 24 11:52:17.310: INFO: Container started at 2023-08-24 11:51:56 +0000 UTC, pod became ready at 2023-08-24 11:52:15 +0000 UTC
  Aug 24 11:52:17.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-2983" for this suite. @ 08/24/23 11:52:17.331
• [22.245 seconds]
------------------------------
S
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 08/24/23 11:52:17.345
  Aug 24 11:52:17.345: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename limitrange @ 08/24/23 11:52:17.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:52:17.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:52:17.395
  STEP: Creating a LimitRange @ 08/24/23 11:52:17.401
  STEP: Setting up watch @ 08/24/23 11:52:17.402
  STEP: Submitting a LimitRange @ 08/24/23 11:52:17.513
  STEP: Verifying LimitRange creation was observed @ 08/24/23 11:52:17.525
  STEP: Fetching the LimitRange to ensure it has proper values @ 08/24/23 11:52:17.525
  Aug 24 11:52:17.533: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug 24 11:52:17.534: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 08/24/23 11:52:17.534
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 08/24/23 11:52:17.546
  Aug 24 11:52:17.554: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug 24 11:52:17.554: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 08/24/23 11:52:17.554
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 08/24/23 11:52:17.565
  Aug 24 11:52:17.574: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Aug 24 11:52:17.574: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 08/24/23 11:52:17.575
  STEP: Failing to create a Pod with more than max resources @ 08/24/23 11:52:17.581
  STEP: Updating a LimitRange @ 08/24/23 11:52:17.59
  STEP: Verifying LimitRange updating is effective @ 08/24/23 11:52:17.603
  STEP: Creating a Pod with less than former min resources @ 08/24/23 11:52:19.611
  STEP: Failing to create a Pod with more than max resources @ 08/24/23 11:52:19.623
  STEP: Deleting a LimitRange @ 08/24/23 11:52:19.63
  STEP: Verifying the LimitRange was deleted @ 08/24/23 11:52:19.642
  Aug 24 11:52:24.654: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 08/24/23 11:52:24.654
  Aug 24 11:52:24.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-2830" for this suite. @ 08/24/23 11:52:24.683
• [7.355 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 08/24/23 11:52:24.702
  Aug 24 11:52:24.702: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 11:52:24.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:52:24.736
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:52:24.742
  STEP: fetching the /apis discovery document @ 08/24/23 11:52:24.748
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 08/24/23 11:52:24.75
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 08/24/23 11:52:24.75
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 08/24/23 11:52:24.75
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 08/24/23 11:52:24.752
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 08/24/23 11:52:24.753
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 08/24/23 11:52:24.756
  Aug 24 11:52:24.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4203" for this suite. @ 08/24/23 11:52:24.767
• [0.077 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 08/24/23 11:52:24.781
  Aug 24 11:52:24.781: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 11:52:24.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:52:24.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:52:24.833
  STEP: creating a replication controller @ 08/24/23 11:52:24.839
  Aug 24 11:52:24.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 create -f -'
  Aug 24 11:52:25.537: INFO: stderr: ""
  Aug 24 11:52:25.537: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/24/23 11:52:25.537
  Aug 24 11:52:25.538: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 11:52:25.722: INFO: stderr: ""
  Aug 24 11:52:25.722: INFO: stdout: "update-demo-nautilus-dv9nm update-demo-nautilus-gtm8s "
  Aug 24 11:52:25.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods update-demo-nautilus-dv9nm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 11:52:25.884: INFO: stderr: ""
  Aug 24 11:52:25.884: INFO: stdout: ""
  Aug 24 11:52:25.884: INFO: update-demo-nautilus-dv9nm is created but not running
  Aug 24 11:52:30.884: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 11:52:31.061: INFO: stderr: ""
  Aug 24 11:52:31.061: INFO: stdout: "update-demo-nautilus-dv9nm update-demo-nautilus-gtm8s "
  Aug 24 11:52:31.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods update-demo-nautilus-dv9nm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 11:52:31.254: INFO: stderr: ""
  Aug 24 11:52:31.254: INFO: stdout: ""
  Aug 24 11:52:31.254: INFO: update-demo-nautilus-dv9nm is created but not running
  Aug 24 11:52:36.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 11:52:36.416: INFO: stderr: ""
  Aug 24 11:52:36.416: INFO: stdout: "update-demo-nautilus-dv9nm update-demo-nautilus-gtm8s "
  Aug 24 11:52:36.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods update-demo-nautilus-dv9nm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 11:52:36.586: INFO: stderr: ""
  Aug 24 11:52:36.586: INFO: stdout: ""
  Aug 24 11:52:36.586: INFO: update-demo-nautilus-dv9nm is created but not running
  Aug 24 11:52:41.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 11:52:41.742: INFO: stderr: ""
  Aug 24 11:52:41.742: INFO: stdout: "update-demo-nautilus-dv9nm update-demo-nautilus-gtm8s "
  Aug 24 11:52:41.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods update-demo-nautilus-dv9nm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 11:52:41.904: INFO: stderr: ""
  Aug 24 11:52:41.904: INFO: stdout: "true"
  Aug 24 11:52:41.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods update-demo-nautilus-dv9nm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 11:52:42.070: INFO: stderr: ""
  Aug 24 11:52:42.070: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 11:52:42.070: INFO: validating pod update-demo-nautilus-dv9nm
  Aug 24 11:52:42.112: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 11:52:42.113: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 11:52:42.113: INFO: update-demo-nautilus-dv9nm is verified up and running
  Aug 24 11:52:42.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods update-demo-nautilus-gtm8s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 11:52:42.245: INFO: stderr: ""
  Aug 24 11:52:42.245: INFO: stdout: "true"
  Aug 24 11:52:42.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods update-demo-nautilus-gtm8s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 11:52:42.390: INFO: stderr: ""
  Aug 24 11:52:42.390: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 11:52:42.390: INFO: validating pod update-demo-nautilus-gtm8s
  Aug 24 11:52:42.401: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 11:52:42.401: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 11:52:42.401: INFO: update-demo-nautilus-gtm8s is verified up and running
  STEP: using delete to clean up resources @ 08/24/23 11:52:42.401
  Aug 24 11:52:42.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 delete --grace-period=0 --force -f -'
  Aug 24 11:52:42.539: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 11:52:42.539: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug 24 11:52:42.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get rc,svc -l name=update-demo --no-headers'
  Aug 24 11:52:42.764: INFO: stderr: "No resources found in kubectl-9160 namespace.\n"
  Aug 24 11:52:42.764: INFO: stdout: ""
  Aug 24 11:52:42.774: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9160 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 24 11:52:42.933: INFO: stderr: ""
  Aug 24 11:52:42.933: INFO: stdout: ""
  Aug 24 11:52:42.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9160" for this suite. @ 08/24/23 11:52:42.944
• [18.174 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 08/24/23 11:52:42.957
  Aug 24 11:52:42.957: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 11:52:42.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:52:42.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:52:42.994
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/24/23 11:52:43
  STEP: Saw pod success @ 08/24/23 11:52:47.04
  Aug 24 11:52:47.047: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-9013c414-233b-4014-a35f-7a86406b8c81 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 11:52:47.067
  Aug 24 11:52:47.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2957" for this suite. @ 08/24/23 11:52:47.101
• [4.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 08/24/23 11:52:47.123
  Aug 24 11:52:47.123: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 11:52:47.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:52:47.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:52:47.162
  STEP: creating the pod @ 08/24/23 11:52:47.17
  STEP: waiting for pod running @ 08/24/23 11:52:47.187
  STEP: creating a file in subpath @ 08/24/23 11:52:49.24
  Aug 24 11:52:49.248: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-1553 PodName:var-expansion-95519d8e-56be-40c9-97a8-701241d823f2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:52:49.248: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:52:49.252: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:52:49.252: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1553/pods/var-expansion-95519d8e-56be-40c9-97a8-701241d823f2/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 08/24/23 11:52:49.377
  Aug 24 11:52:49.385: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-1553 PodName:var-expansion-95519d8e-56be-40c9-97a8-701241d823f2 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:52:49.386: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:52:49.388: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:52:49.388: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-1553/pods/var-expansion-95519d8e-56be-40c9-97a8-701241d823f2/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 08/24/23 11:52:49.503
  Aug 24 11:52:50.067: INFO: Successfully updated pod "var-expansion-95519d8e-56be-40c9-97a8-701241d823f2"
  STEP: waiting for annotated pod running @ 08/24/23 11:52:50.068
  STEP: deleting the pod gracefully @ 08/24/23 11:52:50.076
  Aug 24 11:52:50.076: INFO: Deleting pod "var-expansion-95519d8e-56be-40c9-97a8-701241d823f2" in namespace "var-expansion-1553"
  Aug 24 11:52:50.093: INFO: Wait up to 5m0s for pod "var-expansion-95519d8e-56be-40c9-97a8-701241d823f2" to be fully deleted
  Aug 24 11:53:22.240: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1553" for this suite. @ 08/24/23 11:53:22.252
• [35.150 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 08/24/23 11:53:22.278
  Aug 24 11:53:22.278: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 11:53:22.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:53:22.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:53:22.315
  STEP: Creating secret with name secret-test-b4ccf0db-d0b2-4f09-a067-a128687030e5 @ 08/24/23 11:53:22.322
  STEP: Creating a pod to test consume secrets @ 08/24/23 11:53:22.332
  STEP: Saw pod success @ 08/24/23 11:53:26.374
  Aug 24 11:53:26.380: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-secrets-e97a843f-f364-490c-9fbe-af740a764f9a container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 11:53:26.393
  Aug 24 11:53:26.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3600" for this suite. @ 08/24/23 11:53:26.439
• [4.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 08/24/23 11:53:26.461
  Aug 24 11:53:26.461: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename disruption @ 08/24/23 11:53:26.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:53:26.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:53:26.502
  STEP: creating the pdb @ 08/24/23 11:53:26.507
  STEP: Waiting for the pdb to be processed @ 08/24/23 11:53:26.519
  STEP: updating the pdb @ 08/24/23 11:53:28.536
  STEP: Waiting for the pdb to be processed @ 08/24/23 11:53:28.553
  STEP: patching the pdb @ 08/24/23 11:53:30.57
  STEP: Waiting for the pdb to be processed @ 08/24/23 11:53:30.59
  STEP: Waiting for the pdb to be deleted @ 08/24/23 11:53:32.617
  Aug 24 11:53:32.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8909" for this suite. @ 08/24/23 11:53:32.631
• [6.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 08/24/23 11:53:32.648
  Aug 24 11:53:32.648: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 11:53:32.65
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:53:32.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:53:32.688
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 11:53:32.695
  STEP: Saw pod success @ 08/24/23 11:53:36.733
  Aug 24 11:53:36.747: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-e5868a85-4d1b-4e1c-9891-cfd32eca0b4b container client-container: <nil>
  STEP: delete the pod @ 08/24/23 11:53:36.761
  Aug 24 11:53:36.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8262" for this suite. @ 08/24/23 11:53:36.799
• [4.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 08/24/23 11:53:36.83
  Aug 24 11:53:36.830: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/24/23 11:53:36.832
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:53:36.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:53:36.88
  STEP: creating @ 08/24/23 11:53:36.885
  STEP: getting @ 08/24/23 11:53:36.999
  STEP: listing @ 08/24/23 11:53:37.014
  STEP: deleting @ 08/24/23 11:53:37.021
  Aug 24 11:53:37.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-2213" for this suite. @ 08/24/23 11:53:37.068
• [0.253 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 08/24/23 11:53:37.098
  Aug 24 11:53:37.098: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 11:53:37.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:53:37.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:53:37.146
  STEP: Creating secret with name secret-test-8921a832-4649-47d8-907a-bb62548758bc @ 08/24/23 11:53:37.154
  STEP: Creating a pod to test consume secrets @ 08/24/23 11:53:37.162
  STEP: Saw pod success @ 08/24/23 11:53:41.226
  Aug 24 11:53:41.235: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-secrets-2a7525f8-6049-413d-a119-17f08df14f47 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 11:53:41.247
  Aug 24 11:53:41.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5746" for this suite. @ 08/24/23 11:53:41.298
• [4.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 08/24/23 11:53:41.321
  Aug 24 11:53:41.322: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename init-container @ 08/24/23 11:53:41.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:53:41.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:53:41.369
  STEP: creating the pod @ 08/24/23 11:53:41.374
  Aug 24 11:53:41.374: INFO: PodSpec: initContainers in spec.initContainers
  Aug 24 11:53:44.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-3306" for this suite. @ 08/24/23 11:53:44.785
• [3.477 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 08/24/23 11:53:44.809
  Aug 24 11:53:44.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename podtemplate @ 08/24/23 11:53:44.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:53:44.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:53:44.852
  Aug 24 11:53:44.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8347" for this suite. @ 08/24/23 11:53:44.969
• [0.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 08/24/23 11:53:44.989
  Aug 24 11:53:44.989: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename watch @ 08/24/23 11:53:44.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:53:45.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:53:45.026
  STEP: creating a watch on configmaps with label A @ 08/24/23 11:53:45.031
  STEP: creating a watch on configmaps with label B @ 08/24/23 11:53:45.033
  STEP: creating a watch on configmaps with label A or B @ 08/24/23 11:53:45.035
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 08/24/23 11:53:45.037
  Aug 24 11:53:45.049: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8791  656ec84a-1e07-4bde-ae61-31b74fd6340a 7572 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:53:45.050: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8791  656ec84a-1e07-4bde-ae61-31b74fd6340a 7572 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 08/24/23 11:53:45.05
  Aug 24 11:53:45.072: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8791  656ec84a-1e07-4bde-ae61-31b74fd6340a 7574 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:53:45.072: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8791  656ec84a-1e07-4bde-ae61-31b74fd6340a 7574 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 08/24/23 11:53:45.072
  Aug 24 11:53:45.084: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8791  656ec84a-1e07-4bde-ae61-31b74fd6340a 7575 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:53:45.084: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8791  656ec84a-1e07-4bde-ae61-31b74fd6340a 7575 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 08/24/23 11:53:45.085
  Aug 24 11:53:45.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8791  656ec84a-1e07-4bde-ae61-31b74fd6340a 7576 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:53:45.094: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-8791  656ec84a-1e07-4bde-ae61-31b74fd6340a 7576 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 08/24/23 11:53:45.094
  Aug 24 11:53:45.104: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8791  f31301f7-f142-434a-ba13-79af0c647667 7577 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:53:45.104: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8791  f31301f7-f142-434a-ba13-79af0c647667 7577 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 08/24/23 11:53:55.105
  Aug 24 11:53:55.122: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8791  f31301f7-f142-434a-ba13-79af0c647667 7630 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:53:55.123: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-8791  f31301f7-f142-434a-ba13-79af0c647667 7630 0 2023-08-24 11:53:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-24 11:53:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:54:05.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8791" for this suite. @ 08/24/23 11:54:05.138
• [20.161 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 08/24/23 11:54:05.151
  Aug 24 11:54:05.152: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 11:54:05.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:05.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:05.231
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 11:54:05.236
  STEP: Saw pod success @ 08/24/23 11:54:09.283
  Aug 24 11:54:09.290: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-eae4d62d-b4ed-421e-8747-93e8b6d66cd2 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 11:54:09.301
  Aug 24 11:54:09.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1615" for this suite. @ 08/24/23 11:54:09.335
• [4.194 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 08/24/23 11:54:09.349
  Aug 24 11:54:09.349: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 11:54:09.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:09.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:09.389
  STEP: Creating a pod to test substitution in volume subpath @ 08/24/23 11:54:09.393
  STEP: Saw pod success @ 08/24/23 11:54:13.431
  Aug 24 11:54:13.439: INFO: Trying to get logs from node quohp9aeph3i-3 pod var-expansion-5d133a93-6fb8-445c-920f-f5c3ae7f58ed container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 11:54:13.455
  Aug 24 11:54:13.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4516" for this suite. @ 08/24/23 11:54:13.49
• [4.157 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 08/24/23 11:54:13.511
  Aug 24 11:54:13.511: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename gc @ 08/24/23 11:54:13.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:13.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:13.548
  STEP: create the rc1 @ 08/24/23 11:54:13.563
  STEP: create the rc2 @ 08/24/23 11:54:13.576
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 08/24/23 11:54:19.728
  STEP: delete the rc simpletest-rc-to-be-deleted @ 08/24/23 11:54:25.406
  STEP: wait for the rc to be deleted @ 08/24/23 11:54:25.682
  Aug 24 11:54:30.804: INFO: 92 pods remaining
  Aug 24 11:54:30.805: INFO: 69 pods has nil DeletionTimestamp
  Aug 24 11:54:30.805: INFO: 
  Aug 24 11:54:35.745: INFO: 78 pods remaining
  Aug 24 11:54:35.745: INFO: 50 pods has nil DeletionTimestamp
  Aug 24 11:54:35.745: INFO: 
  Aug 24 11:54:40.720: INFO: 50 pods remaining
  Aug 24 11:54:40.720: INFO: 50 pods has nil DeletionTimestamp
  Aug 24 11:54:40.720: INFO: 
  STEP: Gathering metrics @ 08/24/23 11:54:45.72
  Aug 24 11:54:45.968: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 11:54:45.968: INFO: Deleting pod "simpletest-rc-to-be-deleted-24mq5" in namespace "gc-1670"
  Aug 24 11:54:46.020: INFO: Deleting pod "simpletest-rc-to-be-deleted-2msth" in namespace "gc-1670"
  Aug 24 11:54:46.082: INFO: Deleting pod "simpletest-rc-to-be-deleted-2xrt6" in namespace "gc-1670"
  Aug 24 11:54:46.110: INFO: Deleting pod "simpletest-rc-to-be-deleted-48rn7" in namespace "gc-1670"
  Aug 24 11:54:46.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-4c2hg" in namespace "gc-1670"
  Aug 24 11:54:46.210: INFO: Deleting pod "simpletest-rc-to-be-deleted-4fkpk" in namespace "gc-1670"
  Aug 24 11:54:46.253: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jmjm" in namespace "gc-1670"
  Aug 24 11:54:46.342: INFO: Deleting pod "simpletest-rc-to-be-deleted-4lhjd" in namespace "gc-1670"
  Aug 24 11:54:46.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-4slsx" in namespace "gc-1670"
  Aug 24 11:54:46.483: INFO: Deleting pod "simpletest-rc-to-be-deleted-4zx9q" in namespace "gc-1670"
  Aug 24 11:54:46.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-59hzq" in namespace "gc-1670"
  Aug 24 11:54:46.785: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bwf8" in namespace "gc-1670"
  Aug 24 11:54:46.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-5klpv" in namespace "gc-1670"
  Aug 24 11:54:47.020: INFO: Deleting pod "simpletest-rc-to-be-deleted-5rssd" in namespace "gc-1670"
  Aug 24 11:54:47.147: INFO: Deleting pod "simpletest-rc-to-be-deleted-5vcln" in namespace "gc-1670"
  Aug 24 11:54:47.275: INFO: Deleting pod "simpletest-rc-to-be-deleted-65xbh" in namespace "gc-1670"
  Aug 24 11:54:47.323: INFO: Deleting pod "simpletest-rc-to-be-deleted-6sdw8" in namespace "gc-1670"
  Aug 24 11:54:47.379: INFO: Deleting pod "simpletest-rc-to-be-deleted-6tbss" in namespace "gc-1670"
  Aug 24 11:54:47.498: INFO: Deleting pod "simpletest-rc-to-be-deleted-7724m" in namespace "gc-1670"
  Aug 24 11:54:47.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-77pv2" in namespace "gc-1670"
  Aug 24 11:54:47.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-7c7p9" in namespace "gc-1670"
  Aug 24 11:54:47.639: INFO: Deleting pod "simpletest-rc-to-be-deleted-87lx4" in namespace "gc-1670"
  Aug 24 11:54:47.689: INFO: Deleting pod "simpletest-rc-to-be-deleted-87v4l" in namespace "gc-1670"
  Aug 24 11:54:47.744: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cdq2" in namespace "gc-1670"
  Aug 24 11:54:47.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dhrs" in namespace "gc-1670"
  Aug 24 11:54:47.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-9flv8" in namespace "gc-1670"
  Aug 24 11:54:48.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-9fp89" in namespace "gc-1670"
  Aug 24 11:54:48.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-9ks5j" in namespace "gc-1670"
  Aug 24 11:54:48.271: INFO: Deleting pod "simpletest-rc-to-be-deleted-9s4wl" in namespace "gc-1670"
  Aug 24 11:54:48.324: INFO: Deleting pod "simpletest-rc-to-be-deleted-9tm8q" in namespace "gc-1670"
  Aug 24 11:54:48.398: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zrx5" in namespace "gc-1670"
  Aug 24 11:54:48.482: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7zvm" in namespace "gc-1670"
  Aug 24 11:54:48.532: INFO: Deleting pod "simpletest-rc-to-be-deleted-bgjkn" in namespace "gc-1670"
  Aug 24 11:54:48.586: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4xb2" in namespace "gc-1670"
  Aug 24 11:54:48.630: INFO: Deleting pod "simpletest-rc-to-be-deleted-chzkz" in namespace "gc-1670"
  Aug 24 11:54:48.738: INFO: Deleting pod "simpletest-rc-to-be-deleted-cnvjf" in namespace "gc-1670"
  Aug 24 11:54:48.846: INFO: Deleting pod "simpletest-rc-to-be-deleted-cqs2w" in namespace "gc-1670"
  Aug 24 11:54:49.150: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwb78" in namespace "gc-1670"
  Aug 24 11:54:49.248: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwwts" in namespace "gc-1670"
  Aug 24 11:54:49.369: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2bkb" in namespace "gc-1670"
  Aug 24 11:54:49.460: INFO: Deleting pod "simpletest-rc-to-be-deleted-dms6z" in namespace "gc-1670"
  Aug 24 11:54:49.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmwmj" in namespace "gc-1670"
  Aug 24 11:54:49.640: INFO: Deleting pod "simpletest-rc-to-be-deleted-fcgq2" in namespace "gc-1670"
  Aug 24 11:54:49.676: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5cdj" in namespace "gc-1670"
  Aug 24 11:54:49.736: INFO: Deleting pod "simpletest-rc-to-be-deleted-g678v" in namespace "gc-1670"
  Aug 24 11:54:49.802: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfhpd" in namespace "gc-1670"
  Aug 24 11:54:49.861: INFO: Deleting pod "simpletest-rc-to-be-deleted-gl7j9" in namespace "gc-1670"
  Aug 24 11:54:49.946: INFO: Deleting pod "simpletest-rc-to-be-deleted-hgwkm" in namespace "gc-1670"
  Aug 24 11:54:50.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-hvvwt" in namespace "gc-1670"
  Aug 24 11:54:50.118: INFO: Deleting pod "simpletest-rc-to-be-deleted-j8q7q" in namespace "gc-1670"
  Aug 24 11:54:50.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1670" for this suite. @ 08/24/23 11:54:50.23
• [36.796 seconds]
------------------------------
SSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 08/24/23 11:54:50.307
  Aug 24 11:54:50.307: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sysctl @ 08/24/23 11:54:50.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:50.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:50.367
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 08/24/23 11:54:50.373
  STEP: Watching for error events or started pod @ 08/24/23 11:54:50.411
  STEP: Waiting for pod completion @ 08/24/23 11:54:52.42
  STEP: Checking that the pod succeeded @ 08/24/23 11:54:54.462
  STEP: Getting logs from the pod @ 08/24/23 11:54:54.462
  STEP: Checking that the sysctl is actually updated @ 08/24/23 11:54:54.481
  Aug 24 11:54:54.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-9666" for this suite. @ 08/24/23 11:54:54.493
• [4.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 08/24/23 11:54:54.523
  Aug 24 11:54:54.523: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename job @ 08/24/23 11:54:54.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:54:54.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:54:54.565
  STEP: Creating Indexed job @ 08/24/23 11:54:54.571
  STEP: Ensuring job reaches completions @ 08/24/23 11:54:54.584
  STEP: Ensuring pods with index for job exist @ 08/24/23 11:55:02.59
  Aug 24 11:55:02.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-884" for this suite. @ 08/24/23 11:55:02.607
• [8.096 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 08/24/23 11:55:02.622
  Aug 24 11:55:02.622: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 11:55:02.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:55:02.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:55:02.665
  STEP: Creating service test in namespace statefulset-6215 @ 08/24/23 11:55:02.67
  Aug 24 11:55:02.705: INFO: Found 0 stateful pods, waiting for 1
  Aug 24 11:55:12.713: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 08/24/23 11:55:12.727
  W0824 11:55:12.743587      15 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 24 11:55:12.756: INFO: Found 1 stateful pods, waiting for 2
  Aug 24 11:55:22.784: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 11:55:22.784: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 08/24/23 11:55:22.807
  STEP: Delete all of the StatefulSets @ 08/24/23 11:55:22.817
  STEP: Verify that StatefulSets have been deleted @ 08/24/23 11:55:22.831
  Aug 24 11:55:22.838: INFO: Deleting all statefulset in ns statefulset-6215
  Aug 24 11:55:22.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6215" for this suite. @ 08/24/23 11:55:22.87
• [20.271 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 08/24/23 11:55:22.893
  Aug 24 11:55:22.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename containers @ 08/24/23 11:55:22.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:55:22.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:55:22.978
  STEP: Creating a pod to test override arguments @ 08/24/23 11:55:22.985
  STEP: Saw pod success @ 08/24/23 11:55:27.043
  Aug 24 11:55:27.051: INFO: Trying to get logs from node quohp9aeph3i-3 pod client-containers-108bb0d4-d7f3-4181-b438-96ecc262dc84 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 11:55:27.064
  Aug 24 11:55:27.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3003" for this suite. @ 08/24/23 11:55:27.101
• [4.230 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 08/24/23 11:55:27.127
  Aug 24 11:55:27.127: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 11:55:27.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:55:27.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:55:27.166
  STEP: Creating a ResourceQuota with best effort scope @ 08/24/23 11:55:27.169
  STEP: Ensuring ResourceQuota status is calculated @ 08/24/23 11:55:27.185
  STEP: Creating a ResourceQuota with not best effort scope @ 08/24/23 11:55:29.195
  STEP: Ensuring ResourceQuota status is calculated @ 08/24/23 11:55:29.238
  STEP: Creating a best-effort pod @ 08/24/23 11:55:31.245
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 08/24/23 11:55:31.263
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 08/24/23 11:55:33.276
  STEP: Deleting the pod @ 08/24/23 11:55:35.286
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 11:55:35.307
  STEP: Creating a not best-effort pod @ 08/24/23 11:55:37.314
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 08/24/23 11:55:37.336
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 08/24/23 11:55:39.343
  STEP: Deleting the pod @ 08/24/23 11:55:41.352
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 11:55:41.376
  Aug 24 11:55:43.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4808" for this suite. @ 08/24/23 11:55:43.396
• [16.281 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 08/24/23 11:55:43.418
  Aug 24 11:55:43.418: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 11:55:43.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:55:43.454
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:55:43.46
  STEP: set up a multi version CRD @ 08/24/23 11:55:43.466
  Aug 24 11:55:43.467: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: mark a version not serverd @ 08/24/23 11:55:48.106
  STEP: check the unserved version gets removed @ 08/24/23 11:55:48.14
  STEP: check the other version is not changed @ 08/24/23 11:55:50.392
  Aug 24 11:55:54.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-139" for this suite. @ 08/24/23 11:55:54.46
• [11.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 08/24/23 11:55:54.482
  Aug 24 11:55:54.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 11:55:54.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:55:54.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:55:54.527
  STEP: apply creating a deployment @ 08/24/23 11:55:54.533
  Aug 24 11:55:54.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6149" for this suite. @ 08/24/23 11:55:54.587
• [0.118 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 08/24/23 11:55:54.611
  Aug 24 11:55:54.611: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 11:55:54.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:55:54.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:55:54.657
  STEP: Creating configMap with name configmap-test-volume-map-03e9c9ef-1398-411f-b09a-25d1b3e4e367 @ 08/24/23 11:55:54.668
  STEP: Creating a pod to test consume configMaps @ 08/24/23 11:55:54.677
  STEP: Saw pod success @ 08/24/23 11:55:58.728
  Aug 24 11:55:58.735: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-c5ca8c20-d383-40cf-91f2-fc2d04bec063 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 11:55:58.749
  Aug 24 11:55:58.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4396" for this suite. @ 08/24/23 11:55:58.789
• [4.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 08/24/23 11:55:58.82
  Aug 24 11:55:58.820: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 11:55:58.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:55:58.856
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:55:58.86
  Aug 24 11:55:58.867: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6935 create -f -'
  Aug 24 11:56:00.331: INFO: stderr: ""
  Aug 24 11:56:00.331: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Aug 24 11:56:00.332: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6935 create -f -'
  Aug 24 11:56:01.090: INFO: stderr: ""
  Aug 24 11:56:01.090: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/24/23 11:56:01.09
  Aug 24 11:56:02.098: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 11:56:02.099: INFO: Found 1 / 1
  Aug 24 11:56:02.099: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug 24 11:56:02.106: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 11:56:02.106: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 24 11:56:02.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6935 describe pod agnhost-primary-bh4nx'
  Aug 24 11:56:02.315: INFO: stderr: ""
  Aug 24 11:56:02.315: INFO: stdout: "Name:             agnhost-primary-bh4nx\nNamespace:        kubectl-6935\nPriority:         0\nService Account:  default\nNode:             quohp9aeph3i-3/192.168.121.15\nStart Time:       Thu, 24 Aug 2023 11:56:00 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.16\nIPs:\n  IP:           10.233.66.16\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://bf28442f33e496d91dc8b752cd4425636f650fc7609a5cdf95368361136c18a3\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 24 Aug 2023 11:56:01 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-vzkv9 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-vzkv9:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6935/agnhost-primary-bh4nx to quohp9aeph3i-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Aug 24 11:56:02.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6935 describe rc agnhost-primary'
  Aug 24 11:56:02.486: INFO: stderr: ""
  Aug 24 11:56:02.486: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6935\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-bh4nx\n"
  Aug 24 11:56:02.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6935 describe service agnhost-primary'
  Aug 24 11:56:02.650: INFO: stderr: ""
  Aug 24 11:56:02.650: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6935\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.16.43\nIPs:               10.233.16.43\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.16:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Aug 24 11:56:02.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6935 describe node quohp9aeph3i-1'
  Aug 24 11:56:02.921: INFO: stderr: ""
  Aug 24 11:56:02.921: INFO: stdout: "Name:               quohp9aeph3i-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=quohp9aeph3i-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 24 Aug 2023 11:20:44 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  quohp9aeph3i-1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 24 Aug 2023 11:55:55 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 24 Aug 2023 11:23:45 +0000   Thu, 24 Aug 2023 11:23:45 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Thu, 24 Aug 2023 11:54:52 +0000   Thu, 24 Aug 2023 11:20:36 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 24 Aug 2023 11:54:52 +0000   Thu, 24 Aug 2023 11:20:36 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 24 Aug 2023 11:54:52 +0000   Thu, 24 Aug 2023 11:20:36 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 24 Aug 2023 11:54:52 +0000   Thu, 24 Aug 2023 11:24:53 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.64\n  Hostname:    quohp9aeph3i-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8123912Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3274248Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 0bc9523b2eb849439b0e87f0c912d82e\n  System UUID:                0bc9523b-2eb8-4943-9b0e-87f0c912d82e\n  Boot ID:                    daf86e2f-2489-4196-a0c6-5b7e646b1812\n  Kernel Version:             6.2.0-26-generic\n  OS Image:                   Ubuntu 22.04.3 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.1\n  Kubelet Version:            v1.27.5\n  Kube-Proxy Version:         v1.27.5\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-2mlv4                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         33m\n  kube-system                 cilium-node-init-wh7d5                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         33m\n  kube-system                 coredns-5d78c9869d-xp68m                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     32m\n  kube-system                 kube-addon-manager-quohp9aeph3i-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         33m\n  kube-system                 kube-apiserver-quohp9aeph3i-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         35m\n  kube-system                 kube-controller-manager-quohp9aeph3i-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         35m\n  kube-system                 kube-proxy-ljqfg                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         34m\n  kube-system                 kube-scheduler-quohp9aeph3i-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         35m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-5c6ln    0 (0%)        0 (0%)      0 (0%)           0 (0%)         17m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                855m (53%)   0 (0%)\n  memory             320Mi (10%)  170Mi (5%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 34m                kube-proxy       \n  Normal  NodeHasSufficientMemory  35m (x8 over 35m)  kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    35m (x8 over 35m)  kubelet          Node quohp9aeph3i-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     35m (x7 over 35m)  kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  35m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeHasSufficientMemory  35m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    35m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     35m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             35m                kubelet          Node quohp9aeph3i-1 status is now: NodeNotReady\n  Normal  NodeReady                35m                kubelet          Node quohp9aeph3i-1 status is now: NodeReady\n  Normal  NodeAllocatableEnforced  35m                kubelet          Updated Node Allocatable limit across pods\n  Normal  Starting                 35m                kubelet          Starting kubelet.\n  Normal  RegisteredNode           34m                node-controller  Node quohp9aeph3i-1 event: Registered Node quohp9aeph3i-1 in Controller\n  Normal  NodeHasSufficientPID     33m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientPID\n  Normal  NodeHasSufficientMemory  33m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    33m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasNoDiskPressure\n  Normal  Starting                 33m                kubelet          Starting kubelet.\n  Normal  NodeNotReady             33m                kubelet          Node quohp9aeph3i-1 status is now: NodeNotReady\n  Normal  NodeReady                33m                kubelet          Node quohp9aeph3i-1 status is now: NodeReady\n  Normal  NodeAllocatableEnforced  33m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeNotReady             31m                kubelet          Node quohp9aeph3i-1 status is now: NodeNotReady\n  Normal  NodeHasSufficientMemory  31m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    31m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     31m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientPID\n  Normal  Starting                 31m                kubelet          Starting kubelet.\n  Normal  NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                31m                kubelet          Node quohp9aeph3i-1 status is now: NodeReady\n  Normal  Starting                 31m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  31m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    31m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     31m                kubelet          Node quohp9aeph3i-1 status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             31m                kubelet          Node quohp9aeph3i-1 status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  31m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                31m                kubelet          Node quohp9aeph3i-1 status is now: NodeReady\n"
  Aug 24 11:56:02.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6935 describe namespace kubectl-6935'
  Aug 24 11:56:03.072: INFO: stderr: ""
  Aug 24 11:56:03.072: INFO: stdout: "Name:         kubectl-6935\nLabels:       e2e-framework=kubectl\n              e2e-run=388bc1eb-53bb-4308-9fd0-f4aa3ff2edb9\n              kubernetes.io/metadata.name=kubectl-6935\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Aug 24 11:56:03.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6935" for this suite. @ 08/24/23 11:56:03.081
• [4.273 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 08/24/23 11:56:03.096
  Aug 24 11:56:03.096: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 11:56:03.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:56:03.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:56:03.129
  Aug 24 11:56:05.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8173" for this suite. @ 08/24/23 11:56:05.202
• [2.148 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 08/24/23 11:56:05.246
  Aug 24 11:56:05.246: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename proxy @ 08/24/23 11:56:05.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:56:05.281
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:56:05.286
  Aug 24 11:56:05.290: INFO: Creating pod...
  Aug 24 11:56:07.337: INFO: Creating service...
  Aug 24 11:56:07.366: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/pods/agnhost/proxy/some/path/with/DELETE
  Aug 24 11:56:07.385: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 24 11:56:07.385: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/pods/agnhost/proxy/some/path/with/GET
  Aug 24 11:56:07.393: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug 24 11:56:07.393: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/pods/agnhost/proxy/some/path/with/HEAD
  Aug 24 11:56:07.399: INFO: http.Client request:HEAD | StatusCode:200
  Aug 24 11:56:07.399: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/pods/agnhost/proxy/some/path/with/OPTIONS
  Aug 24 11:56:07.405: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 24 11:56:07.405: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/pods/agnhost/proxy/some/path/with/PATCH
  Aug 24 11:56:07.412: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 24 11:56:07.412: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/pods/agnhost/proxy/some/path/with/POST
  Aug 24 11:56:07.416: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 24 11:56:07.416: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/pods/agnhost/proxy/some/path/with/PUT
  Aug 24 11:56:07.421: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 24 11:56:07.421: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/services/test-service/proxy/some/path/with/DELETE
  Aug 24 11:56:07.430: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 24 11:56:07.430: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/services/test-service/proxy/some/path/with/GET
  Aug 24 11:56:07.443: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug 24 11:56:07.443: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/services/test-service/proxy/some/path/with/HEAD
  Aug 24 11:56:07.453: INFO: http.Client request:HEAD | StatusCode:200
  Aug 24 11:56:07.454: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/services/test-service/proxy/some/path/with/OPTIONS
  Aug 24 11:56:07.465: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 24 11:56:07.465: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/services/test-service/proxy/some/path/with/PATCH
  Aug 24 11:56:07.476: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 24 11:56:07.477: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/services/test-service/proxy/some/path/with/POST
  Aug 24 11:56:07.488: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 24 11:56:07.489: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-2304/services/test-service/proxy/some/path/with/PUT
  Aug 24 11:56:07.499: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 24 11:56:07.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2304" for this suite. @ 08/24/23 11:56:07.512
• [2.278 seconds]
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 08/24/23 11:56:07.525
  Aug 24 11:56:07.526: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename ingressclass @ 08/24/23 11:56:07.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:56:07.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:56:07.56
  STEP: getting /apis @ 08/24/23 11:56:07.569
  STEP: getting /apis/networking.k8s.io @ 08/24/23 11:56:07.578
  STEP: getting /apis/networking.k8s.iov1 @ 08/24/23 11:56:07.581
  STEP: creating @ 08/24/23 11:56:07.583
  STEP: getting @ 08/24/23 11:56:07.609
  STEP: listing @ 08/24/23 11:56:07.614
  STEP: watching @ 08/24/23 11:56:07.618
  Aug 24 11:56:07.618: INFO: starting watch
  STEP: patching @ 08/24/23 11:56:07.62
  STEP: updating @ 08/24/23 11:56:07.629
  Aug 24 11:56:07.638: INFO: waiting for watch events with expected annotations
  Aug 24 11:56:07.638: INFO: saw patched and updated annotations
  STEP: deleting @ 08/24/23 11:56:07.639
  STEP: deleting a collection @ 08/24/23 11:56:07.657
  Aug 24 11:56:07.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-9116" for this suite. @ 08/24/23 11:56:07.69
• [0.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 08/24/23 11:56:07.707
  Aug 24 11:56:07.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 11:56:07.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:56:07.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:56:07.746
  STEP: Creating configMap with name configmap-test-upd-af1faf9a-3f84-40df-8812-03c25acfa09f @ 08/24/23 11:56:07.761
  STEP: Creating the pod @ 08/24/23 11:56:07.771
  STEP: Updating configmap configmap-test-upd-af1faf9a-3f84-40df-8812-03c25acfa09f @ 08/24/23 11:56:09.836
  STEP: waiting to observe update in volume @ 08/24/23 11:56:09.85
  Aug 24 11:57:22.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8844" for this suite. @ 08/24/23 11:57:22.513
• [74.827 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 08/24/23 11:57:22.536
  Aug 24 11:57:22.536: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 11:57:22.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:57:22.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:57:22.574
  STEP: Creating secret with name projected-secret-test-e5147c2f-0130-4040-8dda-dc89017a0900 @ 08/24/23 11:57:22.579
  STEP: Creating a pod to test consume secrets @ 08/24/23 11:57:22.591
  STEP: Saw pod success @ 08/24/23 11:57:26.633
  Aug 24 11:57:26.642: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-secrets-6f609b4e-3664-4d07-b59a-19ee9f4fbb97 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 11:57:26.653
  Aug 24 11:57:26.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5643" for this suite. @ 08/24/23 11:57:26.686
• [4.161 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 08/24/23 11:57:26.7
  Aug 24 11:57:26.700: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pod-network-test @ 08/24/23 11:57:26.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:57:26.73
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:57:26.736
  STEP: Performing setup for networking test in namespace pod-network-test-9368 @ 08/24/23 11:57:26.74
  STEP: creating a selector @ 08/24/23 11:57:26.74
  STEP: Creating the service pods in kubernetes @ 08/24/23 11:57:26.74
  Aug 24 11:57:26.740: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 08/24/23 11:57:48.948
  Aug 24 11:57:51.033: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 24 11:57:51.033: INFO: Going to poll 10.233.65.227 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 11:57:51.039: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.227:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9368 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:57:51.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:57:51.041: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:57:51.041: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9368/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.227%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 11:57:51.218: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug 24 11:57:51.218: INFO: Going to poll 10.233.64.113 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 11:57:51.225: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.113:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9368 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:57:51.225: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:57:51.227: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:57:51.227: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9368/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.113%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 11:57:51.360: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug 24 11:57:51.360: INFO: Going to poll 10.233.66.148 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 11:57:51.367: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.148:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-9368 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:57:51.368: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:57:51.370: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:57:51.370: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-9368/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.148%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 11:57:51.496: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug 24 11:57:51.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9368" for this suite. @ 08/24/23 11:57:51.506
• [24.821 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 08/24/23 11:57:51.522
  Aug 24 11:57:51.522: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 11:57:51.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:57:51.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:57:51.569
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 11:57:51.576
  STEP: Saw pod success @ 08/24/23 11:57:55.627
  Aug 24 11:57:55.637: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-364cd530-9163-4994-a970-1b081ddad2a7 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 11:57:55.657
  Aug 24 11:57:55.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-231" for this suite. @ 08/24/23 11:57:55.688
• [4.187 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 08/24/23 11:57:55.71
  Aug 24 11:57:55.710: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 11:57:55.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:57:55.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:57:55.743
  STEP: Creating secret with name secret-test-525a22ea-d61f-480d-b853-4d373ae24f34 @ 08/24/23 11:57:55.749
  STEP: Creating a pod to test consume secrets @ 08/24/23 11:57:55.756
  STEP: Saw pod success @ 08/24/23 11:57:59.798
  Aug 24 11:57:59.806: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-secrets-afaaa828-9028-4dc9-9e0b-c4f8af10ed58 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 11:57:59.821
  Aug 24 11:57:59.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5426" for this suite. @ 08/24/23 11:57:59.87
• [4.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 08/24/23 11:57:59.893
  Aug 24 11:57:59.894: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 11:57:59.896
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:57:59.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:57:59.941
  STEP: Creating a pod to test downward api env vars @ 08/24/23 11:57:59.948
  STEP: Saw pod success @ 08/24/23 11:58:04.017
  Aug 24 11:58:04.024: INFO: Trying to get logs from node quohp9aeph3i-3 pod downward-api-48f8fc10-f4e9-4b58-a199-da4021533859 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 11:58:04.036
  Aug 24 11:58:04.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9920" for this suite. @ 08/24/23 11:58:04.083
• [4.203 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 08/24/23 11:58:04.098
  Aug 24 11:58:04.098: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename deployment @ 08/24/23 11:58:04.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:04.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:04.15
  STEP: creating a Deployment @ 08/24/23 11:58:04.165
  STEP: waiting for Deployment to be created @ 08/24/23 11:58:04.177
  STEP: waiting for all Replicas to be Ready @ 08/24/23 11:58:04.18
  Aug 24 11:58:04.184: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 11:58:04.184: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 11:58:04.200: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 11:58:04.201: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 11:58:04.238: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 11:58:04.238: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 11:58:04.318: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 11:58:04.318: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug 24 11:58:06.176: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug 24 11:58:06.177: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug 24 11:58:06.798: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 08/24/23 11:58:06.799
  W0824 11:58:06.819376      15 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug 24 11:58:06.822: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 08/24/23 11:58:06.823
  Aug 24 11:58:06.829: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 0
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.830: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.831: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.831: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.851: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.851: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.896: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.897: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.949: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.950: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:06.979: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  Aug 24 11:58:06.979: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  Aug 24 11:58:08.222: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:08.223: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:08.271: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  STEP: listing Deployments @ 08/24/23 11:58:08.271
  Aug 24 11:58:08.280: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 08/24/23 11:58:08.28
  Aug 24 11:58:08.302: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 08/24/23 11:58:08.302
  Aug 24 11:58:08.318: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 11:58:08.331: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 11:58:08.411: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 11:58:08.451: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 11:58:09.866: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 11:58:10.230: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 11:58:10.303: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 11:58:10.370: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug 24 11:58:11.889: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 08/24/23 11:58:11.975
  STEP: fetching the DeploymentStatus @ 08/24/23 11:58:11.994
  Aug 24 11:58:12.008: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  Aug 24 11:58:12.009: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  Aug 24 11:58:12.009: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  Aug 24 11:58:12.010: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 1
  Aug 24 11:58:12.010: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:12.011: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 3
  Aug 24 11:58:12.012: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:12.012: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 2
  Aug 24 11:58:12.013: INFO: observed Deployment test-deployment in namespace deployment-1966 with ReadyReplicas 3
  STEP: deleting the Deployment @ 08/24/23 11:58:12.013
  Aug 24 11:58:12.039: INFO: observed event type MODIFIED
  Aug 24 11:58:12.039: INFO: observed event type MODIFIED
  Aug 24 11:58:12.039: INFO: observed event type MODIFIED
  Aug 24 11:58:12.040: INFO: observed event type MODIFIED
  Aug 24 11:58:12.040: INFO: observed event type MODIFIED
  Aug 24 11:58:12.040: INFO: observed event type MODIFIED
  Aug 24 11:58:12.040: INFO: observed event type MODIFIED
  Aug 24 11:58:12.041: INFO: observed event type MODIFIED
  Aug 24 11:58:12.041: INFO: observed event type MODIFIED
  Aug 24 11:58:12.041: INFO: observed event type MODIFIED
  Aug 24 11:58:12.049: INFO: Log out all the ReplicaSets if there is no deployment created
  Aug 24 11:58:12.082: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-1966  9c436440-b9fa-48ab-9c76-3df0097f7aa7 11117 3 2023-08-24 11:58:04 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment ff210c90-1949-48ae-820e-6565f159a4a5 0xc0003a15f7 0xc0003a15f8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 11:58:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff210c90-1949-48ae-820e-6565f159a4a5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 11:58:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003a1680 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Aug 24 11:58:12.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1966" for this suite. @ 08/24/23 11:58:12.125
• [8.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 08/24/23 11:58:12.146
  Aug 24 11:58:12.146: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 11:58:12.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:12.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:12.198
  STEP: Creating simple DaemonSet "daemon-set" @ 08/24/23 11:58:12.268
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 11:58:12.284
  Aug 24 11:58:12.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 11:58:12.307: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 11:58:13.363: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 11:58:13.363: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 11:58:14.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 11:58:14.327: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 08/24/23 11:58:14.333
  STEP: DeleteCollection of the DaemonSets @ 08/24/23 11:58:14.341
  STEP: Verify that ReplicaSets have been deleted @ 08/24/23 11:58:14.354
  Aug 24 11:58:14.397: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11292"},"items":null}

  Aug 24 11:58:14.406: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11293"},"items":[{"metadata":{"name":"daemon-set-f6mjg","generateName":"daemon-set-","namespace":"daemonsets-9321","uid":"0bec6492-8936-4fa4-a043-dcf4ab1ff02a","resourceVersion":"11290","creationTimestamp":"2023-08-24T11:58:12Z","deletionTimestamp":"2023-08-24T11:58:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e44d3720-dece-40cb-8107-d7f66dcb1094","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T11:58:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e44d3720-dece-40cb-8107-d7f66dcb1094\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T11:58:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.7\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-4hgzx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-4hgzx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"quohp9aeph3i-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["quohp9aeph3i-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:12Z"}],"hostIP":"192.168.121.64","podIP":"10.233.65.7","podIPs":[{"ip":"10.233.65.7"}],"startTime":"2023-08-24T11:58:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T11:58:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://62f77d92dfa5f8d20d9d3b063e6fa54c5a01b4c8dfeb7b4ea02613b1a337c0fe","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-ksgpz","generateName":"daemon-set-","namespace":"daemonsets-9321","uid":"7740c80e-6a7a-49d8-b614-6f7181b5a038","resourceVersion":"11292","creationTimestamp":"2023-08-24T11:58:12Z","deletionTimestamp":"2023-08-24T11:58:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e44d3720-dece-40cb-8107-d7f66dcb1094","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T11:58:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e44d3720-dece-40cb-8107-d7f66dcb1094\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T11:58:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-w9tt5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-w9tt5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"quohp9aeph3i-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["quohp9aeph3i-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:12Z"}],"hostIP":"192.168.121.15","podIP":"10.233.66.139","podIPs":[{"ip":"10.233.66.139"}],"startTime":"2023-08-24T11:58:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T11:58:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://54555e72a5fb7d9afc02bb4836a6873004592decb1f661fc22008917278d5289","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qqbfm","generateName":"daemon-set-","namespace":"daemonsets-9321","uid":"98944b58-4449-467f-8bbd-7e93b559f3e0","resourceVersion":"11291","creationTimestamp":"2023-08-24T11:58:12Z","deletionTimestamp":"2023-08-24T11:58:44Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"e44d3720-dece-40cb-8107-d7f66dcb1094","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-24T11:58:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e44d3720-dece-40cb-8107-d7f66dcb1094\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-24T11:58:14Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9x5nz","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9x5nz","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"quohp9aeph3i-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["quohp9aeph3i-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:12Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:14Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:14Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-24T11:58:12Z"}],"hostIP":"192.168.121.19","podIP":"10.233.64.83","podIPs":[{"ip":"10.233.64.83"}],"startTime":"2023-08-24T11:58:12Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-24T11:58:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://b5303c1b38b96a21593d31768e4b3b3acf0ae95c581e473b640e9474640c4541","started":true}],"qosClass":"BestEffort"}}]}

  Aug 24 11:58:14.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9321" for this suite. @ 08/24/23 11:58:14.44
• [2.305 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 08/24/23 11:58:14.451
  Aug 24 11:58:14.451: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename proxy @ 08/24/23 11:58:14.453
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:14.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:14.499
  Aug 24 11:58:14.505: INFO: Creating pod...
  Aug 24 11:58:16.549: INFO: Creating service...
  Aug 24 11:58:16.571: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/pods/agnhost/proxy?method=DELETE
  Aug 24 11:58:16.586: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 24 11:58:16.586: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/pods/agnhost/proxy?method=OPTIONS
  Aug 24 11:58:16.594: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 24 11:58:16.594: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/pods/agnhost/proxy?method=PATCH
  Aug 24 11:58:16.602: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 24 11:58:16.602: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/pods/agnhost/proxy?method=POST
  Aug 24 11:58:16.609: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 24 11:58:16.609: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/pods/agnhost/proxy?method=PUT
  Aug 24 11:58:16.618: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 24 11:58:16.618: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/services/e2e-proxy-test-service/proxy?method=DELETE
  Aug 24 11:58:16.630: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug 24 11:58:16.630: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Aug 24 11:58:16.648: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug 24 11:58:16.648: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/services/e2e-proxy-test-service/proxy?method=PATCH
  Aug 24 11:58:16.657: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug 24 11:58:16.657: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/services/e2e-proxy-test-service/proxy?method=POST
  Aug 24 11:58:16.664: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug 24 11:58:16.665: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/services/e2e-proxy-test-service/proxy?method=PUT
  Aug 24 11:58:16.673: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug 24 11:58:16.673: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/pods/agnhost/proxy?method=GET
  Aug 24 11:58:16.677: INFO: http.Client request:GET StatusCode:301
  Aug 24 11:58:16.677: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/services/e2e-proxy-test-service/proxy?method=GET
  Aug 24 11:58:16.684: INFO: http.Client request:GET StatusCode:301
  Aug 24 11:58:16.684: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/pods/agnhost/proxy?method=HEAD
  Aug 24 11:58:16.688: INFO: http.Client request:HEAD StatusCode:301
  Aug 24 11:58:16.688: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-4450/services/e2e-proxy-test-service/proxy?method=HEAD
  Aug 24 11:58:16.695: INFO: http.Client request:HEAD StatusCode:301
  Aug 24 11:58:16.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-4450" for this suite. @ 08/24/23 11:58:16.704
• [2.269 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 08/24/23 11:58:16.724
  Aug 24 11:58:16.724: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 11:58:16.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:16.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:16.766
  STEP: creating a ServiceAccount @ 08/24/23 11:58:16.77
  STEP: watching for the ServiceAccount to be added @ 08/24/23 11:58:16.786
  STEP: patching the ServiceAccount @ 08/24/23 11:58:16.792
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 08/24/23 11:58:16.802
  STEP: deleting the ServiceAccount @ 08/24/23 11:58:16.807
  Aug 24 11:58:16.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8450" for this suite. @ 08/24/23 11:58:16.843
• [0.133 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 08/24/23 11:58:16.86
  Aug 24 11:58:16.860: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 11:58:16.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:16.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:16.909
  STEP: Creating simple DaemonSet "daemon-set" @ 08/24/23 11:58:16.958
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 11:58:16.974
  Aug 24 11:58:17.001: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 11:58:17.002: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 11:58:18.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 11:58:18.024: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 11:58:19.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 24 11:58:19.028: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 11:58:20.048: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 11:58:20.048: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 08/24/23 11:58:20.054
  Aug 24 11:58:20.062: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 08/24/23 11:58:20.062
  Aug 24 11:58:20.079: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 08/24/23 11:58:20.079
  Aug 24 11:58:20.083: INFO: Observed &DaemonSet event: ADDED
  Aug 24 11:58:20.083: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:58:20.083: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:58:20.084: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:58:20.084: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:58:20.084: INFO: Found daemon set daemon-set in namespace daemonsets-7277 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 24 11:58:20.085: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 08/24/23 11:58:20.085
  STEP: watching for the daemon set status to be patched @ 08/24/23 11:58:20.099
  Aug 24 11:58:20.103: INFO: Observed &DaemonSet event: ADDED
  Aug 24 11:58:20.104: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:58:20.104: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:58:20.105: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:58:20.106: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:58:20.106: INFO: Observed daemon set daemon-set in namespace daemonsets-7277 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 24 11:58:20.107: INFO: Observed &DaemonSet event: MODIFIED
  Aug 24 11:58:20.107: INFO: Found daemon set daemon-set in namespace daemonsets-7277 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Aug 24 11:58:20.107: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 11:58:20.115
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7277, will wait for the garbage collector to delete the pods @ 08/24/23 11:58:20.116
  Aug 24 11:58:20.192: INFO: Deleting DaemonSet.extensions daemon-set took: 15.914562ms
  Aug 24 11:58:20.293: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.601453ms
  Aug 24 11:58:22.401: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 11:58:22.401: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 11:58:22.408: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"11506"},"items":null}

  Aug 24 11:58:22.415: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"11506"},"items":null}

  Aug 24 11:58:22.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7277" for this suite. @ 08/24/23 11:58:22.466
• [5.624 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 08/24/23 11:58:22.485
  Aug 24 11:58:22.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename watch @ 08/24/23 11:58:22.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:22.539
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:22.549
  STEP: creating a watch on configmaps @ 08/24/23 11:58:22.566
  STEP: creating a new configmap @ 08/24/23 11:58:22.568
  STEP: modifying the configmap once @ 08/24/23 11:58:22.576
  STEP: closing the watch once it receives two notifications @ 08/24/23 11:58:22.604
  Aug 24 11:58:22.604: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9423  b4949b4f-5a07-4480-9aa0-0ec016d970ff 11515 0 2023-08-24 11:58:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 11:58:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:58:22.604: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9423  b4949b4f-5a07-4480-9aa0-0ec016d970ff 11516 0 2023-08-24 11:58:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 11:58:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 08/24/23 11:58:22.604
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 08/24/23 11:58:22.626
  STEP: deleting the configmap @ 08/24/23 11:58:22.628
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 08/24/23 11:58:22.639
  Aug 24 11:58:22.639: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9423  b4949b4f-5a07-4480-9aa0-0ec016d970ff 11519 0 2023-08-24 11:58:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 11:58:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:58:22.639: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-9423  b4949b4f-5a07-4480-9aa0-0ec016d970ff 11520 0 2023-08-24 11:58:22 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-24 11:58:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 11:58:22.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9423" for this suite. @ 08/24/23 11:58:22.65
• [0.178 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 08/24/23 11:58:22.665
  Aug 24 11:58:22.665: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename job @ 08/24/23 11:58:22.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:22.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:22.7
  STEP: Creating a job @ 08/24/23 11:58:22.705
  STEP: Ensuring active pods == parallelism @ 08/24/23 11:58:22.719
  STEP: delete a job @ 08/24/23 11:58:24.731
  STEP: deleting Job.batch foo in namespace job-288, will wait for the garbage collector to delete the pods @ 08/24/23 11:58:24.731
  Aug 24 11:58:24.804: INFO: Deleting Job.batch foo took: 15.872306ms
  Aug 24 11:58:24.904: INFO: Terminating Job.batch foo pods took: 100.922655ms
  STEP: Ensuring job was deleted @ 08/24/23 11:58:56.506
  Aug 24 11:58:56.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-288" for this suite. @ 08/24/23 11:58:56.528
• [33.874 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 08/24/23 11:58:56.541
  Aug 24 11:58:56.541: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 08/24/23 11:58:56.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:58:56.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:58:56.58
  STEP: Setting up the test @ 08/24/23 11:58:56.585
  STEP: Creating hostNetwork=false pod @ 08/24/23 11:58:56.585
  STEP: Creating hostNetwork=true pod @ 08/24/23 11:58:58.639
  STEP: Running the test @ 08/24/23 11:59:00.672
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 08/24/23 11:59:00.672
  Aug 24 11:59:00.672: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:00.672: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:00.674: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:00.674: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 24 11:59:00.822: INFO: Exec stderr: ""
  Aug 24 11:59:00.822: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:00.822: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:00.823: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:00.824: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 24 11:59:00.963: INFO: Exec stderr: ""
  Aug 24 11:59:00.963: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:00.964: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:00.965: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:00.966: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 24 11:59:01.092: INFO: Exec stderr: ""
  Aug 24 11:59:01.092: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:01.092: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:01.094: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:01.095: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 24 11:59:01.204: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 08/24/23 11:59:01.204
  Aug 24 11:59:01.204: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:01.204: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:01.205: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:01.205: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug 24 11:59:01.356: INFO: Exec stderr: ""
  Aug 24 11:59:01.356: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:01.357: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:01.358: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:01.358: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug 24 11:59:01.489: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 08/24/23 11:59:01.49
  Aug 24 11:59:01.490: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:01.491: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:01.493: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:01.493: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 24 11:59:01.601: INFO: Exec stderr: ""
  Aug 24 11:59:01.601: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:01.601: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:01.603: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:01.603: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug 24 11:59:01.697: INFO: Exec stderr: ""
  Aug 24 11:59:01.698: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:01.698: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:01.699: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:01.700: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 24 11:59:01.820: INFO: Exec stderr: ""
  Aug 24 11:59:01.820: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-3315 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 11:59:01.820: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 11:59:01.821: INFO: ExecWithOptions: Clientset creation
  Aug 24 11:59:01.822: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-3315/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug 24 11:59:01.959: INFO: Exec stderr: ""
  Aug 24 11:59:01.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-3315" for this suite. @ 08/24/23 11:59:01.969
• [5.446 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 08/24/23 11:59:01.999
  Aug 24 11:59:01.999: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 11:59:02.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 11:59:02.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 11:59:02.047
  STEP: creating the pod with failed condition @ 08/24/23 11:59:02.054
  STEP: updating the pod @ 08/24/23 12:01:02.093
  Aug 24 12:01:02.626: INFO: Successfully updated pod "var-expansion-c40a9d04-e60b-4cd8-9fd6-0f8a79d37b80"
  STEP: waiting for pod running @ 08/24/23 12:01:02.626
  STEP: deleting the pod gracefully @ 08/24/23 12:01:04.644
  Aug 24 12:01:04.644: INFO: Deleting pod "var-expansion-c40a9d04-e60b-4cd8-9fd6-0f8a79d37b80" in namespace "var-expansion-7170"
  Aug 24 12:01:04.659: INFO: Wait up to 5m0s for pod "var-expansion-c40a9d04-e60b-4cd8-9fd6-0f8a79d37b80" to be fully deleted
  Aug 24 12:01:36.823: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7170" for this suite. @ 08/24/23 12:01:36.837
• [154.861 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 08/24/23 12:01:36.861
  Aug 24 12:01:36.861: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 12:01:36.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:01:36.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:01:36.93
  STEP: reading a file in the container @ 08/24/23 12:01:38.988
  Aug 24 12:01:38.988: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2947 pod-service-account-4362501b-4f1f-4a7e-8391-f126d0ef580a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 08/24/23 12:01:39.263
  Aug 24 12:01:39.263: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2947 pod-service-account-4362501b-4f1f-4a7e-8391-f126d0ef580a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 08/24/23 12:01:39.5
  Aug 24 12:01:39.501: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-2947 pod-service-account-4362501b-4f1f-4a7e-8391-f126d0ef580a -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Aug 24 12:01:39.741: INFO: Got root ca configmap in namespace "svcaccounts-2947"
  Aug 24 12:01:39.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2947" for this suite. @ 08/24/23 12:01:39.755
• [2.913 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 08/24/23 12:01:39.777
  Aug 24 12:01:39.778: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:01:39.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:01:39.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:01:39.829
  Aug 24 12:01:39.834: INFO: Creating simple deployment test-new-deployment
  Aug 24 12:01:39.878: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 08/24/23 12:01:41.913
  STEP: updating a scale subresource @ 08/24/23 12:01:41.921
  STEP: verifying the deployment Spec.Replicas was modified @ 08/24/23 12:01:41.935
  STEP: Patch a scale subresource @ 08/24/23 12:01:41.942
  Aug 24 12:01:41.976: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-9591  111479af-c96c-41c2-8db6-3ba435ed4d24 12198 3 2023-08-24 12:01:39 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-24 12:01:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:01:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002245c58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:01:41 +0000 UTC,LastTransitionTime:2023-08-24 12:01:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-08-24 12:01:41 +0000 UTC,LastTransitionTime:2023-08-24 12:01:39 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 24 12:01:41.986: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-9591  661af1d3-cf98-42e9-8851-825d7c8ae463 12199 2 2023-08-24 12:01:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 111479af-c96c-41c2-8db6-3ba435ed4d24 0xc00205a077 0xc00205a078}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:01:41 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"111479af-c96c-41c2-8db6-3ba435ed4d24\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:01:41 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00205a108 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:01:42.002: INFO: Pod "test-new-deployment-67bd4bf6dc-8jjx5" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-8jjx5 test-new-deployment-67bd4bf6dc- deployment-9591  a1a9a725-3a27-4351-897b-2b322a0b6c07 12192 0 2023-08-24 12:01:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 661af1d3-cf98-42e9-8851-825d7c8ae463 0xc00205a5f7 0xc00205a5f8}] [] [{kube-controller-manager Update v1 2023-08-24 12:01:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"661af1d3-cf98-42e9-8851-825d7c8ae463\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:01:40 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mglb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mglb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:01:39 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:01:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:01:40 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:01:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:10.233.66.24,StartTime:2023-08-24 12:01:39 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:01:40 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b99e0a31c254382864e86fb4d195d9d476dd7a8c6b22e4898db0a7c2be3149fd,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.24,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:01:42.003: INFO: Pod "test-new-deployment-67bd4bf6dc-kg6vr" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-kg6vr test-new-deployment-67bd4bf6dc- deployment-9591  d3cfc443-5eff-4283-a81a-25b39370358d 12200 0 2023-08-24 12:01:42 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 661af1d3-cf98-42e9-8851-825d7c8ae463 0xc00205a847 0xc00205a848}] [] [{kube-controller-manager Update v1 2023-08-24 12:01:42 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"661af1d3-cf98-42e9-8851-825d7c8ae463\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jfsgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jfsgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:01:42.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9591" for this suite. @ 08/24/23 12:01:42.02
• [2.274 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 08/24/23 12:01:42.055
  Aug 24 12:01:42.055: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:01:42.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:01:42.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:01:42.14
  STEP: creating the pod @ 08/24/23 12:01:42.145
  STEP: submitting the pod to kubernetes @ 08/24/23 12:01:42.146
  STEP: verifying the pod is in kubernetes @ 08/24/23 12:01:44.194
  STEP: updating the pod @ 08/24/23 12:01:44.201
  Aug 24 12:01:44.721: INFO: Successfully updated pod "pod-update-fa0f4a2f-6ecc-4791-a466-7fd6c90c0278"
  STEP: verifying the updated pod is in kubernetes @ 08/24/23 12:01:44.736
  Aug 24 12:01:44.751: INFO: Pod update OK
  Aug 24 12:01:44.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7321" for this suite. @ 08/24/23 12:01:44.763
• [2.720 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 08/24/23 12:01:44.776
  Aug 24 12:01:44.776: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename subpath @ 08/24/23 12:01:44.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:01:44.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:01:44.843
  STEP: Setting up data @ 08/24/23 12:01:44.848
  STEP: Creating pod pod-subpath-test-projected-zgd8 @ 08/24/23 12:01:44.865
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 12:01:44.866
  STEP: Saw pod success @ 08/24/23 12:02:09.006
  Aug 24 12:02:09.013: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-subpath-test-projected-zgd8 container test-container-subpath-projected-zgd8: <nil>
  STEP: delete the pod @ 08/24/23 12:02:09.048
  STEP: Deleting pod pod-subpath-test-projected-zgd8 @ 08/24/23 12:02:09.077
  Aug 24 12:02:09.078: INFO: Deleting pod "pod-subpath-test-projected-zgd8" in namespace "subpath-8396"
  Aug 24 12:02:09.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8396" for this suite. @ 08/24/23 12:02:09.095
• [24.334 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 08/24/23 12:02:09.124
  Aug 24 12:02:09.124: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:02:09.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:02:09.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:02:09.178
  STEP: Creating projection with secret that has name projected-secret-test-map-c0f7216f-6727-4176-b552-f742faae7c37 @ 08/24/23 12:02:09.183
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:02:09.224
  STEP: Saw pod success @ 08/24/23 12:02:13.272
  Aug 24 12:02:13.280: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-secrets-0b68eb61-253f-4a47-a8e5-a7f1d99fcf90 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:02:13.295
  Aug 24 12:02:13.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4713" for this suite. @ 08/24/23 12:02:13.331
• [4.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 08/24/23 12:02:13.35
  Aug 24 12:02:13.350: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:02:13.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:02:13.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:02:13.397
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/24/23 12:02:13.403
  STEP: Saw pod success @ 08/24/23 12:02:17.456
  Aug 24 12:02:17.462: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-1aba145f-be1a-4d50-816d-7f5654cbd647 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:02:17.484
  Aug 24 12:02:17.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9544" for this suite. @ 08/24/23 12:02:17.526
• [4.190 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 08/24/23 12:02:17.544
  Aug 24 12:02:17.545: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:02:17.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:02:17.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:02:17.585
  STEP: Creating a ResourceQuota with terminating scope @ 08/24/23 12:02:17.591
  STEP: Ensuring ResourceQuota status is calculated @ 08/24/23 12:02:17.6
  STEP: Creating a ResourceQuota with not terminating scope @ 08/24/23 12:02:19.609
  STEP: Ensuring ResourceQuota status is calculated @ 08/24/23 12:02:19.617
  STEP: Creating a long running pod @ 08/24/23 12:02:21.625
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 08/24/23 12:02:21.648
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 08/24/23 12:02:23.658
  STEP: Deleting the pod @ 08/24/23 12:02:25.666
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 12:02:25.684
  STEP: Creating a terminating pod @ 08/24/23 12:02:27.692
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 08/24/23 12:02:27.717
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 08/24/23 12:02:29.728
  STEP: Deleting the pod @ 08/24/23 12:02:31.738
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 12:02:31.78
  Aug 24 12:02:33.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2602" for this suite. @ 08/24/23 12:02:33.8
• [16.272 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 08/24/23 12:02:33.817
  Aug 24 12:02:33.817: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:02:33.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:02:33.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:02:33.856
  STEP: creating a ConfigMap @ 08/24/23 12:02:33.862
  STEP: fetching the ConfigMap @ 08/24/23 12:02:33.873
  STEP: patching the ConfigMap @ 08/24/23 12:02:33.882
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 08/24/23 12:02:33.901
  STEP: deleting the ConfigMap by collection with a label selector @ 08/24/23 12:02:33.915
  STEP: listing all ConfigMaps in test namespace @ 08/24/23 12:02:33.936
  Aug 24 12:02:33.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9993" for this suite. @ 08/24/23 12:02:33.955
• [0.152 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 08/24/23 12:02:33.973
  Aug 24 12:02:33.973: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:02:33.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:02:34.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:02:34.016
  Aug 24 12:02:34.020: INFO: Creating deployment "test-recreate-deployment"
  Aug 24 12:02:34.030: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Aug 24 12:02:34.043: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  Aug 24 12:02:36.063: INFO: Waiting deployment "test-recreate-deployment" to complete
  Aug 24 12:02:36.069: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Aug 24 12:02:36.088: INFO: Updating deployment test-recreate-deployment
  Aug 24 12:02:36.089: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Aug 24 12:02:36.301: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3418  e7d7ecc3-7ad4-4181-b57b-2b279f54d460 12570 2 2023-08-24 12:02:34 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 12:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005468f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 12:02:36 +0000 UTC,LastTransitionTime:2023-08-24 12:02:36 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-08-24 12:02:36 +0000 UTC,LastTransitionTime:2023-08-24 12:02:34 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Aug 24 12:02:36.308: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-3418  292015dc-3bc9-4a55-9fe9-cbfc3c422f88 12569 1 2023-08-24 12:02:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment e7d7ecc3-7ad4-4181-b57b-2b279f54d460 0xc0050c3577 0xc0050c3578}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7d7ecc3-7ad4-4181-b57b-2b279f54d460\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:02:36 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050c3618 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:02:36.308: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Aug 24 12:02:36.309: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-3418  88a39610-c21a-4b33-821e-c7b66264c692 12557 2 2023-08-24 12:02:34 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment e7d7ecc3-7ad4-4181-b57b-2b279f54d460 0xc0050c3687 0xc0050c3688}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e7d7ecc3-7ad4-4181-b57b-2b279f54d460\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:02:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0050c3738 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:02:36.315: INFO: Pod "test-recreate-deployment-54757ffd6c-phl4v" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-phl4v test-recreate-deployment-54757ffd6c- deployment-3418  63260141-5fe1-48c2-86e2-88c2c9aed238 12571 0 2023-08-24 12:02:36 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 292015dc-3bc9-4a55-9fe9-cbfc3c422f88 0xc0050c3c17 0xc0050c3c18}] [] [{kube-controller-manager Update v1 2023-08-24 12:02:36 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"292015dc-3bc9-4a55-9fe9-cbfc3c422f88\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:02:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b64bz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b64bz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:02:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:02:36 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:02:36 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 12:02:36 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:02:36.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3418" for this suite. @ 08/24/23 12:02:36.324
• [2.364 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:96
  STEP: Creating a kubernetes client @ 08/24/23 12:02:36.341
  Aug 24 12:02:36.341: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename aggregator @ 08/24/23 12:02:36.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:02:36.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:02:36.385
  Aug 24 12:02:36.390: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Registering the sample API server. @ 08/24/23 12:02:36.393
  Aug 24 12:02:36.983: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Aug 24 12:02:37.034: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  Aug 24 12:02:39.121: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:41.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:43.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:45.133: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:47.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:49.131: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:51.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:53.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:55.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:57.130: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:02:59.132: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:03:01.129: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 2, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Aug 24 12:03:03.275: INFO: Waited 130.624998ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 08/24/23 12:03:03.349
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 08/24/23 12:03:03.361
  STEP: List APIServices @ 08/24/23 12:03:03.379
  Aug 24 12:03:03.396: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 08/24/23 12:03:03.396
  Aug 24 12:03:03.422: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 08/24/23 12:03:03.422
  Aug 24 12:03:03.443: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.August, 24, 12, 3, 3, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 08/24/23 12:03:03.444
  Aug 24 12:03:03.452: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-08-24 12:03:03 +0000 UTC Passed all checks passed}
  Aug 24 12:03:03.452: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:03:03.453: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 08/24/23 12:03:03.453
  Aug 24 12:03:03.474: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1470005896" @ 08/24/23 12:03:03.475
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 08/24/23 12:03:03.503
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 08/24/23 12:03:03.516
  STEP: Patch APIService Status @ 08/24/23 12:03:03.528
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 08/24/23 12:03:03.543
  Aug 24 12:03:03.554: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-08-24 12:03:03 +0000 UTC Passed all checks passed}
  Aug 24 12:03:03.555: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:03:03.555: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Aug 24 12:03:03.555: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 08/24/23 12:03:03.556
  STEP: Confirm that the generated APIService has been deleted @ 08/24/23 12:03:03.567
  Aug 24 12:03:03.568: INFO: Requesting list of APIServices to confirm quantity
  Aug 24 12:03:03.579: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Aug 24 12:03:03.580: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Aug 24 12:03:03.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-8154" for this suite. @ 08/24/23 12:03:03.822
• [27.491 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 08/24/23 12:03:03.837
  Aug 24 12:03:03.838: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:03:03.841
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:03.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:03.876
  STEP: Creating secret with name secret-test-e917c3d8-9920-4679-9ae0-d6377d29fbf9 @ 08/24/23 12:03:03.879
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:03:03.894
  STEP: Saw pod success @ 08/24/23 12:03:07.949
  Aug 24 12:03:07.955: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-secrets-f2fb217c-d14b-44b9-8195-c722e2be2fa5 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:03:07.973
  Aug 24 12:03:08.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4276" for this suite. @ 08/24/23 12:03:08.025
• [4.201 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 08/24/23 12:03:08.043
  Aug 24 12:03:08.043: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:03:08.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:08.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:08.092
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/24/23 12:03:08.099
  STEP: Saw pod success @ 08/24/23 12:03:12.143
  Aug 24 12:03:12.150: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-fabe6295-8c57-4345-8744-b07e51d1aef2 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:03:12.163
  Aug 24 12:03:12.193: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4866" for this suite. @ 08/24/23 12:03:12.202
• [4.169 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 08/24/23 12:03:12.215
  Aug 24 12:03:12.215: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:03:12.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:12.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:12.259
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 08/24/23 12:03:12.264
  Aug 24 12:03:12.265: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 08/24/23 12:03:19.993
  Aug 24 12:03:19.995: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:03:22.266: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:03:30.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2250" for this suite. @ 08/24/23 12:03:30.571
• [18.374 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 08/24/23 12:03:30.592
  Aug 24 12:03:30.592: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 08/24/23 12:03:30.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:30.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:30.632
  STEP: creating a target pod @ 08/24/23 12:03:30.638
  STEP: adding an ephemeral container @ 08/24/23 12:03:32.679
  STEP: checking pod container endpoints @ 08/24/23 12:03:34.715
  Aug 24 12:03:34.716: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3994 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:03:34.716: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:03:34.718: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:03:34.718: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-3994/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Aug 24 12:03:34.867: INFO: Exec stderr: ""
  Aug 24 12:03:34.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-3994" for this suite. @ 08/24/23 12:03:34.911
• [4.329 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 08/24/23 12:03:34.926
  Aug 24 12:03:34.926: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:03:34.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:34.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:34.964
  STEP: Creating configMap with name configmap-test-volume-c1e2d1d1-f986-41b4-981b-7c8fe5a9e1b7 @ 08/24/23 12:03:34.969
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:03:34.982
  STEP: Saw pod success @ 08/24/23 12:03:39.029
  Aug 24 12:03:39.036: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-434f7bbe-1e05-421e-ad1b-146c80e93151 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:03:39.05
  Aug 24 12:03:39.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7083" for this suite. @ 08/24/23 12:03:39.083
• [4.169 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 08/24/23 12:03:39.097
  Aug 24 12:03:39.098: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:03:39.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:39.15
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:39.155
  STEP: create deployment with httpd image @ 08/24/23 12:03:39.162
  Aug 24 12:03:39.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1434 create -f -'
  Aug 24 12:03:39.838: INFO: stderr: ""
  Aug 24 12:03:39.838: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 08/24/23 12:03:39.839
  Aug 24 12:03:39.839: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1434 diff -f -'
  Aug 24 12:03:40.502: INFO: rc: 1
  Aug 24 12:03:40.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1434 delete -f -'
  Aug 24 12:03:40.720: INFO: stderr: ""
  Aug 24 12:03:40.720: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Aug 24 12:03:40.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1434" for this suite. @ 08/24/23 12:03:40.735
• [1.660 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 08/24/23 12:03:40.759
  Aug 24 12:03:40.759: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-pred @ 08/24/23 12:03:40.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:03:40.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:03:40.814
  Aug 24 12:03:40.820: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 24 12:03:40.854: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:03:40.862: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-1 before test
  Aug 24 12:03:40.882: INFO: cilium-2mlv4 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.882: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: cilium-node-init-wh7d5 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.882: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: coredns-5d78c9869d-xp68m from kube-system started at 2023-08-24 11:23:49 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.882: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: kube-addon-manager-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.882: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: kube-apiserver-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.882: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: kube-controller-manager-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.882: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: kube-proxy-ljqfg from kube-system started at 2023-08-24 11:21:03 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.882: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: kube-scheduler-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.882: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-5c6ln from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:03:40.882: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:03:40.882: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-2 before test
  Aug 24 12:03:40.901: INFO: cilium-krjld from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: cilium-node-init-ldwv5 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: cilium-operator-b8f479cd9-78ztl from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container cilium-operator ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: coredns-5d78c9869d-bvl77 from kube-system started at 2023-08-24 11:23:40 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: kube-addon-manager-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: kube-apiserver-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: kube-controller-manager-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: kube-proxy-dr4nl from kube-system started at 2023-08-24 11:21:33 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: kube-scheduler-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-bccng from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:03:40.901: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:03:40.901: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-3 before test
  Aug 24 12:03:40.921: INFO: ephemeral-containers-target-pod from ephemeral-containers-test-3994 started at 2023-08-24 12:03:30 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.921: INFO: 	Container test-container-1 ready: true, restart count 0
  Aug 24 12:03:40.921: INFO: cilium-node-init-hkrgc from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.921: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:03:40.921: INFO: cilium-qwwtg from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.921: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:03:40.921: INFO: kube-proxy-9cptc from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.921: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:03:40.921: INFO: httpd-deployment-5cd84d4f9-hjbc9 from kubectl-1434 started at 2023-08-24 12:03:39 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.921: INFO: 	Container httpd ready: false, restart count 0
  Aug 24 12:03:40.921: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:22 +0000 UTC (1 container statuses recorded)
  Aug 24 12:03:40.921: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 24 12:03:40.921: INFO: sonobuoy-e2e-job-06f57e3e53464656 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:03:40.921: INFO: 	Container e2e ready: true, restart count 0
  Aug 24 12:03:40.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:03:40.921: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-wfrkl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:03:40.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:03:40.921: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/24/23 12:03:40.921
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/24/23 12:03:42.985
  STEP: Trying to apply a random label on the found node. @ 08/24/23 12:03:43.017
  STEP: verifying the node has the label kubernetes.io/e2e-3c3248ff-6b3e-4420-b75d-02e9672ea99e 95 @ 08/24/23 12:03:43.039
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 08/24/23 12:03:43.057
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.15 on the node which pod4 resides and expect not scheduled @ 08/24/23 12:03:45.085
  STEP: removing the label kubernetes.io/e2e-3c3248ff-6b3e-4420-b75d-02e9672ea99e off the node quohp9aeph3i-3 @ 08/24/23 12:08:45.1
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-3c3248ff-6b3e-4420-b75d-02e9672ea99e @ 08/24/23 12:08:45.13
  Aug 24 12:08:45.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6445" for this suite. @ 08/24/23 12:08:45.169
• [304.445 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 08/24/23 12:08:45.213
  Aug 24 12:08:45.213: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:08:45.217
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:08:45.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:08:45.269
  STEP: Counting existing ResourceQuota @ 08/24/23 12:08:45.273
  STEP: Creating a ResourceQuota @ 08/24/23 12:08:50.283
  STEP: Ensuring resource quota status is calculated @ 08/24/23 12:08:50.291
  STEP: Creating a ReplicaSet @ 08/24/23 12:08:52.301
  STEP: Ensuring resource quota status captures replicaset creation @ 08/24/23 12:08:52.323
  STEP: Deleting a ReplicaSet @ 08/24/23 12:08:54.33
  STEP: Ensuring resource quota status released usage @ 08/24/23 12:08:54.344
  Aug 24 12:08:56.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5871" for this suite. @ 08/24/23 12:08:56.363
• [11.162 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 08/24/23 12:08:56.377
  Aug 24 12:08:56.377: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-pred @ 08/24/23 12:08:56.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:08:56.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:08:56.414
  Aug 24 12:08:56.418: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 24 12:08:56.433: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:08:56.441: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-1 before test
  Aug 24 12:08:56.460: INFO: cilium-2mlv4 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.461: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:08:56.461: INFO: cilium-node-init-wh7d5 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.462: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:08:56.462: INFO: coredns-5d78c9869d-xp68m from kube-system started at 2023-08-24 11:23:49 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.463: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:08:56.463: INFO: kube-addon-manager-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.463: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:08:56.464: INFO: kube-apiserver-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.464: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:08:56.465: INFO: kube-controller-manager-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.465: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:08:56.465: INFO: kube-proxy-ljqfg from kube-system started at 2023-08-24 11:21:03 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.466: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:08:56.466: INFO: kube-scheduler-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.467: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:08:56.467: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-5c6ln from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:08:56.467: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:08:56.468: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:08:56.468: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-2 before test
  Aug 24 12:08:56.484: INFO: cilium-krjld from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: cilium-node-init-ldwv5 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: cilium-operator-b8f479cd9-78ztl from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container cilium-operator ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: coredns-5d78c9869d-bvl77 from kube-system started at 2023-08-24 11:23:40 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: kube-addon-manager-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: kube-apiserver-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: kube-controller-manager-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: kube-proxy-dr4nl from kube-system started at 2023-08-24 11:21:33 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: kube-scheduler-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-bccng from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:08:56.485: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:08:56.485: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-3 before test
  Aug 24 12:08:56.499: INFO: cilium-node-init-hkrgc from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.500: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:08:56.500: INFO: cilium-qwwtg from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.501: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:08:56.501: INFO: kube-proxy-9cptc from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.502: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:08:56.502: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:22 +0000 UTC (1 container statuses recorded)
  Aug 24 12:08:56.502: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 24 12:08:56.503: INFO: sonobuoy-e2e-job-06f57e3e53464656 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:08:56.503: INFO: 	Container e2e ready: true, restart count 0
  Aug 24 12:08:56.503: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:08:56.504: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-wfrkl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:08:56.504: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:08:56.505: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 08/24/23 12:08:56.505
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.177e506c6faf721e], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 08/24/23 12:08:56.579
  Aug 24 12:08:57.564: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4779" for this suite. @ 08/24/23 12:08:57.575
• [1.211 seconds]
------------------------------
SSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 08/24/23 12:08:57.591
  Aug 24 12:08:57.591: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:08:57.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:08:57.624
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:08:57.629
  STEP: Creating projection with secret that has name secret-emptykey-test-6b05a408-6b66-4730-a424-83d6937df139 @ 08/24/23 12:08:57.633
  Aug 24 12:08:57.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7911" for this suite. @ 08/24/23 12:08:57.649
• [0.074 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 08/24/23 12:08:57.665
  Aug 24 12:08:57.665: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:08:57.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:08:57.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:08:57.7
  STEP: Creating resourceQuota "e2e-rq-status-cvlpw" @ 08/24/23 12:08:57.711
  Aug 24 12:08:57.741: INFO: Resource quota "e2e-rq-status-cvlpw" reports spec: hard cpu limit of 500m
  Aug 24 12:08:57.741: INFO: Resource quota "e2e-rq-status-cvlpw" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-cvlpw" /status @ 08/24/23 12:08:57.741
  STEP: Confirm /status for "e2e-rq-status-cvlpw" resourceQuota via watch @ 08/24/23 12:08:57.757
  Aug 24 12:08:57.762: INFO: observed resourceQuota "e2e-rq-status-cvlpw" in namespace "resourcequota-1902" with hard status: v1.ResourceList(nil)
  Aug 24 12:08:57.763: INFO: Found resourceQuota "e2e-rq-status-cvlpw" in namespace "resourcequota-1902" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug 24 12:08:57.763: INFO: ResourceQuota "e2e-rq-status-cvlpw" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 08/24/23 12:08:57.77
  Aug 24 12:08:57.782: INFO: Resource quota "e2e-rq-status-cvlpw" reports spec: hard cpu limit of 1
  Aug 24 12:08:57.782: INFO: Resource quota "e2e-rq-status-cvlpw" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-cvlpw" /status @ 08/24/23 12:08:57.782
  STEP: Confirm /status for "e2e-rq-status-cvlpw" resourceQuota via watch @ 08/24/23 12:08:57.797
  Aug 24 12:08:57.801: INFO: observed resourceQuota "e2e-rq-status-cvlpw" in namespace "resourcequota-1902" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug 24 12:08:57.801: INFO: Found resourceQuota "e2e-rq-status-cvlpw" in namespace "resourcequota-1902" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Aug 24 12:08:57.801: INFO: ResourceQuota "e2e-rq-status-cvlpw" /status was patched
  STEP: Get "e2e-rq-status-cvlpw" /status @ 08/24/23 12:08:57.802
  Aug 24 12:08:57.810: INFO: Resourcequota "e2e-rq-status-cvlpw" reports status: hard cpu of 1
  Aug 24 12:08:57.811: INFO: Resourcequota "e2e-rq-status-cvlpw" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-cvlpw" /status before checking Spec is unchanged @ 08/24/23 12:08:57.818
  Aug 24 12:08:57.825: INFO: Resourcequota "e2e-rq-status-cvlpw" reports status: hard cpu of 2
  Aug 24 12:08:57.826: INFO: Resourcequota "e2e-rq-status-cvlpw" reports status: hard memory of 2Gi
  Aug 24 12:08:57.829: INFO: Found resourceQuota "e2e-rq-status-cvlpw" in namespace "resourcequota-1902" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Aug 24 12:11:07.842: INFO: ResourceQuota "e2e-rq-status-cvlpw" Spec was unchanged and /status reset
  Aug 24 12:11:07.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1902" for this suite. @ 08/24/23 12:11:07.856
• [130.204 seconds]
------------------------------
SSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 08/24/23 12:11:07.872
  Aug 24 12:11:07.872: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename disruption @ 08/24/23 12:11:07.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:11:07.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:11:07.946
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:11:07.961
  STEP: Waiting for all pods to be running @ 08/24/23 12:11:08.032
  Aug 24 12:11:08.045: INFO: running pods: 0 < 3
  Aug 24 12:11:10.055: INFO: running pods: 0 < 3
  Aug 24 12:11:12.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1491" for this suite. @ 08/24/23 12:11:12.065
• [4.202 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 08/24/23 12:11:12.076
  Aug 24 12:11:12.076: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename job @ 08/24/23 12:11:12.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:11:12.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:11:12.11
  STEP: Creating a job @ 08/24/23 12:11:12.114
  STEP: Ensuring job reaches completions @ 08/24/23 12:11:12.126
  Aug 24 12:11:24.134: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9598" for this suite. @ 08/24/23 12:11:24.144
• [12.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 08/24/23 12:11:24.165
  Aug 24 12:11:24.165: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename dns @ 08/24/23 12:11:24.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:11:24.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:11:24.199
  STEP: Creating a test headless service @ 08/24/23 12:11:24.203
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6107 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6107;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6107 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6107;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6107.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6107.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6107.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6107.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6107.svc;check="$$(dig +notcp +noall +answer +search 142.12.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.12.142_udp@PTR;check="$$(dig +tcp +noall +answer +search 142.12.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.12.142_tcp@PTR;sleep 1; done
   @ 08/24/23 12:11:24.24
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6107 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6107;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6107 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6107;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6107.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6107.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6107.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6107.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6107.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6107.svc;check="$$(dig +notcp +noall +answer +search 142.12.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.12.142_udp@PTR;check="$$(dig +tcp +noall +answer +search 142.12.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.12.142_tcp@PTR;sleep 1; done
   @ 08/24/23 12:11:24.241
  STEP: creating a pod to probe DNS @ 08/24/23 12:11:24.241
  STEP: submitting the pod to kubernetes @ 08/24/23 12:11:24.244
  STEP: retrieving the pod @ 08/24/23 12:11:28.31
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:11:28.315
  Aug 24 12:11:28.326: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.334: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.340: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.347: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.353: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.358: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.367: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.376: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.411: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.421: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.430: INFO: Unable to read jessie_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.440: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.446: INFO: Unable to read jessie_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.453: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.459: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.466: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:28.490: INFO: Lookups using dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6107 wheezy_tcp@dns-test-service.dns-6107 wheezy_udp@dns-test-service.dns-6107.svc wheezy_tcp@dns-test-service.dns-6107.svc wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6107 jessie_tcp@dns-test-service.dns-6107 jessie_udp@dns-test-service.dns-6107.svc jessie_tcp@dns-test-service.dns-6107.svc jessie_udp@_http._tcp.dns-test-service.dns-6107.svc jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc]

  Aug 24 12:11:33.500: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.509: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.521: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.528: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.536: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.544: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.551: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.562: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.600: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.608: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.616: INFO: Unable to read jessie_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.623: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.631: INFO: Unable to read jessie_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.638: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.648: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.654: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:33.680: INFO: Lookups using dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6107 wheezy_tcp@dns-test-service.dns-6107 wheezy_udp@dns-test-service.dns-6107.svc wheezy_tcp@dns-test-service.dns-6107.svc wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6107 jessie_tcp@dns-test-service.dns-6107 jessie_udp@dns-test-service.dns-6107.svc jessie_tcp@dns-test-service.dns-6107.svc jessie_udp@_http._tcp.dns-test-service.dns-6107.svc jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc]

  Aug 24 12:11:38.501: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.510: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.519: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.526: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.535: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.544: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.551: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.566: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.613: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.620: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.627: INFO: Unable to read jessie_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.635: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.643: INFO: Unable to read jessie_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.649: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.657: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.667: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:38.698: INFO: Lookups using dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6107 wheezy_tcp@dns-test-service.dns-6107 wheezy_udp@dns-test-service.dns-6107.svc wheezy_tcp@dns-test-service.dns-6107.svc wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6107 jessie_tcp@dns-test-service.dns-6107 jessie_udp@dns-test-service.dns-6107.svc jessie_tcp@dns-test-service.dns-6107.svc jessie_udp@_http._tcp.dns-test-service.dns-6107.svc jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc]

  Aug 24 12:11:43.499: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.504: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.509: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.516: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.522: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.528: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.534: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.540: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.585: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.594: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.602: INFO: Unable to read jessie_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.612: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.623: INFO: Unable to read jessie_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.635: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.648: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.654: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:43.718: INFO: Lookups using dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6107 wheezy_tcp@dns-test-service.dns-6107 wheezy_udp@dns-test-service.dns-6107.svc wheezy_tcp@dns-test-service.dns-6107.svc wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6107 jessie_tcp@dns-test-service.dns-6107 jessie_udp@dns-test-service.dns-6107.svc jessie_tcp@dns-test-service.dns-6107.svc jessie_udp@_http._tcp.dns-test-service.dns-6107.svc jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc]

  Aug 24 12:11:48.500: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.505: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.518: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.524: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.529: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.534: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.542: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.548: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.583: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.588: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.594: INFO: Unable to read jessie_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.598: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.604: INFO: Unable to read jessie_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.609: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.615: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.621: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:48.645: INFO: Lookups using dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6107 wheezy_tcp@dns-test-service.dns-6107 wheezy_udp@dns-test-service.dns-6107.svc wheezy_tcp@dns-test-service.dns-6107.svc wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6107 jessie_tcp@dns-test-service.dns-6107 jessie_udp@dns-test-service.dns-6107.svc jessie_tcp@dns-test-service.dns-6107.svc jessie_udp@_http._tcp.dns-test-service.dns-6107.svc jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc]

  Aug 24 12:11:53.501: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.508: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.520: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.530: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.540: INFO: Unable to read wheezy_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.549: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.558: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.566: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.600: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.606: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.612: INFO: Unable to read jessie_udp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.627: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107 from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.634: INFO: Unable to read jessie_udp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.641: INFO: Unable to read jessie_tcp@dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.647: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.654: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc from pod dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2: the server could not find the requested resource (get pods dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2)
  Aug 24 12:11:53.689: INFO: Lookups using dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6107 wheezy_tcp@dns-test-service.dns-6107 wheezy_udp@dns-test-service.dns-6107.svc wheezy_tcp@dns-test-service.dns-6107.svc wheezy_udp@_http._tcp.dns-test-service.dns-6107.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6107.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6107 jessie_tcp@dns-test-service.dns-6107 jessie_udp@dns-test-service.dns-6107.svc jessie_tcp@dns-test-service.dns-6107.svc jessie_udp@_http._tcp.dns-test-service.dns-6107.svc jessie_tcp@_http._tcp.dns-test-service.dns-6107.svc]

  Aug 24 12:11:58.661: INFO: DNS probes using dns-6107/dns-test-b2b7a61f-755f-48f2-a403-7d73d27ecaf2 succeeded

  Aug 24 12:11:58.662: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:11:58.67
  STEP: deleting the test service @ 08/24/23 12:11:58.706
  STEP: deleting the test headless service @ 08/24/23 12:11:58.815
  STEP: Destroying namespace "dns-6107" for this suite. @ 08/24/23 12:11:58.845
• [34.692 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 08/24/23 12:11:58.861
  Aug 24 12:11:58.863: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:11:58.867
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:11:58.899
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:11:58.912
  Aug 24 12:11:58.918: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  W0824 12:12:01.647975      15 warnings.go:70] unknown field "alpha"
  W0824 12:12:01.648032      15 warnings.go:70] unknown field "beta"
  W0824 12:12:01.648041      15 warnings.go:70] unknown field "delta"
  W0824 12:12:01.648050      15 warnings.go:70] unknown field "epsilon"
  W0824 12:12:01.648061      15 warnings.go:70] unknown field "gamma"
  Aug 24 12:12:02.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4897" for this suite. @ 08/24/23 12:12:02.232
• [3.409 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 08/24/23 12:12:02.278
  Aug 24 12:12:02.278: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 12:12:02.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:02.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:02.311
  Aug 24 12:12:02.315: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:12:05.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7353" for this suite. @ 08/24/23 12:12:05.754
• [3.487 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 08/24/23 12:12:05.77
  Aug 24 12:12:05.770: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:12:05.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:05.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:05.807
  STEP: Creating configMap with name configmap-test-volume-map-63730e88-fa5c-423c-ac94-dd64991d5d22 @ 08/24/23 12:12:05.812
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:12:05.822
  STEP: Saw pod success @ 08/24/23 12:12:09.892
  Aug 24 12:12:09.898: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-4bdc97ca-002a-4206-9836-274c912d20aa container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:12:09.928
  Aug 24 12:12:09.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6553" for this suite. @ 08/24/23 12:12:09.967
• [4.207 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 08/24/23 12:12:09.98
  Aug 24 12:12:09.980: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:12:09.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:10.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:10.013
  STEP: Counting existing ResourceQuota @ 08/24/23 12:12:10.019
  STEP: Creating a ResourceQuota @ 08/24/23 12:12:15.025
  STEP: Ensuring resource quota status is calculated @ 08/24/23 12:12:15.037
  STEP: Creating a Service @ 08/24/23 12:12:17.045
  STEP: Creating a NodePort Service @ 08/24/23 12:12:17.075
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 08/24/23 12:12:17.129
  STEP: Ensuring resource quota status captures service creation @ 08/24/23 12:12:17.176
  STEP: Deleting Services @ 08/24/23 12:12:19.184
  STEP: Ensuring resource quota status released usage @ 08/24/23 12:12:19.267
  Aug 24 12:12:21.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6896" for this suite. @ 08/24/23 12:12:21.285
• [11.326 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 08/24/23 12:12:21.309
  Aug 24 12:12:21.309: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename dns @ 08/24/23 12:12:21.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:21.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:21.368
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/24/23 12:12:21.386
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/24/23 12:12:21.386
  STEP: creating a pod to probe DNS @ 08/24/23 12:12:21.387
  STEP: submitting the pod to kubernetes @ 08/24/23 12:12:21.387
  STEP: retrieving the pod @ 08/24/23 12:12:25.437
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:12:25.445
  Aug 24 12:12:25.473: INFO: DNS probes using dns-9042/dns-test-3c732681-58b1-40c2-a342-9dba05f710fd succeeded

  Aug 24 12:12:25.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:12:25.48
  STEP: Destroying namespace "dns-9042" for this suite. @ 08/24/23 12:12:25.5
• [4.205 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 08/24/23 12:12:25.517
  Aug 24 12:12:25.517: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:12:25.52
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:25.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:25.552
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/24/23 12:12:25.556
  STEP: Saw pod success @ 08/24/23 12:12:29.604
  Aug 24 12:12:29.611: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-75d4456a-ef01-44af-8669-34f9ff088ac7 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:12:29.626
  Aug 24 12:12:29.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4247" for this suite. @ 08/24/23 12:12:29.668
• [4.169 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 08/24/23 12:12:29.689
  Aug 24 12:12:29.689: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-pred @ 08/24/23 12:12:29.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:29.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:29.742
  Aug 24 12:12:29.750: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 24 12:12:29.773: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:12:29.781: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-1 before test
  Aug 24 12:12:29.811: INFO: cilium-2mlv4 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.811: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: cilium-node-init-wh7d5 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.811: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: coredns-5d78c9869d-xp68m from kube-system started at 2023-08-24 11:23:49 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.811: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: kube-addon-manager-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.811: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: kube-apiserver-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.811: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: kube-controller-manager-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.811: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: kube-proxy-ljqfg from kube-system started at 2023-08-24 11:21:03 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.811: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: kube-scheduler-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.811: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-5c6ln from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:12:29.811: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:12:29.811: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-2 before test
  Aug 24 12:12:29.828: INFO: cilium-krjld from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.829: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:12:29.829: INFO: cilium-node-init-ldwv5 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.830: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:12:29.830: INFO: cilium-operator-b8f479cd9-78ztl from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.831: INFO: 	Container cilium-operator ready: true, restart count 0
  Aug 24 12:12:29.831: INFO: coredns-5d78c9869d-bvl77 from kube-system started at 2023-08-24 11:23:40 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.831: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:12:29.832: INFO: kube-addon-manager-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.832: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:12:29.832: INFO: kube-apiserver-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.833: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:12:29.833: INFO: kube-controller-manager-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.834: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:12:29.834: INFO: kube-proxy-dr4nl from kube-system started at 2023-08-24 11:21:33 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.834: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:12:29.835: INFO: kube-scheduler-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.835: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:12:29.835: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-bccng from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:12:29.836: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:12:29.836: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:12:29.836: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-3 before test
  Aug 24 12:12:29.860: INFO: cilium-node-init-hkrgc from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.860: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:12:29.861: INFO: cilium-qwwtg from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.861: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:12:29.861: INFO: kube-proxy-9cptc from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.862: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:12:29.862: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:22 +0000 UTC (1 container statuses recorded)
  Aug 24 12:12:29.863: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 24 12:12:29.863: INFO: sonobuoy-e2e-job-06f57e3e53464656 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:12:29.863: INFO: 	Container e2e ready: true, restart count 0
  Aug 24 12:12:29.864: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:12:29.864: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-wfrkl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:12:29.864: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:12:29.865: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node quohp9aeph3i-1 @ 08/24/23 12:12:29.913
  STEP: verifying the node has the label node quohp9aeph3i-2 @ 08/24/23 12:12:29.96
  STEP: verifying the node has the label node quohp9aeph3i-3 @ 08/24/23 12:12:29.989
  Aug 24 12:12:30.031: INFO: Pod cilium-2mlv4 requesting resource cpu=0m on Node quohp9aeph3i-1
  Aug 24 12:12:30.032: INFO: Pod cilium-krjld requesting resource cpu=0m on Node quohp9aeph3i-2
  Aug 24 12:12:30.032: INFO: Pod cilium-node-init-hkrgc requesting resource cpu=100m on Node quohp9aeph3i-3
  Aug 24 12:12:30.032: INFO: Pod cilium-node-init-ldwv5 requesting resource cpu=100m on Node quohp9aeph3i-2
  Aug 24 12:12:30.033: INFO: Pod cilium-node-init-wh7d5 requesting resource cpu=100m on Node quohp9aeph3i-1
  Aug 24 12:12:30.033: INFO: Pod cilium-operator-b8f479cd9-78ztl requesting resource cpu=0m on Node quohp9aeph3i-2
  Aug 24 12:12:30.033: INFO: Pod cilium-qwwtg requesting resource cpu=0m on Node quohp9aeph3i-3
  Aug 24 12:12:30.034: INFO: Pod coredns-5d78c9869d-bvl77 requesting resource cpu=100m on Node quohp9aeph3i-2
  Aug 24 12:12:30.034: INFO: Pod coredns-5d78c9869d-xp68m requesting resource cpu=100m on Node quohp9aeph3i-1
  Aug 24 12:12:30.035: INFO: Pod kube-addon-manager-quohp9aeph3i-1 requesting resource cpu=5m on Node quohp9aeph3i-1
  Aug 24 12:12:30.035: INFO: Pod kube-addon-manager-quohp9aeph3i-2 requesting resource cpu=5m on Node quohp9aeph3i-2
  Aug 24 12:12:30.035: INFO: Pod kube-apiserver-quohp9aeph3i-1 requesting resource cpu=250m on Node quohp9aeph3i-1
  Aug 24 12:12:30.036: INFO: Pod kube-apiserver-quohp9aeph3i-2 requesting resource cpu=250m on Node quohp9aeph3i-2
  Aug 24 12:12:30.036: INFO: Pod kube-controller-manager-quohp9aeph3i-1 requesting resource cpu=200m on Node quohp9aeph3i-1
  Aug 24 12:12:30.037: INFO: Pod kube-controller-manager-quohp9aeph3i-2 requesting resource cpu=200m on Node quohp9aeph3i-2
  Aug 24 12:12:30.037: INFO: Pod kube-proxy-9cptc requesting resource cpu=0m on Node quohp9aeph3i-3
  Aug 24 12:12:30.037: INFO: Pod kube-proxy-dr4nl requesting resource cpu=0m on Node quohp9aeph3i-2
  Aug 24 12:12:30.038: INFO: Pod kube-proxy-ljqfg requesting resource cpu=0m on Node quohp9aeph3i-1
  Aug 24 12:12:30.038: INFO: Pod kube-scheduler-quohp9aeph3i-1 requesting resource cpu=100m on Node quohp9aeph3i-1
  Aug 24 12:12:30.038: INFO: Pod kube-scheduler-quohp9aeph3i-2 requesting resource cpu=100m on Node quohp9aeph3i-2
  Aug 24 12:12:30.039: INFO: Pod sonobuoy requesting resource cpu=0m on Node quohp9aeph3i-3
  Aug 24 12:12:30.039: INFO: Pod sonobuoy-e2e-job-06f57e3e53464656 requesting resource cpu=0m on Node quohp9aeph3i-3
  Aug 24 12:12:30.039: INFO: Pod sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-5c6ln requesting resource cpu=0m on Node quohp9aeph3i-1
  Aug 24 12:12:30.040: INFO: Pod sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-bccng requesting resource cpu=0m on Node quohp9aeph3i-2
  Aug 24 12:12:30.040: INFO: Pod sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-wfrkl requesting resource cpu=0m on Node quohp9aeph3i-3
  STEP: Starting Pods to consume most of the cluster CPU. @ 08/24/23 12:12:30.041
  Aug 24 12:12:30.041: INFO: Creating a pod which consumes cpu=591m on Node quohp9aeph3i-1
  Aug 24 12:12:30.110: INFO: Creating a pod which consumes cpu=591m on Node quohp9aeph3i-2
  Aug 24 12:12:30.129: INFO: Creating a pod which consumes cpu=1050m on Node quohp9aeph3i-3
  STEP: Creating another pod that requires unavailable amount of CPU. @ 08/24/23 12:12:34.204
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-172f7298-0982-4d00-8c24-5d57de5a2ef0.177e509e2a6e372e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2660/filler-pod-172f7298-0982-4d00-8c24-5d57de5a2ef0 to quohp9aeph3i-2] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-172f7298-0982-4d00-8c24-5d57de5a2ef0.177e509e778ce1f1], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-172f7298-0982-4d00-8c24-5d57de5a2ef0.177e509e8b2f0beb], Reason = [Created], Message = [Created container filler-pod-172f7298-0982-4d00-8c24-5d57de5a2ef0] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-172f7298-0982-4d00-8c24-5d57de5a2ef0.177e509e932c1531], Reason = [Started], Message = [Started container filler-pod-172f7298-0982-4d00-8c24-5d57de5a2ef0] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3d100331-1612-4931-b40a-ee7bda7ba041.177e509e27c2eb65], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2660/filler-pod-3d100331-1612-4931-b40a-ee7bda7ba041 to quohp9aeph3i-1] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3d100331-1612-4931-b40a-ee7bda7ba041.177e509e76153368], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3d100331-1612-4931-b40a-ee7bda7ba041.177e509e8cb664fc], Reason = [Created], Message = [Created container filler-pod-3d100331-1612-4931-b40a-ee7bda7ba041] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-3d100331-1612-4931-b40a-ee7bda7ba041.177e509e92a98b5d], Reason = [Started], Message = [Started container filler-pod-3d100331-1612-4931-b40a-ee7bda7ba041] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1721e2a-6970-49cd-8eeb-a828045b970a.177e509e2c469e4c], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2660/filler-pod-b1721e2a-6970-49cd-8eeb-a828045b970a to quohp9aeph3i-3] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1721e2a-6970-49cd-8eeb-a828045b970a.177e509e62b9acc5], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1721e2a-6970-49cd-8eeb-a828045b970a.177e509e70b3d1a0], Reason = [Created], Message = [Created container filler-pod-b1721e2a-6970-49cd-8eeb-a828045b970a] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b1721e2a-6970-49cd-8eeb-a828045b970a.177e509e755c0673], Reason = [Started], Message = [Started container filler-pod-b1721e2a-6970-49cd-8eeb-a828045b970a] @ 08/24/23 12:12:34.213
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.177e509f1e4877c8], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 08/24/23 12:12:34.238
  STEP: removing the label node off the node quohp9aeph3i-1 @ 08/24/23 12:12:35.239
  STEP: verifying the node doesn't have the label node @ 08/24/23 12:12:35.271
  STEP: removing the label node off the node quohp9aeph3i-2 @ 08/24/23 12:12:35.284
  STEP: verifying the node doesn't have the label node @ 08/24/23 12:12:35.31
  STEP: removing the label node off the node quohp9aeph3i-3 @ 08/24/23 12:12:35.322
  STEP: verifying the node doesn't have the label node @ 08/24/23 12:12:35.353
  Aug 24 12:12:35.363: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2660" for this suite. @ 08/24/23 12:12:35.376
• [5.724 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 08/24/23 12:12:35.415
  Aug 24 12:12:35.415: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:12:35.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:35.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:35.47
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/24/23 12:12:35.476
  STEP: Saw pod success @ 08/24/23 12:12:39.545
  Aug 24 12:12:39.556: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-bf9c228c-a2e4-4f16-813f-a84f517e12b7 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:12:39.577
  Aug 24 12:12:39.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4668" for this suite. @ 08/24/23 12:12:39.627
• [4.228 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 08/24/23 12:12:39.645
  Aug 24 12:12:39.645: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:12:39.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:39.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:39.69
  STEP: creating the pod @ 08/24/23 12:12:39.694
  Aug 24 12:12:39.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1174 create -f -'
  Aug 24 12:12:40.628: INFO: stderr: ""
  Aug 24 12:12:40.628: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 08/24/23 12:12:42.676
  Aug 24 12:12:42.677: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1174 label pods pause testing-label=testing-label-value'
  Aug 24 12:12:42.858: INFO: stderr: ""
  Aug 24 12:12:42.858: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 08/24/23 12:12:42.858
  Aug 24 12:12:42.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1174 get pod pause -L testing-label'
  Aug 24 12:12:43.007: INFO: stderr: ""
  Aug 24 12:12:43.007: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 08/24/23 12:12:43.007
  Aug 24 12:12:43.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1174 label pods pause testing-label-'
  Aug 24 12:12:43.157: INFO: stderr: ""
  Aug 24 12:12:43.157: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 08/24/23 12:12:43.157
  Aug 24 12:12:43.157: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1174 get pod pause -L testing-label'
  Aug 24 12:12:43.296: INFO: stderr: ""
  Aug 24 12:12:43.296: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 08/24/23 12:12:43.297
  Aug 24 12:12:43.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1174 delete --grace-period=0 --force -f -'
  Aug 24 12:12:43.444: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 12:12:43.444: INFO: stdout: "pod \"pause\" force deleted\n"
  Aug 24 12:12:43.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1174 get rc,svc -l name=pause --no-headers'
  Aug 24 12:12:43.611: INFO: stderr: "No resources found in kubectl-1174 namespace.\n"
  Aug 24 12:12:43.611: INFO: stdout: ""
  Aug 24 12:12:43.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-1174 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 24 12:12:43.772: INFO: stderr: ""
  Aug 24 12:12:43.772: INFO: stdout: ""
  Aug 24 12:12:43.772: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1174" for this suite. @ 08/24/23 12:12:43.782
• [4.160 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 08/24/23 12:12:43.806
  Aug 24 12:12:43.806: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename controllerrevisions @ 08/24/23 12:12:43.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:43.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:43.845
  STEP: Creating DaemonSet "e2e-7vvwv-daemon-set" @ 08/24/23 12:12:43.902
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 12:12:43.912
  Aug 24 12:12:43.946: INFO: Number of nodes with available pods controlled by daemonset e2e-7vvwv-daemon-set: 0
  Aug 24 12:12:43.946: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:12:44.982: INFO: Number of nodes with available pods controlled by daemonset e2e-7vvwv-daemon-set: 0
  Aug 24 12:12:44.982: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:12:45.973: INFO: Number of nodes with available pods controlled by daemonset e2e-7vvwv-daemon-set: 2
  Aug 24 12:12:45.973: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:12:46.971: INFO: Number of nodes with available pods controlled by daemonset e2e-7vvwv-daemon-set: 3
  Aug 24 12:12:46.971: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-7vvwv-daemon-set
  STEP: Confirm DaemonSet "e2e-7vvwv-daemon-set" successfully created with "daemonset-name=e2e-7vvwv-daemon-set" label @ 08/24/23 12:12:46.976
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-7vvwv-daemon-set" @ 08/24/23 12:12:46.992
  Aug 24 12:12:46.999: INFO: Located ControllerRevision: "e2e-7vvwv-daemon-set-8c697f98f"
  STEP: Patching ControllerRevision "e2e-7vvwv-daemon-set-8c697f98f" @ 08/24/23 12:12:47.006
  Aug 24 12:12:47.018: INFO: e2e-7vvwv-daemon-set-8c697f98f has been patched
  STEP: Create a new ControllerRevision @ 08/24/23 12:12:47.018
  Aug 24 12:12:47.033: INFO: Created ControllerRevision: e2e-7vvwv-daemon-set-78578fc59c
  STEP: Confirm that there are two ControllerRevisions @ 08/24/23 12:12:47.033
  Aug 24 12:12:47.033: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 24 12:12:47.041: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-7vvwv-daemon-set-8c697f98f" @ 08/24/23 12:12:47.041
  STEP: Confirm that there is only one ControllerRevision @ 08/24/23 12:12:47.054
  Aug 24 12:12:47.054: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 24 12:12:47.061: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-7vvwv-daemon-set-78578fc59c" @ 08/24/23 12:12:47.068
  Aug 24 12:12:47.087: INFO: e2e-7vvwv-daemon-set-78578fc59c has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 08/24/23 12:12:47.087
  W0824 12:12:47.099418      15 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 08/24/23 12:12:47.099
  Aug 24 12:12:47.099: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 24 12:12:48.106: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 24 12:12:48.119: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-7vvwv-daemon-set-78578fc59c=updated" @ 08/24/23 12:12:48.119
  STEP: Confirm that there is only one ControllerRevision @ 08/24/23 12:12:48.142
  Aug 24 12:12:48.143: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug 24 12:12:48.148: INFO: Found 1 ControllerRevisions
  Aug 24 12:12:48.154: INFO: ControllerRevision "e2e-7vvwv-daemon-set-5ffd54f568" has revision 3
  STEP: Deleting DaemonSet "e2e-7vvwv-daemon-set" @ 08/24/23 12:12:48.16
  STEP: deleting DaemonSet.extensions e2e-7vvwv-daemon-set in namespace controllerrevisions-8063, will wait for the garbage collector to delete the pods @ 08/24/23 12:12:48.16
  Aug 24 12:12:48.231: INFO: Deleting DaemonSet.extensions e2e-7vvwv-daemon-set took: 14.124947ms
  Aug 24 12:12:48.331: INFO: Terminating DaemonSet.extensions e2e-7vvwv-daemon-set pods took: 100.678977ms
  Aug 24 12:12:50.036: INFO: Number of nodes with available pods controlled by daemonset e2e-7vvwv-daemon-set: 0
  Aug 24 12:12:50.036: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-7vvwv-daemon-set
  Aug 24 12:12:50.044: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15000"},"items":null}

  Aug 24 12:12:50.050: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15000"},"items":null}

  Aug 24 12:12:50.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-8063" for this suite. @ 08/24/23 12:12:50.081
• [6.285 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 08/24/23 12:12:50.091
  Aug 24 12:12:50.091: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:12:50.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:12:50.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:12:50.124
  STEP: creating all guestbook components @ 08/24/23 12:12:50.132
  Aug 24 12:12:50.132: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Aug 24 12:12:50.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 create -f -'
  Aug 24 12:12:50.898: INFO: stderr: ""
  Aug 24 12:12:50.898: INFO: stdout: "service/agnhost-replica created\n"
  Aug 24 12:12:50.898: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Aug 24 12:12:50.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 create -f -'
  Aug 24 12:12:51.457: INFO: stderr: ""
  Aug 24 12:12:51.458: INFO: stdout: "service/agnhost-primary created\n"
  Aug 24 12:12:51.458: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Aug 24 12:12:51.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 create -f -'
  Aug 24 12:12:52.001: INFO: stderr: ""
  Aug 24 12:12:52.001: INFO: stdout: "service/frontend created\n"
  Aug 24 12:12:52.003: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Aug 24 12:12:52.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 create -f -'
  Aug 24 12:12:52.535: INFO: stderr: ""
  Aug 24 12:12:52.535: INFO: stdout: "deployment.apps/frontend created\n"
  Aug 24 12:12:52.535: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug 24 12:12:52.535: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 create -f -'
  Aug 24 12:12:53.357: INFO: stderr: ""
  Aug 24 12:12:53.357: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Aug 24 12:12:53.357: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug 24 12:12:53.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 create -f -'
  Aug 24 12:12:54.327: INFO: stderr: ""
  Aug 24 12:12:54.327: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 08/24/23 12:12:54.327
  Aug 24 12:12:54.327: INFO: Waiting for all frontend pods to be Running.
  Aug 24 12:12:59.379: INFO: Waiting for frontend to serve content.
  Aug 24 12:12:59.402: INFO: Trying to add a new entry to the guestbook.
  Aug 24 12:12:59.430: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 08/24/23 12:12:59.447
  Aug 24 12:12:59.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 delete --grace-period=0 --force -f -'
  Aug 24 12:12:59.613: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 12:12:59.613: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 12:12:59.613
  Aug 24 12:12:59.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 delete --grace-period=0 --force -f -'
  Aug 24 12:12:59.793: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 12:12:59.793: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 12:12:59.793
  Aug 24 12:12:59.794: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 delete --grace-period=0 --force -f -'
  Aug 24 12:12:59.988: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 12:12:59.989: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 12:12:59.989
  Aug 24 12:12:59.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 delete --grace-period=0 --force -f -'
  Aug 24 12:13:00.125: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 12:13:00.125: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 12:13:00.125
  Aug 24 12:13:00.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 delete --grace-period=0 --force -f -'
  Aug 24 12:13:00.360: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 12:13:00.360: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/24/23 12:13:00.36
  Aug 24 12:13:00.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-9288 delete --grace-period=0 --force -f -'
  Aug 24 12:13:00.573: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 12:13:00.573: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Aug 24 12:13:00.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9288" for this suite. @ 08/24/23 12:13:00.582
• [10.516 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 08/24/23 12:13:00.609
  Aug 24 12:13:00.609: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 12:13:00.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:00.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:00.665
  STEP: creating service multi-endpoint-test in namespace services-1369 @ 08/24/23 12:13:00.67
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1369 to expose endpoints map[] @ 08/24/23 12:13:00.7
  Aug 24 12:13:00.733: INFO: successfully validated that service multi-endpoint-test in namespace services-1369 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-1369 @ 08/24/23 12:13:00.733
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1369 to expose endpoints map[pod1:[100]] @ 08/24/23 12:13:02.779
  Aug 24 12:13:02.801: INFO: successfully validated that service multi-endpoint-test in namespace services-1369 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-1369 @ 08/24/23 12:13:02.801
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1369 to expose endpoints map[pod1:[100] pod2:[101]] @ 08/24/23 12:13:04.834
  Aug 24 12:13:04.862: INFO: successfully validated that service multi-endpoint-test in namespace services-1369 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 08/24/23 12:13:04.863
  Aug 24 12:13:04.863: INFO: Creating new exec pod
  Aug 24 12:13:07.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-1369 exec execpodspxxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Aug 24 12:13:08.277: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Aug 24 12:13:08.277: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:13:08.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-1369 exec execpodspxxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.20.199 80'
  Aug 24 12:13:08.549: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.20.199 80\nConnection to 10.233.20.199 80 port [tcp/http] succeeded!\n"
  Aug 24 12:13:08.549: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:13:08.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-1369 exec execpodspxxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Aug 24 12:13:08.830: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Aug 24 12:13:08.830: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:13:08.831: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-1369 exec execpodspxxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.20.199 81'
  Aug 24 12:13:09.134: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.20.199 81\nConnection to 10.233.20.199 81 port [tcp/*] succeeded!\n"
  Aug 24 12:13:09.134: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-1369 @ 08/24/23 12:13:09.134
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1369 to expose endpoints map[pod2:[101]] @ 08/24/23 12:13:09.174
  Aug 24 12:13:09.264: INFO: successfully validated that service multi-endpoint-test in namespace services-1369 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-1369 @ 08/24/23 12:13:09.265
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-1369 to expose endpoints map[] @ 08/24/23 12:13:09.348
  Aug 24 12:13:10.389: INFO: successfully validated that service multi-endpoint-test in namespace services-1369 exposes endpoints map[]
  Aug 24 12:13:10.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1369" for this suite. @ 08/24/23 12:13:10.447
• [9.862 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 08/24/23 12:13:10.473
  Aug 24 12:13:10.474: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:13:10.478
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:10.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:10.528
  STEP: Creating a ResourceQuota @ 08/24/23 12:13:10.531
  STEP: Getting a ResourceQuota @ 08/24/23 12:13:10.54
  STEP: Listing all ResourceQuotas with LabelSelector @ 08/24/23 12:13:10.548
  STEP: Patching the ResourceQuota @ 08/24/23 12:13:10.559
  STEP: Deleting a Collection of ResourceQuotas @ 08/24/23 12:13:10.571
  STEP: Verifying the deleted ResourceQuota @ 08/24/23 12:13:10.594
  Aug 24 12:13:10.601: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-427" for this suite. @ 08/24/23 12:13:10.611
• [0.155 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 08/24/23 12:13:10.63
  Aug 24 12:13:10.631: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pod-network-test @ 08/24/23 12:13:10.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:10.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:10.675
  STEP: Performing setup for networking test in namespace pod-network-test-953 @ 08/24/23 12:13:10.68
  STEP: creating a selector @ 08/24/23 12:13:10.68
  STEP: Creating the service pods in kubernetes @ 08/24/23 12:13:10.68
  Aug 24 12:13:10.681: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 08/24/23 12:13:22.87
  Aug 24 12:13:24.966: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 24 12:13:24.967: INFO: Going to poll 10.233.65.193 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 12:13:24.974: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.193 8081 | grep -v '^\s*$'] Namespace:pod-network-test-953 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:13:24.975: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:13:24.978: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:13:24.979: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-953/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.193+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 12:13:26.177: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug 24 12:13:26.178: INFO: Going to poll 10.233.64.140 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 12:13:26.187: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.140 8081 | grep -v '^\s*$'] Namespace:pod-network-test-953 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:13:26.187: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:13:26.189: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:13:26.189: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-953/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.140+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 12:13:27.316: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug 24 12:13:27.316: INFO: Going to poll 10.233.66.192 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Aug 24 12:13:27.323: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.192 8081 | grep -v '^\s*$'] Namespace:pod-network-test-953 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:13:27.324: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:13:27.326: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:13:27.326: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-953/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.192+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 12:13:28.441: INFO: Found all 1 expected endpoints: [netserver-2]
  Aug 24 12:13:28.441: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-953" for this suite. @ 08/24/23 12:13:28.451
• [17.838 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 08/24/23 12:13:28.472
  Aug 24 12:13:28.473: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:13:28.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:28.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:28.518
  STEP: Creating configMap with name configmap-test-volume-04b8d9dc-4a5e-4019-9aa8-100b371aa69a @ 08/24/23 12:13:28.522
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:13:28.531
  STEP: Saw pod success @ 08/24/23 12:13:32.581
  Aug 24 12:13:32.589: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-747970e8-1261-422d-8945-e049cfbc85a3 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:13:32.603
  Aug 24 12:13:32.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8347" for this suite. @ 08/24/23 12:13:32.645
• [4.188 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 08/24/23 12:13:32.662
  Aug 24 12:13:32.662: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 12:13:32.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:32.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:32.7
  STEP: create the container @ 08/24/23 12:13:32.705
  W0824 12:13:32.723329      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 08/24/23 12:13:32.724
  STEP: get the container status @ 08/24/23 12:13:35.761
  STEP: the container should be terminated @ 08/24/23 12:13:35.768
  STEP: the termination message should be set @ 08/24/23 12:13:35.768
  Aug 24 12:13:35.768: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/24/23 12:13:35.768
  Aug 24 12:13:35.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-596" for this suite. @ 08/24/23 12:13:35.825
• [3.184 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 08/24/23 12:13:35.852
  Aug 24 12:13:35.852: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:13:35.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:35.895
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:35.9
  Aug 24 12:13:35.907: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 08/24/23 12:13:37.939
  Aug 24 12:13:37.940: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 --namespace=crd-publish-openapi-7198 create -f -'
  Aug 24 12:13:39.533: INFO: stderr: ""
  Aug 24 12:13:39.533: INFO: stdout: "e2e-test-crd-publish-openapi-8917-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug 24 12:13:39.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 --namespace=crd-publish-openapi-7198 delete e2e-test-crd-publish-openapi-8917-crds test-foo'
  Aug 24 12:13:39.740: INFO: stderr: ""
  Aug 24 12:13:39.740: INFO: stdout: "e2e-test-crd-publish-openapi-8917-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Aug 24 12:13:39.740: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 --namespace=crd-publish-openapi-7198 apply -f -'
  Aug 24 12:13:40.355: INFO: stderr: ""
  Aug 24 12:13:40.355: INFO: stdout: "e2e-test-crd-publish-openapi-8917-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug 24 12:13:40.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 --namespace=crd-publish-openapi-7198 delete e2e-test-crd-publish-openapi-8917-crds test-foo'
  Aug 24 12:13:40.502: INFO: stderr: ""
  Aug 24 12:13:40.502: INFO: stdout: "e2e-test-crd-publish-openapi-8917-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 08/24/23 12:13:40.502
  Aug 24 12:13:40.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 --namespace=crd-publish-openapi-7198 create -f -'
  Aug 24 12:13:41.915: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 08/24/23 12:13:41.915
  Aug 24 12:13:41.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 --namespace=crd-publish-openapi-7198 create -f -'
  Aug 24 12:13:42.392: INFO: rc: 1
  Aug 24 12:13:42.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 --namespace=crd-publish-openapi-7198 apply -f -'
  Aug 24 12:13:42.841: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 08/24/23 12:13:42.842
  Aug 24 12:13:42.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 --namespace=crd-publish-openapi-7198 create -f -'
  Aug 24 12:13:43.328: INFO: rc: 1
  Aug 24 12:13:43.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 --namespace=crd-publish-openapi-7198 apply -f -'
  Aug 24 12:13:43.817: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 08/24/23 12:13:43.817
  Aug 24 12:13:43.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 explain e2e-test-crd-publish-openapi-8917-crds'
  Aug 24 12:13:44.339: INFO: stderr: ""
  Aug 24 12:13:44.339: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8917-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 08/24/23 12:13:44.339
  Aug 24 12:13:44.339: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 explain e2e-test-crd-publish-openapi-8917-crds.metadata'
  Aug 24 12:13:44.820: INFO: stderr: ""
  Aug 24 12:13:44.820: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8917-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Aug 24 12:13:44.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 explain e2e-test-crd-publish-openapi-8917-crds.spec'
  Aug 24 12:13:45.295: INFO: stderr: ""
  Aug 24 12:13:45.295: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8917-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Aug 24 12:13:45.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 explain e2e-test-crd-publish-openapi-8917-crds.spec.bars'
  Aug 24 12:13:45.887: INFO: stderr: ""
  Aug 24 12:13:45.887: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8917-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 08/24/23 12:13:45.887
  Aug 24 12:13:45.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7198 explain e2e-test-crd-publish-openapi-8917-crds.spec.bars2'
  Aug 24 12:13:46.336: INFO: rc: 1
  Aug 24 12:13:48.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7198" for this suite. @ 08/24/23 12:13:48.128
• [12.289 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 08/24/23 12:13:48.151
  Aug 24 12:13:48.151: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 12:13:48.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:13:48.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:13:48.183
  STEP: Creating service test in namespace statefulset-5280 @ 08/24/23 12:13:48.189
  STEP: Creating a new StatefulSet @ 08/24/23 12:13:48.2
  Aug 24 12:13:48.235: INFO: Found 0 stateful pods, waiting for 3
  Aug 24 12:13:58.245: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:13:58.245: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:13:58.246: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/24/23 12:13:58.266
  Aug 24 12:13:58.296: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/24/23 12:13:58.296
  STEP: Not applying an update when the partition is greater than the number of replicas @ 08/24/23 12:14:08.335
  STEP: Performing a canary update @ 08/24/23 12:14:08.335
  Aug 24 12:14:08.368: INFO: Updating stateful set ss2
  Aug 24 12:14:08.390: INFO: Waiting for Pod statefulset-5280/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 08/24/23 12:14:18.417
  Aug 24 12:14:18.509: INFO: Found 1 stateful pods, waiting for 3
  Aug 24 12:14:28.527: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:14:28.527: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:14:28.527: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 08/24/23 12:14:28.541
  Aug 24 12:14:28.573: INFO: Updating stateful set ss2
  Aug 24 12:14:28.617: INFO: Waiting for Pod statefulset-5280/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug 24 12:14:38.667: INFO: Updating stateful set ss2
  Aug 24 12:14:38.693: INFO: Waiting for StatefulSet statefulset-5280/ss2 to complete update
  Aug 24 12:14:38.693: INFO: Waiting for Pod statefulset-5280/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Aug 24 12:14:48.709: INFO: Deleting all statefulset in ns statefulset-5280
  Aug 24 12:14:48.717: INFO: Scaling statefulset ss2 to 0
  Aug 24 12:14:58.750: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:14:58.755: INFO: Deleting statefulset ss2
  Aug 24 12:14:58.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5280" for this suite. @ 08/24/23 12:14:58.79
• [70.655 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 08/24/23 12:14:58.809
  Aug 24 12:14:58.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:14:58.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:14:58.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:14:58.847
  STEP: creating Agnhost RC @ 08/24/23 12:14:58.853
  Aug 24 12:14:58.854: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-36 create -f -'
  Aug 24 12:14:59.566: INFO: stderr: ""
  Aug 24 12:14:59.566: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/24/23 12:14:59.566
  Aug 24 12:15:00.574: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:15:00.574: INFO: Found 0 / 1
  Aug 24 12:15:01.573: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:15:01.573: INFO: Found 0 / 1
  Aug 24 12:15:02.578: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:15:02.578: INFO: Found 1 / 1
  Aug 24 12:15:02.579: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 08/24/23 12:15:02.579
  Aug 24 12:15:02.588: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:15:02.588: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 24 12:15:02.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-36 patch pod agnhost-primary-r6pc8 -p {"metadata":{"annotations":{"x":"y"}}}'
  Aug 24 12:15:02.767: INFO: stderr: ""
  Aug 24 12:15:02.767: INFO: stdout: "pod/agnhost-primary-r6pc8 patched\n"
  STEP: checking annotations @ 08/24/23 12:15:02.767
  Aug 24 12:15:02.773: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug 24 12:15:02.773: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug 24 12:15:02.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-36" for this suite. @ 08/24/23 12:15:02.782
• [3.988 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 08/24/23 12:15:02.811
  Aug 24 12:15:02.811: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:15:02.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:15:02.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:15:02.842
  STEP: Setting up server cert @ 08/24/23 12:15:02.882
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:15:03.201
  STEP: Deploying the webhook pod @ 08/24/23 12:15:03.212
  STEP: Wait for the deployment to be ready @ 08/24/23 12:15:03.235
  Aug 24 12:15:03.245: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 08/24/23 12:15:05.267
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:15:05.285
  Aug 24 12:15:06.285: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/24/23 12:15:06.424
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:15:06.499
  STEP: Deleting the collection of validation webhooks @ 08/24/23 12:15:06.558
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:15:06.668
  Aug 24 12:15:06.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3641" for this suite. @ 08/24/23 12:15:06.799
  STEP: Destroying namespace "webhook-markers-58" for this suite. @ 08/24/23 12:15:06.814
• [4.025 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 08/24/23 12:15:06.841
  Aug 24 12:15:06.841: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:15:06.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:15:06.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:15:06.879
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/24/23 12:15:06.884
  STEP: Saw pod success @ 08/24/23 12:15:10.937
  Aug 24 12:15:10.944: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-f53f9b7f-2db5-421b-9545-330127254aaf container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:15:10.969
  Aug 24 12:15:10.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6040" for this suite. @ 08/24/23 12:15:11.004
• [4.178 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 08/24/23 12:15:11.023
  Aug 24 12:15:11.023: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:15:11.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:15:11.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:15:11.058
  Aug 24 12:15:11.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6095 version'
  Aug 24 12:15:11.224: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Aug 24 12:15:11.224: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.5\", GitCommit:\"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:48:26Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.5\", GitCommit:\"93e0d7146fb9c3e9f68aa41b2b4265b2fcdb0a4c\", GitTreeState:\"clean\", BuildDate:\"2023-08-24T00:42:11Z\", GoVersion:\"go1.20.7\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Aug 24 12:15:11.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6095" for this suite. @ 08/24/23 12:15:11.233
• [0.222 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 08/24/23 12:15:11.246
  Aug 24 12:15:11.246: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 12:15:11.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:15:11.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:15:11.28
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3647 @ 08/24/23 12:15:11.286
  STEP: changing the ExternalName service to type=ClusterIP @ 08/24/23 12:15:11.297
  STEP: creating replication controller externalname-service in namespace services-3647 @ 08/24/23 12:15:11.329
  I0824 12:15:11.348258      15 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3647, replica count: 2
  I0824 12:15:14.401197      15 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:15:14.401: INFO: Creating new exec pod
  Aug 24 12:15:17.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-3647 exec execpod9mgcn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 24 12:15:17.722: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 24 12:15:17.722: INFO: stdout: "externalname-service-48t8l"
  Aug 24 12:15:17.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-3647 exec execpod9mgcn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.44.35 80'
  Aug 24 12:15:17.971: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.44.35 80\nConnection to 10.233.44.35 80 port [tcp/http] succeeded!\n"
  Aug 24 12:15:17.972: INFO: stdout: "externalname-service-48t8l"
  Aug 24 12:15:17.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:15:17.985: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-3647" for this suite. @ 08/24/23 12:15:18.021
• [6.794 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 08/24/23 12:15:18.041
  Aug 24 12:15:18.041: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:15:18.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:15:18.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:15:18.105
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 08/24/23 12:15:18.112
  Aug 24 12:15:18.113: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:15:19.928: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:15:27.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3155" for this suite. @ 08/24/23 12:15:27.8
• [9.775 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 08/24/23 12:15:27.817
  Aug 24 12:15:27.817: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 12:15:27.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:15:27.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:15:27.859
  STEP: Creating a cronjob @ 08/24/23 12:15:27.867
  STEP: Ensuring more than one job is running at a time @ 08/24/23 12:15:27.877
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 08/24/23 12:17:01.889
  STEP: Removing cronjob @ 08/24/23 12:17:01.897
  Aug 24 12:17:01.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3545" for this suite. @ 08/24/23 12:17:01.927
• [94.131 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 08/24/23 12:17:01.951
  Aug 24 12:17:01.951: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:17:01.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:17:01.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:17:02.003
  STEP: creating a Deployment @ 08/24/23 12:17:02.026
  Aug 24 12:17:02.026: INFO: Creating simple deployment test-deployment-dgf2r
  Aug 24 12:17:02.051: INFO: new replicaset for deployment "test-deployment-dgf2r" is yet to be created
  Aug 24 12:17:04.072: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 17, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 17, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 17, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 17, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-deployment-dgf2r-5994cf9475\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Getting /status @ 08/24/23 12:17:06.088
  Aug 24 12:17:06.100: INFO: Deployment test-deployment-dgf2r has Conditions: [{Available True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dgf2r-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 08/24/23 12:17:06.1
  Aug 24 12:17:06.125: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 17, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 17, 4, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 17, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 17, 2, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-dgf2r-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 08/24/23 12:17:06.125
  Aug 24 12:17:06.131: INFO: Observed &Deployment event: ADDED
  Aug 24 12:17:06.131: INFO: Observed Deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dgf2r-5994cf9475"}
  Aug 24 12:17:06.131: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:17:06.131: INFO: Observed Deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dgf2r-5994cf9475"}
  Aug 24 12:17:06.132: INFO: Observed Deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 24 12:17:06.132: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:17:06.132: INFO: Observed Deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 24 12:17:06.133: INFO: Observed Deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dgf2r-5994cf9475" is progressing.}
  Aug 24 12:17:06.135: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:17:06.136: INFO: Observed Deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 24 12:17:06.137: INFO: Observed Deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dgf2r-5994cf9475" has successfully progressed.}
  Aug 24 12:17:06.138: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:17:06.139: INFO: Observed Deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 24 12:17:06.140: INFO: Observed Deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dgf2r-5994cf9475" has successfully progressed.}
  Aug 24 12:17:06.141: INFO: Found Deployment test-deployment-dgf2r in namespace deployment-1477 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:17:06.141: INFO: Deployment test-deployment-dgf2r has an updated status
  STEP: patching the Statefulset Status @ 08/24/23 12:17:06.141
  Aug 24 12:17:06.142: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 24 12:17:06.156: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 08/24/23 12:17:06.157
  Aug 24 12:17:06.161: INFO: Observed &Deployment event: ADDED
  Aug 24 12:17:06.161: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dgf2r-5994cf9475"}
  Aug 24 12:17:06.161: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:17:06.161: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dgf2r-5994cf9475"}
  Aug 24 12:17:06.162: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 24 12:17:06.162: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:17:06.162: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug 24 12:17:06.162: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:02 +0000 UTC 2023-08-24 12:17:02 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dgf2r-5994cf9475" is progressing.}
  Aug 24 12:17:06.163: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:17:06.163: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 24 12:17:06.163: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dgf2r-5994cf9475" has successfully progressed.}
  Aug 24 12:17:06.164: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:17:06.164: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:04 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug 24 12:17:06.164: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-24 12:17:04 +0000 UTC 2023-08-24 12:17:02 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dgf2r-5994cf9475" has successfully progressed.}
  Aug 24 12:17:06.164: INFO: Observed deployment test-deployment-dgf2r in namespace deployment-1477 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:17:06.165: INFO: Observed &Deployment event: MODIFIED
  Aug 24 12:17:06.165: INFO: Found deployment test-deployment-dgf2r in namespace deployment-1477 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Aug 24 12:17:06.165: INFO: Deployment test-deployment-dgf2r has a patched status
  Aug 24 12:17:06.173: INFO: Deployment "test-deployment-dgf2r":
  &Deployment{ObjectMeta:{test-deployment-dgf2r  deployment-1477  7ef3720e-ac45-4e8e-bdd7-a2f1b7200b09 16749 1 2023-08-24 12:17:02 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-24 12:17:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-08-24 12:17:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-08-24 12:17:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0083fb4e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-dgf2r-5994cf9475",LastUpdateTime:2023-08-24 12:17:06 +0000 UTC,LastTransitionTime:2023-08-24 12:17:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 24 12:17:06.182: INFO: New ReplicaSet "test-deployment-dgf2r-5994cf9475" of Deployment "test-deployment-dgf2r":
  &ReplicaSet{ObjectMeta:{test-deployment-dgf2r-5994cf9475  deployment-1477  02448d39-8363-42b0-a0d0-ec8101e5d296 16739 1 2023-08-24 12:17:02 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-dgf2r 7ef3720e-ac45-4e8e-bdd7-a2f1b7200b09 0xc00835d970 0xc00835d971}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:17:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7ef3720e-ac45-4e8e-bdd7-a2f1b7200b09\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:17:04 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00835da18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:17:06.190: INFO: Pod "test-deployment-dgf2r-5994cf9475-5j5th" is available:
  &Pod{ObjectMeta:{test-deployment-dgf2r-5994cf9475-5j5th test-deployment-dgf2r-5994cf9475- deployment-1477  45583096-eeae-489f-8800-7e3fcaeec923 16738 0 2023-08-24 12:17:02 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-dgf2r-5994cf9475 02448d39-8363-42b0-a0d0-ec8101e5d296 0xc00835ddd0 0xc00835ddd1}] [] [{kube-controller-manager Update v1 2023-08-24 12:17:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"02448d39-8363-42b0-a0d0-ec8101e5d296\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:17:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.65\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrd76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrd76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:17:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:17:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:17:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:17:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:10.233.66.65,StartTime:2023-08-24 12:17:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:17:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://78bc6f41238f70a55984e6670e9e3865056b998883e05d2bdb9baba825641c57,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.65,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:17:06.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1477" for this suite. @ 08/24/23 12:17:06.207
• [4.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 08/24/23 12:17:06.232
  Aug 24 12:17:06.232: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:17:06.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:17:06.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:17:06.272
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 08/24/23 12:17:06.275
  STEP: Saw pod success @ 08/24/23 12:17:10.33
  Aug 24 12:17:10.336: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-9d32f13b-db23-4ea3-b6a2-a4d0f4aab22e container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:17:10.366
  Aug 24 12:17:10.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5785" for this suite. @ 08/24/23 12:17:10.402
• [4.182 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 08/24/23 12:17:10.415
  Aug 24 12:17:10.416: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 12:17:10.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:17:10.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:17:10.454
  Aug 24 12:17:10.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6201" for this suite. @ 08/24/23 12:17:10.527
• [0.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 08/24/23 12:17:10.548
  Aug 24 12:17:10.548: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:17:10.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:17:10.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:17:10.584
  STEP: creating the pod @ 08/24/23 12:17:10.587
  STEP: submitting the pod to kubernetes @ 08/24/23 12:17:10.587
  STEP: verifying QOS class is set on the pod @ 08/24/23 12:17:10.601
  Aug 24 12:17:10.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5406" for this suite. @ 08/24/23 12:17:10.636
• [0.104 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 08/24/23 12:17:10.654
  Aug 24 12:17:10.654: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:17:10.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:17:10.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:17:10.693
  Aug 24 12:17:10.704: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: creating the pod @ 08/24/23 12:17:10.705
  STEP: submitting the pod to kubernetes @ 08/24/23 12:17:10.705
  Aug 24 12:17:12.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6281" for this suite. @ 08/24/23 12:17:12.78
• [2.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 08/24/23 12:17:12.819
  Aug 24 12:17:12.819: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:17:12.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:17:12.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:17:12.864
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/24/23 12:17:12.869
  STEP: Saw pod success @ 08/24/23 12:17:16.958
  Aug 24 12:17:16.969: INFO: Trying to get logs from node quohp9aeph3i-1 pod pod-08fc24fa-a936-4b35-8d03-659f43ab6a39 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:17:17.004
  Aug 24 12:17:17.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9269" for this suite. @ 08/24/23 12:17:17.047
• [4.240 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 08/24/23 12:17:17.061
  Aug 24 12:17:17.061: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:17:17.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:17:17.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:17:17.103
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:17:17.107
  STEP: Saw pod success @ 08/24/23 12:17:21.157
  Aug 24 12:17:21.162: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-4c0c1697-8b82-45b6-8071-0f91c313ac82 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:17:21.178
  Aug 24 12:17:21.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9955" for this suite. @ 08/24/23 12:17:21.26
• [4.215 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 08/24/23 12:17:21.277
  Aug 24 12:17:21.277: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:17:21.28
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:17:21.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:17:21.315
  STEP: Creating pod test-grpc-e662a2d4-2263-49e8-9af2-0d72264ebc51 in namespace container-probe-337 @ 08/24/23 12:17:21.323
  Aug 24 12:17:23.356: INFO: Started pod test-grpc-e662a2d4-2263-49e8-9af2-0d72264ebc51 in namespace container-probe-337
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 12:17:23.356
  Aug 24 12:17:23.363: INFO: Initial restart count of pod test-grpc-e662a2d4-2263-49e8-9af2-0d72264ebc51 is 0
  Aug 24 12:21:24.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:21:24.478
  STEP: Destroying namespace "container-probe-337" for this suite. @ 08/24/23 12:21:24.509
• [243.269 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 08/24/23 12:21:24.549
  Aug 24 12:21:24.549: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:21:24.553
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:21:24.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:21:24.607
  STEP: Creating a pod to test emptydir volume type on node default medium @ 08/24/23 12:21:24.612
  STEP: Saw pod success @ 08/24/23 12:21:28.669
  Aug 24 12:21:28.676: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-6379dea2-f1b4-48bd-9767-af18e46c564f container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:21:28.707
  Aug 24 12:21:28.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7336" for this suite. @ 08/24/23 12:21:28.748
• [4.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 08/24/23 12:21:28.773
  Aug 24 12:21:28.773: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename tables @ 08/24/23 12:21:28.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:21:28.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:21:28.823
  Aug 24 12:21:28.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-1233" for this suite. @ 08/24/23 12:21:28.848
• [0.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 08/24/23 12:21:28.869
  Aug 24 12:21:28.869: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:21:28.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:21:28.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:21:28.914
  STEP: Creating pod test-webserver-2235f3fc-cf41-4aef-b884-7e95f8d4224d in namespace container-probe-3000 @ 08/24/23 12:21:28.919
  Aug 24 12:21:30.957: INFO: Started pod test-webserver-2235f3fc-cf41-4aef-b884-7e95f8d4224d in namespace container-probe-3000
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 12:21:30.957
  Aug 24 12:21:30.965: INFO: Initial restart count of pod test-webserver-2235f3fc-cf41-4aef-b884-7e95f8d4224d is 0
  Aug 24 12:25:32.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:25:32.08
  STEP: Destroying namespace "container-probe-3000" for this suite. @ 08/24/23 12:25:32.106
• [243.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 08/24/23 12:25:32.135
  Aug 24 12:25:32.135: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename endpointslice @ 08/24/23 12:25:32.145
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:32.182
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:32.187
  Aug 24 12:25:32.211: INFO: Endpoints addresses: [192.168.121.19 192.168.121.64] , ports: [6443]
  Aug 24 12:25:32.211: INFO: EndpointSlices addresses: [192.168.121.19 192.168.121.64] , ports: [6443]
  Aug 24 12:25:32.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8951" for this suite. @ 08/24/23 12:25:32.218
• [0.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 08/24/23 12:25:32.233
  Aug 24 12:25:32.233: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:25:32.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:32.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:32.276
  STEP: Create set of pods @ 08/24/23 12:25:32.281
  Aug 24 12:25:32.305: INFO: created test-pod-1
  Aug 24 12:25:32.321: INFO: created test-pod-2
  Aug 24 12:25:32.336: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 08/24/23 12:25:32.336
  STEP: waiting for all pods to be deleted @ 08/24/23 12:25:36.482
  Aug 24 12:25:36.490: INFO: Pod quantity 3 is different from expected quantity 0
  Aug 24 12:25:37.503: INFO: Pod quantity 2 is different from expected quantity 0
  Aug 24 12:25:38.498: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9254" for this suite. @ 08/24/23 12:25:38.509
• [6.288 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 08/24/23 12:25:38.535
  Aug 24 12:25:38.535: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:25:38.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:38.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:38.569
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/24/23 12:25:38.573
  Aug 24 12:25:38.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-5554 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug 24 12:25:38.727: INFO: stderr: ""
  Aug 24 12:25:38.727: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 08/24/23 12:25:38.727
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/24/23 12:25:43.78
  Aug 24 12:25:43.780: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-5554 get pod e2e-test-httpd-pod -o json'
  Aug 24 12:25:43.965: INFO: stderr: ""
  Aug 24 12:25:43.965: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-24T12:25:38Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-5554\",\n        \"resourceVersion\": \"18282\",\n        \"uid\": \"f42c2030-7421-4be4-85a7-d1aeaf47b7f7\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-xl2df\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"quohp9aeph3i-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-xl2df\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T12:25:38Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T12:25:40Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T12:25:40Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-24T12:25:38Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://6fd96893d99654ba686f8567a752c3f2bbc8f85234e0d13954b956299466ee61\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-24T12:25:39Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.15\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.26\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.26\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-24T12:25:38Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 08/24/23 12:25:43.965
  Aug 24 12:25:43.965: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-5554 replace -f -'
  Aug 24 12:25:44.683: INFO: stderr: ""
  Aug 24 12:25:44.683: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 08/24/23 12:25:44.683
  Aug 24 12:25:44.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-5554 delete pods e2e-test-httpd-pod'
  Aug 24 12:25:47.128: INFO: stderr: ""
  Aug 24 12:25:47.128: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 24 12:25:47.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5554" for this suite. @ 08/24/23 12:25:47.138
• [8.618 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 08/24/23 12:25:47.153
  Aug 24 12:25:47.153: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/24/23 12:25:47.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:47.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:47.188
  STEP: create the container to handle the HTTPGet hook request. @ 08/24/23 12:25:47.2
  STEP: create the pod with lifecycle hook @ 08/24/23 12:25:51.251
  STEP: delete the pod with lifecycle hook @ 08/24/23 12:25:53.288
  STEP: check prestop hook @ 08/24/23 12:25:55.323
  Aug 24 12:25:55.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8794" for this suite. @ 08/24/23 12:25:55.369
• [8.233 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 08/24/23 12:25:55.387
  Aug 24 12:25:55.387: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 12:25:55.39
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:25:55.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:25:55.422
  STEP: Creating a simple DaemonSet "daemon-set" @ 08/24/23 12:25:55.471
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 12:25:55.486
  Aug 24 12:25:55.505: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:25:55.506: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:25:56.536: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:25:56.536: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:25:57.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:25:57.552: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:25:58.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:25:58.534: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 08/24/23 12:25:58.542
  Aug 24 12:25:58.602: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:25:58.602: INFO: Node quohp9aeph3i-3 is running 0 daemon pod, expected 1
  Aug 24 12:25:59.626: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:25:59.626: INFO: Node quohp9aeph3i-3 is running 0 daemon pod, expected 1
  Aug 24 12:26:00.617: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:26:00.618: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 08/24/23 12:26:00.618
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 12:26:00.63
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5433, will wait for the garbage collector to delete the pods @ 08/24/23 12:26:00.63
  Aug 24 12:26:00.714: INFO: Deleting DaemonSet.extensions daemon-set took: 25.479957ms
  Aug 24 12:26:00.915: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.870705ms
  Aug 24 12:26:02.222: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:26:02.222: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 12:26:02.235: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18510"},"items":null}

  Aug 24 12:26:02.241: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18510"},"items":null}

  Aug 24 12:26:02.277: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5433" for this suite. @ 08/24/23 12:26:02.285
• [6.913 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 08/24/23 12:26:02.3
  Aug 24 12:26:02.300: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/24/23 12:26:02.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:02.328
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:02.333
  STEP: create the container to handle the HTTPGet hook request. @ 08/24/23 12:26:02.345
  STEP: create the pod with lifecycle hook @ 08/24/23 12:26:04.388
  STEP: check poststart hook @ 08/24/23 12:26:06.433
  STEP: delete the pod with lifecycle hook @ 08/24/23 12:26:06.47
  Aug 24 12:26:08.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4132" for this suite. @ 08/24/23 12:26:08.515
• [6.226 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 08/24/23 12:26:08.532
  Aug 24 12:26:08.532: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 12:26:08.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:26:08.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:26:08.563
  STEP: Creating service test in namespace statefulset-702 @ 08/24/23 12:26:08.568
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 08/24/23 12:26:08.584
  STEP: Creating stateful set ss in namespace statefulset-702 @ 08/24/23 12:26:08.591
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-702 @ 08/24/23 12:26:08.603
  Aug 24 12:26:08.610: INFO: Found 0 stateful pods, waiting for 1
  Aug 24 12:26:18.621: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 08/24/23 12:26:18.621
  Aug 24 12:26:18.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-702 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:26:18.966: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:26:18.966: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:26:18.966: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:26:18.975: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Aug 24 12:26:28.983: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:26:28.983: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:26:29.014: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999387s
  Aug 24 12:26:30.022: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990739957s
  Aug 24 12:26:31.030: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.982461048s
  Aug 24 12:26:32.044: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.973874129s
  Aug 24 12:26:33.053: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.960504846s
  Aug 24 12:26:34.062: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.951356046s
  Aug 24 12:26:35.071: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.94197226s
  Aug 24 12:26:36.079: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.933485001s
  Aug 24 12:26:37.088: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.925978743s
  Aug 24 12:26:38.096: INFO: Verifying statefulset ss doesn't scale past 1 for another 916.906169ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-702 @ 08/24/23 12:26:39.097
  Aug 24 12:26:39.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-702 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 12:26:39.434: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 12:26:39.435: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:26:39.435: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:26:39.445: INFO: Found 1 stateful pods, waiting for 3
  Aug 24 12:26:49.458: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:26:49.458: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:26:49.458: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 08/24/23 12:26:49.458
  STEP: Scale down will halt with unhealthy stateful pod @ 08/24/23 12:26:49.458
  Aug 24 12:26:49.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-702 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:26:49.748: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:26:49.748: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:26:49.748: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:26:49.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-702 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:26:50.067: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:26:50.067: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:26:50.067: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:26:50.067: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-702 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:26:50.355: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:26:50.355: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:26:50.355: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:26:50.355: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:26:50.363: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Aug 24 12:27:00.378: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:27:00.378: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:27:00.378: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:27:00.399: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999479s
  Aug 24 12:27:01.408: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993552989s
  Aug 24 12:27:02.418: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.984610036s
  Aug 24 12:27:03.427: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.97462562s
  Aug 24 12:27:04.443: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.965791201s
  Aug 24 12:27:05.453: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.950380155s
  Aug 24 12:27:06.462: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.939582041s
  Aug 24 12:27:07.474: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.930282624s
  Aug 24 12:27:08.482: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.919365394s
  Aug 24 12:27:09.492: INFO: Verifying statefulset ss doesn't scale past 3 for another 910.92899ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-702 @ 08/24/23 12:27:10.492
  Aug 24 12:27:10.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-702 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 12:27:10.824: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 12:27:10.824: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:27:10.824: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:27:10.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-702 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 12:27:11.149: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 12:27:11.149: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:27:11.149: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:27:11.150: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-702 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 12:27:11.430: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 12:27:11.430: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:27:11.430: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:27:11.430: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 08/24/23 12:27:21.474
  Aug 24 12:27:21.475: INFO: Deleting all statefulset in ns statefulset-702
  Aug 24 12:27:21.484: INFO: Scaling statefulset ss to 0
  Aug 24 12:27:21.503: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:27:21.508: INFO: Deleting statefulset ss
  Aug 24 12:27:21.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-702" for this suite. @ 08/24/23 12:27:21.555
• [73.039 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 08/24/23 12:27:21.573
  Aug 24 12:27:21.573: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 12:27:21.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:27:21.622
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:27:21.628
  Aug 24 12:27:21.678: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 12:27:21.689
  Aug 24 12:27:21.705: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:27:21.705: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:27:22.733: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:27:22.733: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:27:23.725: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:27:23.726: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 08/24/23 12:27:23.761
  STEP: Check that daemon pods images are updated. @ 08/24/23 12:27:23.794
  Aug 24 12:27:23.804: INFO: Wrong image for pod: daemon-set-45mcv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 12:27:23.805: INFO: Wrong image for pod: daemon-set-pdbzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 12:27:23.807: INFO: Wrong image for pod: daemon-set-tqrrw. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 12:27:24.843: INFO: Pod daemon-set-2tzdj is not available
  Aug 24 12:27:24.843: INFO: Wrong image for pod: daemon-set-45mcv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 12:27:24.843: INFO: Wrong image for pod: daemon-set-pdbzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 12:27:25.836: INFO: Pod daemon-set-2tzdj is not available
  Aug 24 12:27:25.838: INFO: Wrong image for pod: daemon-set-45mcv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 12:27:25.838: INFO: Wrong image for pod: daemon-set-pdbzd. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 12:27:26.847: INFO: Wrong image for pod: daemon-set-45mcv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 12:27:27.838: INFO: Wrong image for pod: daemon-set-45mcv. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug 24 12:27:27.838: INFO: Pod daemon-set-vvv4t is not available
  Aug 24 12:27:28.854: INFO: Pod daemon-set-k765z is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 08/24/23 12:27:28.863
  Aug 24 12:27:28.952: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:27:28.952: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:27:29.971: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:27:29.971: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:27:30.978: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:27:30.978: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 12:27:31.027
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6983, will wait for the garbage collector to delete the pods @ 08/24/23 12:27:31.027
  Aug 24 12:27:31.106: INFO: Deleting DaemonSet.extensions daemon-set took: 20.565255ms
  Aug 24 12:27:31.207: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.774896ms
  Aug 24 12:27:34.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:27:34.014: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 12:27:34.018: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19165"},"items":null}

  Aug 24 12:27:34.025: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19165"},"items":null}

  Aug 24 12:27:34.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6983" for this suite. @ 08/24/23 12:27:34.065
• [12.503 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 08/24/23 12:27:34.078
  Aug 24 12:27:34.078: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 12:27:34.08
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:27:34.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:27:34.106
  STEP: Creating service test in namespace statefulset-1497 @ 08/24/23 12:27:34.112
  STEP: Creating statefulset ss in namespace statefulset-1497 @ 08/24/23 12:27:34.12
  Aug 24 12:27:34.144: INFO: Found 0 stateful pods, waiting for 1
  Aug 24 12:27:44.153: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 08/24/23 12:27:44.166
  STEP: updating a scale subresource @ 08/24/23 12:27:44.172
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/24/23 12:27:44.182
  STEP: Patch a scale subresource @ 08/24/23 12:27:44.189
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/24/23 12:27:44.212
  Aug 24 12:27:44.224: INFO: Deleting all statefulset in ns statefulset-1497
  Aug 24 12:27:44.258: INFO: Scaling statefulset ss to 0
  Aug 24 12:27:54.301: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:27:54.307: INFO: Deleting statefulset ss
  Aug 24 12:27:54.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1497" for this suite. @ 08/24/23 12:27:54.352
• [20.287 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 08/24/23 12:27:54.366
  Aug 24 12:27:54.367: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:27:54.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:27:54.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:27:54.405
  STEP: Creating configMap configmap-2779/configmap-test-1c3732bf-4367-4eec-b938-c1dc4555f352 @ 08/24/23 12:27:54.41
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:27:54.421
  STEP: Saw pod success @ 08/24/23 12:27:58.466
  Aug 24 12:27:58.472: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-dfd6ae26-a1db-417f-93b9-0544a6ce24a1 container env-test: <nil>
  STEP: delete the pod @ 08/24/23 12:27:58.506
  Aug 24 12:27:58.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2779" for this suite. @ 08/24/23 12:27:58.542
• [4.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 08/24/23 12:27:58.564
  Aug 24 12:27:58.564: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:27:58.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:27:58.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:27:58.597
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:27:58.603
  STEP: Saw pod success @ 08/24/23 12:28:02.651
  Aug 24 12:28:02.656: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-0b11bbb4-fa5f-43b2-8f3e-af2adbf83e95 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:28:02.67
  Aug 24 12:28:02.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-735" for this suite. @ 08/24/23 12:28:02.705
• [4.152 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 08/24/23 12:28:02.717
  Aug 24 12:28:02.717: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/24/23 12:28:02.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:28:02.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:28:02.747
  STEP: create the container to handle the HTTPGet hook request. @ 08/24/23 12:28:02.761
  STEP: create the pod with lifecycle hook @ 08/24/23 12:28:04.813
  STEP: check poststart hook @ 08/24/23 12:28:06.846
  STEP: delete the pod with lifecycle hook @ 08/24/23 12:28:06.873
  Aug 24 12:28:08.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1239" for this suite. @ 08/24/23 12:28:08.921
• [6.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 08/24/23 12:28:08.941
  Aug 24 12:28:08.941: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 12:28:08.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:28:08.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:28:08.977
  Aug 24 12:28:11.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7899" for this suite. @ 08/24/23 12:28:11.05
• [2.121 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 08/24/23 12:28:11.064
  Aug 24 12:28:11.064: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 12:28:11.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:28:11.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:28:11.098
  STEP: Creating service test in namespace statefulset-5772 @ 08/24/23 12:28:11.104
  STEP: Creating stateful set ss in namespace statefulset-5772 @ 08/24/23 12:28:11.115
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5772 @ 08/24/23 12:28:11.133
  Aug 24 12:28:11.150: INFO: Found 0 stateful pods, waiting for 1
  Aug 24 12:28:21.156: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 08/24/23 12:28:21.156
  Aug 24 12:28:21.162: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-5772 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:28:21.477: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:28:21.477: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:28:21.477: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:28:21.490: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Aug 24 12:28:31.497: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:28:31.497: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:28:31.523: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Aug 24 12:28:31.524: INFO: ss-0  quohp9aeph3i-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:11 +0000 UTC  }]
  Aug 24 12:28:31.524: INFO: 
  Aug 24 12:28:31.524: INFO: StatefulSet ss has not reached scale 3, at 1
  Aug 24 12:28:32.532: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.990241741s
  Aug 24 12:28:33.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980342657s
  Aug 24 12:28:34.554: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970150485s
  Aug 24 12:28:35.563: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.960216467s
  Aug 24 12:28:36.571: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.952146065s
  Aug 24 12:28:37.582: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.943980912s
  Aug 24 12:28:38.591: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.933291002s
  Aug 24 12:28:39.600: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.92408048s
  Aug 24 12:28:40.611: INFO: Verifying statefulset ss doesn't scale past 3 for another 914.662856ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5772 @ 08/24/23 12:28:41.611
  Aug 24 12:28:41.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-5772 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 12:28:41.927: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 12:28:41.927: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:28:41.927: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:28:41.928: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-5772 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 12:28:42.220: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug 24 12:28:42.220: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:28:42.220: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:28:42.221: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-5772 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 12:28:42.535: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug 24 12:28:42.535: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 12:28:42.535: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug 24 12:28:42.552: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  Aug 24 12:28:52.565: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:28:52.566: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 12:28:52.566: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 08/24/23 12:28:52.566
  Aug 24 12:28:52.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-5772 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:28:52.858: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:28:52.858: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:28:52.858: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:28:52.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-5772 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:28:53.128: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:28:53.128: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:28:53.128: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:28:53.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-5772 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 12:28:53.411: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 12:28:53.411: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 12:28:53.411: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug 24 12:28:53.411: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:28:53.419: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Aug 24 12:29:03.434: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:29:03.434: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:29:03.434: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug 24 12:29:03.462: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Aug 24 12:29:03.462: INFO: ss-0  quohp9aeph3i-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:11 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:11 +0000 UTC  }]
  Aug 24 12:29:03.463: INFO: ss-1  quohp9aeph3i-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:31 +0000 UTC  }]
  Aug 24 12:29:03.463: INFO: ss-2  quohp9aeph3i-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:31 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:31 +0000 UTC  }]
  Aug 24 12:29:03.464: INFO: 
  Aug 24 12:29:03.464: INFO: StatefulSet ss has not reached scale 0, at 3
  Aug 24 12:29:04.473: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Aug 24 12:29:04.473: INFO: ss-0  quohp9aeph3i-3  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:11 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:11 +0000 UTC  }]
  Aug 24 12:29:04.473: INFO: ss-1  quohp9aeph3i-2  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:31 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:53 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 12:28:31 +0000 UTC  }]
  Aug 24 12:29:04.473: INFO: 
  Aug 24 12:29:04.474: INFO: StatefulSet ss has not reached scale 0, at 2
  Aug 24 12:29:05.482: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.978339458s
  Aug 24 12:29:06.490: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.969824671s
  Aug 24 12:29:07.495: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.962087852s
  Aug 24 12:29:08.503: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.955815097s
  Aug 24 12:29:09.510: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.949196985s
  Aug 24 12:29:10.519: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.941789723s
  Aug 24 12:29:11.527: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.931898346s
  Aug 24 12:29:12.535: INFO: Verifying statefulset ss doesn't scale past 0 for another 924.721393ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5772 @ 08/24/23 12:29:13.536
  Aug 24 12:29:13.546: INFO: Scaling statefulset ss to 0
  Aug 24 12:29:13.568: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:29:13.575: INFO: Deleting all statefulset in ns statefulset-5772
  Aug 24 12:29:13.583: INFO: Scaling statefulset ss to 0
  Aug 24 12:29:13.611: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:29:13.617: INFO: Deleting statefulset ss
  Aug 24 12:29:13.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5772" for this suite. @ 08/24/23 12:29:13.651
• [62.599 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 08/24/23 12:29:13.666
  Aug 24 12:29:13.666: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename deployment @ 08/24/23 12:29:13.669
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:29:13.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:29:13.708
  Aug 24 12:29:13.713: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Aug 24 12:29:13.732: INFO: Pod name sample-pod: Found 0 pods out of 1
  Aug 24 12:29:18.741: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 12:29:18.741
  Aug 24 12:29:18.741: INFO: Creating deployment "test-rolling-update-deployment"
  Aug 24 12:29:18.753: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Aug 24 12:29:18.770: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Aug 24 12:29:20.785: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Aug 24 12:29:20.790: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Aug 24 12:29:20.809: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-4589  c99c8890-5ef3-49f7-82a7-1fe9c482e65f 19929 1 2023-08-24 12:29:18 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-24 12:29:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:29:20 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003da3278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 12:29:18 +0000 UTC,LastTransitionTime:2023-08-24 12:29:18 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-08-24 12:29:20 +0000 UTC,LastTransitionTime:2023-08-24 12:29:18 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 24 12:29:20.820: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-4589  5d244304-b53d-41d0-9a45-4f1859179dcf 19918 1 2023-08-24 12:29:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment c99c8890-5ef3-49f7-82a7-1fe9c482e65f 0xc003e16a07 0xc003e16a08}] [] [{kube-controller-manager Update apps/v1 2023-08-24 12:29:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99c8890-5ef3-49f7-82a7-1fe9c482e65f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:29:19 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e16ab8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:29:20.820: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Aug 24 12:29:20.821: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-4589  46731d3b-69aa-402d-b781-c228afa8ce4f 19927 2 2023-08-24 12:29:13 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment c99c8890-5ef3-49f7-82a7-1fe9c482e65f 0xc003e168d7 0xc003e168d8}] [] [{e2e.test Update apps/v1 2023-08-24 12:29:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:29:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c99c8890-5ef3-49f7-82a7-1fe9c482e65f\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 12:29:20 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e16998 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 12:29:20.829: INFO: Pod "test-rolling-update-deployment-656d657cd8-h5zzn" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-h5zzn test-rolling-update-deployment-656d657cd8- deployment-4589  612504be-ae99-4852-a1d1-c37573610c55 19917 0 2023-08-24 12:29:18 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 5d244304-b53d-41d0-9a45-4f1859179dcf 0xc003e17517 0xc003e17518}] [] [{kube-controller-manager Update v1 2023-08-24 12:29:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5d244304-b53d-41d0-9a45-4f1859179dcf\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 12:29:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-f6q58,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-f6q58,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:29:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:29:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 12:29:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:10.233.66.190,StartTime:2023-08-24 12:29:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 12:29:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://3a22a6881d9cb161606ab37d2985bf18fd149c4fa648fe14e8469214ed8dbb9d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.190,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 12:29:20.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4589" for this suite. @ 08/24/23 12:29:20.842
• [7.191 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 08/24/23 12:29:20.867
  Aug 24 12:29:20.867: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 12:29:20.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:29:20.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:29:20.931
  Aug 24 12:29:20.991: INFO: Create a RollingUpdate DaemonSet
  Aug 24 12:29:21.001: INFO: Check that daemon pods launch on every node of the cluster
  Aug 24 12:29:21.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:29:21.026: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:29:22.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:29:22.055: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  Aug 24 12:29:23.046: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:29:23.047: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Aug 24 12:29:23.047: INFO: Update the DaemonSet to trigger a rollout
  Aug 24 12:29:23.067: INFO: Updating DaemonSet daemon-set
  Aug 24 12:29:24.110: INFO: Roll back the DaemonSet before rollout is complete
  Aug 24 12:29:24.140: INFO: Updating DaemonSet daemon-set
  Aug 24 12:29:24.140: INFO: Make sure DaemonSet rollback is complete
  Aug 24 12:29:24.153: INFO: Wrong image for pod: daemon-set-wbzkv. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Aug 24 12:29:24.154: INFO: Pod daemon-set-wbzkv is not available
  Aug 24 12:29:30.179: INFO: Pod daemon-set-l9jtn is not available
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 12:29:30.2
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-114, will wait for the garbage collector to delete the pods @ 08/24/23 12:29:30.2
  Aug 24 12:29:30.273: INFO: Deleting DaemonSet.extensions daemon-set took: 15.71604ms
  Aug 24 12:29:30.375: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.698071ms
  Aug 24 12:29:33.485: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:29:33.485: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 12:29:33.491: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"20095"},"items":null}

  Aug 24 12:29:33.500: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"20095"},"items":null}

  Aug 24 12:29:33.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-114" for this suite. @ 08/24/23 12:29:33.535
• [12.684 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 08/24/23 12:29:33.57
  Aug 24 12:29:33.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:29:33.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:29:33.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:29:33.605
  STEP: Setting up server cert @ 08/24/23 12:29:33.645
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:29:34.713
  STEP: Deploying the webhook pod @ 08/24/23 12:29:34.73
  STEP: Wait for the deployment to be ready @ 08/24/23 12:29:34.747
  Aug 24 12:29:34.760: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/24/23 12:29:36.781
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:29:36.802
  Aug 24 12:29:37.802: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:29:37.813: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7056-crds.webhook.example.com via the AdmissionRegistration API @ 08/24/23 12:29:38.338
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/24/23 12:29:38.38
  Aug 24 12:29:40.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2649" for this suite. @ 08/24/23 12:29:41.2
  STEP: Destroying namespace "webhook-markers-6836" for this suite. @ 08/24/23 12:29:41.231
• [7.676 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 08/24/23 12:29:41.258
  Aug 24 12:29:41.258: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:29:41.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:29:41.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:29:41.306
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/24/23 12:29:41.312
  Aug 24 12:29:41.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-7783 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug 24 12:29:41.471: INFO: stderr: ""
  Aug 24 12:29:41.471: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 08/24/23 12:29:41.471
  Aug 24 12:29:41.472: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-7783 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Aug 24 12:29:41.640: INFO: stderr: ""
  Aug 24 12:29:41.640: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/24/23 12:29:41.64
  Aug 24 12:29:41.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-7783 delete pods e2e-test-httpd-pod'
  Aug 24 12:29:44.046: INFO: stderr: ""
  Aug 24 12:29:44.046: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 24 12:29:44.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7783" for this suite. @ 08/24/23 12:29:44.056
• [2.815 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 08/24/23 12:29:44.082
  Aug 24 12:29:44.083: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:29:44.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:29:44.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:29:44.131
  STEP: Counting existing ResourceQuota @ 08/24/23 12:30:01.144
  STEP: Creating a ResourceQuota @ 08/24/23 12:30:06.154
  STEP: Ensuring resource quota status is calculated @ 08/24/23 12:30:06.164
  STEP: Creating a ConfigMap @ 08/24/23 12:30:08.173
  STEP: Ensuring resource quota status captures configMap creation @ 08/24/23 12:30:08.193
  STEP: Deleting a ConfigMap @ 08/24/23 12:30:10.201
  STEP: Ensuring resource quota status released usage @ 08/24/23 12:30:10.22
  Aug 24 12:30:12.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1633" for this suite. @ 08/24/23 12:30:12.243
• [28.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 08/24/23 12:30:12.267
  Aug 24 12:30:12.267: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename gc @ 08/24/23 12:30:12.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:30:12.3
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:30:12.304
  STEP: create the deployment @ 08/24/23 12:30:12.308
  W0824 12:30:12.320471      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/24/23 12:30:12.32
  STEP: delete the deployment @ 08/24/23 12:30:12.872
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 08/24/23 12:30:12.93
  STEP: Gathering metrics @ 08/24/23 12:30:13.507
  Aug 24 12:30:13.676: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 12:30:13.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5858" for this suite. @ 08/24/23 12:30:13.687
• [1.434 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 08/24/23 12:30:13.71
  Aug 24 12:30:13.711: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename containers @ 08/24/23 12:30:13.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:30:13.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:30:13.748
  STEP: Creating a pod to test override command @ 08/24/23 12:30:13.753
  STEP: Saw pod success @ 08/24/23 12:30:17.791
  Aug 24 12:30:17.799: INFO: Trying to get logs from node quohp9aeph3i-3 pod client-containers-e66542b2-dc9b-4b01-b8d9-d31dc1dda8aa container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:30:17.827
  Aug 24 12:30:17.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6897" for this suite. @ 08/24/23 12:30:17.857
• [4.164 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 08/24/23 12:30:17.876
  Aug 24 12:30:17.876: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:30:17.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:30:17.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:30:17.915
  STEP: Setting up server cert @ 08/24/23 12:30:17.961
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:30:18.594
  STEP: Deploying the webhook pod @ 08/24/23 12:30:18.608
  STEP: Wait for the deployment to be ready @ 08/24/23 12:30:18.626
  Aug 24 12:30:18.652: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/24/23 12:30:20.67
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:30:20.688
  Aug 24 12:30:21.688: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 08/24/23 12:30:21.697
  STEP: create a pod that should be updated by the webhook @ 08/24/23 12:30:21.729
  Aug 24 12:30:21.764: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1201" for this suite. @ 08/24/23 12:30:21.911
  STEP: Destroying namespace "webhook-markers-3995" for this suite. @ 08/24/23 12:30:21.924
• [4.066 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 08/24/23 12:30:21.943
  Aug 24 12:30:21.943: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:30:21.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:30:21.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:30:21.983
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:30:21.988
  STEP: Saw pod success @ 08/24/23 12:30:26.079
  Aug 24 12:30:26.087: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-c1152c00-d637-478d-9f96-e1995407f3e8 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:30:26.099
  Aug 24 12:30:26.132: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7299" for this suite. @ 08/24/23 12:30:26.143
• [4.217 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 08/24/23 12:30:26.166
  Aug 24 12:30:26.166: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 12:30:26.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:30:26.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:30:26.201
  STEP: getting /apis @ 08/24/23 12:30:26.206
  STEP: getting /apis/node.k8s.io @ 08/24/23 12:30:26.214
  STEP: getting /apis/node.k8s.io/v1 @ 08/24/23 12:30:26.216
  STEP: creating @ 08/24/23 12:30:26.218
  STEP: watching @ 08/24/23 12:30:26.249
  Aug 24 12:30:26.249: INFO: starting watch
  STEP: getting @ 08/24/23 12:30:26.262
  STEP: listing @ 08/24/23 12:30:26.269
  STEP: patching @ 08/24/23 12:30:26.274
  STEP: updating @ 08/24/23 12:30:26.285
  Aug 24 12:30:26.295: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 08/24/23 12:30:26.296
  STEP: deleting a collection @ 08/24/23 12:30:26.32
  Aug 24 12:30:26.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5686" for this suite. @ 08/24/23 12:30:26.359
• [0.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 08/24/23 12:30:26.373
  Aug 24 12:30:26.374: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:30:26.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:30:26.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:30:26.409
  STEP: Creating configMap with name configmap-test-volume-00ea9e47-373c-4de2-9a25-fa2207bfcab1 @ 08/24/23 12:30:26.412
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:30:26.42
  STEP: Saw pod success @ 08/24/23 12:30:30.466
  Aug 24 12:30:30.473: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-2c1f1070-1cec-4cc9-86a6-39796947419e container configmap-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:30:30.489
  Aug 24 12:30:30.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2466" for this suite. @ 08/24/23 12:30:30.535
• [4.173 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 08/24/23 12:30:30.551
  Aug 24 12:30:30.551: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:30:30.554
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:30:30.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:30:30.588
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:30:30.593
  STEP: Saw pod success @ 08/24/23 12:30:34.629
  Aug 24 12:30:34.636: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-e4f50e84-89d9-47fa-bd97-a05c54527134 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:30:34.648
  Aug 24 12:30:34.677: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3838" for this suite. @ 08/24/23 12:30:34.686
• [4.148 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 08/24/23 12:30:34.701
  Aug 24 12:30:34.701: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 12:30:34.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:30:34.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:30:34.741
  STEP: Updating Namespace "namespaces-2881" @ 08/24/23 12:30:34.747
  Aug 24 12:30:34.770: INFO: Namespace "namespaces-2881" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"388bc1eb-53bb-4308-9fd0-f4aa3ff2edb9", "kubernetes.io/metadata.name":"namespaces-2881", "namespaces-2881":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Aug 24 12:30:34.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2881" for this suite. @ 08/24/23 12:30:34.783
• [0.094 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 08/24/23 12:30:34.8
  Aug 24 12:30:34.800: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-preemption @ 08/24/23 12:30:34.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:30:34.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:30:34.844
  Aug 24 12:30:34.877: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug 24 12:31:34.934: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/24/23 12:31:34.941
  Aug 24 12:31:34.942: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/24/23 12:31:34.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:34.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:34.977
  Aug 24 12:31:35.009: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Aug 24 12:31:35.016: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Aug 24 12:31:35.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:31:35.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-8286" for this suite. @ 08/24/23 12:31:35.164
  STEP: Destroying namespace "sched-preemption-1646" for this suite. @ 08/24/23 12:31:35.175
• [60.386 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 08/24/23 12:31:35.186
  Aug 24 12:31:35.186: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 12:31:35.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:35.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:35.215
  Aug 24 12:31:35.226: INFO: Got root ca configmap in namespace "svcaccounts-3866"
  Aug 24 12:31:35.237: INFO: Deleted root ca configmap in namespace "svcaccounts-3866"
  STEP: waiting for a new root ca configmap created @ 08/24/23 12:31:35.739
  Aug 24 12:31:35.747: INFO: Recreated root ca configmap in namespace "svcaccounts-3866"
  Aug 24 12:31:35.756: INFO: Updated root ca configmap in namespace "svcaccounts-3866"
  STEP: waiting for the root ca configmap reconciled @ 08/24/23 12:31:36.257
  Aug 24 12:31:36.265: INFO: Reconciled root ca configmap in namespace "svcaccounts-3866"
  Aug 24 12:31:36.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3866" for this suite. @ 08/24/23 12:31:36.273
• [1.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 08/24/23 12:31:36.289
  Aug 24 12:31:36.289: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:31:36.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:36.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:36.322
  Aug 24 12:31:36.328: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/24/23 12:31:38.401
  Aug 24 12:31:38.402: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-392 --namespace=crd-publish-openapi-392 create -f -'
  Aug 24 12:31:39.822: INFO: stderr: ""
  Aug 24 12:31:39.822: INFO: stdout: "e2e-test-crd-publish-openapi-459-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug 24 12:31:39.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-392 --namespace=crd-publish-openapi-392 delete e2e-test-crd-publish-openapi-459-crds test-cr'
  Aug 24 12:31:39.990: INFO: stderr: ""
  Aug 24 12:31:39.990: INFO: stdout: "e2e-test-crd-publish-openapi-459-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Aug 24 12:31:39.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-392 --namespace=crd-publish-openapi-392 apply -f -'
  Aug 24 12:31:40.548: INFO: stderr: ""
  Aug 24 12:31:40.548: INFO: stdout: "e2e-test-crd-publish-openapi-459-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug 24 12:31:40.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-392 --namespace=crd-publish-openapi-392 delete e2e-test-crd-publish-openapi-459-crds test-cr'
  Aug 24 12:31:40.737: INFO: stderr: ""
  Aug 24 12:31:40.737: INFO: stdout: "e2e-test-crd-publish-openapi-459-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/24/23 12:31:40.737
  Aug 24 12:31:40.738: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-392 explain e2e-test-crd-publish-openapi-459-crds'
  Aug 24 12:31:41.209: INFO: stderr: ""
  Aug 24 12:31:41.209: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-459-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Aug 24 12:31:44.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-392" for this suite. @ 08/24/23 12:31:44.495
• [8.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 08/24/23 12:31:44.512
  Aug 24 12:31:44.512: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename security-context-test @ 08/24/23 12:31:44.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:44.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:44.556
  Aug 24 12:31:48.604: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8412" for this suite. @ 08/24/23 12:31:48.612
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 08/24/23 12:31:48.627
  Aug 24 12:31:48.627: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:31:48.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:48.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:48.67
  STEP: creating pod @ 08/24/23 12:31:48.676
  Aug 24 12:31:50.724: INFO: Pod pod-hostip-f0fc1227-dbc9-42d1-adb7-82977c4888ae has hostIP: 192.168.121.15
  Aug 24 12:31:50.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7584" for this suite. @ 08/24/23 12:31:50.733
• [2.117 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 08/24/23 12:31:50.744
  Aug 24 12:31:50.744: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:31:50.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:50.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:50.781
  STEP: Setting up server cert @ 08/24/23 12:31:50.843
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:31:51.972
  STEP: Deploying the webhook pod @ 08/24/23 12:31:51.984
  STEP: Wait for the deployment to be ready @ 08/24/23 12:31:52.028
  Aug 24 12:31:52.051: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/24/23 12:31:54.073
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:31:54.097
  Aug 24 12:31:55.098: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 08/24/23 12:31:55.111
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 08/24/23 12:31:55.151
  STEP: Creating a configMap that should not be mutated @ 08/24/23 12:31:55.162
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 08/24/23 12:31:55.181
  STEP: Creating a configMap that should be mutated @ 08/24/23 12:31:55.192
  Aug 24 12:31:55.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4243" for this suite. @ 08/24/23 12:31:55.333
  STEP: Destroying namespace "webhook-markers-4606" for this suite. @ 08/24/23 12:31:55.351
• [4.623 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 08/24/23 12:31:55.367
  Aug 24 12:31:55.368: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 12:31:55.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:55.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:55.407
  STEP: create the container @ 08/24/23 12:31:55.411
  W0824 12:31:55.426214      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/24/23 12:31:55.427
  STEP: get the container status @ 08/24/23 12:31:59.467
  STEP: the container should be terminated @ 08/24/23 12:31:59.476
  STEP: the termination message should be set @ 08/24/23 12:31:59.476
  Aug 24 12:31:59.476: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 08/24/23 12:31:59.476
  Aug 24 12:31:59.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2728" for this suite. @ 08/24/23 12:31:59.518
• [4.162 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 08/24/23 12:31:59.531
  Aug 24 12:31:59.531: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:31:59.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:31:59.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:31:59.573
  STEP: Creating pod liveness-45c9dd76-4772-4f61-8f20-8262d939e0f7 in namespace container-probe-8951 @ 08/24/23 12:31:59.581
  Aug 24 12:32:03.625: INFO: Started pod liveness-45c9dd76-4772-4f61-8f20-8262d939e0f7 in namespace container-probe-8951
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 12:32:03.626
  Aug 24 12:32:03.633: INFO: Initial restart count of pod liveness-45c9dd76-4772-4f61-8f20-8262d939e0f7 is 0
  Aug 24 12:32:21.725: INFO: Restart count of pod container-probe-8951/liveness-45c9dd76-4772-4f61-8f20-8262d939e0f7 is now 1 (18.091604364s elapsed)
  Aug 24 12:32:21.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:32:21.736
  STEP: Destroying namespace "container-probe-8951" for this suite. @ 08/24/23 12:32:21.768
• [22.288 seconds]
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 08/24/23 12:32:21.82
  Aug 24 12:32:21.820: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:32:21.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:32:21.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:32:21.884
  Aug 24 12:33:21.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3644" for this suite. @ 08/24/23 12:33:21.931
• [60.129 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 08/24/23 12:33:21.95
  Aug 24 12:33:21.950: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:33:21.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:21.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:22.001
  STEP: Creating the pod @ 08/24/23 12:33:22.006
  Aug 24 12:33:24.600: INFO: Successfully updated pod "annotationupdate78a9691d-0d57-41b9-b7a9-60dfc9632fa1"
  Aug 24 12:33:26.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9758" for this suite. @ 08/24/23 12:33:26.644
• [4.706 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 08/24/23 12:33:26.658
  Aug 24 12:33:26.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:33:26.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:26.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:26.7
  STEP: Setting up server cert @ 08/24/23 12:33:26.751
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:33:27.679
  STEP: Deploying the webhook pod @ 08/24/23 12:33:27.695
  STEP: Wait for the deployment to be ready @ 08/24/23 12:33:27.72
  Aug 24 12:33:27.739: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 08/24/23 12:33:29.761
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:33:29.782
  Aug 24 12:33:30.783: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 08/24/23 12:33:30.793
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:33:30.833
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 08/24/23 12:33:30.849
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:33:30.866
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 08/24/23 12:33:30.892
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/24/23 12:33:30.907
  Aug 24 12:33:30.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4428" for this suite. @ 08/24/23 12:33:31.037
  STEP: Destroying namespace "webhook-markers-4178" for this suite. @ 08/24/23 12:33:31.058
• [4.414 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 08/24/23 12:33:31.074
  Aug 24 12:33:31.074: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename disruption @ 08/24/23 12:33:31.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:31.12
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:31.127
  STEP: Creating a kubernetes client @ 08/24/23 12:33:31.136
  Aug 24 12:33:31.136: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename disruption-2 @ 08/24/23 12:33:31.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:31.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:31.18
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:33:31.195
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:33:33.254
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:33:33.268
  STEP: listing a collection of PDBs across all namespaces @ 08/24/23 12:33:33.282
  STEP: listing a collection of PDBs in namespace disruption-8427 @ 08/24/23 12:33:33.287
  STEP: deleting a collection of PDBs @ 08/24/23 12:33:33.294
  STEP: Waiting for the PDB collection to be deleted @ 08/24/23 12:33:33.322
  Aug 24 12:33:33.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:33:33.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-8548" for this suite. @ 08/24/23 12:33:33.351
  STEP: Destroying namespace "disruption-8427" for this suite. @ 08/24/23 12:33:33.364
• [2.304 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 08/24/23 12:33:33.388
  Aug 24 12:33:33.388: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:33:33.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:33.419
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:33.426
  STEP: creating a secret @ 08/24/23 12:33:33.431
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 08/24/23 12:33:33.441
  STEP: patching the secret @ 08/24/23 12:33:33.447
  STEP: deleting the secret using a LabelSelector @ 08/24/23 12:33:33.465
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 08/24/23 12:33:33.482
  Aug 24 12:33:33.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4855" for this suite. @ 08/24/23 12:33:33.499
• [0.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 08/24/23 12:33:33.517
  Aug 24 12:33:33.518: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 12:33:33.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:33.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:33.551
  Aug 24 12:33:33.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1017" for this suite. @ 08/24/23 12:33:33.635
• [0.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 08/24/23 12:33:33.657
  Aug 24 12:33:33.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:33:33.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:33.687
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:33.692
  STEP: Setting up server cert @ 08/24/23 12:33:33.739
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:33:34.458
  STEP: Deploying the webhook pod @ 08/24/23 12:33:34.47
  STEP: Wait for the deployment to be ready @ 08/24/23 12:33:34.493
  Aug 24 12:33:34.504: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 08/24/23 12:33:36.522
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:33:36.542
  Aug 24 12:33:37.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:33:37.553: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9680-crds.webhook.example.com via the AdmissionRegistration API @ 08/24/23 12:33:38.076
  STEP: Creating a custom resource while v1 is storage version @ 08/24/23 12:33:38.112
  STEP: Patching Custom Resource Definition to set v2 as storage @ 08/24/23 12:33:40.377
  STEP: Patching the custom resource while v2 is storage version @ 08/24/23 12:33:40.408
  Aug 24 12:33:40.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2013" for this suite. @ 08/24/23 12:33:41.07
  STEP: Destroying namespace "webhook-markers-4376" for this suite. @ 08/24/23 12:33:41.083
• [7.437 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 08/24/23 12:33:41.101
  Aug 24 12:33:41.101: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:33:41.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:41.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:41.139
  STEP: validating api versions @ 08/24/23 12:33:41.143
  Aug 24 12:33:41.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-8047 api-versions'
  Aug 24 12:33:41.309: INFO: stderr: ""
  Aug 24 12:33:41.309: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Aug 24 12:33:41.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8047" for this suite. @ 08/24/23 12:33:41.32
• [0.232 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 08/24/23 12:33:41.335
  Aug 24 12:33:41.335: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 12:33:41.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:41.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:41.378
  STEP: creating a ReplicationController @ 08/24/23 12:33:41.389
  STEP: waiting for RC to be added @ 08/24/23 12:33:41.401
  STEP: waiting for available Replicas @ 08/24/23 12:33:41.402
  STEP: patching ReplicationController @ 08/24/23 12:33:43.028
  STEP: waiting for RC to be modified @ 08/24/23 12:33:43.044
  STEP: patching ReplicationController status @ 08/24/23 12:33:43.045
  STEP: waiting for RC to be modified @ 08/24/23 12:33:43.056
  STEP: waiting for available Replicas @ 08/24/23 12:33:43.057
  STEP: fetching ReplicationController status @ 08/24/23 12:33:43.068
  STEP: patching ReplicationController scale @ 08/24/23 12:33:43.075
  STEP: waiting for RC to be modified @ 08/24/23 12:33:43.087
  STEP: waiting for ReplicationController's scale to be the max amount @ 08/24/23 12:33:43.088
  STEP: fetching ReplicationController; ensuring that it's patched @ 08/24/23 12:33:45.159
  STEP: updating ReplicationController status @ 08/24/23 12:33:45.18
  STEP: waiting for RC to be modified @ 08/24/23 12:33:45.191
  STEP: listing all ReplicationControllers @ 08/24/23 12:33:45.192
  STEP: checking that ReplicationController has expected values @ 08/24/23 12:33:45.236
  STEP: deleting ReplicationControllers by collection @ 08/24/23 12:33:45.236
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 08/24/23 12:33:45.257
  Aug 24 12:33:45.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 12:33:45.368535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-7295" for this suite. @ 08/24/23 12:33:45.378
• [4.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 08/24/23 12:33:45.405
  Aug 24 12:33:45.405: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 12:33:45.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:45.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:45.454
  STEP: creating service in namespace services-6233 @ 08/24/23 12:33:45.46
  STEP: creating service affinity-nodeport in namespace services-6233 @ 08/24/23 12:33:45.461
  STEP: creating replication controller affinity-nodeport in namespace services-6233 @ 08/24/23 12:33:45.497
  I0824 12:33:45.514020      15 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-6233, replica count: 3
  E0824 12:33:46.373737      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:47.371035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:48.374503      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:33:48.577683      15 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:33:48.611: INFO: Creating new exec pod
  E0824 12:33:49.372732      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:50.373202      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:51.373196      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:33:51.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-6233 exec execpod-affinityx2bvc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Aug 24 12:33:51.998: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Aug 24 12:33:51.998: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:33:51.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-6233 exec execpod-affinityx2bvc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.218 80'
  Aug 24 12:33:52.245: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.218 80\nConnection to 10.233.52.218 80 port [tcp/http] succeeded!\n"
  Aug 24 12:33:52.245: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:33:52.245: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-6233 exec execpod-affinityx2bvc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.15 32561'
  E0824 12:33:52.373563      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:33:52.511: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.15 32561\nConnection to 192.168.121.15 32561 port [tcp/*] succeeded!\n"
  Aug 24 12:33:52.511: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:33:52.512: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-6233 exec execpod-affinityx2bvc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.64 32561'
  Aug 24 12:33:52.762: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.64 32561\nConnection to 192.168.121.64 32561 port [tcp/*] succeeded!\n"
  Aug 24 12:33:52.762: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:33:52.762: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-6233 exec execpod-affinityx2bvc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.64:32561/ ; done'
  Aug 24 12:33:53.253: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:32561/\n"
  Aug 24 12:33:53.253: INFO: stdout: "\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh\naffinity-nodeport-zjmqh"
  Aug 24 12:33:53.253: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.253: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.253: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.253: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.253: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Received response from host: affinity-nodeport-zjmqh
  Aug 24 12:33:53.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:33:53.264: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-6233, will wait for the garbage collector to delete the pods @ 08/24/23 12:33:53.29
  Aug 24 12:33:53.364: INFO: Deleting ReplicationController affinity-nodeport took: 13.315001ms
  E0824 12:33:53.374570      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:33:53.465: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.307708ms
  E0824 12:33:54.375284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:55.375990      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-6233" for this suite. @ 08/24/23 12:33:56.016
• [10.629 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 08/24/23 12:33:56.047
  Aug 24 12:33:56.047: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/24/23 12:33:56.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:56.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:56.088
  Aug 24 12:33:56.093: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 12:33:56.376078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:33:56.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-820" for this suite. @ 08/24/23 12:33:56.695
• [0.664 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 08/24/23 12:33:56.713
  Aug 24 12:33:56.713: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename subpath @ 08/24/23 12:33:56.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:33:56.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:33:56.766
  STEP: Setting up data @ 08/24/23 12:33:56.769
  STEP: Creating pod pod-subpath-test-configmap-22tz @ 08/24/23 12:33:56.79
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 12:33:56.79
  E0824 12:33:57.376945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:58.377567      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:33:59.378158      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:00.378777      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:01.379517      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:02.379978      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:03.380741      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:04.381528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:05.382183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:06.382817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:07.383723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:08.384525      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:09.384837      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:10.385524      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:11.386484      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:12.387214      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:13.388337      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:14.388479      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:15.389065      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:16.389816      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:17.390805      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:18.391615      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:19.392307      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:20.392487      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:34:20.94
  Aug 24 12:34:20.948: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-subpath-test-configmap-22tz container test-container-subpath-configmap-22tz: <nil>
  STEP: delete the pod @ 08/24/23 12:34:20.972
  STEP: Deleting pod pod-subpath-test-configmap-22tz @ 08/24/23 12:34:21.009
  Aug 24 12:34:21.009: INFO: Deleting pod "pod-subpath-test-configmap-22tz" in namespace "subpath-1747"
  Aug 24 12:34:21.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1747" for this suite. @ 08/24/23 12:34:21.029
• [24.333 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 08/24/23 12:34:21.049
  Aug 24 12:34:21.049: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:34:21.052
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:34:21.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:34:21.102
  STEP: Creating secret with name s-test-opt-del-0a121f16-eef6-4789-bd5d-ff653fc661b9 @ 08/24/23 12:34:21.123
  STEP: Creating secret with name s-test-opt-upd-b5c0ef3e-ece9-4b58-bea6-5c455aab2f19 @ 08/24/23 12:34:21.131
  STEP: Creating the pod @ 08/24/23 12:34:21.145
  E0824 12:34:21.392702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:22.393149      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:23.394099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:24.394502      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-0a121f16-eef6-4789-bd5d-ff653fc661b9 @ 08/24/23 12:34:25.263
  STEP: Updating secret s-test-opt-upd-b5c0ef3e-ece9-4b58-bea6-5c455aab2f19 @ 08/24/23 12:34:25.274
  STEP: Creating secret with name s-test-opt-create-134744ca-e841-4b14-9ed1-aa8c5ad2cfcf @ 08/24/23 12:34:25.284
  STEP: waiting to observe update in volume @ 08/24/23 12:34:25.292
  E0824 12:34:25.394732      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:26.395223      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:27.395985      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:28.397014      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:29.397970      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:30.398443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:31.399098      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:32.399280      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:33.399808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:34.400029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:35.400148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:36.400350      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:37.401070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:38.401630      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:39.402384      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:40.402686      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:41.402923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:42.403233      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:43.403846      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:44.404490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:45.405231      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:46.405375      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:47.405977      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:48.406549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:49.406710      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:50.406907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:51.407512      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:52.407748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:53.408298      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:54.408691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:55.408949      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:56.409072      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:57.409191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:58.409540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:34:59.409887      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:00.410036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:01.410754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:02.410717      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:03.411434      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:04.411718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:05.411833      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:06.412036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:07.412627      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:08.413620      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:09.414298      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:10.414742      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:11.415641      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:12.416669      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:13.417567      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:14.417708      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:15.419095      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:16.419844      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:17.420414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:18.420703      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:19.421290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:20.421534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:21.421755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:22.421818      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:23.422274      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:24.422682      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:25.422950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:26.423217      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:27.429996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:28.426145      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:29.426297      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:30.426697      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:31.426758      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:32.427750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:33.429661      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:34.429963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:35.428785      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:36.428941      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:37.429505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:38.430602      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:39.431319      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:40.432313      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:41.432435      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:42.432745      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:43.433474      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:44.434268      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:45.434936      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:46.435629      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:47.435973      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:48.436849      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:49.437547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:35:50.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2268" for this suite. @ 08/24/23 12:35:50.089
• [89.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 08/24/23 12:35:50.11
  Aug 24 12:35:50.110: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:35:50.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:35:50.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:35:50.147
  STEP: Counting existing ResourceQuota @ 08/24/23 12:35:50.151
  E0824 12:35:50.438094      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:51.438796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:52.438895      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:53.439029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:54.439912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 12:35:55.161
  STEP: Ensuring resource quota status is calculated @ 08/24/23 12:35:55.175
  E0824 12:35:55.441042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:56.441581      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:35:57.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6495" for this suite. @ 08/24/23 12:35:57.228
• [7.131 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 08/24/23 12:35:57.241
  Aug 24 12:35:57.241: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-pred @ 08/24/23 12:35:57.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:35:57.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:35:57.284
  Aug 24 12:35:57.291: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug 24 12:35:57.312: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:35:57.321: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-1 before test
  Aug 24 12:35:57.348: INFO: cilium-2mlv4 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.348: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: cilium-node-init-wh7d5 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.348: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: coredns-5d78c9869d-xp68m from kube-system started at 2023-08-24 11:23:49 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.348: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: kube-addon-manager-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.348: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: kube-apiserver-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.348: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: kube-controller-manager-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.348: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: kube-proxy-ljqfg from kube-system started at 2023-08-24 11:21:03 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.348: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: kube-scheduler-quohp9aeph3i-1 from kube-system started at 2023-08-24 11:24:44 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.348: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-5c6ln from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:35:57.348: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:35:57.348: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-2 before test
  Aug 24 12:35:57.366: INFO: cilium-krjld from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.367: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:35:57.367: INFO: cilium-node-init-ldwv5 from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.367: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:35:57.368: INFO: cilium-operator-b8f479cd9-78ztl from kube-system started at 2023-08-24 11:22:43 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.368: INFO: 	Container cilium-operator ready: true, restart count 0
  Aug 24 12:35:57.369: INFO: coredns-5d78c9869d-bvl77 from kube-system started at 2023-08-24 11:23:40 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.369: INFO: 	Container coredns ready: true, restart count 0
  Aug 24 12:35:57.370: INFO: kube-addon-manager-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.370: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Aug 24 12:35:57.371: INFO: kube-apiserver-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.371: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug 24 12:35:57.372: INFO: kube-controller-manager-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.372: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug 24 12:35:57.373: INFO: kube-proxy-dr4nl from kube-system started at 2023-08-24 11:21:33 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.373: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:35:57.374: INFO: kube-scheduler-quohp9aeph3i-2 from kube-system started at 2023-08-24 11:25:04 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.375: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug 24 12:35:57.375: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-bccng from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:35:57.376: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:35:57.377: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug 24 12:35:57.377: INFO: 
  Logging pods the apiserver thinks is on node quohp9aeph3i-3 before test
  Aug 24 12:35:57.394: INFO: cilium-node-init-hkrgc from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.394: INFO: 	Container node-init ready: true, restart count 0
  Aug 24 12:35:57.394: INFO: cilium-qwwtg from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.394: INFO: 	Container cilium-agent ready: true, restart count 0
  Aug 24 12:35:57.394: INFO: kube-proxy-9cptc from kube-system started at 2023-08-24 11:25:41 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.394: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug 24 12:35:57.394: INFO: sonobuoy from sonobuoy started at 2023-08-24 11:38:22 +0000 UTC (1 container statuses recorded)
  Aug 24 12:35:57.394: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug 24 12:35:57.394: INFO: sonobuoy-e2e-job-06f57e3e53464656 from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:35:57.394: INFO: 	Container e2e ready: true, restart count 0
  Aug 24 12:35:57.394: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:35:57.394: INFO: sonobuoy-systemd-logs-daemon-set-cdc726afb4864600-wfrkl from sonobuoy started at 2023-08-24 11:38:32 +0000 UTC (2 container statuses recorded)
  Aug 24 12:35:57.394: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug 24 12:35:57.394: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/24/23 12:35:57.394
  E0824 12:35:57.441907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:35:58.442869      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/24/23 12:35:59.428
  E0824 12:35:59.443702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to apply a random label on the found node. @ 08/24/23 12:35:59.452
  STEP: verifying the node has the label kubernetes.io/e2e-24020872-3ae8-41ad-9caa-cce4ec339e75 42 @ 08/24/23 12:35:59.466
  STEP: Trying to relaunch the pod, now with labels. @ 08/24/23 12:35:59.473
  E0824 12:36:00.443831      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:01.443961      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-24020872-3ae8-41ad-9caa-cce4ec339e75 off the node quohp9aeph3i-3 @ 08/24/23 12:36:01.534
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-24020872-3ae8-41ad-9caa-cce4ec339e75 @ 08/24/23 12:36:01.56
  Aug 24 12:36:01.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4876" for this suite. @ 08/24/23 12:36:01.577
• [4.348 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 08/24/23 12:36:01.602
  Aug 24 12:36:01.602: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pod-network-test @ 08/24/23 12:36:01.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:36:01.637
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:36:01.652
  STEP: Performing setup for networking test in namespace pod-network-test-1838 @ 08/24/23 12:36:01.662
  STEP: creating a selector @ 08/24/23 12:36:01.662
  STEP: Creating the service pods in kubernetes @ 08/24/23 12:36:01.663
  Aug 24 12:36:01.663: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0824 12:36:02.447344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:03.448557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:04.448700      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:05.448942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:06.449913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:07.450762      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:08.450756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:09.451266      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:10.451554      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:11.452428      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:12.452278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:13.452402      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:14.452656      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:15.455442      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:16.454670      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:17.454881      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:18.455290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:19.455434      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:20.456460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:21.456969      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:22.457121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:23.457509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/24/23 12:36:23.894
  E0824 12:36:24.459130      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:25.459602      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:36:25.931: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Aug 24 12:36:25.931: INFO: Breadth first check of 10.233.65.91 on host 192.168.121.64...
  Aug 24 12:36:25.939: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.25:9080/dial?request=hostname&protocol=udp&host=10.233.65.91&port=8081&tries=1'] Namespace:pod-network-test-1838 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:36:25.940: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:36:25.942: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:36:25.943: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1838/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.25%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.91%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 12:36:26.149: INFO: Waiting for responses: map[]
  Aug 24 12:36:26.150: INFO: reached 10.233.65.91 after 0/1 tries
  Aug 24 12:36:26.150: INFO: Breadth first check of 10.233.64.66 on host 192.168.121.19...
  Aug 24 12:36:26.159: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.25:9080/dial?request=hostname&protocol=udp&host=10.233.64.66&port=8081&tries=1'] Namespace:pod-network-test-1838 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:36:26.159: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:36:26.161: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:36:26.161: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1838/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.25%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.66%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug 24 12:36:26.311: INFO: Waiting for responses: map[]
  Aug 24 12:36:26.312: INFO: reached 10.233.64.66 after 0/1 tries
  Aug 24 12:36:26.312: INFO: Breadth first check of 10.233.66.244 on host 192.168.121.15...
  Aug 24 12:36:26.321: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.25:9080/dial?request=hostname&protocol=udp&host=10.233.66.244&port=8081&tries=1'] Namespace:pod-network-test-1838 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:36:26.321: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:36:26.323: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:36:26.323: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1838/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.25%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.244%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0824 12:36:26.461560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:36:26.461: INFO: Waiting for responses: map[]
  Aug 24 12:36:26.461: INFO: reached 10.233.66.244 after 0/1 tries
  Aug 24 12:36:26.461: INFO: Going to retry 0 out of 3 pods....
  Aug 24 12:36:26.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1838" for this suite. @ 08/24/23 12:36:26.479
• [24.890 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 08/24/23 12:36:26.496
  Aug 24 12:36:26.496: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:36:26.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:36:26.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:36:26.54
  STEP: Creating a pod to test downward api env vars @ 08/24/23 12:36:26.546
  E0824 12:36:27.462070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:28.462503      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:29.462496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:30.462964      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:36:30.587
  Aug 24 12:36:30.596: INFO: Trying to get logs from node quohp9aeph3i-3 pod downward-api-20b786f3-44de-4541-94bc-527d1936cc75 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:36:30.616
  Aug 24 12:36:30.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2651" for this suite. @ 08/24/23 12:36:30.676
• [4.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 08/24/23 12:36:30.701
  Aug 24 12:36:30.701: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 12:36:30.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:36:30.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:36:30.745
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 08/24/23 12:36:30.772
  E0824 12:36:31.462897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:32.463884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:33.464305      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:34.465286      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:35.465402      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:36.465811      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:37.466974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:38.466713      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:39.466808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:40.467222      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:41.467421      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:42.467632      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:43.468319      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:44.468438      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:45.468777      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:46.469036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:47.470461      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:48.471016      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 08/24/23 12:36:48.962
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 08/24/23 12:36:48.971
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 08/24/23 12:36:48.986
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 08/24/23 12:36:48.986
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 08/24/23 12:36:49.031
  E0824 12:36:49.471174      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:50.472192      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:51.472484      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 08/24/23 12:36:52.059
  E0824 12:36:52.472384      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 08/24/23 12:36:53.076
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 08/24/23 12:36:53.09
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 08/24/23 12:36:53.09
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 08/24/23 12:36:53.126
  E0824 12:36:53.472911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 08/24/23 12:36:54.14
  E0824 12:36:54.472870      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:55.473909      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 08/24/23 12:36:56.17
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 08/24/23 12:36:56.186
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 08/24/23 12:36:56.186
  Aug 24 12:36:56.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5228" for this suite. @ 08/24/23 12:36:56.255
• [25.566 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 08/24/23 12:36:56.277
  Aug 24 12:36:56.278: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:36:56.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:36:56.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:36:56.327
  STEP: Creating pod liveness-1d64fec0-954e-4172-bebf-3539c1d7b126 in namespace container-probe-7741 @ 08/24/23 12:36:56.335
  E0824 12:36:56.474997      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:57.475252      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:36:58.383: INFO: Started pod liveness-1d64fec0-954e-4172-bebf-3539c1d7b126 in namespace container-probe-7741
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 12:36:58.383
  Aug 24 12:36:58.390: INFO: Initial restart count of pod liveness-1d64fec0-954e-4172-bebf-3539c1d7b126 is 0
  E0824 12:36:58.475550      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:36:59.496360      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:00.484181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:01.484279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:02.484932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:03.485491      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:04.486477      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:05.486856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:06.487176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:07.487301      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:08.488147      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:09.488344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:10.488476      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:11.488683      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:12.489351      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:13.490254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:14.491082      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:15.491287      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:16.491749      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:17.492275      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:37:18.483: INFO: Restart count of pod container-probe-7741/liveness-1d64fec0-954e-4172-bebf-3539c1d7b126 is now 1 (20.09289024s elapsed)
  E0824 12:37:18.492752      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:19.492896      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:20.493467      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:21.494916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:22.494810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:23.495494      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:24.495914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:25.496502      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:26.496714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:27.497536      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:28.498591      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:29.501659      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:30.500528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:31.501045      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:32.501035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:33.501804      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:34.504323      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:35.501837      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:36.502506      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:37.502918      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:38.503342      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:37:38.581: INFO: Restart count of pod container-probe-7741/liveness-1d64fec0-954e-4172-bebf-3539c1d7b126 is now 2 (40.190861146s elapsed)
  E0824 12:37:39.503808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:40.504442      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:41.504517      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:42.504836      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:43.505708      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:44.511195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:45.508348      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:46.508729      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:47.508982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:48.509783      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:49.509981      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:50.510489      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:51.510614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:52.511230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:53.511559      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:54.511781      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:55.512166      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:56.512823      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:57.513090      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:37:58.513767      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:37:58.672: INFO: Restart count of pod container-probe-7741/liveness-1d64fec0-954e-4172-bebf-3539c1d7b126 is now 3 (1m0.281803903s elapsed)
  E0824 12:37:59.514178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:00.514659      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:01.514696      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:02.515034      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:03.515488      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:04.515686      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:05.516303      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:06.516536      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:07.517102      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:08.518378      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:09.518760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:10.519029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:11.520111      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:12.520452      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:13.521665      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:14.521816      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:15.522038      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:16.522213      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:17.523420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:18.523875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:38:18.758: INFO: Restart count of pod container-probe-7741/liveness-1d64fec0-954e-4172-bebf-3539c1d7b126 is now 4 (1m20.368345923s elapsed)
  E0824 12:38:19.524481      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:20.524627      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:21.525416      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:22.526174      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:23.527154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:24.527594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:25.528054      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:26.528347      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:27.528538      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:28.529470      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:29.529614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:30.529809      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:31.529951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:32.530368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:33.531631      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:34.531483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:35.532090      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:36.532781      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:37.533513      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:38.534598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:39.534843      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:40.535063      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:41.535542      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:42.536360      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:43.537412      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:44.537635      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:45.537881      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:46.538033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:47.539060      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:48.539745      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:49.540525      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:50.541200      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:51.542071      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:52.542458      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:53.543307      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:54.544132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:55.544353      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:56.544540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:57.545394      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:58.545524      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:38:59.546606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:00.546825      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:01.547232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:02.547269      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:03.547892      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:04.548088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:05.548679      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:06.548537      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:07.549685      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:08.550606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:09.550649      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:10.551116      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:11.552121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:12.554044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:13.553325      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:14.553726      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:15.553864      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:16.554122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:17.555115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:18.555575      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:19.555935      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:20.556096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:21.556944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:22.557674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:23.558270      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:24.558562      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:25.560295      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:26.559838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:27.080: INFO: Restart count of pod container-probe-7741/liveness-1d64fec0-954e-4172-bebf-3539c1d7b126 is now 5 (2m28.689882037s elapsed)
  Aug 24 12:39:27.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:39:27.093
  STEP: Destroying namespace "container-probe-7741" for this suite. @ 08/24/23 12:39:27.123
• [150.869 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 08/24/23 12:39:27.169
  Aug 24 12:39:27.169: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:39:27.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:39:27.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:39:27.218
  STEP: creating a replication controller @ 08/24/23 12:39:27.226
  Aug 24 12:39:27.227: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 create -f -'
  E0824 12:39:27.560539      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:28.561077      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:29.154: INFO: stderr: ""
  Aug 24 12:39:29.154: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/24/23 12:39:29.154
  Aug 24 12:39:29.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 12:39:29.367: INFO: stderr: ""
  Aug 24 12:39:29.367: INFO: stdout: "update-demo-nautilus-cn44s update-demo-nautilus-f6jtk "
  Aug 24 12:39:29.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-cn44s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 12:39:29.531: INFO: stderr: ""
  Aug 24 12:39:29.531: INFO: stdout: ""
  Aug 24 12:39:29.531: INFO: update-demo-nautilus-cn44s is created but not running
  E0824 12:39:29.561544      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:30.561826      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:31.561948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:32.562229      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:33.562480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:34.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0824 12:39:34.562980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:34.704: INFO: stderr: ""
  Aug 24 12:39:34.704: INFO: stdout: "update-demo-nautilus-cn44s update-demo-nautilus-f6jtk "
  Aug 24 12:39:34.704: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-cn44s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 12:39:34.885: INFO: stderr: ""
  Aug 24 12:39:34.885: INFO: stdout: "true"
  Aug 24 12:39:34.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-cn44s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 12:39:35.096: INFO: stderr: ""
  Aug 24 12:39:35.096: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 12:39:35.096: INFO: validating pod update-demo-nautilus-cn44s
  Aug 24 12:39:35.113: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 12:39:35.113: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 12:39:35.113: INFO: update-demo-nautilus-cn44s is verified up and running
  Aug 24 12:39:35.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-f6jtk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 12:39:35.278: INFO: stderr: ""
  Aug 24 12:39:35.278: INFO: stdout: "true"
  Aug 24 12:39:35.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-f6jtk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 12:39:35.419: INFO: stderr: ""
  Aug 24 12:39:35.420: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 12:39:35.420: INFO: validating pod update-demo-nautilus-f6jtk
  Aug 24 12:39:35.436: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 12:39:35.436: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 12:39:35.436: INFO: update-demo-nautilus-f6jtk is verified up and running
  STEP: scaling down the replication controller @ 08/24/23 12:39:35.436
  Aug 24 12:39:35.450: INFO: scanned /root for discovery docs: <nil>
  Aug 24 12:39:35.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0824 12:39:35.563791      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:36.564252      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:36.669: INFO: stderr: ""
  Aug 24 12:39:36.669: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/24/23 12:39:36.669
  Aug 24 12:39:36.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 12:39:36.802: INFO: stderr: ""
  Aug 24 12:39:36.802: INFO: stdout: "update-demo-nautilus-cn44s "
  Aug 24 12:39:36.802: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-cn44s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 12:39:36.959: INFO: stderr: ""
  Aug 24 12:39:36.959: INFO: stdout: "true"
  Aug 24 12:39:36.960: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-cn44s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 12:39:37.097: INFO: stderr: ""
  Aug 24 12:39:37.097: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 12:39:37.097: INFO: validating pod update-demo-nautilus-cn44s
  Aug 24 12:39:37.104: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 12:39:37.104: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 12:39:37.104: INFO: update-demo-nautilus-cn44s is verified up and running
  STEP: scaling up the replication controller @ 08/24/23 12:39:37.104
  Aug 24 12:39:37.116: INFO: scanned /root for discovery docs: <nil>
  Aug 24 12:39:37.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0824 12:39:37.564284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:38.320: INFO: stderr: ""
  Aug 24 12:39:38.320: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/24/23 12:39:38.321
  Aug 24 12:39:38.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug 24 12:39:38.452: INFO: stderr: ""
  Aug 24 12:39:38.452: INFO: stdout: "update-demo-nautilus-cn44s update-demo-nautilus-z694c "
  Aug 24 12:39:38.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-cn44s -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0824 12:39:38.564691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:38.589: INFO: stderr: ""
  Aug 24 12:39:38.589: INFO: stdout: "true"
  Aug 24 12:39:38.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-cn44s -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 12:39:38.727: INFO: stderr: ""
  Aug 24 12:39:38.727: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 12:39:38.727: INFO: validating pod update-demo-nautilus-cn44s
  Aug 24 12:39:38.735: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 12:39:38.735: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 12:39:38.735: INFO: update-demo-nautilus-cn44s is verified up and running
  Aug 24 12:39:38.735: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-z694c -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug 24 12:39:38.873: INFO: stderr: ""
  Aug 24 12:39:38.873: INFO: stdout: "true"
  Aug 24 12:39:38.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods update-demo-nautilus-z694c -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug 24 12:39:39.025: INFO: stderr: ""
  Aug 24 12:39:39.025: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug 24 12:39:39.025: INFO: validating pod update-demo-nautilus-z694c
  Aug 24 12:39:39.042: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug 24 12:39:39.042: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug 24 12:39:39.042: INFO: update-demo-nautilus-z694c is verified up and running
  STEP: using delete to clean up resources @ 08/24/23 12:39:39.042
  Aug 24 12:39:39.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 delete --grace-period=0 --force -f -'
  Aug 24 12:39:39.193: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug 24 12:39:39.193: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug 24 12:39:39.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get rc,svc -l name=update-demo --no-headers'
  Aug 24 12:39:39.354: INFO: stderr: "No resources found in kubectl-6622 namespace.\n"
  Aug 24 12:39:39.354: INFO: stdout: ""
  Aug 24 12:39:39.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6622 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug 24 12:39:39.545: INFO: stderr: ""
  Aug 24 12:39:39.545: INFO: stdout: ""
  Aug 24 12:39:39.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6622" for this suite. @ 08/24/23 12:39:39.556
  E0824 12:39:39.565167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [12.400 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 08/24/23 12:39:39.57
  Aug 24 12:39:39.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:39:39.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:39:39.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:39:39.641
  STEP: Creating projection with secret that has name projected-secret-test-e1edcd43-23ee-4619-aa02-bfacbd7e2152 @ 08/24/23 12:39:39.647
  STEP: Creating a pod to test consume secrets @ 08/24/23 12:39:39.656
  E0824 12:39:40.565364      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:41.565838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:42.565927      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:43.566015      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:39:43.694
  Aug 24 12:39:43.698: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-secrets-c15d94af-afce-408f-b326-526dee965aff container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:39:43.729
  Aug 24 12:39:43.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4391" for this suite. @ 08/24/23 12:39:43.76
• [4.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 08/24/23 12:39:43.779
  Aug 24 12:39:43.779: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename security-context-test @ 08/24/23 12:39:43.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:39:43.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:39:43.814
  E0824 12:39:44.567167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:45.567342      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:46.567902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:47.567771      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:47.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5251" for this suite. @ 08/24/23 12:39:47.865
• [4.099 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 08/24/23 12:39:47.879
  Aug 24 12:39:47.879: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 12:39:47.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:39:47.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:39:47.924
  Aug 24 12:39:47.927: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 12:39:48.568832      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:49.569492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/24/23 12:39:50.157
  Aug 24 12:39:50.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-9007 --namespace=crd-publish-openapi-9007 create -f -'
  E0824 12:39:50.570079      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:51.552: INFO: stderr: ""
  Aug 24 12:39:51.552: INFO: stdout: "e2e-test-crd-publish-openapi-3062-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug 24 12:39:51.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-9007 --namespace=crd-publish-openapi-9007 delete e2e-test-crd-publish-openapi-3062-crds test-cr'
  E0824 12:39:51.570550      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:51.691: INFO: stderr: ""
  Aug 24 12:39:51.691: INFO: stdout: "e2e-test-crd-publish-openapi-3062-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Aug 24 12:39:51.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-9007 --namespace=crd-publish-openapi-9007 apply -f -'
  E0824 12:39:52.570595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:52.857: INFO: stderr: ""
  Aug 24 12:39:52.857: INFO: stdout: "e2e-test-crd-publish-openapi-3062-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug 24 12:39:52.858: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-9007 --namespace=crd-publish-openapi-9007 delete e2e-test-crd-publish-openapi-3062-crds test-cr'
  Aug 24 12:39:53.025: INFO: stderr: ""
  Aug 24 12:39:53.025: INFO: stdout: "e2e-test-crd-publish-openapi-3062-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 08/24/23 12:39:53.025
  Aug 24 12:39:53.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-9007 explain e2e-test-crd-publish-openapi-3062-crds'
  Aug 24 12:39:53.546: INFO: stderr: ""
  Aug 24 12:39:53.546: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-3062-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0824 12:39:53.571122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:54.571276      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:55.571976      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:39:55.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9007" for this suite. @ 08/24/23 12:39:55.761
• [7.899 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 08/24/23 12:39:55.779
  Aug 24 12:39:55.779: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 12:39:55.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:39:55.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:39:55.818
  Aug 24 12:39:55.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-311" for this suite. @ 08/24/23 12:39:55.842
• [0.073 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 08/24/23 12:39:55.855
  Aug 24 12:39:55.855: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename job @ 08/24/23 12:39:55.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:39:55.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:39:55.888
  STEP: Creating a suspended job @ 08/24/23 12:39:55.9
  STEP: Patching the Job @ 08/24/23 12:39:55.911
  STEP: Watching for Job to be patched @ 08/24/23 12:39:55.935
  Aug 24 12:39:55.938: INFO: Event ADDED observed for Job e2e-mbn2m in namespace job-6570 with labels: map[e2e-job-label:e2e-mbn2m] and annotations: map[batch.kubernetes.io/job-tracking:]
  Aug 24 12:39:55.939: INFO: Event MODIFIED found for Job e2e-mbn2m in namespace job-6570 with labels: map[e2e-job-label:e2e-mbn2m e2e-mbn2m:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 08/24/23 12:39:55.939
  STEP: Watching for Job to be updated @ 08/24/23 12:39:55.999
  Aug 24 12:39:56.003: INFO: Event MODIFIED found for Job e2e-mbn2m in namespace job-6570 with labels: map[e2e-job-label:e2e-mbn2m e2e-mbn2m:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:39:56.003: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 08/24/23 12:39:56.004
  Aug 24 12:39:56.021: INFO: Job: e2e-mbn2m as labels: map[e2e-job-label:e2e-mbn2m e2e-mbn2m:patched]
  STEP: Waiting for job to complete @ 08/24/23 12:39:56.021
  E0824 12:39:56.572041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:57.574018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:58.574248      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:39:59.575001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:00.575092      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:01.575263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:02.575485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:03.576497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 08/24/23 12:40:04.03
  STEP: Watching for Job to be deleted @ 08/24/23 12:40:04.045
  Aug 24 12:40:04.048: INFO: Event MODIFIED observed for Job e2e-mbn2m in namespace job-6570 with labels: map[e2e-job-label:e2e-mbn2m e2e-mbn2m:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:40:04.048: INFO: Event MODIFIED observed for Job e2e-mbn2m in namespace job-6570 with labels: map[e2e-job-label:e2e-mbn2m e2e-mbn2m:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:40:04.048: INFO: Event MODIFIED observed for Job e2e-mbn2m in namespace job-6570 with labels: map[e2e-job-label:e2e-mbn2m e2e-mbn2m:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:40:04.049: INFO: Event MODIFIED observed for Job e2e-mbn2m in namespace job-6570 with labels: map[e2e-job-label:e2e-mbn2m e2e-mbn2m:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:40:04.049: INFO: Event MODIFIED observed for Job e2e-mbn2m in namespace job-6570 with labels: map[e2e-job-label:e2e-mbn2m e2e-mbn2m:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug 24 12:40:04.049: INFO: Event DELETED found for Job e2e-mbn2m in namespace job-6570 with labels: map[e2e-job-label:e2e-mbn2m e2e-mbn2m:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 08/24/23 12:40:04.049
  Aug 24 12:40:04.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6570" for this suite. @ 08/24/23 12:40:04.074
• [8.235 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 08/24/23 12:40:04.092
  Aug 24 12:40:04.092: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:40:04.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:40:04.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:40:04.133
  STEP: Creating the pod @ 08/24/23 12:40:04.138
  E0824 12:40:04.577207      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:05.577911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:06.578916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:06.738: INFO: Successfully updated pod "annotationupdate07b7dbe5-6112-4a45-b87c-0f90f1866b2e"
  E0824 12:40:07.579831      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:08.580777      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:08.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2684" for this suite. @ 08/24/23 12:40:08.778
• [4.701 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 08/24/23 12:40:08.797
  Aug 24 12:40:08.798: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:40:08.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:40:08.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:40:08.837
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:40:08.842
  E0824 12:40:09.581659      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:10.581971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:11.582194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:12.583048      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:40:12.897
  Aug 24 12:40:12.901: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-578f45c6-0f72-49c5-b13f-e71af6e24e57 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:40:12.92
  Aug 24 12:40:12.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6457" for this suite. @ 08/24/23 12:40:12.953
• [4.164 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 08/24/23 12:40:12.963
  Aug 24 12:40:12.963: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 12:40:12.964
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:40:12.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:40:12.997
  STEP: Creating service test in namespace statefulset-291 @ 08/24/23 12:40:13.001
  STEP: Creating statefulset ss in namespace statefulset-291 @ 08/24/23 12:40:13.018
  Aug 24 12:40:13.050: INFO: Found 0 stateful pods, waiting for 1
  E0824 12:40:13.583087      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:14.583913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:15.584398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:16.584255      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:17.585141      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:18.585874      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:19.586211      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:20.587171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:21.587566      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:22.587915      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:23.057: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 08/24/23 12:40:23.069
  STEP: Getting /status @ 08/24/23 12:40:23.098
  Aug 24 12:40:23.113: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 08/24/23 12:40:23.114
  Aug 24 12:40:23.135: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 08/24/23 12:40:23.135
  Aug 24 12:40:23.140: INFO: Observed &StatefulSet event: ADDED
  Aug 24 12:40:23.141: INFO: Found Statefulset ss in namespace statefulset-291 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:40:23.141: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 08/24/23 12:40:23.142
  Aug 24 12:40:23.142: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 24 12:40:23.160: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 08/24/23 12:40:23.16
  Aug 24 12:40:23.166: INFO: Observed &StatefulSet event: ADDED
  Aug 24 12:40:23.166: INFO: Observed Statefulset ss in namespace statefulset-291 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:40:23.167: INFO: Observed &StatefulSet event: MODIFIED
  Aug 24 12:40:23.167: INFO: Deleting all statefulset in ns statefulset-291
  Aug 24 12:40:23.174: INFO: Scaling statefulset ss to 0
  E0824 12:40:23.588316      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:24.588284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:25.588517      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:26.588569      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:27.588813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:28.589623      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:29.589797      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:30.590422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:31.590576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:32.591078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:33.232: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 12:40:33.238: INFO: Deleting statefulset ss
  Aug 24 12:40:33.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-291" for this suite. @ 08/24/23 12:40:33.27
• [20.321 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 08/24/23 12:40:33.294
  Aug 24 12:40:33.295: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:40:33.297
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:40:33.325
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:40:33.329
  Aug 24 12:40:33.333: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 12:40:33.592132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:34.592847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:35.593056      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0824 12:40:36.054793      15 warnings.go:70] unknown field "alpha"
  W0824 12:40:36.054996      15 warnings.go:70] unknown field "beta"
  W0824 12:40:36.055116      15 warnings.go:70] unknown field "delta"
  W0824 12:40:36.055342      15 warnings.go:70] unknown field "epsilon"
  W0824 12:40:36.055513      15 warnings.go:70] unknown field "gamma"
  E0824 12:40:36.594387      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:36.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4767" for this suite. @ 08/24/23 12:40:36.652
• [3.370 seconds]
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 08/24/23 12:40:36.665
  Aug 24 12:40:36.665: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename podtemplate @ 08/24/23 12:40:36.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:40:36.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:40:36.7
  STEP: Create set of pod templates @ 08/24/23 12:40:36.704
  Aug 24 12:40:36.715: INFO: created test-podtemplate-1
  Aug 24 12:40:36.725: INFO: created test-podtemplate-2
  Aug 24 12:40:36.735: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 08/24/23 12:40:36.735
  STEP: delete collection of pod templates @ 08/24/23 12:40:36.743
  Aug 24 12:40:36.743: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 08/24/23 12:40:36.78
  Aug 24 12:40:36.780: INFO: requesting list of pod templates to confirm quantity
  Aug 24 12:40:36.789: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2543" for this suite. @ 08/24/23 12:40:36.799
• [0.151 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 08/24/23 12:40:36.817
  Aug 24 12:40:36.818: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename job @ 08/24/23 12:40:36.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:40:36.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:40:36.881
  STEP: Creating a job @ 08/24/23 12:40:36.885
  STEP: Ensure pods equal to parallelism count is attached to the job @ 08/24/23 12:40:36.925
  E0824 12:40:37.622418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:38.623736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 08/24/23 12:40:38.933
  STEP: updating /status @ 08/24/23 12:40:38.947
  STEP: get /status @ 08/24/23 12:40:39
  Aug 24 12:40:39.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6731" for this suite. @ 08/24/23 12:40:39.018
• [2.215 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 08/24/23 12:40:39.034
  Aug 24 12:40:39.035: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 12:40:39.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:40:39.073
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:40:39.08
  STEP: Creating a ResourceQuota @ 08/24/23 12:40:39.087
  STEP: Getting a ResourceQuota @ 08/24/23 12:40:39.101
  STEP: Updating a ResourceQuota @ 08/24/23 12:40:39.11
  STEP: Verifying a ResourceQuota was modified @ 08/24/23 12:40:39.126
  STEP: Deleting a ResourceQuota @ 08/24/23 12:40:39.132
  STEP: Verifying the deleted ResourceQuota @ 08/24/23 12:40:39.146
  Aug 24 12:40:39.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9451" for this suite. @ 08/24/23 12:40:39.16
• [0.148 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 08/24/23 12:40:39.184
  Aug 24 12:40:39.185: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 12:40:39.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:40:39.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:40:39.238
  STEP: Creating a pod to test service account token:  @ 08/24/23 12:40:39.243
  E0824 12:40:39.634881      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:40.627293      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:41.627407      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:42.629371      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:40:43.291
  Aug 24 12:40:43.299: INFO: Trying to get logs from node quohp9aeph3i-3 pod test-pod-0467c92b-a854-4cbd-b5e1-62e9f4f39580 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:40:43.313
  Aug 24 12:40:43.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9721" for this suite. @ 08/24/23 12:40:43.355
• [4.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 08/24/23 12:40:43.373
  Aug 24 12:40:43.373: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename gc @ 08/24/23 12:40:43.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:40:43.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:40:43.411
  STEP: create the rc @ 08/24/23 12:40:43.424
  W0824 12:40:43.435272      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0824 12:40:43.627964      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:44.630197      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:45.630555      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:46.646667      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:47.647295      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:48.671746      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/24/23 12:40:49.469
  E0824 12:40:49.698688      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for the rc to be deleted @ 08/24/23 12:40:50.027
  E0824 12:40:50.688247      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:51.660: INFO: 89 pods remaining
  Aug 24 12:40:51.664: INFO: 80 pods has nil DeletionTimestamp
  Aug 24 12:40:51.664: INFO: 
  E0824 12:40:51.688320      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:40:52.709202      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:52.960: INFO: 82 pods remaining
  Aug 24 12:40:53.106: INFO: 61 pods has nil DeletionTimestamp
  Aug 24 12:40:53.106: INFO: 
  E0824 12:40:53.710096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:54.134: INFO: 73 pods remaining
  Aug 24 12:40:54.135: INFO: 43 pods has nil DeletionTimestamp
  Aug 24 12:40:54.135: INFO: 
  E0824 12:40:54.711173      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:55.263: INFO: 64 pods remaining
  Aug 24 12:40:55.263: INFO: 26 pods has nil DeletionTimestamp
  Aug 24 12:40:55.263: INFO: 
  E0824 12:40:55.741159      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:56.349: INFO: 60 pods remaining
  Aug 24 12:40:56.350: INFO: 17 pods has nil DeletionTimestamp
  Aug 24 12:40:56.350: INFO: 
  E0824 12:40:56.719501      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:57.514: INFO: 53 pods remaining
  Aug 24 12:40:57.514: INFO: 2 pods has nil DeletionTimestamp
  Aug 24 12:40:57.514: INFO: 
  E0824 12:40:57.722829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:58.216: INFO: 46 pods remaining
  Aug 24 12:40:58.220: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 12:40:58.220: INFO: 
  E0824 12:40:58.728943      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:40:59.120: INFO: 41 pods remaining
  Aug 24 12:40:59.120: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 12:40:59.121: INFO: 
  E0824 12:40:59.741743      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:00.461: INFO: 33 pods remaining
  Aug 24 12:41:00.461: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 12:41:00.461: INFO: 
  E0824 12:41:00.734754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:01.111: INFO: 28 pods remaining
  Aug 24 12:41:01.111: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 12:41:01.114: INFO: 
  E0824 12:41:01.735789      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:02.412: INFO: 20 pods remaining
  Aug 24 12:41:02.412: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 12:41:02.413: INFO: 
  E0824 12:41:02.736125      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:03.078: INFO: 15 pods remaining
  Aug 24 12:41:03.078: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 12:41:03.078: INFO: 
  E0824 12:41:03.736328      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:04.263: INFO: 8 pods remaining
  Aug 24 12:41:04.263: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 12:41:04.263: INFO: 
  E0824 12:41:04.736584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:05.088: INFO: 1 pods remaining
  Aug 24 12:41:05.088: INFO: 0 pods has nil DeletionTimestamp
  Aug 24 12:41:05.088: INFO: 
  E0824 12:41:05.736756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/24/23 12:41:06.072
  Aug 24 12:41:06.349: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 12:41:06.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-664" for this suite. @ 08/24/23 12:41:06.363
• [23.006 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 08/24/23 12:41:06.381
  Aug 24 12:41:06.381: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 12:41:06.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:06.46
  E0824 12:41:06.858303      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:07.148
  STEP: creating service in namespace services-9663 @ 08/24/23 12:41:07.628
  STEP: creating service affinity-clusterip-transition in namespace services-9663 @ 08/24/23 12:41:07.629
  E0824 12:41:07.980410      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating replication controller affinity-clusterip-transition in namespace services-9663 @ 08/24/23 12:41:07.981
  I0824 12:41:08.165333      15 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-9663, replica count: 3
  E0824 12:41:08.981246      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:09.984901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:10.991271      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:41:11.250014      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 0 running, 3 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 12:41:11.993515      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:12.994438      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:13.994714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:41:14.250870      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 12:41:14.995018      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:15.995164      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:16.995500      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:41:17.252271      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 12:41:17.995505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:18.996453      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:19.998197      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:41:20.253372      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 12:41:20.997926      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:21.998559      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:22.998772      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:41:23.254888      15 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:41:23.275: INFO: Creating new exec pod
  E0824 12:41:24.002984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:25.003492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:26.004063      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:26.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-9663 exec execpod-affinitydkzjt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Aug 24 12:41:26.648: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Aug 24 12:41:26.648: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:41:26.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-9663 exec execpod-affinitydkzjt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.21.211 80'
  Aug 24 12:41:26.935: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.21.211 80\nConnection to 10.233.21.211 80 port [tcp/http] succeeded!\n"
  Aug 24 12:41:26.935: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:41:26.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-9663 exec execpod-affinitydkzjt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.21.211:80/ ; done'
  E0824 12:41:27.004165      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:27.445: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n"
  Aug 24 12:41:27.445: INFO: stdout: "\naffinity-clusterip-transition-fwbs4\naffinity-clusterip-transition-fwbs4\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-fwbs4\naffinity-clusterip-transition-7cs6t\naffinity-clusterip-transition-7cs6t\naffinity-clusterip-transition-fwbs4\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-7cs6t\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-fwbs4\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-fwbs4\naffinity-clusterip-transition-7cs6t"
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-fwbs4
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-fwbs4
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-fwbs4
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-7cs6t
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-7cs6t
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-fwbs4
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-7cs6t
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-fwbs4
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-fwbs4
  Aug 24 12:41:27.445: INFO: Received response from host: affinity-clusterip-transition-7cs6t
  Aug 24 12:41:27.464: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-9663 exec execpod-affinitydkzjt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.21.211:80/ ; done'
  Aug 24 12:41:27.930: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.211:80/\n"
  Aug 24 12:41:27.930: INFO: stdout: "\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz\naffinity-clusterip-transition-9gccz"
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Received response from host: affinity-clusterip-transition-9gccz
  Aug 24 12:41:27.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:41:27.939: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-9663, will wait for the garbage collector to delete the pods @ 08/24/23 12:41:27.969
  E0824 12:41:28.004838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:28.045: INFO: Deleting ReplicationController affinity-clusterip-transition took: 20.668517ms
  Aug 24 12:41:28.147: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 101.611847ms
  E0824 12:41:29.005376      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:30.006089      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9663" for this suite. @ 08/24/23 12:41:30.278
• [23.916 seconds]
------------------------------
S
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 08/24/23 12:41:30.3
  Aug 24 12:41:30.300: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename svc-latency @ 08/24/23 12:41:30.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:30.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:30.347
  Aug 24 12:41:30.354: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-7735 @ 08/24/23 12:41:30.356
  I0824 12:41:30.372762      15 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7735, replica count: 1
  E0824 12:41:31.006614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:41:31.424691      15 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 12:41:32.006947      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:41:32.425821      15 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:41:32.554: INFO: Created: latency-svc-5gwb6
  Aug 24 12:41:32.571: INFO: Got endpoints: latency-svc-5gwb6 [44.660759ms]
  Aug 24 12:41:32.596: INFO: Created: latency-svc-cdtnm
  Aug 24 12:41:32.616: INFO: Got endpoints: latency-svc-cdtnm [42.787132ms]
  Aug 24 12:41:32.619: INFO: Created: latency-svc-bcl8q
  Aug 24 12:41:32.633: INFO: Got endpoints: latency-svc-bcl8q [58.067823ms]
  Aug 24 12:41:32.637: INFO: Created: latency-svc-9568v
  Aug 24 12:41:32.652: INFO: Got endpoints: latency-svc-9568v [78.004665ms]
  Aug 24 12:41:32.657: INFO: Created: latency-svc-25jx4
  Aug 24 12:41:32.673: INFO: Created: latency-svc-whmtz
  Aug 24 12:41:32.678: INFO: Got endpoints: latency-svc-25jx4 [99.52356ms]
  Aug 24 12:41:32.696: INFO: Got endpoints: latency-svc-whmtz [120.666083ms]
  Aug 24 12:41:32.809: INFO: Created: latency-svc-bsh5r
  Aug 24 12:41:32.811: INFO: Created: latency-svc-zwbff
  Aug 24 12:41:32.813: INFO: Created: latency-svc-vmsjd
  Aug 24 12:41:32.815: INFO: Created: latency-svc-2vc4j
  Aug 24 12:41:32.820: INFO: Created: latency-svc-fg4md
  Aug 24 12:41:32.820: INFO: Created: latency-svc-blvtg
  Aug 24 12:41:32.822: INFO: Created: latency-svc-q6mdz
  Aug 24 12:41:32.822: INFO: Created: latency-svc-rxqkc
  Aug 24 12:41:32.823: INFO: Created: latency-svc-pllkv
  Aug 24 12:41:32.823: INFO: Created: latency-svc-l7fc7
  Aug 24 12:41:32.856: INFO: Created: latency-svc-c2dth
  Aug 24 12:41:32.857: INFO: Created: latency-svc-fqd5s
  Aug 24 12:41:32.857: INFO: Created: latency-svc-b97fg
  Aug 24 12:41:32.857: INFO: Created: latency-svc-w7swk
  Aug 24 12:41:32.858: INFO: Created: latency-svc-rmvpl
  Aug 24 12:41:32.915: INFO: Got endpoints: latency-svc-l7fc7 [333.710098ms]
  Aug 24 12:41:32.942: INFO: Got endpoints: latency-svc-bsh5r [366.116025ms]
  Aug 24 12:41:32.942: INFO: Got endpoints: latency-svc-zwbff [362.576362ms]
  Aug 24 12:41:32.962: INFO: Created: latency-svc-5pwb5
  Aug 24 12:41:32.978: INFO: Got endpoints: latency-svc-2vc4j [399.433362ms]
  Aug 24 12:41:32.980: INFO: Got endpoints: latency-svc-blvtg [402.578473ms]
  Aug 24 12:41:32.980: INFO: Got endpoints: latency-svc-vmsjd [346.688184ms]
  Aug 24 12:41:32.981: INFO: Got endpoints: latency-svc-fg4md [399.530569ms]
  Aug 24 12:41:32.981: INFO: Got endpoints: latency-svc-q6mdz [284.607715ms]
  Aug 24 12:41:32.994: INFO: Got endpoints: latency-svc-rxqkc [341.583823ms]
  E0824 12:41:33.007176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:33.013: INFO: Got endpoints: latency-svc-pllkv [432.49406ms]
  Aug 24 12:41:33.013: INFO: Got endpoints: latency-svc-w7swk [335.336384ms]
  Aug 24 12:41:33.029: INFO: Got endpoints: latency-svc-rmvpl [450.085515ms]
  Aug 24 12:41:33.032: INFO: Got endpoints: latency-svc-b97fg [415.763776ms]
  Aug 24 12:41:33.036: INFO: Got endpoints: latency-svc-5pwb5 [120.587558ms]
  Aug 24 12:41:33.041: INFO: Got endpoints: latency-svc-fqd5s [464.818862ms]
  Aug 24 12:41:33.047: INFO: Got endpoints: latency-svc-c2dth [466.741249ms]
  Aug 24 12:41:33.078: INFO: Created: latency-svc-b8wmm
  Aug 24 12:41:33.090: INFO: Got endpoints: latency-svc-b8wmm [147.527619ms]
  Aug 24 12:41:33.098: INFO: Created: latency-svc-74hlh
  Aug 24 12:41:33.114: INFO: Got endpoints: latency-svc-74hlh [171.363025ms]
  Aug 24 12:41:33.117: INFO: Created: latency-svc-czjnp
  Aug 24 12:41:33.134: INFO: Got endpoints: latency-svc-czjnp [155.736077ms]
  Aug 24 12:41:33.135: INFO: Created: latency-svc-bcq2t
  Aug 24 12:41:33.149: INFO: Got endpoints: latency-svc-bcq2t [169.250969ms]
  Aug 24 12:41:33.154: INFO: Created: latency-svc-7q5zl
  Aug 24 12:41:33.177: INFO: Created: latency-svc-rjmf2
  Aug 24 12:41:33.195: INFO: Got endpoints: latency-svc-7q5zl [214.007267ms]
  Aug 24 12:41:33.212: INFO: Got endpoints: latency-svc-rjmf2 [231.534361ms]
  Aug 24 12:41:33.246: INFO: Created: latency-svc-64nr9
  Aug 24 12:41:33.310: INFO: Got endpoints: latency-svc-64nr9 [328.330405ms]
  Aug 24 12:41:33.335: INFO: Created: latency-svc-z66nx
  Aug 24 12:41:33.343: INFO: Created: latency-svc-ndnkk
  Aug 24 12:41:33.349: INFO: Got endpoints: latency-svc-ndnkk [354.766536ms]
  Aug 24 12:41:33.353: INFO: Created: latency-svc-5blgp
  Aug 24 12:41:33.376: INFO: Got endpoints: latency-svc-5blgp [362.21412ms]
  Aug 24 12:41:33.380: INFO: Created: latency-svc-xh5z5
  Aug 24 12:41:33.397: INFO: Got endpoints: latency-svc-z66nx [383.951935ms]
  Aug 24 12:41:33.400: INFO: Created: latency-svc-gswlz
  Aug 24 12:41:33.409: INFO: Got endpoints: latency-svc-xh5z5 [378.696082ms]
  Aug 24 12:41:33.414: INFO: Created: latency-svc-n4cxk
  Aug 24 12:41:33.425: INFO: Got endpoints: latency-svc-gswlz [392.749197ms]
  Aug 24 12:41:33.432: INFO: Got endpoints: latency-svc-n4cxk [395.735605ms]
  Aug 24 12:41:33.434: INFO: Created: latency-svc-zznms
  Aug 24 12:41:33.448: INFO: Got endpoints: latency-svc-zznms [406.199116ms]
  Aug 24 12:41:33.448: INFO: Created: latency-svc-5gpvn
  Aug 24 12:41:33.472: INFO: Created: latency-svc-l2mtf
  Aug 24 12:41:33.484: INFO: Got endpoints: latency-svc-5gpvn [436.304694ms]
  Aug 24 12:41:33.484: INFO: Got endpoints: latency-svc-l2mtf [393.274415ms]
  Aug 24 12:41:33.496: INFO: Created: latency-svc-56b8k
  Aug 24 12:41:33.513: INFO: Created: latency-svc-hjwbp
  Aug 24 12:41:33.522: INFO: Got endpoints: latency-svc-56b8k [407.64342ms]
  Aug 24 12:41:33.534: INFO: Created: latency-svc-p8blv
  Aug 24 12:41:33.539: INFO: Got endpoints: latency-svc-hjwbp [403.596105ms]
  Aug 24 12:41:33.549: INFO: Created: latency-svc-7bl9c
  Aug 24 12:41:33.550: INFO: Got endpoints: latency-svc-p8blv [400.722086ms]
  Aug 24 12:41:33.555: INFO: Created: latency-svc-hqj4l
  Aug 24 12:41:33.567: INFO: Got endpoints: latency-svc-7bl9c [371.079172ms]
  Aug 24 12:41:33.585: INFO: Got endpoints: latency-svc-hqj4l [372.156619ms]
  Aug 24 12:41:33.586: INFO: Created: latency-svc-2k9l4
  Aug 24 12:41:33.597: INFO: Got endpoints: latency-svc-2k9l4 [287.042019ms]
  Aug 24 12:41:33.606: INFO: Created: latency-svc-k5qxv
  Aug 24 12:41:33.615: INFO: Created: latency-svc-rmkkp
  Aug 24 12:41:33.618: INFO: Got endpoints: latency-svc-k5qxv [268.730888ms]
  Aug 24 12:41:33.632: INFO: Got endpoints: latency-svc-rmkkp [256.445735ms]
  Aug 24 12:41:33.737: INFO: Created: latency-svc-8cbmc
  Aug 24 12:41:33.754: INFO: Created: latency-svc-2n2ng
  Aug 24 12:41:33.756: INFO: Created: latency-svc-c4cgp
  Aug 24 12:41:33.757: INFO: Created: latency-svc-gmknt
  Aug 24 12:41:33.758: INFO: Created: latency-svc-wqgjc
  Aug 24 12:41:33.766: INFO: Created: latency-svc-4mzxz
  Aug 24 12:41:33.768: INFO: Created: latency-svc-qpxqv
  Aug 24 12:41:33.773: INFO: Created: latency-svc-gsxjj
  Aug 24 12:41:33.785: INFO: Created: latency-svc-bdwks
  Aug 24 12:41:33.786: INFO: Created: latency-svc-x64lx
  Aug 24 12:41:33.787: INFO: Created: latency-svc-st9gk
  Aug 24 12:41:33.792: INFO: Created: latency-svc-zxgpf
  Aug 24 12:41:33.792: INFO: Got endpoints: latency-svc-8cbmc [366.850539ms]
  Aug 24 12:41:33.793: INFO: Created: latency-svc-t2npw
  Aug 24 12:41:33.793: INFO: Created: latency-svc-wnxcg
  Aug 24 12:41:33.794: INFO: Created: latency-svc-vb9cc
  Aug 24 12:41:33.802: INFO: Got endpoints: latency-svc-4mzxz [234.862778ms]
  Aug 24 12:41:33.819: INFO: Got endpoints: latency-svc-gsxjj [334.665255ms]
  Aug 24 12:41:33.819: INFO: Got endpoints: latency-svc-bdwks [201.03195ms]
  Aug 24 12:41:33.824: INFO: Created: latency-svc-vks6q
  Aug 24 12:41:33.834: INFO: Got endpoints: latency-svc-2n2ng [386.376876ms]
  Aug 24 12:41:33.853: INFO: Created: latency-svc-466jg
  Aug 24 12:41:33.864: INFO: Got endpoints: latency-svc-wqgjc [431.443619ms]
  Aug 24 12:41:33.874: INFO: Got endpoints: latency-svc-vb9cc [276.543383ms]
  Aug 24 12:41:33.874: INFO: Got endpoints: latency-svc-c4cgp [334.173135ms]
  Aug 24 12:41:33.875: INFO: Got endpoints: latency-svc-zxgpf [477.522882ms]
  Aug 24 12:41:33.877: INFO: Got endpoints: latency-svc-t2npw [244.831927ms]
  Aug 24 12:41:33.884: INFO: Created: latency-svc-xrlzf
  Aug 24 12:41:33.894: INFO: Created: latency-svc-7tn5v
  Aug 24 12:41:33.908: INFO: Created: latency-svc-n8djb
  Aug 24 12:41:33.921: INFO: Created: latency-svc-65brf
  Aug 24 12:41:33.927: INFO: Got endpoints: latency-svc-wnxcg [518.790204ms]
  Aug 24 12:41:33.936: INFO: Created: latency-svc-kc8v9
  Aug 24 12:41:33.955: INFO: Created: latency-svc-4dj68
  Aug 24 12:41:33.966: INFO: Created: latency-svc-svd8p
  Aug 24 12:41:33.970: INFO: Got endpoints: latency-svc-st9gk [447.781422ms]
  Aug 24 12:41:33.985: INFO: Created: latency-svc-p5s6c
  Aug 24 12:41:33.998: INFO: Created: latency-svc-65hmw
  E0824 12:41:34.007700      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:34.021: INFO: Got endpoints: latency-svc-gmknt [471.22777ms]
  Aug 24 12:41:34.023: INFO: Created: latency-svc-cwdjl
  Aug 24 12:41:34.056: INFO: Created: latency-svc-cf5wk
  Aug 24 12:41:34.066: INFO: Got endpoints: latency-svc-x64lx [481.001226ms]
  Aug 24 12:41:34.089: INFO: Created: latency-svc-v9nc5
  Aug 24 12:41:34.121: INFO: Got endpoints: latency-svc-qpxqv [637.35912ms]
  Aug 24 12:41:34.138: INFO: Created: latency-svc-t26w6
  Aug 24 12:41:34.170: INFO: Got endpoints: latency-svc-vks6q [377.095457ms]
  Aug 24 12:41:34.185: INFO: Created: latency-svc-gc8hs
  Aug 24 12:41:34.215: INFO: Got endpoints: latency-svc-466jg [412.823299ms]
  Aug 24 12:41:34.233: INFO: Created: latency-svc-h7lkz
  Aug 24 12:41:34.266: INFO: Got endpoints: latency-svc-xrlzf [446.325271ms]
  Aug 24 12:41:34.294: INFO: Created: latency-svc-2z49m
  Aug 24 12:41:34.319: INFO: Got endpoints: latency-svc-7tn5v [499.003806ms]
  Aug 24 12:41:34.338: INFO: Created: latency-svc-22wx4
  Aug 24 12:41:34.368: INFO: Got endpoints: latency-svc-n8djb [533.892056ms]
  Aug 24 12:41:34.392: INFO: Created: latency-svc-m7p8q
  Aug 24 12:41:34.421: INFO: Got endpoints: latency-svc-65brf [557.505085ms]
  Aug 24 12:41:34.447: INFO: Created: latency-svc-rdksd
  Aug 24 12:41:34.473: INFO: Got endpoints: latency-svc-kc8v9 [599.026543ms]
  Aug 24 12:41:34.496: INFO: Created: latency-svc-q4msb
  Aug 24 12:41:34.517: INFO: Got endpoints: latency-svc-4dj68 [641.537696ms]
  Aug 24 12:41:34.540: INFO: Created: latency-svc-hhtz8
  Aug 24 12:41:34.567: INFO: Got endpoints: latency-svc-svd8p [692.67846ms]
  Aug 24 12:41:34.596: INFO: Created: latency-svc-w8nsl
  Aug 24 12:41:34.624: INFO: Got endpoints: latency-svc-p5s6c [746.906004ms]
  Aug 24 12:41:34.647: INFO: Created: latency-svc-2mdg6
  Aug 24 12:41:34.667: INFO: Got endpoints: latency-svc-65hmw [739.447825ms]
  Aug 24 12:41:34.686: INFO: Created: latency-svc-9hf79
  Aug 24 12:41:34.720: INFO: Got endpoints: latency-svc-cwdjl [748.632573ms]
  Aug 24 12:41:34.742: INFO: Created: latency-svc-fnqlt
  Aug 24 12:41:34.769: INFO: Got endpoints: latency-svc-cf5wk [747.324096ms]
  Aug 24 12:41:34.790: INFO: Created: latency-svc-b2kj6
  Aug 24 12:41:34.822: INFO: Got endpoints: latency-svc-v9nc5 [756.329317ms]
  Aug 24 12:41:34.845: INFO: Created: latency-svc-sn7kd
  Aug 24 12:41:34.869: INFO: Got endpoints: latency-svc-t26w6 [747.138402ms]
  Aug 24 12:41:34.888: INFO: Created: latency-svc-4dcwh
  Aug 24 12:41:34.917: INFO: Got endpoints: latency-svc-gc8hs [747.270383ms]
  Aug 24 12:41:34.942: INFO: Created: latency-svc-dggd4
  Aug 24 12:41:34.968: INFO: Got endpoints: latency-svc-h7lkz [752.664783ms]
  Aug 24 12:41:34.999: INFO: Created: latency-svc-flztc
  E0824 12:41:35.008099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:35.023: INFO: Got endpoints: latency-svc-2z49m [756.836559ms]
  Aug 24 12:41:35.044: INFO: Created: latency-svc-r9hsp
  Aug 24 12:41:35.079: INFO: Got endpoints: latency-svc-22wx4 [759.191752ms]
  Aug 24 12:41:35.098: INFO: Created: latency-svc-5jvjv
  Aug 24 12:41:35.130: INFO: Got endpoints: latency-svc-m7p8q [761.775665ms]
  Aug 24 12:41:35.152: INFO: Created: latency-svc-7vjfs
  Aug 24 12:41:35.170: INFO: Got endpoints: latency-svc-rdksd [747.847834ms]
  Aug 24 12:41:35.188: INFO: Created: latency-svc-tbc58
  Aug 24 12:41:35.222: INFO: Got endpoints: latency-svc-q4msb [748.574005ms]
  Aug 24 12:41:35.248: INFO: Created: latency-svc-psmww
  Aug 24 12:41:35.264: INFO: Got endpoints: latency-svc-hhtz8 [747.604576ms]
  Aug 24 12:41:35.288: INFO: Created: latency-svc-c4nxr
  Aug 24 12:41:35.320: INFO: Got endpoints: latency-svc-w8nsl [752.168129ms]
  Aug 24 12:41:35.340: INFO: Created: latency-svc-xhk2m
  Aug 24 12:41:35.365: INFO: Got endpoints: latency-svc-2mdg6 [741.220386ms]
  Aug 24 12:41:35.383: INFO: Created: latency-svc-qfqfs
  Aug 24 12:41:35.414: INFO: Got endpoints: latency-svc-9hf79 [746.209427ms]
  Aug 24 12:41:35.453: INFO: Created: latency-svc-wcd2b
  Aug 24 12:41:35.468: INFO: Got endpoints: latency-svc-fnqlt [746.99579ms]
  Aug 24 12:41:35.485: INFO: Created: latency-svc-7cm6b
  Aug 24 12:41:35.515: INFO: Got endpoints: latency-svc-b2kj6 [745.647732ms]
  Aug 24 12:41:35.536: INFO: Created: latency-svc-2vj5f
  Aug 24 12:41:35.570: INFO: Got endpoints: latency-svc-sn7kd [746.553563ms]
  Aug 24 12:41:35.591: INFO: Created: latency-svc-xf5zt
  Aug 24 12:41:35.615: INFO: Got endpoints: latency-svc-4dcwh [746.260258ms]
  Aug 24 12:41:35.632: INFO: Created: latency-svc-jn4r5
  Aug 24 12:41:35.661: INFO: Got endpoints: latency-svc-dggd4 [743.537271ms]
  Aug 24 12:41:35.682: INFO: Created: latency-svc-pl6k5
  Aug 24 12:41:35.724: INFO: Got endpoints: latency-svc-flztc [755.392357ms]
  Aug 24 12:41:35.745: INFO: Created: latency-svc-frbfq
  Aug 24 12:41:35.763: INFO: Got endpoints: latency-svc-r9hsp [739.374531ms]
  Aug 24 12:41:35.786: INFO: Created: latency-svc-ss6xq
  Aug 24 12:41:35.813: INFO: Got endpoints: latency-svc-5jvjv [733.970847ms]
  Aug 24 12:41:35.841: INFO: Created: latency-svc-p4svw
  Aug 24 12:41:35.875: INFO: Got endpoints: latency-svc-7vjfs [744.782091ms]
  Aug 24 12:41:35.909: INFO: Created: latency-svc-xrzs9
  Aug 24 12:41:35.930: INFO: Got endpoints: latency-svc-tbc58 [760.164166ms]
  Aug 24 12:41:35.950: INFO: Created: latency-svc-s8lhn
  Aug 24 12:41:35.968: INFO: Got endpoints: latency-svc-psmww [745.612532ms]
  Aug 24 12:41:36.004: INFO: Created: latency-svc-mrjdk
  E0824 12:41:36.008395      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:36.019: INFO: Got endpoints: latency-svc-c4nxr [753.764955ms]
  Aug 24 12:41:36.070: INFO: Created: latency-svc-l4dbr
  Aug 24 12:41:36.092: INFO: Got endpoints: latency-svc-xhk2m [772.484627ms]
  Aug 24 12:41:36.127: INFO: Created: latency-svc-gqjnt
  Aug 24 12:41:36.129: INFO: Got endpoints: latency-svc-qfqfs [763.382033ms]
  Aug 24 12:41:36.171: INFO: Created: latency-svc-bjrkr
  Aug 24 12:41:36.175: INFO: Got endpoints: latency-svc-wcd2b [761.192678ms]
  Aug 24 12:41:36.205: INFO: Created: latency-svc-m7mx9
  Aug 24 12:41:36.223: INFO: Got endpoints: latency-svc-7cm6b [754.856494ms]
  Aug 24 12:41:36.257: INFO: Created: latency-svc-84sqh
  Aug 24 12:41:36.269: INFO: Got endpoints: latency-svc-2vj5f [754.081607ms]
  Aug 24 12:41:36.292: INFO: Created: latency-svc-cvq5w
  Aug 24 12:41:36.314: INFO: Got endpoints: latency-svc-xf5zt [743.756629ms]
  Aug 24 12:41:36.334: INFO: Created: latency-svc-rgzfx
  Aug 24 12:41:36.371: INFO: Got endpoints: latency-svc-jn4r5 [755.683597ms]
  Aug 24 12:41:36.397: INFO: Created: latency-svc-qxvjr
  Aug 24 12:41:36.418: INFO: Got endpoints: latency-svc-pl6k5 [756.765924ms]
  Aug 24 12:41:36.437: INFO: Created: latency-svc-c5rjq
  Aug 24 12:41:36.469: INFO: Got endpoints: latency-svc-frbfq [744.928575ms]
  Aug 24 12:41:36.491: INFO: Created: latency-svc-zzlnh
  Aug 24 12:41:36.515: INFO: Got endpoints: latency-svc-ss6xq [751.655909ms]
  Aug 24 12:41:36.536: INFO: Created: latency-svc-t49v4
  Aug 24 12:41:36.574: INFO: Got endpoints: latency-svc-p4svw [760.472084ms]
  Aug 24 12:41:36.598: INFO: Created: latency-svc-wbwh5
  Aug 24 12:41:36.616: INFO: Got endpoints: latency-svc-xrzs9 [740.47497ms]
  Aug 24 12:41:36.637: INFO: Created: latency-svc-s7d2t
  Aug 24 12:41:36.668: INFO: Got endpoints: latency-svc-s8lhn [738.051833ms]
  Aug 24 12:41:36.686: INFO: Created: latency-svc-2gnbp
  Aug 24 12:41:36.720: INFO: Got endpoints: latency-svc-mrjdk [751.419615ms]
  Aug 24 12:41:36.744: INFO: Created: latency-svc-4dght
  Aug 24 12:41:36.768: INFO: Got endpoints: latency-svc-l4dbr [749.376232ms]
  Aug 24 12:41:36.791: INFO: Created: latency-svc-dfbtf
  Aug 24 12:41:36.815: INFO: Got endpoints: latency-svc-gqjnt [721.487166ms]
  Aug 24 12:41:36.837: INFO: Created: latency-svc-qcpfr
  Aug 24 12:41:36.868: INFO: Got endpoints: latency-svc-bjrkr [739.04119ms]
  Aug 24 12:41:36.890: INFO: Created: latency-svc-v6vzk
  Aug 24 12:41:36.925: INFO: Got endpoints: latency-svc-m7mx9 [749.576505ms]
  Aug 24 12:41:36.948: INFO: Created: latency-svc-prpkh
  Aug 24 12:41:36.965: INFO: Got endpoints: latency-svc-84sqh [742.036657ms]
  Aug 24 12:41:36.992: INFO: Created: latency-svc-76g8k
  E0824 12:41:37.009390      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:37.022: INFO: Got endpoints: latency-svc-cvq5w [753.18097ms]
  Aug 24 12:41:37.049: INFO: Created: latency-svc-hm2kj
  Aug 24 12:41:37.078: INFO: Got endpoints: latency-svc-rgzfx [764.543266ms]
  Aug 24 12:41:37.103: INFO: Created: latency-svc-4s92g
  Aug 24 12:41:37.118: INFO: Got endpoints: latency-svc-qxvjr [746.812987ms]
  Aug 24 12:41:37.138: INFO: Created: latency-svc-56dmc
  Aug 24 12:41:37.165: INFO: Got endpoints: latency-svc-c5rjq [746.28773ms]
  Aug 24 12:41:37.188: INFO: Created: latency-svc-6wk7k
  Aug 24 12:41:37.235: INFO: Got endpoints: latency-svc-zzlnh [765.944011ms]
  Aug 24 12:41:37.262: INFO: Created: latency-svc-hctmw
  Aug 24 12:41:37.268: INFO: Got endpoints: latency-svc-t49v4 [753.168616ms]
  Aug 24 12:41:37.293: INFO: Created: latency-svc-ck2lc
  Aug 24 12:41:37.321: INFO: Got endpoints: latency-svc-wbwh5 [746.610852ms]
  Aug 24 12:41:37.347: INFO: Created: latency-svc-84c7v
  Aug 24 12:41:37.376: INFO: Got endpoints: latency-svc-s7d2t [759.432607ms]
  Aug 24 12:41:37.397: INFO: Created: latency-svc-crfrt
  Aug 24 12:41:37.422: INFO: Got endpoints: latency-svc-2gnbp [753.796733ms]
  Aug 24 12:41:37.451: INFO: Created: latency-svc-2fw5m
  Aug 24 12:41:37.475: INFO: Got endpoints: latency-svc-4dght [755.099168ms]
  Aug 24 12:41:37.495: INFO: Created: latency-svc-zm4sr
  Aug 24 12:41:37.526: INFO: Got endpoints: latency-svc-dfbtf [757.202761ms]
  Aug 24 12:41:37.555: INFO: Created: latency-svc-g5cqv
  Aug 24 12:41:37.567: INFO: Got endpoints: latency-svc-qcpfr [751.005096ms]
  Aug 24 12:41:37.594: INFO: Created: latency-svc-9kvvh
  Aug 24 12:41:37.622: INFO: Got endpoints: latency-svc-v6vzk [753.263676ms]
  Aug 24 12:41:37.643: INFO: Created: latency-svc-dwd8t
  Aug 24 12:41:37.667: INFO: Got endpoints: latency-svc-prpkh [741.800261ms]
  Aug 24 12:41:37.685: INFO: Created: latency-svc-485lc
  Aug 24 12:41:37.717: INFO: Got endpoints: latency-svc-76g8k [751.160728ms]
  Aug 24 12:41:37.738: INFO: Created: latency-svc-gskhx
  Aug 24 12:41:37.774: INFO: Got endpoints: latency-svc-hm2kj [751.900506ms]
  Aug 24 12:41:37.799: INFO: Created: latency-svc-hpsmn
  Aug 24 12:41:37.823: INFO: Got endpoints: latency-svc-4s92g [744.574836ms]
  Aug 24 12:41:37.850: INFO: Created: latency-svc-rhdvp
  Aug 24 12:41:37.863: INFO: Got endpoints: latency-svc-56dmc [744.804671ms]
  Aug 24 12:41:37.884: INFO: Created: latency-svc-2sqxt
  Aug 24 12:41:37.921: INFO: Got endpoints: latency-svc-6wk7k [756.682793ms]
  Aug 24 12:41:37.944: INFO: Created: latency-svc-xvcsr
  Aug 24 12:41:37.963: INFO: Got endpoints: latency-svc-hctmw [728.061037ms]
  Aug 24 12:41:37.988: INFO: Created: latency-svc-zvfn2
  E0824 12:41:38.009556      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:38.023: INFO: Got endpoints: latency-svc-ck2lc [754.663175ms]
  Aug 24 12:41:38.046: INFO: Created: latency-svc-7c2g7
  Aug 24 12:41:38.067: INFO: Got endpoints: latency-svc-84c7v [745.252207ms]
  Aug 24 12:41:38.105: INFO: Created: latency-svc-m62jl
  Aug 24 12:41:38.122: INFO: Got endpoints: latency-svc-crfrt [746.348475ms]
  Aug 24 12:41:38.146: INFO: Created: latency-svc-qts9p
  Aug 24 12:41:38.178: INFO: Got endpoints: latency-svc-2fw5m [755.599611ms]
  Aug 24 12:41:38.205: INFO: Created: latency-svc-pr29v
  Aug 24 12:41:38.220: INFO: Got endpoints: latency-svc-zm4sr [744.418647ms]
  Aug 24 12:41:38.240: INFO: Created: latency-svc-qrcgx
  Aug 24 12:41:38.266: INFO: Got endpoints: latency-svc-g5cqv [740.300719ms]
  Aug 24 12:41:38.286: INFO: Created: latency-svc-fqbs6
  Aug 24 12:41:38.315: INFO: Got endpoints: latency-svc-9kvvh [747.817685ms]
  Aug 24 12:41:38.332: INFO: Created: latency-svc-6wv85
  Aug 24 12:41:38.365: INFO: Got endpoints: latency-svc-dwd8t [742.562484ms]
  Aug 24 12:41:38.386: INFO: Created: latency-svc-vdfcg
  Aug 24 12:41:38.413: INFO: Got endpoints: latency-svc-485lc [746.12109ms]
  Aug 24 12:41:38.432: INFO: Created: latency-svc-gqtv8
  Aug 24 12:41:38.470: INFO: Got endpoints: latency-svc-gskhx [753.602987ms]
  Aug 24 12:41:38.491: INFO: Created: latency-svc-ltzzt
  Aug 24 12:41:38.514: INFO: Got endpoints: latency-svc-hpsmn [739.308045ms]
  Aug 24 12:41:38.533: INFO: Created: latency-svc-kxw96
  Aug 24 12:41:38.569: INFO: Got endpoints: latency-svc-rhdvp [745.52889ms]
  Aug 24 12:41:38.585: INFO: Created: latency-svc-cn9vb
  Aug 24 12:41:38.614: INFO: Got endpoints: latency-svc-2sqxt [750.714823ms]
  Aug 24 12:41:38.634: INFO: Created: latency-svc-swrq9
  Aug 24 12:41:38.674: INFO: Got endpoints: latency-svc-xvcsr [751.980621ms]
  Aug 24 12:41:38.694: INFO: Created: latency-svc-zshpz
  Aug 24 12:41:38.721: INFO: Got endpoints: latency-svc-zvfn2 [757.908619ms]
  Aug 24 12:41:38.740: INFO: Created: latency-svc-pwwm9
  Aug 24 12:41:38.770: INFO: Got endpoints: latency-svc-7c2g7 [746.403898ms]
  Aug 24 12:41:38.791: INFO: Created: latency-svc-kktpc
  Aug 24 12:41:38.820: INFO: Got endpoints: latency-svc-m62jl [753.092693ms]
  Aug 24 12:41:38.838: INFO: Created: latency-svc-54gzc
  Aug 24 12:41:38.873: INFO: Got endpoints: latency-svc-qts9p [750.343619ms]
  Aug 24 12:41:38.896: INFO: Created: latency-svc-rqvv6
  Aug 24 12:41:38.916: INFO: Got endpoints: latency-svc-pr29v [736.972629ms]
  Aug 24 12:41:38.942: INFO: Created: latency-svc-bhjdh
  Aug 24 12:41:38.968: INFO: Got endpoints: latency-svc-qrcgx [748.242054ms]
  Aug 24 12:41:39.000: INFO: Created: latency-svc-p6qz2
  E0824 12:41:39.010688      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:39.016: INFO: Got endpoints: latency-svc-fqbs6 [749.526209ms]
  Aug 24 12:41:39.045: INFO: Created: latency-svc-jtz89
  Aug 24 12:41:39.075: INFO: Got endpoints: latency-svc-6wv85 [759.90652ms]
  Aug 24 12:41:39.096: INFO: Created: latency-svc-xd6cz
  Aug 24 12:41:39.124: INFO: Got endpoints: latency-svc-vdfcg [759.466895ms]
  Aug 24 12:41:39.163: INFO: Created: latency-svc-bbnl5
  Aug 24 12:41:39.168: INFO: Got endpoints: latency-svc-gqtv8 [754.543639ms]
  Aug 24 12:41:39.198: INFO: Created: latency-svc-rtxdh
  Aug 24 12:41:39.217: INFO: Got endpoints: latency-svc-ltzzt [746.487525ms]
  Aug 24 12:41:39.238: INFO: Created: latency-svc-78pvt
  Aug 24 12:41:39.268: INFO: Got endpoints: latency-svc-kxw96 [754.126615ms]
  Aug 24 12:41:39.285: INFO: Created: latency-svc-vxrvs
  Aug 24 12:41:39.319: INFO: Got endpoints: latency-svc-cn9vb [749.508125ms]
  Aug 24 12:41:39.342: INFO: Created: latency-svc-497cv
  Aug 24 12:41:39.369: INFO: Got endpoints: latency-svc-swrq9 [754.396694ms]
  Aug 24 12:41:39.388: INFO: Created: latency-svc-jrfc8
  Aug 24 12:41:39.416: INFO: Got endpoints: latency-svc-zshpz [742.443772ms]
  Aug 24 12:41:39.438: INFO: Created: latency-svc-82mph
  Aug 24 12:41:39.465: INFO: Got endpoints: latency-svc-pwwm9 [743.145717ms]
  Aug 24 12:41:39.491: INFO: Created: latency-svc-rgj4q
  Aug 24 12:41:39.517: INFO: Got endpoints: latency-svc-kktpc [747.039536ms]
  Aug 24 12:41:39.541: INFO: Created: latency-svc-f968g
  Aug 24 12:41:39.565: INFO: Got endpoints: latency-svc-54gzc [744.64626ms]
  Aug 24 12:41:39.586: INFO: Created: latency-svc-2wl7r
  Aug 24 12:41:39.618: INFO: Got endpoints: latency-svc-rqvv6 [744.700396ms]
  Aug 24 12:41:39.639: INFO: Created: latency-svc-jht8d
  Aug 24 12:41:39.664: INFO: Got endpoints: latency-svc-bhjdh [747.918053ms]
  Aug 24 12:41:39.689: INFO: Created: latency-svc-vfll9
  Aug 24 12:41:39.717: INFO: Got endpoints: latency-svc-p6qz2 [747.359483ms]
  Aug 24 12:41:39.735: INFO: Created: latency-svc-vcv6r
  Aug 24 12:41:39.766: INFO: Got endpoints: latency-svc-jtz89 [749.360468ms]
  Aug 24 12:41:39.791: INFO: Created: latency-svc-7dvwg
  Aug 24 12:41:39.818: INFO: Got endpoints: latency-svc-xd6cz [742.447992ms]
  Aug 24 12:41:39.847: INFO: Created: latency-svc-n45kz
  Aug 24 12:41:39.865: INFO: Got endpoints: latency-svc-bbnl5 [740.807407ms]
  Aug 24 12:41:39.885: INFO: Created: latency-svc-f48kc
  Aug 24 12:41:39.917: INFO: Got endpoints: latency-svc-rtxdh [748.239631ms]
  Aug 24 12:41:39.941: INFO: Created: latency-svc-bvf48
  Aug 24 12:41:39.992: INFO: Got endpoints: latency-svc-78pvt [774.719631ms]
  E0824 12:41:40.010764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:40.019: INFO: Created: latency-svc-gk25h
  Aug 24 12:41:40.026: INFO: Got endpoints: latency-svc-vxrvs [757.699366ms]
  Aug 24 12:41:40.053: INFO: Created: latency-svc-x9h2w
  Aug 24 12:41:40.077: INFO: Got endpoints: latency-svc-497cv [758.496996ms]
  Aug 24 12:41:40.097: INFO: Created: latency-svc-j2vrj
  Aug 24 12:41:40.117: INFO: Got endpoints: latency-svc-jrfc8 [748.411172ms]
  Aug 24 12:41:40.153: INFO: Created: latency-svc-95gr6
  Aug 24 12:41:40.169: INFO: Got endpoints: latency-svc-82mph [752.849131ms]
  Aug 24 12:41:40.193: INFO: Created: latency-svc-b97r5
  Aug 24 12:41:40.221: INFO: Got endpoints: latency-svc-rgj4q [756.248551ms]
  Aug 24 12:41:40.240: INFO: Created: latency-svc-hnd2w
  Aug 24 12:41:40.265: INFO: Got endpoints: latency-svc-f968g [748.003153ms]
  Aug 24 12:41:40.284: INFO: Created: latency-svc-742b6
  Aug 24 12:41:40.319: INFO: Got endpoints: latency-svc-2wl7r [753.619512ms]
  Aug 24 12:41:40.340: INFO: Created: latency-svc-p6zgg
  Aug 24 12:41:40.372: INFO: Got endpoints: latency-svc-jht8d [754.33503ms]
  Aug 24 12:41:40.394: INFO: Created: latency-svc-4vccs
  Aug 24 12:41:40.421: INFO: Got endpoints: latency-svc-vfll9 [757.253365ms]
  Aug 24 12:41:40.469: INFO: Got endpoints: latency-svc-vcv6r [751.865172ms]
  Aug 24 12:41:40.517: INFO: Got endpoints: latency-svc-7dvwg [750.515752ms]
  Aug 24 12:41:40.568: INFO: Got endpoints: latency-svc-n45kz [750.124464ms]
  Aug 24 12:41:40.617: INFO: Got endpoints: latency-svc-f48kc [751.55141ms]
  Aug 24 12:41:40.666: INFO: Got endpoints: latency-svc-bvf48 [748.805858ms]
  Aug 24 12:41:40.717: INFO: Got endpoints: latency-svc-gk25h [725.078369ms]
  Aug 24 12:41:40.766: INFO: Got endpoints: latency-svc-x9h2w [739.757762ms]
  Aug 24 12:41:40.819: INFO: Got endpoints: latency-svc-j2vrj [741.336781ms]
  Aug 24 12:41:40.877: INFO: Got endpoints: latency-svc-95gr6 [759.692032ms]
  Aug 24 12:41:40.916: INFO: Got endpoints: latency-svc-b97r5 [746.973434ms]
  Aug 24 12:41:40.965: INFO: Got endpoints: latency-svc-hnd2w [743.97413ms]
  E0824 12:41:41.011771      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:41.017: INFO: Got endpoints: latency-svc-742b6 [751.29367ms]
  Aug 24 12:41:41.066: INFO: Got endpoints: latency-svc-p6zgg [747.3161ms]
  Aug 24 12:41:41.123: INFO: Got endpoints: latency-svc-4vccs [751.184486ms]
  Aug 24 12:41:41.124: INFO: Latencies: [42.787132ms 58.067823ms 78.004665ms 99.52356ms 120.587558ms 120.666083ms 147.527619ms 155.736077ms 169.250969ms 171.363025ms 201.03195ms 214.007267ms 231.534361ms 234.862778ms 244.831927ms 256.445735ms 268.730888ms 276.543383ms 284.607715ms 287.042019ms 328.330405ms 333.710098ms 334.173135ms 334.665255ms 335.336384ms 341.583823ms 346.688184ms 354.766536ms 362.21412ms 362.576362ms 366.116025ms 366.850539ms 371.079172ms 372.156619ms 377.095457ms 378.696082ms 383.951935ms 386.376876ms 392.749197ms 393.274415ms 395.735605ms 399.433362ms 399.530569ms 400.722086ms 402.578473ms 403.596105ms 406.199116ms 407.64342ms 412.823299ms 415.763776ms 431.443619ms 432.49406ms 436.304694ms 446.325271ms 447.781422ms 450.085515ms 464.818862ms 466.741249ms 471.22777ms 477.522882ms 481.001226ms 499.003806ms 518.790204ms 533.892056ms 557.505085ms 599.026543ms 637.35912ms 641.537696ms 692.67846ms 721.487166ms 725.078369ms 728.061037ms 733.970847ms 736.972629ms 738.051833ms 739.04119ms 739.308045ms 739.374531ms 739.447825ms 739.757762ms 740.300719ms 740.47497ms 740.807407ms 741.220386ms 741.336781ms 741.800261ms 742.036657ms 742.443772ms 742.447992ms 742.562484ms 743.145717ms 743.537271ms 743.756629ms 743.97413ms 744.418647ms 744.574836ms 744.64626ms 744.700396ms 744.782091ms 744.804671ms 744.928575ms 745.252207ms 745.52889ms 745.612532ms 745.647732ms 746.12109ms 746.209427ms 746.260258ms 746.28773ms 746.348475ms 746.403898ms 746.487525ms 746.553563ms 746.610852ms 746.812987ms 746.906004ms 746.973434ms 746.99579ms 747.039536ms 747.138402ms 747.270383ms 747.3161ms 747.324096ms 747.359483ms 747.604576ms 747.817685ms 747.847834ms 747.918053ms 748.003153ms 748.239631ms 748.242054ms 748.411172ms 748.574005ms 748.632573ms 748.805858ms 749.360468ms 749.376232ms 749.508125ms 749.526209ms 749.576505ms 750.124464ms 750.343619ms 750.515752ms 750.714823ms 751.005096ms 751.160728ms 751.184486ms 751.29367ms 751.419615ms 751.55141ms 751.655909ms 751.865172ms 751.900506ms 751.980621ms 752.168129ms 752.664783ms 752.849131ms 753.092693ms 753.168616ms 753.18097ms 753.263676ms 753.602987ms 753.619512ms 753.764955ms 753.796733ms 754.081607ms 754.126615ms 754.33503ms 754.396694ms 754.543639ms 754.663175ms 754.856494ms 755.099168ms 755.392357ms 755.599611ms 755.683597ms 756.248551ms 756.329317ms 756.682793ms 756.765924ms 756.836559ms 757.202761ms 757.253365ms 757.699366ms 757.908619ms 758.496996ms 759.191752ms 759.432607ms 759.466895ms 759.692032ms 759.90652ms 760.164166ms 760.472084ms 761.192678ms 761.775665ms 763.382033ms 764.543266ms 765.944011ms 772.484627ms 774.719631ms]
  Aug 24 12:41:41.125: INFO: 50 %ile: 744.928575ms
  Aug 24 12:41:41.125: INFO: 90 %ile: 756.836559ms
  Aug 24 12:41:41.126: INFO: 99 %ile: 772.484627ms
  Aug 24 12:41:41.126: INFO: Total sample count: 200
  Aug 24 12:41:41.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-7735" for this suite. @ 08/24/23 12:41:41.147
• [10.872 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 08/24/23 12:41:41.173
  Aug 24 12:41:41.173: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 12:41:41.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:41.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:41.25
  STEP: starting the proxy server @ 08/24/23 12:41:41.255
  Aug 24 12:41:41.255: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-8226 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 08/24/23 12:41:41.371
  Aug 24 12:41:41.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8226" for this suite. @ 08/24/23 12:41:41.405
• [0.245 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 08/24/23 12:41:41.423
  Aug 24 12:41:41.423: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 12:41:41.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:41.456
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:41.46
  STEP: Read namespace status @ 08/24/23 12:41:41.464
  Aug 24 12:41:41.474: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 08/24/23 12:41:41.476
  Aug 24 12:41:41.491: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 08/24/23 12:41:41.492
  Aug 24 12:41:41.518: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Aug 24 12:41:41.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4016" for this suite. @ 08/24/23 12:41:41.531
• [0.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 08/24/23 12:41:41.57
  Aug 24 12:41:41.570: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename events @ 08/24/23 12:41:41.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:41.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:41.626
  STEP: Create set of events @ 08/24/23 12:41:41.636
  Aug 24 12:41:41.656: INFO: created test-event-1
  Aug 24 12:41:41.670: INFO: created test-event-2
  Aug 24 12:41:41.680: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 08/24/23 12:41:41.681
  STEP: delete collection of events @ 08/24/23 12:41:41.693
  Aug 24 12:41:41.693: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/24/23 12:41:41.74
  Aug 24 12:41:41.740: INFO: requesting list of events to confirm quantity
  Aug 24 12:41:41.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2033" for this suite. @ 08/24/23 12:41:41.758
• [0.209 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 08/24/23 12:41:41.788
  Aug 24 12:41:41.788: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename job @ 08/24/23 12:41:41.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:41.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:41.849
  STEP: Creating a job @ 08/24/23 12:41:41.854
  STEP: Ensuring active pods == parallelism @ 08/24/23 12:41:41.882
  E0824 12:41:42.012000      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:43.024869      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:44.022808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:45.022964      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 08/24/23 12:41:45.891
  E0824 12:41:46.023674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:46.484: INFO: Successfully updated pod "adopt-release-2b58h"
  STEP: Checking that the Job readopts the Pod @ 08/24/23 12:41:46.484
  E0824 12:41:47.024171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:48.024411      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 08/24/23 12:41:48.565
  E0824 12:41:49.024916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:49.101: INFO: Successfully updated pod "adopt-release-2b58h"
  STEP: Checking that the Job releases the Pod @ 08/24/23 12:41:49.101
  E0824 12:41:50.025362      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:51.025309      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:41:51.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1010" for this suite. @ 08/24/23 12:41:51.156
• [9.385 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 08/24/23 12:41:51.178
  Aug 24 12:41:51.178: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:41:51.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:51.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:51.21
  STEP: Creating configMap with name projected-configmap-test-volume-map-0c3b86f9-a4d2-4dc0-bf20-a3d7ecb93d94 @ 08/24/23 12:41:51.216
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:41:51.223
  E0824 12:41:52.034690      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:53.027368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:54.027031      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:55.027281      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:41:55.277
  Aug 24 12:41:55.302: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-configmaps-83956887-fafc-46db-98f5-50c0bff0ff46 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:41:55.336
  Aug 24 12:41:55.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-184" for this suite. @ 08/24/23 12:41:55.413
• [4.249 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 08/24/23 12:41:55.43
  Aug 24 12:41:55.430: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:41:55.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:55.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:55.471
  Aug 24 12:41:55.477: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  W0824 12:41:55.480509      15 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc008615580 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0824 12:41:56.028171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:57.028775      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:41:58.029392      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0824 12:41:58.360565      15 warnings.go:70] unknown field "alpha"
  W0824 12:41:58.360624      15 warnings.go:70] unknown field "beta"
  W0824 12:41:58.360648      15 warnings.go:70] unknown field "delta"
  W0824 12:41:58.360672      15 warnings.go:70] unknown field "epsilon"
  W0824 12:41:58.360747      15 warnings.go:70] unknown field "gamma"
  Aug 24 12:41:58.907: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5082" for this suite. @ 08/24/23 12:41:58.965
• [3.552 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 08/24/23 12:41:58.982
  Aug 24 12:41:58.983: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:41:58.984
  E0824 12:41:59.029844      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:41:59.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:41:59.047
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-56835ab4-2eec-4565-8b59-6d24f17fddb9 @ 08/24/23 12:41:59.079
  STEP: Creating the pod @ 08/24/23 12:41:59.096
  E0824 12:42:00.030215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:01.030313      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-56835ab4-2eec-4565-8b59-6d24f17fddb9 @ 08/24/23 12:42:01.179
  STEP: waiting to observe update in volume @ 08/24/23 12:42:01.189
  E0824 12:42:02.030851      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:03.030786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:04.031133      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:05.031576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:06.032366      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:07.033468      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:08.033454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:09.034132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:10.034939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:11.034805      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:12.034913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:13.036150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:14.036121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:15.036393      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:16.037450      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:17.038896      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:18.039167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:19.039406      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:20.039854      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:21.041083      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:22.040711      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:23.040872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:24.042161      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:25.042813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:26.043698      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:27.044468      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:28.045317      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:29.045546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:30.045889      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:31.046806      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:32.047834      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:33.048172      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:34.048883      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:35.049534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:36.050267      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:37.050714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:38.051464      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:39.052259      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:40.052521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:41.053275      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:42.053611      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:43.054442      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:44.055268      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:45.054966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:46.055287      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:47.055786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:48.056479      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:49.059546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:50.059230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:51.059330      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:52.059729      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:53.059724      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:54.059919      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:55.060041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:56.061607      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:57.062168      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:58.062431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:42:59.063483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:00.063555      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:01.064500      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:02.064705      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:03.065596      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:04.065887      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:05.066451      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:06.067541      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:07.067625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:08.067855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:09.068223      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:10.069225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:11.070268      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:12.071203      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:13.072454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:14.072374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:15.072520      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:16.073261      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:17.074201      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:18.075928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:19.076318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:20.076470      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:21.076810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:22.077496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:23.077549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:24.078252      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:25.079216      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:26.080910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:27.079852      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:28.080216      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:29.080421      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:30.080826      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:31.081014      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:32.081361      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:33.081474      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:34.081528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:34.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5332" for this suite. @ 08/24/23 12:43:34.089
• [95.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 08/24/23 12:43:34.109
  Aug 24 12:43:34.109: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 12:43:34.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:34.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:34.145
  STEP: Creating simple DaemonSet "daemon-set" @ 08/24/23 12:43:34.184
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/24/23 12:43:34.193
  Aug 24 12:43:34.214: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:43:34.214: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  E0824 12:43:35.083931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:35.239: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:43:35.239: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  E0824 12:43:36.083930      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:36.233: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:43:36.233: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 08/24/23 12:43:36.24
  Aug 24 12:43:36.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:43:36.275: INFO: Node quohp9aeph3i-3 is running 0 daemon pod, expected 1
  E0824 12:43:37.085045      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:37.296: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:43:37.296: INFO: Node quohp9aeph3i-3 is running 0 daemon pod, expected 1
  E0824 12:43:38.084562      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:38.292: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug 24 12:43:38.292: INFO: Node quohp9aeph3i-3 is running 0 daemon pod, expected 1
  E0824 12:43:39.084958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:39.291: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Aug 24 12:43:39.292: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 12:43:39.3
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5593, will wait for the garbage collector to delete the pods @ 08/24/23 12:43:39.3
  Aug 24 12:43:39.372: INFO: Deleting DaemonSet.extensions daemon-set took: 13.224521ms
  Aug 24 12:43:39.474: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.696286ms
  E0824 12:43:40.085208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:40.888: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:43:40.888: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 12:43:40.893: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27892"},"items":null}

  Aug 24 12:43:40.915: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27892"},"items":null}

  Aug 24 12:43:40.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5593" for this suite. @ 08/24/23 12:43:40.964
• [6.880 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 08/24/23 12:43:40.997
  Aug 24 12:43:40.997: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 12:43:40.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:41.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:41.03
  STEP: Creating a test namespace @ 08/24/23 12:43:41.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:41.058
  STEP: Creating a pod in the namespace @ 08/24/23 12:43:41.064
  E0824 12:43:41.085248      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pod to have running status @ 08/24/23 12:43:41.085
  E0824 12:43:42.085405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:43.085691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 08/24/23 12:43:43.109
  STEP: Waiting for the namespace to be removed. @ 08/24/23 12:43:43.126
  E0824 12:43:44.086072      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:45.086296      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:46.086581      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:47.086694      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:48.086856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:49.088185      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:50.088219      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:51.088538      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:52.088620      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:53.088704      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:54.089708      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 08/24/23 12:43:54.135
  STEP: Verifying there are no pods in the namespace @ 08/24/23 12:43:54.161
  Aug 24 12:43:54.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8538" for this suite. @ 08/24/23 12:43:54.174
  STEP: Destroying namespace "nsdeletetest-9543" for this suite. @ 08/24/23 12:43:54.183
  Aug 24 12:43:54.188: INFO: Namespace nsdeletetest-9543 was already deleted
  STEP: Destroying namespace "nsdeletetest-5346" for this suite. @ 08/24/23 12:43:54.189
• [13.202 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 08/24/23 12:43:54.201
  Aug 24 12:43:54.201: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:43:54.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:54.226
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:54.231
  E0824 12:43:55.090469      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:56.090886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:43:56.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:43:56.279: INFO: Deleting pod "var-expansion-90f40bc0-8968-4bb2-b00b-010e3e0e4dc4" in namespace "var-expansion-3017"
  Aug 24 12:43:56.296: INFO: Wait up to 5m0s for pod "var-expansion-90f40bc0-8968-4bb2-b00b-010e3e0e4dc4" to be fully deleted
  E0824 12:43:57.091079      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:43:58.091081      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-3017" for this suite. @ 08/24/23 12:43:58.31
• [4.122 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 08/24/23 12:43:58.325
  Aug 24 12:43:58.325: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 12:43:58.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:43:58.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:43:58.36
  STEP: Creating a ForbidConcurrent cronjob @ 08/24/23 12:43:58.366
  STEP: Ensuring a job is scheduled @ 08/24/23 12:43:58.377
  E0824 12:43:59.091602      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:00.091727      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 08/24/23 12:44:00.387
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/24/23 12:44:00.392
  STEP: Ensuring no more jobs are scheduled @ 08/24/23 12:44:00.397
  E0824 12:44:01.092006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:02.092117      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:03.092413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:04.093776      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:05.093726      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:06.093943      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:07.094108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:08.094663      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:09.095609      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:10.095717      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:11.096836      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:12.097483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:13.097866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:14.097991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:15.098666      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:16.099764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:17.100583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:18.101176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:19.101449      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:20.101735      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:21.102255      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:22.102511      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:23.102678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:24.102854      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:25.103561      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:26.103948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:27.107198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:28.105035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:29.105789      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:30.107443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:31.107044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:32.107049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:33.107251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:34.107632      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:35.108605      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:36.109035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:37.109184      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:38.109323      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:39.110558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:40.110809      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:41.111725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:42.112308      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:43.112411      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:44.112744      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:45.113178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:46.113505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:47.113582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:48.114030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:49.114496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:50.114758      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:51.114976      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:52.115585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:53.115704      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:54.115892      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:55.116858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:56.117041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:57.117384      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:58.117525      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:44:59.118042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:00.118908      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:01.119582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:02.120266      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:03.120467      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:04.120536      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:05.120585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:06.120801      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:07.121027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:08.121152      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:09.121885      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:10.122719      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:11.122932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:12.123026      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:13.123165      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:14.123332      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:15.123554      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:16.124061      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:17.124628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:18.125312      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:19.126447      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:20.127712      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:21.127506      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:22.127299      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:23.127753      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:24.128754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:25.128993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:26.129687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:27.130684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:28.131280      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:29.132115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:30.133170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:31.133250      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:32.133383      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:33.133539      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:34.134622      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:35.134963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:36.134994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:37.135422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:38.135859      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:39.137807      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:40.136821      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:41.137647      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:42.137420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:43.137562      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:44.139626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:45.138885      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:46.139293      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:47.139573      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:48.140006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:49.140273      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:50.141481      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:51.141721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:52.144969      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:53.144753      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:54.145179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:55.146447      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:56.146537      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:57.146834      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:58.146627      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:45:59.146900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:00.146993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:01.147297      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:02.149254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:03.149433      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:04.149607      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:05.149754      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:06.149981      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:07.151078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:08.151856      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:09.152083      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:10.152212      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:11.152376      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:12.152643      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:13.152828      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:14.152971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:15.154010      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:16.154840      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:17.155094      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:18.155787      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:19.155894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:20.156214      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:21.156758      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:22.157080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:23.157612      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:24.158574      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:25.159601      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:26.159906      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:27.161077      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:28.161419      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:29.162444      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:30.162583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:31.162823      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:32.163002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:33.163087      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:34.164093      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:35.165118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:36.165333      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:37.165420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:38.165630      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:39.166023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:40.166150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:41.166535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:42.166729      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:43.166907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:44.168012      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:45.168171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:46.168879      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:47.169033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:48.169662      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:49.169725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:50.169887      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:51.170246      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:52.171162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:53.171909      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:54.172195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:55.172292      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:56.173161      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:57.174230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:58.174734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:46:59.176070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:00.176245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:01.176736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:02.177114      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:03.177340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:04.177851      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:05.177916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:06.178283      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:07.178849      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:08.179913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:09.179443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:10.179826      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:11.180240      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:12.180891      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:13.181724      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:14.182183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:15.182636      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:16.183059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:17.183017      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:18.183810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:19.184449      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:20.184637      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:21.185059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:22.185680      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:23.187132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:24.187596      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:25.187745      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:26.188103      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:27.188557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:28.189345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:29.189156      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:30.189549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:31.189901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:32.190813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:33.190479      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:34.190631      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:35.191232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:36.191637      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:37.191783      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:38.192046      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:39.192297      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:40.193231      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:41.193910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:42.194096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:43.194367      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:44.194506      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:45.194956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:46.195037      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:47.195264      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:48.195885      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:49.196226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:50.196532      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:51.197222      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:52.197527      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:53.202622      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:54.201689      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:55.200855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:56.201091      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:57.202861      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:58.202411      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:47:59.202485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:00.203074      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:01.203331      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:02.203482      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:03.203690      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:04.203913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:05.203956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:06.204185      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:07.204474      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:08.204572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:09.204765      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:10.205014      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:11.205591      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:12.206473      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:13.206584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:14.206829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:15.207034      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:16.207932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:17.207928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:18.208284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:19.208501      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:20.209709      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:21.209704      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:22.210699      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:23.211497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:24.211648      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:25.211853      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:26.212340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:27.212528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:28.213147      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:29.213343      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:30.213548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:31.213716      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:32.214211      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:33.214636      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:34.215599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:35.215829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:36.216281      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:37.216505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:38.217013      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:39.217221      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:40.217487      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:41.218524      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:42.218614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:43.218756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:44.219690      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:45.219847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:46.220507      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:47.220521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:48.220872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:49.221007      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:50.221245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:51.222644      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:52.222877      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:53.223266      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:54.223304      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:55.224280      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:56.224399      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:57.224688      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:58.225456      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:48:59.225624      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:00.226433      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 08/24/23 12:49:00.409
  Aug 24 12:49:00.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9316" for this suite. @ 08/24/23 12:49:00.439
• [302.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 08/24/23 12:49:00.454
  Aug 24 12:49:00.454: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename taint-single-pod @ 08/24/23 12:49:00.458
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:49:00.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:49:00.531
  Aug 24 12:49:00.537: INFO: Waiting up to 1m0s for all nodes to be ready
  E0824 12:49:01.226906      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:02.227194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:03.228263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:04.229037      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:05.229548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:06.230211      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:07.231118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:08.231335      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:09.231820      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:10.231965      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:11.232162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:12.232597      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:13.233451      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:14.234353      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:15.235294      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:16.235586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:17.235890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:18.236225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:19.236517      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:20.237213      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:21.237455      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:22.237625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:23.237763      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:24.238720      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:25.239649      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:26.239854      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:27.240110      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:28.240239      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:29.240403      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:30.240698      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:31.240788      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:32.240946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:33.241432      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:34.241752      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:35.241897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:36.242083      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:37.242544      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:38.242863      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:39.242906      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:40.243080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:41.243672      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:42.244329      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:43.244670      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:44.244831      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:45.245384      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:46.245566      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:47.246759      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:48.247150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:49.248088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:50.248141      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:51.248433      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:52.249001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:53.249121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:54.249200      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:55.249337      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:56.250237      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:57.250648      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:58.251131      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:49:59.250885      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:00.251115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:50:00.579: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 12:50:00.586: INFO: Starting informer...
  STEP: Starting pod... @ 08/24/23 12:50:00.587
  Aug 24 12:50:00.813: INFO: Pod is running on quohp9aeph3i-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/24/23 12:50:00.814
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/24/23 12:50:00.84
  STEP: Waiting short time to make sure Pod is queued for deletion @ 08/24/23 12:50:00.85
  Aug 24 12:50:00.851: INFO: Pod wasn't evicted. Proceeding
  Aug 24 12:50:00.851: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/24/23 12:50:00.878
  STEP: Waiting some time to make sure that toleration time passed. @ 08/24/23 12:50:00.887
  E0824 12:50:01.251405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:02.251614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:03.251872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:04.252044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:05.252194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:06.252377      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:07.253489      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:08.254157      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:09.254037      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:10.254665      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:11.254904      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:12.254964      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:13.255231      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:14.256247      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:15.256631      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:16.256921      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:17.257380      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:18.257624      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:19.258656      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:20.259808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:21.260661      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:22.261497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:23.261671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:24.261942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:25.262255      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:26.262395      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:27.262546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:28.262689      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:29.262945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:30.263147      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:31.263269      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:32.264208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:33.264930      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:34.265069      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:35.265218      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:36.265604      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:37.265805      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:38.265944      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:39.266741      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:40.266887      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:41.267686      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:42.267746      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:43.267874      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:44.268583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:45.268956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:46.269309      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:47.269691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:48.270226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:49.270232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:50.270798      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:51.271254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:52.271622      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:53.271855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:54.272062      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:55.272228      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:56.272392      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:57.272582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:58.272932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:50:59.273505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:00.273673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:01.273804      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:02.274780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:03.275066      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:04.275468      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:05.275204      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:06.276285      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:07.276757      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:08.277901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:09.278690      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:10.278827      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:11.279002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:12.279183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:13.280781      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:14.280750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:15.281033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:51:15.887: INFO: Pod wasn't evicted. Test successful
  Aug 24 12:51:15.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-297" for this suite. @ 08/24/23 12:51:15.904
• [135.465 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 08/24/23 12:51:15.921
  Aug 24 12:51:15.921: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:51:15.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:51:15.972
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:51:15.977
  STEP: Setting up server cert @ 08/24/23 12:51:16.052
  E0824 12:51:16.281734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:51:16.603
  STEP: Deploying the webhook pod @ 08/24/23 12:51:16.62
  STEP: Wait for the deployment to be ready @ 08/24/23 12:51:16.646
  Aug 24 12:51:16.662: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 12:51:17.283775      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:18.284565      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:51:18.684
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:51:18.714
  E0824 12:51:19.284717      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:51:19.716: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/24/23 12:51:19.869
  STEP: Creating a configMap that should be mutated @ 08/24/23 12:51:19.905
  STEP: Deleting the collection of validation webhooks @ 08/24/23 12:51:19.977
  STEP: Creating a configMap that should not be mutated @ 08/24/23 12:51:20.084
  Aug 24 12:51:20.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4854" for this suite. @ 08/24/23 12:51:20.21
  STEP: Destroying namespace "webhook-markers-2894" for this suite. @ 08/24/23 12:51:20.222
• [4.313 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 08/24/23 12:51:20.235
  Aug 24 12:51:20.236: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:51:20.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:51:20.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:51:20.281
  E0824 12:51:20.284807      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/24/23 12:51:20.287
  E0824 12:51:21.285107      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:22.285398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:23.285960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:24.286404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:51:24.326
  Aug 24 12:51:24.332: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-80ee273f-87bd-4663-a4f9-ba38ece67bce container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:51:24.362
  Aug 24 12:51:24.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5638" for this suite. @ 08/24/23 12:51:24.4
• [4.174 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 08/24/23 12:51:24.413
  Aug 24 12:51:24.413: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:51:24.414
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:51:24.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:51:24.444
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:51:24.452
  E0824 12:51:25.287639      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:26.286606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:27.286979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:28.287271      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:51:28.501
  Aug 24 12:51:28.507: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-d8d6acaf-e293-404d-a006-030fe9f88edd container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:51:28.519
  Aug 24 12:51:28.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2999" for this suite. @ 08/24/23 12:51:28.557
• [4.158 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 08/24/23 12:51:28.581
  Aug 24 12:51:28.581: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 12:51:28.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:51:28.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:51:28.617
  STEP: Create a pod @ 08/24/23 12:51:28.626
  E0824 12:51:29.287748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:30.288809      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 08/24/23 12:51:30.671
  Aug 24 12:51:30.684: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Aug 24 12:51:30.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4878" for this suite. @ 08/24/23 12:51:30.695
• [2.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 08/24/23 12:51:30.718
  Aug 24 12:51:30.719: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename security-context-test @ 08/24/23 12:51:30.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:51:30.743
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:51:30.748
  E0824 12:51:31.293986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:32.291723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:33.292136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:34.293191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:51:34.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-1792" for this suite. @ 08/24/23 12:51:34.826
• [4.117 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 08/24/23 12:51:34.84
  Aug 24 12:51:34.841: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename endpointslice @ 08/24/23 12:51:34.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:51:34.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:51:34.867
  STEP: getting /apis @ 08/24/23 12:51:34.871
  STEP: getting /apis/discovery.k8s.io @ 08/24/23 12:51:34.878
  STEP: getting /apis/discovery.k8s.iov1 @ 08/24/23 12:51:34.879
  STEP: creating @ 08/24/23 12:51:34.881
  STEP: getting @ 08/24/23 12:51:34.913
  STEP: listing @ 08/24/23 12:51:34.922
  STEP: watching @ 08/24/23 12:51:34.929
  Aug 24 12:51:34.929: INFO: starting watch
  STEP: cluster-wide listing @ 08/24/23 12:51:34.932
  STEP: cluster-wide watching @ 08/24/23 12:51:34.937
  Aug 24 12:51:34.937: INFO: starting watch
  STEP: patching @ 08/24/23 12:51:34.939
  STEP: updating @ 08/24/23 12:51:34.953
  Aug 24 12:51:34.968: INFO: waiting for watch events with expected annotations
  Aug 24 12:51:34.968: INFO: saw patched and updated annotations
  STEP: deleting @ 08/24/23 12:51:34.968
  STEP: deleting a collection @ 08/24/23 12:51:34.992
  Aug 24 12:51:35.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3565" for this suite. @ 08/24/23 12:51:35.032
• [0.203 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 08/24/23 12:51:35.051
  Aug 24 12:51:35.051: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename subpath @ 08/24/23 12:51:35.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:51:35.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:51:35.09
  STEP: Setting up data @ 08/24/23 12:51:35.095
  STEP: Creating pod pod-subpath-test-configmap-7srs @ 08/24/23 12:51:35.117
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 12:51:35.117
  E0824 12:51:35.293665      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:36.294829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:37.295765      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:38.296390      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:39.296788      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:40.296998      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:41.297591      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:42.298812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:43.298367      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:44.298991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:45.300030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:46.300571      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:47.301300      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:48.302405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:49.303268      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:50.303529      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:51.304277      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:52.304549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:53.305278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:54.306155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:55.307263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:56.308272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:57.308985      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:51:58.309690      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:51:59.274
  Aug 24 12:51:59.280: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-subpath-test-configmap-7srs container test-container-subpath-configmap-7srs: <nil>
  STEP: delete the pod @ 08/24/23 12:51:59.294
  E0824 12:51:59.309724      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pod pod-subpath-test-configmap-7srs @ 08/24/23 12:51:59.324
  Aug 24 12:51:59.324: INFO: Deleting pod "pod-subpath-test-configmap-7srs" in namespace "subpath-4261"
  Aug 24 12:51:59.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4261" for this suite. @ 08/24/23 12:51:59.345
• [24.306 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 08/24/23 12:51:59.362
  Aug 24 12:51:59.362: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 12:51:59.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:51:59.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:51:59.401
  STEP: creating an Endpoint @ 08/24/23 12:51:59.413
  STEP: waiting for available Endpoint @ 08/24/23 12:51:59.424
  STEP: listing all Endpoints @ 08/24/23 12:51:59.429
  STEP: updating the Endpoint @ 08/24/23 12:51:59.437
  STEP: fetching the Endpoint @ 08/24/23 12:51:59.449
  STEP: patching the Endpoint @ 08/24/23 12:51:59.455
  STEP: fetching the Endpoint @ 08/24/23 12:51:59.475
  STEP: deleting the Endpoint by Collection @ 08/24/23 12:51:59.482
  STEP: waiting for Endpoint deletion @ 08/24/23 12:51:59.501
  STEP: fetching the Endpoint @ 08/24/23 12:51:59.505
  Aug 24 12:51:59.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3369" for this suite. @ 08/24/23 12:51:59.52
• [0.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 08/24/23 12:51:59.536
  Aug 24 12:51:59.536: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-preemption @ 08/24/23 12:51:59.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:51:59.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:51:59.577
  Aug 24 12:51:59.610: INFO: Waiting up to 1m0s for all nodes to be ready
  E0824 12:52:00.310466      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:01.310814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:02.311563      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:03.311958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:04.312763      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:05.313994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:06.314245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:07.314641      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:08.314838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:09.315347      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:10.315751      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:11.315900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:12.316145      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:13.323279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:14.317746      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:15.318578      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:16.318701      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:17.319291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:18.319936      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:19.320113      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:20.320350      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:21.320472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:22.321500      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:23.322216      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:24.322846      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:25.323359      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:26.323632      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:27.323862      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:28.324626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:29.324756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:30.324920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:31.325772      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:32.325991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:33.329545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:34.327510      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:35.327564      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:36.328192      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:37.328650      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:38.328770      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:39.329143      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:40.329734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:41.329903      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:42.330154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:43.330851      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:44.330996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:45.332076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:46.332431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:47.332610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:48.333618      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:49.333927      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:50.334404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:51.334743      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:52.335033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:53.335812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:54.336464      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:55.336504      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:56.337465      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:57.338125      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:58.338197      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:52:59.339458      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:52:59.674: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/24/23 12:52:59.681
  Aug 24 12:52:59.719: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug 24 12:52:59.726: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug 24 12:52:59.804: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug 24 12:52:59.826: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug 24 12:52:59.900: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug 24 12:52:59.922: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/24/23 12:52:59.922
  E0824 12:53:00.339510      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:01.339988      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:02.340370      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:03.340927      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 08/24/23 12:53:03.987
  E0824 12:53:04.340991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:05.341628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:06.342381      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:07.342867      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:08.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1332" for this suite. @ 08/24/23 12:53:08.182
• [68.659 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 08/24/23 12:53:08.196
  Aug 24 12:53:08.196: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:53:08.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:08.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:08.231
  STEP: Creating configMap with name configmap-test-volume-map-08dde5a1-82ad-43ff-bb37-1875f17ca1c1 @ 08/24/23 12:53:08.235
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:53:08.243
  E0824 12:53:08.343382      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:09.343806      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:10.343705      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:11.344396      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:53:12.289
  Aug 24 12:53:12.296: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-17dbcb48-11a1-4543-8d89-a0d96d5b3302 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:53:12.31
  E0824 12:53:12.344321      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:12.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5518" for this suite. @ 08/24/23 12:53:12.356
• [4.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 08/24/23 12:53:12.378
  Aug 24 12:53:12.378: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename daemonsets @ 08/24/23 12:53:12.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:12.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:12.41
  Aug 24 12:53:12.454: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 08/24/23 12:53:12.464
  Aug 24 12:53:12.470: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:12.470: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 08/24/23 12:53:12.47
  Aug 24 12:53:12.519: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:12.519: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  E0824 12:53:13.345158      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:13.537: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:13.538: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  E0824 12:53:14.345178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:14.539: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:14.539: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  E0824 12:53:15.345805      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:15.532: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:15.532: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  E0824 12:53:16.345673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:16.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 24 12:53:16.527: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 08/24/23 12:53:16.533
  Aug 24 12:53:16.589: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 24 12:53:16.590: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0824 12:53:17.346196      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:17.606: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:17.606: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 08/24/23 12:53:17.606
  Aug 24 12:53:17.652: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:17.652: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  E0824 12:53:18.346624      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:18.664: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:18.664: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  E0824 12:53:19.347404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:19.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:19.661: INFO: Node quohp9aeph3i-1 is running 0 daemon pod, expected 1
  E0824 12:53:20.347674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:20.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug 24 12:53:20.662: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/24/23 12:53:20.677
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5885, will wait for the garbage collector to delete the pods @ 08/24/23 12:53:20.677
  Aug 24 12:53:20.747: INFO: Deleting DaemonSet.extensions daemon-set took: 13.533825ms
  Aug 24 12:53:20.848: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.67007ms
  E0824 12:53:21.347971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:21.854: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug 24 12:53:21.854: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug 24 12:53:21.860: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"29857"},"items":null}

  Aug 24 12:53:21.866: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"29857"},"items":null}

  Aug 24 12:53:21.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5885" for this suite. @ 08/24/23 12:53:21.929
• [9.571 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 08/24/23 12:53:21.95
  Aug 24 12:53:21.950: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename ingress @ 08/24/23 12:53:21.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:21.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:21.988
  STEP: getting /apis @ 08/24/23 12:53:21.997
  STEP: getting /apis/networking.k8s.io @ 08/24/23 12:53:22.006
  STEP: getting /apis/networking.k8s.iov1 @ 08/24/23 12:53:22.01
  STEP: creating @ 08/24/23 12:53:22.012
  STEP: getting @ 08/24/23 12:53:22.043
  STEP: listing @ 08/24/23 12:53:22.051
  STEP: watching @ 08/24/23 12:53:22.062
  Aug 24 12:53:22.062: INFO: starting watch
  STEP: cluster-wide listing @ 08/24/23 12:53:22.065
  STEP: cluster-wide watching @ 08/24/23 12:53:22.071
  Aug 24 12:53:22.072: INFO: starting watch
  STEP: patching @ 08/24/23 12:53:22.075
  STEP: updating @ 08/24/23 12:53:22.095
  Aug 24 12:53:22.116: INFO: waiting for watch events with expected annotations
  Aug 24 12:53:22.116: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/24/23 12:53:22.117
  STEP: updating /status @ 08/24/23 12:53:22.131
  STEP: get /status @ 08/24/23 12:53:22.151
  STEP: deleting @ 08/24/23 12:53:22.16
  STEP: deleting a collection @ 08/24/23 12:53:22.191
  Aug 24 12:53:22.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-1833" for this suite. @ 08/24/23 12:53:22.238
• [0.301 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 08/24/23 12:53:22.256
  Aug 24 12:53:22.256: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:53:22.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:22.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:22.31
  STEP: Creating a pod to test downward api env vars @ 08/24/23 12:53:22.321
  E0824 12:53:22.348905      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:23.349538      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:24.350071      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:25.350414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:26.350493      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:53:26.391
  Aug 24 12:53:26.399: INFO: Trying to get logs from node quohp9aeph3i-3 pod downward-api-6da6f5bb-c43d-4a1a-a4c2-3c2ba08fcf13 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:53:26.426
  Aug 24 12:53:26.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9566" for this suite. @ 08/24/23 12:53:26.469
• [4.228 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 08/24/23 12:53:26.487
  Aug 24 12:53:26.487: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:53:26.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:26.526
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:26.534
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:53:26.543
  E0824 12:53:27.350558      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:28.352796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:29.352876      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:30.353040      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:53:30.598
  Aug 24 12:53:30.608: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-2b9afd73-6a2d-485d-a96f-0e264eb4c54c container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:53:30.623
  Aug 24 12:53:30.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5125" for this suite. @ 08/24/23 12:53:30.663
• [4.191 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 08/24/23 12:53:30.678
  Aug 24 12:53:30.678: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 12:53:30.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:30.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:30.712
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 08/24/23 12:53:30.719
  Aug 24 12:53:30.739: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0824 12:53:31.353912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:32.354420      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:33.354960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:34.355125      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:35.355780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:35.765: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 12:53:35.765
  STEP: getting scale subresource @ 08/24/23 12:53:35.766
  STEP: updating a scale subresource @ 08/24/23 12:53:35.775
  STEP: verifying the replicaset Spec.Replicas was modified @ 08/24/23 12:53:35.798
  STEP: Patch a scale subresource @ 08/24/23 12:53:35.807
  Aug 24 12:53:35.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5894" for this suite. @ 08/24/23 12:53:35.902
• [5.296 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 08/24/23 12:53:35.978
  Aug 24 12:53:35.978: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:53:35.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:36.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:36.075
  STEP: Creating configMap with name projected-configmap-test-volume-a8152c2d-5be7-4db8-8be1-51fec9585bf5 @ 08/24/23 12:53:36.087
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:53:36.108
  E0824 12:53:36.356980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:37.357170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:38.357326      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:39.357770      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:53:40.172
  Aug 24 12:53:40.178: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-configmaps-be7f15e6-8904-4ad1-8fc0-4e306dc38391 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:53:40.193
  Aug 24 12:53:40.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4064" for this suite. @ 08/24/23 12:53:40.228
• [4.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 08/24/23 12:53:40.244
  Aug 24 12:53:40.244: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 12:53:40.246
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:40.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:40.28
  Aug 24 12:53:40.319: INFO: created pod pod-service-account-defaultsa
  Aug 24 12:53:40.319: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Aug 24 12:53:40.329: INFO: created pod pod-service-account-mountsa
  Aug 24 12:53:40.329: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Aug 24 12:53:40.341: INFO: created pod pod-service-account-nomountsa
  Aug 24 12:53:40.341: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  E0824 12:53:40.361336      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:40.366: INFO: created pod pod-service-account-defaultsa-mountspec
  Aug 24 12:53:40.367: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Aug 24 12:53:40.382: INFO: created pod pod-service-account-mountsa-mountspec
  Aug 24 12:53:40.382: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Aug 24 12:53:40.394: INFO: created pod pod-service-account-nomountsa-mountspec
  Aug 24 12:53:40.394: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Aug 24 12:53:40.406: INFO: created pod pod-service-account-defaultsa-nomountspec
  Aug 24 12:53:40.406: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Aug 24 12:53:40.450: INFO: created pod pod-service-account-mountsa-nomountspec
  Aug 24 12:53:40.450: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Aug 24 12:53:40.490: INFO: created pod pod-service-account-nomountsa-nomountspec
  Aug 24 12:53:40.490: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Aug 24 12:53:40.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-992" for this suite. @ 08/24/23 12:53:40.514
• [0.335 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 08/24/23 12:53:40.589
  Aug 24 12:53:40.589: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename discovery @ 08/24/23 12:53:40.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:40.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:40.705
  STEP: Setting up server cert @ 08/24/23 12:53:40.72
  E0824 12:53:41.361671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:42.353: INFO: Checking APIGroup: apiregistration.k8s.io
  Aug 24 12:53:42.355: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Aug 24 12:53:42.356: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Aug 24 12:53:42.356: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Aug 24 12:53:42.357: INFO: Checking APIGroup: apps
  Aug 24 12:53:42.360: INFO: PreferredVersion.GroupVersion: apps/v1
  Aug 24 12:53:42.360: INFO: Versions found [{apps/v1 v1}]
  Aug 24 12:53:42.360: INFO: apps/v1 matches apps/v1
  Aug 24 12:53:42.360: INFO: Checking APIGroup: events.k8s.io
  E0824 12:53:42.362153      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:42.362: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Aug 24 12:53:42.362: INFO: Versions found [{events.k8s.io/v1 v1}]
  Aug 24 12:53:42.363: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Aug 24 12:53:42.363: INFO: Checking APIGroup: authentication.k8s.io
  Aug 24 12:53:42.367: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Aug 24 12:53:42.367: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Aug 24 12:53:42.368: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Aug 24 12:53:42.368: INFO: Checking APIGroup: authorization.k8s.io
  Aug 24 12:53:42.371: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Aug 24 12:53:42.371: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Aug 24 12:53:42.372: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Aug 24 12:53:42.372: INFO: Checking APIGroup: autoscaling
  Aug 24 12:53:42.375: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Aug 24 12:53:42.375: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Aug 24 12:53:42.376: INFO: autoscaling/v2 matches autoscaling/v2
  Aug 24 12:53:42.377: INFO: Checking APIGroup: batch
  Aug 24 12:53:42.380: INFO: PreferredVersion.GroupVersion: batch/v1
  Aug 24 12:53:42.380: INFO: Versions found [{batch/v1 v1}]
  Aug 24 12:53:42.380: INFO: batch/v1 matches batch/v1
  Aug 24 12:53:42.380: INFO: Checking APIGroup: certificates.k8s.io
  Aug 24 12:53:42.382: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Aug 24 12:53:42.382: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Aug 24 12:53:42.382: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Aug 24 12:53:42.382: INFO: Checking APIGroup: networking.k8s.io
  Aug 24 12:53:42.384: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Aug 24 12:53:42.384: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Aug 24 12:53:42.385: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Aug 24 12:53:42.385: INFO: Checking APIGroup: policy
  Aug 24 12:53:42.388: INFO: PreferredVersion.GroupVersion: policy/v1
  Aug 24 12:53:42.389: INFO: Versions found [{policy/v1 v1}]
  Aug 24 12:53:42.389: INFO: policy/v1 matches policy/v1
  Aug 24 12:53:42.389: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Aug 24 12:53:42.391: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Aug 24 12:53:42.391: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Aug 24 12:53:42.391: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Aug 24 12:53:42.391: INFO: Checking APIGroup: storage.k8s.io
  Aug 24 12:53:42.393: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Aug 24 12:53:42.393: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Aug 24 12:53:42.394: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Aug 24 12:53:42.394: INFO: Checking APIGroup: admissionregistration.k8s.io
  Aug 24 12:53:42.397: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Aug 24 12:53:42.397: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Aug 24 12:53:42.397: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Aug 24 12:53:42.397: INFO: Checking APIGroup: apiextensions.k8s.io
  Aug 24 12:53:42.398: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Aug 24 12:53:42.398: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Aug 24 12:53:42.398: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Aug 24 12:53:42.398: INFO: Checking APIGroup: scheduling.k8s.io
  Aug 24 12:53:42.400: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Aug 24 12:53:42.401: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Aug 24 12:53:42.401: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Aug 24 12:53:42.401: INFO: Checking APIGroup: coordination.k8s.io
  Aug 24 12:53:42.403: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Aug 24 12:53:42.403: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Aug 24 12:53:42.403: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Aug 24 12:53:42.403: INFO: Checking APIGroup: node.k8s.io
  Aug 24 12:53:42.404: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Aug 24 12:53:42.404: INFO: Versions found [{node.k8s.io/v1 v1}]
  Aug 24 12:53:42.404: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Aug 24 12:53:42.404: INFO: Checking APIGroup: discovery.k8s.io
  Aug 24 12:53:42.406: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Aug 24 12:53:42.406: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Aug 24 12:53:42.406: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Aug 24 12:53:42.406: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Aug 24 12:53:42.408: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Aug 24 12:53:42.408: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Aug 24 12:53:42.408: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Aug 24 12:53:42.408: INFO: Checking APIGroup: cilium.io
  Aug 24 12:53:42.411: INFO: PreferredVersion.GroupVersion: cilium.io/v2
  Aug 24 12:53:42.411: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
  Aug 24 12:53:42.411: INFO: cilium.io/v2 matches cilium.io/v2
  Aug 24 12:53:42.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-8542" for this suite. @ 08/24/23 12:53:42.42
• [1.848 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 08/24/23 12:53:42.437
  Aug 24 12:53:42.437: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:53:42.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:42.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:42.478
  STEP: Creating configMap with name projected-configmap-test-volume-a337214d-e2cc-433d-8e98-7881f08ef252 @ 08/24/23 12:53:42.483
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:53:42.494
  E0824 12:53:43.363102      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:44.363424      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:45.363509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:46.367438      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:53:46.541
  Aug 24 12:53:46.550: INFO: Trying to get logs from node quohp9aeph3i-2 pod pod-projected-configmaps-43698dfa-3cdb-4da7-b477-6d5dfe704fd8 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:53:46.587
  Aug 24 12:53:46.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1044" for this suite. @ 08/24/23 12:53:46.621
• [4.206 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 08/24/23 12:53:46.644
  Aug 24 12:53:46.644: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename gc @ 08/24/23 12:53:46.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:46.677
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:46.683
  STEP: create the deployment @ 08/24/23 12:53:46.688
  W0824 12:53:46.701953      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/24/23 12:53:46.702
  STEP: delete the deployment @ 08/24/23 12:53:47.221
  STEP: wait for all rs to be garbage collected @ 08/24/23 12:53:47.242
  E0824 12:53:47.364733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: expected 0 pods, got 1 pods @ 08/24/23 12:53:47.48
  STEP: Gathering metrics @ 08/24/23 12:53:48.029
  Aug 24 12:53:48.245: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 12:53:48.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1907" for this suite. @ 08/24/23 12:53:48.263
• [1.649 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 08/24/23 12:53:48.307
  Aug 24 12:53:48.307: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:53:48.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:48.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:48.355
  STEP: Creating configMap with name configmap-projected-all-test-volume-d22c15c0-02d6-4ced-af70-383f3aefdc38 @ 08/24/23 12:53:48.359
  E0824 12:53:48.365565      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating secret with name secret-projected-all-test-volume-bc86a5d2-e3db-4123-af2f-61224a539a3a @ 08/24/23 12:53:48.369
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 08/24/23 12:53:48.376
  E0824 12:53:49.366606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:50.367222      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:51.367735      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:52.368834      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:53:52.422
  Aug 24 12:53:52.429: INFO: Trying to get logs from node quohp9aeph3i-1 pod projected-volume-49896e9f-71b4-48b1-ad64-6e13bfb5f545 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 12:53:52.459
  Aug 24 12:53:52.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6754" for this suite. @ 08/24/23 12:53:52.514
• [4.225 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 08/24/23 12:53:52.546
  Aug 24 12:53:52.546: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 12:53:52.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:53:52.581
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:53:52.595
  STEP: Creating pod test-grpc-8d1d7dc9-2ed5-4891-9420-475d250896fa in namespace container-probe-1182 @ 08/24/23 12:53:52.601
  E0824 12:53:53.369721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:54.370753      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:53:54.647: INFO: Started pod test-grpc-8d1d7dc9-2ed5-4891-9420-475d250896fa in namespace container-probe-1182
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 12:53:54.647
  Aug 24 12:53:54.657: INFO: Initial restart count of pod test-grpc-8d1d7dc9-2ed5-4891-9420-475d250896fa is 0
  E0824 12:53:55.370788      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:56.372385      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:57.372015      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:58.373133      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:53:59.373353      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:00.374291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:01.374306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:02.374666      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:03.374772      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:04.375050      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:05.375855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:06.376124      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:07.377059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:08.378971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:09.378213      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:10.378844      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:11.379244      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:12.379824      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:13.380294      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:14.380659      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:15.381161      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:16.382032      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:17.383082      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:18.383459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:19.384272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:20.384475      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:21.385308      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:22.385546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:23.385948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:24.386571      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:25.387490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:26.387683      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:27.387906      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:28.388495      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:29.388609      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:30.388751      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:31.389227      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:32.389192      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:33.390177      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:34.390497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:35.391289      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:36.391495      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:37.391880      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:38.392408      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:39.392534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:40.393630      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:41.394675      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:42.395450      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:43.396438      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:44.396649      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:45.396816      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:46.397052      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:47.397896      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:48.398653      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:49.399436      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:50.399701      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:51.400110      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:52.401521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:53.402406      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:54.402143      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:55.402282      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:56.402796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:57.402905      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:58.403332      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:54:59.403748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:00.404023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:00.975: INFO: Restart count of pod container-probe-1182/test-grpc-8d1d7dc9-2ed5-4891-9420-475d250896fa is now 1 (1m6.318008925s elapsed)
  Aug 24 12:55:00.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:55:00.986
  STEP: Destroying namespace "container-probe-1182" for this suite. @ 08/24/23 12:55:01.028
• [68.495 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 08/24/23 12:55:01.048
  Aug 24 12:55:01.048: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:55:01.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:01.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:01.096
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/24/23 12:55:01.101
  E0824 12:55:01.404291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:02.404430      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:03.405704      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:04.406581      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:55:05.152
  Aug 24 12:55:05.159: INFO: Trying to get logs from node quohp9aeph3i-1 pod pod-2200e9d7-eac6-4a05-98ce-d613c269b3cc container test-container: <nil>
  STEP: delete the pod @ 08/24/23 12:55:05.193
  Aug 24 12:55:05.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7881" for this suite. @ 08/24/23 12:55:05.265
• [4.230 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 08/24/23 12:55:05.287
  Aug 24 12:55:05.287: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:55:05.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:05.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:05.319
  STEP: Setting up server cert @ 08/24/23 12:55:05.353
  E0824 12:55:05.406839      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:55:06.134
  STEP: Deploying the webhook pod @ 08/24/23 12:55:06.151
  STEP: Wait for the deployment to be ready @ 08/24/23 12:55:06.171
  Aug 24 12:55:06.188: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0824 12:55:06.407198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:07.407729      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:08.219: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 12, 55, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 55, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 12, 55, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 12, 55, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 12:55:08.408023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:09.408041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:55:10.228
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:55:10.249
  E0824 12:55:10.409267      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:11.249: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 08/24/23 12:55:11.257
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/24/23 12:55:11.257
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 08/24/23 12:55:11.288
  E0824 12:55:11.410679      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 08/24/23 12:55:12.308
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/24/23 12:55:12.308
  E0824 12:55:12.411400      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 08/24/23 12:55:13.359
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/24/23 12:55:13.359
  E0824 12:55:13.412739      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:14.412718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:15.412894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:16.413650      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:17.414541      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:18.414573      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 08/24/23 12:55:18.421
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/24/23 12:55:18.422
  E0824 12:55:19.415562      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:20.415610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:21.415810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:22.415947      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:23.416645      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:23.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7574" for this suite. @ 08/24/23 12:55:23.662
  STEP: Destroying namespace "webhook-markers-1026" for this suite. @ 08/24/23 12:55:23.704
• [18.442 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 08/24/23 12:55:23.731
  Aug 24 12:55:23.731: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 12:55:23.734
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:23.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:23.776
  Aug 24 12:55:23.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1309" for this suite. @ 08/24/23 12:55:23.875
• [0.166 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 08/24/23 12:55:23.898
  Aug 24 12:55:23.898: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename limitrange @ 08/24/23 12:55:23.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:23.934
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:23.941
  STEP: Creating LimitRange "e2e-limitrange-hlrfs" in namespace "limitrange-5406" @ 08/24/23 12:55:23.949
  STEP: Creating another limitRange in another namespace @ 08/24/23 12:55:23.962
  Aug 24 12:55:24.012: INFO: Namespace "e2e-limitrange-hlrfs-8886" created
  Aug 24 12:55:24.013: INFO: Creating LimitRange "e2e-limitrange-hlrfs" in namespace "e2e-limitrange-hlrfs-8886"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-hlrfs" @ 08/24/23 12:55:24.023
  Aug 24 12:55:24.035: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-hlrfs" in "limitrange-5406" namespace @ 08/24/23 12:55:24.036
  Aug 24 12:55:24.052: INFO: LimitRange "e2e-limitrange-hlrfs" has been patched
  STEP: Delete LimitRange "e2e-limitrange-hlrfs" by Collection with labelSelector: "e2e-limitrange-hlrfs=patched" @ 08/24/23 12:55:24.053
  STEP: Confirm that the limitRange "e2e-limitrange-hlrfs" has been deleted @ 08/24/23 12:55:24.078
  Aug 24 12:55:24.078: INFO: Requesting list of LimitRange to confirm quantity
  Aug 24 12:55:24.083: INFO: Found 0 LimitRange with label "e2e-limitrange-hlrfs=patched"
  Aug 24 12:55:24.084: INFO: LimitRange "e2e-limitrange-hlrfs" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-hlrfs" @ 08/24/23 12:55:24.084
  Aug 24 12:55:24.089: INFO: Found 1 limitRange
  Aug 24 12:55:24.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5406" for this suite. @ 08/24/23 12:55:24.096
  STEP: Destroying namespace "e2e-limitrange-hlrfs-8886" for this suite. @ 08/24/23 12:55:24.108
• [0.231 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 08/24/23 12:55:24.131
  Aug 24 12:55:24.131: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename watch @ 08/24/23 12:55:24.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:24.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:24.158
  STEP: creating a watch on configmaps with a certain label @ 08/24/23 12:55:24.162
  STEP: creating a new configmap @ 08/24/23 12:55:24.164
  STEP: modifying the configmap once @ 08/24/23 12:55:24.174
  STEP: changing the label value of the configmap @ 08/24/23 12:55:24.194
  STEP: Expecting to observe a delete notification for the watched object @ 08/24/23 12:55:24.219
  Aug 24 12:55:24.219: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7003  3ae866aa-6c78-4588-a561-2daf13ebe68b 30744 0 2023-08-24 12:55:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:55:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:55:24.219: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7003  3ae866aa-6c78-4588-a561-2daf13ebe68b 30745 0 2023-08-24 12:55:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:55:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:55:24.220: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7003  3ae866aa-6c78-4588-a561-2daf13ebe68b 30746 0 2023-08-24 12:55:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:55:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 08/24/23 12:55:24.22
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 08/24/23 12:55:24.233
  E0824 12:55:24.417615      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:25.418224      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:26.419319      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:27.419791      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:28.420815      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:29.421338      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:30.422672      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:31.421945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:32.422503      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:33.423350      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 08/24/23 12:55:34.237
  STEP: modifying the configmap a third time @ 08/24/23 12:55:34.259
  STEP: deleting the configmap @ 08/24/23 12:55:34.276
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 08/24/23 12:55:34.287
  Aug 24 12:55:34.288: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7003  3ae866aa-6c78-4588-a561-2daf13ebe68b 30808 0 2023-08-24 12:55:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:55:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:55:34.288: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7003  3ae866aa-6c78-4588-a561-2daf13ebe68b 30809 0 2023-08-24 12:55:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:55:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:55:34.289: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7003  3ae866aa-6c78-4588-a561-2daf13ebe68b 30810 0 2023-08-24 12:55:24 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-24 12:55:34 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 12:55:34.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7003" for this suite. @ 08/24/23 12:55:34.298
• [10.182 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 08/24/23 12:55:34.315
  Aug 24 12:55:34.315: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 12:55:34.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:34.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:34.363
  STEP: Creating Pod @ 08/24/23 12:55:34.37
  E0824 12:55:34.424159      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:35.425091      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 08/24/23 12:55:36.403
  Aug 24 12:55:36.403: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-776 PodName:pod-sharedvolume-42a5fb6e-d69f-489e-9025-2d7b93b22239 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 12:55:36.403: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 12:55:36.405: INFO: ExecWithOptions: Clientset creation
  Aug 24 12:55:36.405: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-776/pods/pod-sharedvolume-42a5fb6e-d69f-489e-9025-2d7b93b22239/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  E0824 12:55:36.426022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:36.526: INFO: Exec stderr: ""
  Aug 24 12:55:36.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-776" for this suite. @ 08/24/23 12:55:36.538
• [2.237 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 08/24/23 12:55:36.555
  Aug 24 12:55:36.555: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename gc @ 08/24/23 12:55:36.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:36.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:36.601
  Aug 24 12:55:36.702: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"85589d28-9d27-4031-8f56-627957569830", Controller:(*bool)(0xc005223526), BlockOwnerDeletion:(*bool)(0xc005223527)}}
  Aug 24 12:55:36.726: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"f5c2efe9-1013-4b7b-96c6-38119f9f23e3", Controller:(*bool)(0xc005223786), BlockOwnerDeletion:(*bool)(0xc005223787)}}
  Aug 24 12:55:36.747: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"562dd542-98be-49c0-bce8-68c204dbfff0", Controller:(*bool)(0xc003ebab6e), BlockOwnerDeletion:(*bool)(0xc003ebab6f)}}
  E0824 12:55:37.457343      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:38.458540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:39.458841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:40.461972      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:41.461443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:41.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9572" for this suite. @ 08/24/23 12:55:41.79
• [5.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 08/24/23 12:55:41.819
  Aug 24 12:55:41.819: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:55:41.823
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:41.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:41.889
  E0824 12:55:42.461730      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:43.462539      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:43.946: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:55:43.959: INFO: Deleting pod "var-expansion-fb9c3231-905a-4395-976a-0bdef80753a2" in namespace "var-expansion-9489"
  Aug 24 12:55:43.978: INFO: Wait up to 5m0s for pod "var-expansion-fb9c3231-905a-4395-976a-0bdef80753a2" to be fully deleted
  E0824 12:55:44.463544      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:45.463947      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-9489" for this suite. @ 08/24/23 12:55:46.001
• [4.198 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 08/24/23 12:55:46.02
  Aug 24 12:55:46.020: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/24/23 12:55:46.022
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:46.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:46.053
  E0824 12:55:46.464627      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:47.465628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:48.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 08/24/23 12:55:48.128
  STEP: Cleaning up the configmap @ 08/24/23 12:55:48.142
  STEP: Cleaning up the pod @ 08/24/23 12:55:48.154
  STEP: Destroying namespace "emptydir-wrapper-5132" for this suite. @ 08/24/23 12:55:48.172
• [2.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 08/24/23 12:55:48.197
  Aug 24 12:55:48.197: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:55:48.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:48.229
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:48.235
  STEP: Setting up server cert @ 08/24/23 12:55:48.284
  E0824 12:55:48.465449      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:55:49.391
  STEP: Deploying the webhook pod @ 08/24/23 12:55:49.4
  STEP: Wait for the deployment to be ready @ 08/24/23 12:55:49.421
  Aug 24 12:55:49.444: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 12:55:49.465912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:50.465997      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:51.466238      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:55:51.48
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:55:51.507
  E0824 12:55:52.467044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:55:52.507: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 08/24/23 12:55:52.515
  STEP: create a namespace for the webhook @ 08/24/23 12:55:52.546
  STEP: create a configmap should be unconditionally rejected by the webhook @ 08/24/23 12:55:52.576
  Aug 24 12:55:52.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3432" for this suite. @ 08/24/23 12:55:52.728
  STEP: Destroying namespace "webhook-markers-5066" for this suite. @ 08/24/23 12:55:52.747
  STEP: Destroying namespace "fail-closed-namespace-6331" for this suite. @ 08/24/23 12:55:52.763
• [4.585 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 08/24/23 12:55:52.817
  Aug 24 12:55:52.817: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 12:55:52.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:52.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:52.869
  STEP: Creating a pod to test env composition @ 08/24/23 12:55:52.881
  E0824 12:55:53.466731      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:54.467697      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:55.468082      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:56.468491      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:55:56.979
  Aug 24 12:55:56.985: INFO: Trying to get logs from node quohp9aeph3i-3 pod var-expansion-4547427d-333a-4e3f-a008-f720ce98bde5 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:55:57.012
  Aug 24 12:55:57.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9184" for this suite. @ 08/24/23 12:55:57.05
• [4.248 seconds]
------------------------------
SS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 08/24/23 12:55:57.066
  Aug 24 12:55:57.066: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename runtimeclass @ 08/24/23 12:55:57.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:57.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:57.105
  STEP: Deleting RuntimeClass runtimeclass-6474-delete-me @ 08/24/23 12:55:57.116
  STEP: Waiting for the RuntimeClass to disappear @ 08/24/23 12:55:57.128
  Aug 24 12:55:57.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6474" for this suite. @ 08/24/23 12:55:57.155
• [0.115 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 08/24/23 12:55:57.182
  Aug 24 12:55:57.182: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:55:57.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:55:57.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:55:57.271
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 12:55:57.279
  E0824 12:55:57.469572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:58.470797      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:55:59.471987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:00.472993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:56:01.336
  Aug 24 12:56:01.346: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-afd7ca24-e8d9-4ae7-bc9a-ad87f722c9fb container client-container: <nil>
  STEP: delete the pod @ 08/24/23 12:56:01.36
  Aug 24 12:56:01.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7487" for this suite. @ 08/24/23 12:56:01.405
• [4.242 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 08/24/23 12:56:01.429
  Aug 24 12:56:01.430: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 12:56:01.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:01.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:01.468
  E0824 12:56:01.472847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating service in namespace services-7222 @ 08/24/23 12:56:01.477
  STEP: creating service affinity-clusterip in namespace services-7222 @ 08/24/23 12:56:01.478
  STEP: creating replication controller affinity-clusterip in namespace services-7222 @ 08/24/23 12:56:01.5
  I0824 12:56:01.517593      15 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-7222, replica count: 3
  E0824 12:56:02.473137      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:03.474761      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:04.474579      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:56:04.572360      15 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:56:04.584: INFO: Creating new exec pod
  E0824 12:56:05.475247      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:06.475557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:07.476159      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:56:07.619: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-7222 exec execpod-affinitybnbrq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Aug 24 12:56:07.967: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Aug 24 12:56:07.967: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:56:07.970: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-7222 exec execpod-affinitybnbrq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.171 80'
  Aug 24 12:56:08.297: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.171 80\nConnection to 10.233.48.171 80 port [tcp/http] succeeded!\n"
  Aug 24 12:56:08.297: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 12:56:08.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-7222 exec execpod-affinitybnbrq -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.48.171:80/ ; done'
  E0824 12:56:08.476530      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:56:08.868: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.48.171:80/\n"
  Aug 24 12:56:08.868: INFO: stdout: "\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq\naffinity-clusterip-jkvnq"
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.868: INFO: Received response from host: affinity-clusterip-jkvnq
  Aug 24 12:56:08.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 12:56:08.879: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-7222, will wait for the garbage collector to delete the pods @ 08/24/23 12:56:08.95
  Aug 24 12:56:09.041: INFO: Deleting ReplicationController affinity-clusterip took: 17.722948ms
  Aug 24 12:56:09.141: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.733636ms
  E0824 12:56:09.477245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:10.477794      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:11.478693      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7222" for this suite. @ 08/24/23 12:56:11.684
• [10.275 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 08/24/23 12:56:11.706
  Aug 24 12:56:11.706: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename gc @ 08/24/23 12:56:11.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:11.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:11.754
  STEP: create the rc @ 08/24/23 12:56:11.761
  W0824 12:56:11.775794      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0824 12:56:12.479757      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:13.480233      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:14.480478      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:15.480722      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:16.480950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/24/23 12:56:16.787
  STEP: wait for all pods to be garbage collected @ 08/24/23 12:56:16.804
  E0824 12:56:17.481404      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:18.481953      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:19.482395      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:20.482658      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:21.482664      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/24/23 12:56:21.825
  Aug 24 12:56:22.031: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 12:56:22.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7786" for this suite. @ 08/24/23 12:56:22.044
• [10.350 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 08/24/23 12:56:22.061
  Aug 24 12:56:22.061: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename disruption @ 08/24/23 12:56:22.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:22.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:22.104
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:56:22.12
  E0824 12:56:22.483534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:23.484703      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 08/24/23 12:56:24.136
  STEP: Waiting for all pods to be running @ 08/24/23 12:56:24.16
  Aug 24 12:56:24.172: INFO: running pods: 0 < 1
  E0824 12:56:24.485386      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:25.485366      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 08/24/23 12:56:26.185
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:56:26.214
  STEP: Patching PodDisruptionBudget status @ 08/24/23 12:56:26.241
  STEP: Waiting for the pdb to be processed @ 08/24/23 12:56:26.263
  Aug 24 12:56:26.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9428" for this suite. @ 08/24/23 12:56:26.28
• [4.233 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 08/24/23 12:56:26.297
  Aug 24 12:56:26.297: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:56:26.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:26.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:26.34
  Aug 24 12:56:26.346: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 12:56:26.485785      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:27.486203      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:28.487030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:29.487998      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:56:29.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9224" for this suite. @ 08/24/23 12:56:29.759
• [3.480 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 08/24/23 12:56:29.782
  Aug 24 12:56:29.782: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename subjectreview @ 08/24/23 12:56:29.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:29.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:29.831
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-9326" @ 08/24/23 12:56:29.838
  Aug 24 12:56:29.851: INFO: saUsername: "system:serviceaccount:subjectreview-9326:e2e"
  Aug 24 12:56:29.851: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-9326"}
  Aug 24 12:56:29.851: INFO: saUID: "ae3b7011-9bc3-4cc1-a56e-9fe855b76367"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-9326:e2e" @ 08/24/23 12:56:29.852
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-9326:e2e" @ 08/24/23 12:56:29.852
  Aug 24 12:56:29.856: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-9326:e2e" api 'list' configmaps in "subjectreview-9326" namespace @ 08/24/23 12:56:29.856
  Aug 24 12:56:29.859: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-9326:e2e" @ 08/24/23 12:56:29.86
  Aug 24 12:56:29.863: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Aug 24 12:56:29.864: INFO: LocalSubjectAccessReview has been verified
  Aug 24 12:56:29.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-9326" for this suite. @ 08/24/23 12:56:29.875
• [0.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 08/24/23 12:56:29.9
  Aug 24 12:56:29.901: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 12:56:29.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:29.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:29.943
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-4202 @ 08/24/23 12:56:29.948
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/24/23 12:56:29.975
  STEP: creating service externalsvc in namespace services-4202 @ 08/24/23 12:56:29.976
  STEP: creating replication controller externalsvc in namespace services-4202 @ 08/24/23 12:56:30.012
  I0824 12:56:30.024755      15 runners.go:194] Created replication controller with name: externalsvc, namespace: services-4202, replica count: 2
  E0824 12:56:30.489861      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:31.489872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:32.490693      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:56:33.075841      15 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 08/24/23 12:56:33.086
  Aug 24 12:56:33.121: INFO: Creating new exec pod
  E0824 12:56:33.491122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:34.491422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:56:35.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4202 exec execpodsfdtw -- /bin/sh -x -c nslookup nodeport-service.services-4202.svc.cluster.local'
  E0824 12:56:35.491414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:56:35.574: INFO: stderr: "+ nslookup nodeport-service.services-4202.svc.cluster.local\n"
  Aug 24 12:56:35.574: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-4202.svc.cluster.local\tcanonical name = externalsvc.services-4202.svc.cluster.local.\nName:\texternalsvc.services-4202.svc.cluster.local\nAddress: 10.233.46.20\n\n"
  Aug 24 12:56:35.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-4202, will wait for the garbage collector to delete the pods @ 08/24/23 12:56:35.584
  Aug 24 12:56:35.653: INFO: Deleting ReplicationController externalsvc took: 11.204319ms
  Aug 24 12:56:35.755: INFO: Terminating ReplicationController externalsvc pods took: 101.63311ms
  E0824 12:56:36.491948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:37.492660      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:56:38.001: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-4202" for this suite. @ 08/24/23 12:56:38.035
• [8.153 seconds]
------------------------------
SS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 08/24/23 12:56:38.054
  Aug 24 12:56:38.054: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename init-container @ 08/24/23 12:56:38.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:38.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:38.1
  STEP: creating the pod @ 08/24/23 12:56:38.106
  Aug 24 12:56:38.107: INFO: PodSpec: initContainers in spec.initContainers
  E0824 12:56:38.493736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:39.497108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:40.495075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:41.495133      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:56:42.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4895" for this suite. @ 08/24/23 12:56:42.027
• [3.988 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 08/24/23 12:56:42.05
  Aug 24 12:56:42.050: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 12:56:42.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:42.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:42.098
  STEP: Creating a pod to test downward api env vars @ 08/24/23 12:56:42.102
  E0824 12:56:42.495929      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:43.496779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:44.497886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:45.498171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:56:46.185
  Aug 24 12:56:46.191: INFO: Trying to get logs from node quohp9aeph3i-3 pod downward-api-96fca1f8-c155-4e25-9de1-5dd9bdc1c9e5 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 12:56:46.212
  Aug 24 12:56:46.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5415" for this suite. @ 08/24/23 12:56:46.244
• [4.204 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 08/24/23 12:56:46.255
  Aug 24 12:56:46.255: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename podtemplate @ 08/24/23 12:56:46.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:46.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:46.289
  STEP: Create a pod template @ 08/24/23 12:56:46.292
  STEP: Replace a pod template @ 08/24/23 12:56:46.301
  Aug 24 12:56:46.313: INFO: Found updated podtemplate annotation: "true"

  Aug 24 12:56:46.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1683" for this suite. @ 08/24/23 12:56:46.321
• [0.078 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 08/24/23 12:56:46.336
  Aug 24 12:56:46.336: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 12:56:46.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:46.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:46.368
  STEP: Setting up server cert @ 08/24/23 12:56:46.401
  E0824 12:56:46.498888      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:47.499691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 12:56:48.494
  E0824 12:56:48.500234      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook pod @ 08/24/23 12:56:48.503
  STEP: Wait for the deployment to be ready @ 08/24/23 12:56:48.522
  Aug 24 12:56:48.543: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 12:56:49.501638      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:50.501952      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 12:56:50.563
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 12:56:50.587
  E0824 12:56:51.501970      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:56:51.588: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 12:56:51.595: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 08/24/23 12:56:52.124
  STEP: Creating a custom resource that should be denied by the webhook @ 08/24/23 12:56:52.165
  E0824 12:56:52.502796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:53.503385      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 08/24/23 12:56:54.263
  STEP: Updating the custom resource with disallowed data should be denied @ 08/24/23 12:56:54.276
  STEP: Deleting the custom resource should be denied @ 08/24/23 12:56:54.294
  STEP: Remove the offending key and value from the custom resource data @ 08/24/23 12:56:54.309
  STEP: Deleting the updated custom resource should be successful @ 08/24/23 12:56:54.325
  Aug 24 12:56:54.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 12:56:54.504726      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-1481" for this suite. @ 08/24/23 12:56:55.037
  STEP: Destroying namespace "webhook-markers-6141" for this suite. @ 08/24/23 12:56:55.064
• [8.748 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 08/24/23 12:56:55.085
  Aug 24 12:56:55.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 12:56:55.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:56:55.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:56:55.162
  STEP: Create a Replicaset @ 08/24/23 12:56:55.172
  STEP: Verify that the required pods have come up. @ 08/24/23 12:56:55.181
  Aug 24 12:56:55.190: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0824 12:56:55.505469      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:56.506144      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:57.507038      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:58.508076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:56:59.507935      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:57:00.197: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 12:57:00.198
  STEP: Getting /status @ 08/24/23 12:57:00.199
  Aug 24 12:57:00.208: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 08/24/23 12:57:00.208
  Aug 24 12:57:00.234: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 08/24/23 12:57:00.235
  Aug 24 12:57:00.241: INFO: Observed &ReplicaSet event: ADDED
  Aug 24 12:57:00.243: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 12:57:00.245: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 12:57:00.246: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 12:57:00.246: INFO: Found replicaset test-rs in namespace replicaset-9350 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 24 12:57:00.247: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 08/24/23 12:57:00.247
  Aug 24 12:57:00.248: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug 24 12:57:00.260: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 08/24/23 12:57:00.26
  Aug 24 12:57:00.265: INFO: Observed &ReplicaSet event: ADDED
  Aug 24 12:57:00.265: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 12:57:00.266: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 12:57:00.269: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 12:57:00.269: INFO: Observed replicaset test-rs in namespace replicaset-9350 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug 24 12:57:00.270: INFO: Observed &ReplicaSet event: MODIFIED
  Aug 24 12:57:00.270: INFO: Found replicaset test-rs in namespace replicaset-9350 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Aug 24 12:57:00.271: INFO: Replicaset test-rs has a patched status
  Aug 24 12:57:00.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9350" for this suite. @ 08/24/23 12:57:00.286
• [5.218 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 08/24/23 12:57:00.308
  Aug 24 12:57:00.309: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 12:57:00.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:00.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:00.346
  STEP: Creating a cronjob @ 08/24/23 12:57:00.356
  STEP: creating @ 08/24/23 12:57:00.356
  STEP: getting @ 08/24/23 12:57:00.369
  STEP: listing @ 08/24/23 12:57:00.374
  STEP: watching @ 08/24/23 12:57:00.38
  Aug 24 12:57:00.380: INFO: starting watch
  STEP: cluster-wide listing @ 08/24/23 12:57:00.384
  STEP: cluster-wide watching @ 08/24/23 12:57:00.39
  Aug 24 12:57:00.391: INFO: starting watch
  STEP: patching @ 08/24/23 12:57:00.393
  STEP: updating @ 08/24/23 12:57:00.404
  Aug 24 12:57:00.428: INFO: waiting for watch events with expected annotations
  Aug 24 12:57:00.428: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/24/23 12:57:00.429
  STEP: updating /status @ 08/24/23 12:57:00.447
  STEP: get /status @ 08/24/23 12:57:00.461
  STEP: deleting @ 08/24/23 12:57:00.475
  E0824 12:57:00.508270      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting a collection @ 08/24/23 12:57:00.51
  Aug 24 12:57:00.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6168" for this suite. @ 08/24/23 12:57:00.553
• [0.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 08/24/23 12:57:00.584
  Aug 24 12:57:00.584: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/24/23 12:57:00.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:00.626
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:00.633
  STEP: create the container to handle the HTTPGet hook request. @ 08/24/23 12:57:00.66
  E0824 12:57:01.508490      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:02.509215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:03.510398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:04.511228      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/24/23 12:57:04.713
  E0824 12:57:05.511684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:06.512000      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 08/24/23 12:57:06.748
  E0824 12:57:07.512176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:08.512781      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 08/24/23 12:57:08.773
  Aug 24 12:57:08.787: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3755" for this suite. @ 08/24/23 12:57:08.803
• [8.239 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 08/24/23 12:57:08.825
  Aug 24 12:57:08.826: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 12:57:08.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:08.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:08.879
  STEP: Creating configMap with name projected-configmap-test-volume-map-f3555bc2-a96e-42b5-8eee-83fdf3a5973e @ 08/24/23 12:57:08.883
  STEP: Creating a pod to test consume configMaps @ 08/24/23 12:57:08.893
  E0824 12:57:09.513176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:10.513351      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:11.513892      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:12.513687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:57:12.933
  Aug 24 12:57:12.940: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-configmaps-4946a2c0-94f4-4a3f-abd9-dc3ff40dfafd container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 12:57:12.957
  Aug 24 12:57:12.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3158" for this suite. @ 08/24/23 12:57:13.011
• [4.204 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 08/24/23 12:57:13.03
  Aug 24 12:57:13.030: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 12:57:13.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:13.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:13.085
  Aug 24 12:57:13.095: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 08/24/23 12:57:13.12
  STEP: Checking rc "condition-test" has the desired failure condition set @ 08/24/23 12:57:13.145
  E0824 12:57:13.514766      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 08/24/23 12:57:14.182
  Aug 24 12:57:14.237: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 08/24/23 12:57:14.238
  Aug 24 12:57:14.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5702" for this suite. @ 08/24/23 12:57:14.265
• [1.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 08/24/23 12:57:14.297
  Aug 24 12:57:14.297: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename events @ 08/24/23 12:57:14.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:14.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:14.372
  STEP: creating a test event @ 08/24/23 12:57:14.38
  STEP: listing events in all namespaces @ 08/24/23 12:57:14.396
  STEP: listing events in test namespace @ 08/24/23 12:57:14.42
  STEP: listing events with field selection filtering on source @ 08/24/23 12:57:14.428
  STEP: listing events with field selection filtering on reportingController @ 08/24/23 12:57:14.436
  STEP: getting the test event @ 08/24/23 12:57:14.442
  STEP: patching the test event @ 08/24/23 12:57:14.489
  STEP: getting the test event @ 08/24/23 12:57:14.514
  E0824 12:57:14.514882      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the test event @ 08/24/23 12:57:14.531
  STEP: getting the test event @ 08/24/23 12:57:14.543
  STEP: deleting the test event @ 08/24/23 12:57:14.579
  STEP: listing events in all namespaces @ 08/24/23 12:57:14.61
  STEP: listing events in test namespace @ 08/24/23 12:57:14.621
  Aug 24 12:57:14.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7759" for this suite. @ 08/24/23 12:57:14.672
• [0.395 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 08/24/23 12:57:14.696
  Aug 24 12:57:14.696: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename subpath @ 08/24/23 12:57:14.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:14.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:14.806
  STEP: Setting up data @ 08/24/23 12:57:14.818
  STEP: Creating pod pod-subpath-test-secret-sjtq @ 08/24/23 12:57:14.852
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 12:57:14.852
  E0824 12:57:15.515689      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:16.515731      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:17.515744      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:18.516124      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:19.517136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:20.517190      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:21.517252      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:22.517417      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:23.518220      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:24.519504      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:25.519993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:26.520560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:27.520724      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:28.520811      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:29.521401      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:30.522768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:31.522815      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:32.523208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:33.524258      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:34.524621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:35.525232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:36.525604      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:37.526221      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:38.526904      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 12:57:39.019
  Aug 24 12:57:39.024: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-subpath-test-secret-sjtq container test-container-subpath-secret-sjtq: <nil>
  STEP: delete the pod @ 08/24/23 12:57:39.045
  STEP: Deleting pod pod-subpath-test-secret-sjtq @ 08/24/23 12:57:39.075
  Aug 24 12:57:39.075: INFO: Deleting pod "pod-subpath-test-secret-sjtq" in namespace "subpath-503"
  Aug 24 12:57:39.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-503" for this suite. @ 08/24/23 12:57:39.09
• [24.410 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 08/24/23 12:57:39.107
  Aug 24 12:57:39.107: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename dns @ 08/24/23 12:57:39.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:57:39.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:57:39.145
  STEP: Creating a test headless service @ 08/24/23 12:57:39.15
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-694.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-694.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 39.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.39_udp@PTR;check="$$(dig +tcp +noall +answer +search 39.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.39_tcp@PTR;sleep 1; done
   @ 08/24/23 12:57:39.187
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-694.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-694.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-694.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-694.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-694.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 39.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.39_udp@PTR;check="$$(dig +tcp +noall +answer +search 39.26.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.26.39_tcp@PTR;sleep 1; done
   @ 08/24/23 12:57:39.187
  STEP: creating a pod to probe DNS @ 08/24/23 12:57:39.187
  STEP: submitting the pod to kubernetes @ 08/24/23 12:57:39.187
  E0824 12:57:39.528084      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:40.531002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:41.531852      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:42.532134      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 12:57:43.257
  STEP: looking for the results for each expected name from probers @ 08/24/23 12:57:43.263
  Aug 24 12:57:43.274: INFO: Unable to read wheezy_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:43.282: INFO: Unable to read wheezy_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:43.289: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:43.296: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:43.334: INFO: Unable to read jessie_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:43.343: INFO: Unable to read jessie_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:43.349: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:43.357: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:43.383: INFO: Lookups using dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048 failed for: [wheezy_udp@dns-test-service.dns-694.svc.cluster.local wheezy_tcp@dns-test-service.dns-694.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_udp@dns-test-service.dns-694.svc.cluster.local jessie_tcp@dns-test-service.dns-694.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local]

  E0824 12:57:43.532544      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:44.532760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:45.532932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:46.533456      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:47.533457      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:57:48.395: INFO: Unable to read wheezy_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:48.403: INFO: Unable to read wheezy_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:48.411: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:48.422: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:48.467: INFO: Unable to read jessie_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:48.477: INFO: Unable to read jessie_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:48.483: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:48.490: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:48.528: INFO: Lookups using dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048 failed for: [wheezy_udp@dns-test-service.dns-694.svc.cluster.local wheezy_tcp@dns-test-service.dns-694.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_udp@dns-test-service.dns-694.svc.cluster.local jessie_tcp@dns-test-service.dns-694.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local]

  E0824 12:57:48.534214      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:49.534699      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:50.535102      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:51.538626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:52.535814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:57:53.401: INFO: Unable to read wheezy_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:53.412: INFO: Unable to read wheezy_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:53.421: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:53.429: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:53.489: INFO: Unable to read jessie_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:53.496: INFO: Unable to read jessie_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:53.506: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:53.515: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  E0824 12:57:53.535677      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:57:53.550: INFO: Lookups using dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048 failed for: [wheezy_udp@dns-test-service.dns-694.svc.cluster.local wheezy_tcp@dns-test-service.dns-694.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_udp@dns-test-service.dns-694.svc.cluster.local jessie_tcp@dns-test-service.dns-694.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local]

  E0824 12:57:54.535929      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:55.536984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:56.536389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:57.536626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:57:58.395: INFO: Unable to read wheezy_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:58.401: INFO: Unable to read wheezy_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:58.407: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:58.412: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:58.439: INFO: Unable to read jessie_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:58.445: INFO: Unable to read jessie_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:58.450: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:58.457: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:57:58.481: INFO: Lookups using dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048 failed for: [wheezy_udp@dns-test-service.dns-694.svc.cluster.local wheezy_tcp@dns-test-service.dns-694.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_udp@dns-test-service.dns-694.svc.cluster.local jessie_tcp@dns-test-service.dns-694.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local]

  E0824 12:57:58.537685      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:57:59.537999      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:00.538284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:01.538626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:02.538731      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:58:03.394: INFO: Unable to read wheezy_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:03.402: INFO: Unable to read wheezy_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:03.407: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:03.413: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:03.441: INFO: Unable to read jessie_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:03.446: INFO: Unable to read jessie_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:03.451: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:03.456: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:03.481: INFO: Lookups using dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048 failed for: [wheezy_udp@dns-test-service.dns-694.svc.cluster.local wheezy_tcp@dns-test-service.dns-694.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_udp@dns-test-service.dns-694.svc.cluster.local jessie_tcp@dns-test-service.dns-694.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local]

  E0824 12:58:03.539609      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:04.541808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:05.541155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:06.541392      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:07.542245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:58:08.394: INFO: Unable to read wheezy_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:08.401: INFO: Unable to read wheezy_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:08.407: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:08.413: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:08.454: INFO: Unable to read jessie_udp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:08.465: INFO: Unable to read jessie_tcp@dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:08.475: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:08.483: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local from pod dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048: the server could not find the requested resource (get pods dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048)
  Aug 24 12:58:08.511: INFO: Lookups using dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048 failed for: [wheezy_udp@dns-test-service.dns-694.svc.cluster.local wheezy_tcp@dns-test-service.dns-694.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_udp@dns-test-service.dns-694.svc.cluster.local jessie_tcp@dns-test-service.dns-694.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-694.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-694.svc.cluster.local]

  E0824 12:58:08.542979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:09.543395      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:10.543417      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:11.543677      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:12.544249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:58:13.501: INFO: DNS probes using dns-694/dns-test-86c7bd81-3ca7-4258-8c95-4d7ced5ea048 succeeded

  Aug 24 12:58:13.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 12:58:13.512
  STEP: deleting the test service @ 08/24/23 12:58:13.544
  E0824 12:58:13.550845      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the test headless service @ 08/24/23 12:58:13.649
  STEP: Destroying namespace "dns-694" for this suite. @ 08/24/23 12:58:13.672
• [34.582 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 08/24/23 12:58:13.692
  Aug 24 12:58:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename field-validation @ 08/24/23 12:58:13.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:58:13.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:58:13.737
  Aug 24 12:58:13.753: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 12:58:14.551816      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:15.551760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:16.551905      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0824 12:58:16.610066      15 warnings.go:70] unknown field "alpha"
  W0824 12:58:16.610114      15 warnings.go:70] unknown field "beta"
  W0824 12:58:16.610123      15 warnings.go:70] unknown field "delta"
  W0824 12:58:16.610132      15 warnings.go:70] unknown field "epsilon"
  W0824 12:58:16.610140      15 warnings.go:70] unknown field "gamma"
  Aug 24 12:58:17.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-275" for this suite. @ 08/24/23 12:58:17.233
• [3.556 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 08/24/23 12:58:17.253
  Aug 24 12:58:17.253: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replicaset @ 08/24/23 12:58:17.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:58:17.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:58:17.288
  Aug 24 12:58:17.294: INFO: Creating ReplicaSet my-hostname-basic-25c574ab-2f52-4b9f-b124-e7573b6dd8a1
  Aug 24 12:58:17.313: INFO: Pod name my-hostname-basic-25c574ab-2f52-4b9f-b124-e7573b6dd8a1: Found 0 pods out of 1
  E0824 12:58:17.552676      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:18.553318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:19.554058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:20.553722      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:21.553925      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:58:22.321: INFO: Pod name my-hostname-basic-25c574ab-2f52-4b9f-b124-e7573b6dd8a1: Found 1 pods out of 1
  Aug 24 12:58:22.321: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-25c574ab-2f52-4b9f-b124-e7573b6dd8a1" is running
  Aug 24 12:58:22.329: INFO: Pod "my-hostname-basic-25c574ab-2f52-4b9f-b124-e7573b6dd8a1-xx27x" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:58:17 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:58:18 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:58:18 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-24 12:58:17 +0000 UTC Reason: Message:}])
  Aug 24 12:58:22.329: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/24/23 12:58:22.329
  Aug 24 12:58:22.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1330" for this suite. @ 08/24/23 12:58:22.368
• [5.129 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 08/24/23 12:58:22.383
  Aug 24 12:58:22.383: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-runtime @ 08/24/23 12:58:22.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:58:22.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:58:22.425
  STEP: create the container @ 08/24/23 12:58:22.43
  W0824 12:58:22.446973      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/24/23 12:58:22.447
  E0824 12:58:22.554076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:23.554946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:24.555908      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:25.556701      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/24/23 12:58:26.493
  STEP: the container should be terminated @ 08/24/23 12:58:26.499
  STEP: the termination message should be set @ 08/24/23 12:58:26.499
  Aug 24 12:58:26.499: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/24/23 12:58:26.5
  Aug 24 12:58:26.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6470" for this suite. @ 08/24/23 12:58:26.545
  E0824 12:58:26.558463      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 08/24/23 12:58:26.567
  Aug 24 12:58:26.567: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 12:58:26.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:58:26.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:58:26.605
  E0824 12:58:27.558658      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:28.560060      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:29.560209      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:30.560639      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:58:30.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1269" for this suite. @ 08/24/23 12:58:30.655
• [4.104 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 08/24/23 12:58:30.673
  Aug 24 12:58:30.673: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename proxy @ 08/24/23 12:58:30.677
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:58:30.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:58:30.709
  STEP: starting an echo server on multiple ports @ 08/24/23 12:58:30.746
  STEP: creating replication controller proxy-service-5nn74 in namespace proxy-6819 @ 08/24/23 12:58:30.747
  I0824 12:58:30.765856      15 runners.go:194] Created replication controller with name: proxy-service-5nn74, namespace: proxy-6819, replica count: 1
  E0824 12:58:31.561519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:58:31.818465      15 runners.go:194] proxy-service-5nn74 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 12:58:32.562581      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 12:58:32.819554      15 runners.go:194] proxy-service-5nn74 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 12:58:32.827: INFO: setup took 2.112248831s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 08/24/23 12:58:32.827
  Aug 24 12:58:32.854: INFO: (0) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 26.321651ms)
  Aug 24 12:58:32.854: INFO: (0) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 26.101314ms)
  Aug 24 12:58:32.858: INFO: (0) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 29.510443ms)
  Aug 24 12:58:32.858: INFO: (0) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 30.144938ms)
  Aug 24 12:58:32.861: INFO: (0) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 32.013023ms)
  Aug 24 12:58:32.866: INFO: (0) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 38.958506ms)
  Aug 24 12:58:32.872: INFO: (0) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 44.591889ms)
  Aug 24 12:58:32.873: INFO: (0) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 44.609004ms)
  Aug 24 12:58:32.873: INFO: (0) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 45.0643ms)
  Aug 24 12:58:32.873: INFO: (0) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 44.251561ms)
  Aug 24 12:58:32.873: INFO: (0) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 45.610673ms)
  Aug 24 12:58:32.873: INFO: (0) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 44.642969ms)
  Aug 24 12:58:32.873: INFO: (0) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 45.21093ms)
  Aug 24 12:58:32.873: INFO: (0) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 45.183279ms)
  Aug 24 12:58:32.873: INFO: (0) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 45.747331ms)
  Aug 24 12:58:32.873: INFO: (0) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 44.930974ms)
  Aug 24 12:58:32.884: INFO: (1) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 9.766146ms)
  Aug 24 12:58:32.893: INFO: (1) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 18.486142ms)
  Aug 24 12:58:32.898: INFO: (1) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 23.324754ms)
  Aug 24 12:58:32.899: INFO: (1) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 23.480123ms)
  Aug 24 12:58:32.900: INFO: (1) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 24.834429ms)
  Aug 24 12:58:32.900: INFO: (1) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 25.190801ms)
  Aug 24 12:58:32.903: INFO: (1) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 27.325415ms)
  Aug 24 12:58:32.904: INFO: (1) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 29.344343ms)
  Aug 24 12:58:32.904: INFO: (1) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 29.075095ms)
  Aug 24 12:58:32.918: INFO: (1) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 43.35103ms)
  Aug 24 12:58:32.919: INFO: (1) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 43.614259ms)
  Aug 24 12:58:32.919: INFO: (1) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 43.328536ms)
  Aug 24 12:58:32.919: INFO: (1) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 44.696554ms)
  Aug 24 12:58:32.922: INFO: (1) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 47.541777ms)
  Aug 24 12:58:32.923: INFO: (1) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 47.922758ms)
  Aug 24 12:58:32.924: INFO: (1) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 48.99898ms)
  Aug 24 12:58:32.932: INFO: (2) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 8.004393ms)
  Aug 24 12:58:32.937: INFO: (2) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 12.470937ms)
  Aug 24 12:58:32.937: INFO: (2) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 11.834352ms)
  Aug 24 12:58:32.941: INFO: (2) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 14.369194ms)
  Aug 24 12:58:32.942: INFO: (2) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 16.870064ms)
  Aug 24 12:58:32.942: INFO: (2) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 17.626739ms)
  Aug 24 12:58:32.943: INFO: (2) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 17.508917ms)
  Aug 24 12:58:32.944: INFO: (2) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 17.886861ms)
  Aug 24 12:58:32.944: INFO: (2) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 16.947412ms)
  Aug 24 12:58:32.947: INFO: (2) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 20.934005ms)
  Aug 24 12:58:32.948: INFO: (2) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 22.660831ms)
  Aug 24 12:58:32.951: INFO: (2) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 24.03944ms)
  Aug 24 12:58:32.951: INFO: (2) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 25.08857ms)
  Aug 24 12:58:32.952: INFO: (2) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 25.790792ms)
  Aug 24 12:58:32.953: INFO: (2) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 26.041822ms)
  Aug 24 12:58:32.953: INFO: (2) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 26.561825ms)
  Aug 24 12:58:32.965: INFO: (3) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 11.413374ms)
  Aug 24 12:58:32.966: INFO: (3) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 11.581335ms)
  Aug 24 12:58:32.968: INFO: (3) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 14.00009ms)
  Aug 24 12:58:32.972: INFO: (3) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 17.154051ms)
  Aug 24 12:58:32.972: INFO: (3) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 16.17408ms)
  Aug 24 12:58:32.972: INFO: (3) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 17.26914ms)
  Aug 24 12:58:32.972: INFO: (3) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 16.857398ms)
  Aug 24 12:58:32.976: INFO: (3) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 20.668959ms)
  Aug 24 12:58:32.977: INFO: (3) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 23.224225ms)
  Aug 24 12:58:32.978: INFO: (3) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 22.507865ms)
  Aug 24 12:58:32.979: INFO: (3) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 24.439888ms)
  Aug 24 12:58:32.979: INFO: (3) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 24.939547ms)
  Aug 24 12:58:32.979: INFO: (3) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 23.233816ms)
  Aug 24 12:58:32.980: INFO: (3) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 24.624604ms)
  Aug 24 12:58:32.980: INFO: (3) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 24.894312ms)
  Aug 24 12:58:32.985: INFO: (3) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 30.801143ms)
  Aug 24 12:58:32.998: INFO: (4) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 12.956339ms)
  Aug 24 12:58:33.002: INFO: (4) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 16.950826ms)
  Aug 24 12:58:33.007: INFO: (4) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 20.879335ms)
  Aug 24 12:58:33.007: INFO: (4) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 21.009918ms)
  Aug 24 12:58:33.010: INFO: (4) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 24.74216ms)
  Aug 24 12:58:33.010: INFO: (4) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 23.736255ms)
  Aug 24 12:58:33.011: INFO: (4) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 25.015782ms)
  Aug 24 12:58:33.011: INFO: (4) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 24.462736ms)
  Aug 24 12:58:33.011: INFO: (4) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 24.617682ms)
  Aug 24 12:58:33.011: INFO: (4) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 24.428755ms)
  Aug 24 12:58:33.011: INFO: (4) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 25.08752ms)
  Aug 24 12:58:33.011: INFO: (4) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 26.06049ms)
  Aug 24 12:58:33.013: INFO: (4) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 26.73961ms)
  Aug 24 12:58:33.015: INFO: (4) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 28.933298ms)
  Aug 24 12:58:33.017: INFO: (4) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 30.929182ms)
  Aug 24 12:58:33.018: INFO: (4) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 31.585211ms)
  Aug 24 12:58:33.030: INFO: (5) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 11.904472ms)
  Aug 24 12:58:33.038: INFO: (5) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 18.921958ms)
  Aug 24 12:58:33.043: INFO: (5) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 23.309298ms)
  Aug 24 12:58:33.043: INFO: (5) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 23.037523ms)
  Aug 24 12:58:33.043: INFO: (5) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 24.722964ms)
  Aug 24 12:58:33.043: INFO: (5) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 24.219039ms)
  Aug 24 12:58:33.043: INFO: (5) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 24.532603ms)
  Aug 24 12:58:33.044: INFO: (5) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 24.277641ms)
  Aug 24 12:58:33.044: INFO: (5) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 24.567628ms)
  Aug 24 12:58:33.044: INFO: (5) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 25.080986ms)
  Aug 24 12:58:33.044: INFO: (5) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 25.474396ms)
  Aug 24 12:58:33.045: INFO: (5) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 25.382988ms)
  Aug 24 12:58:33.045: INFO: (5) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 25.406785ms)
  Aug 24 12:58:33.046: INFO: (5) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 27.012143ms)
  Aug 24 12:58:33.046: INFO: (5) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 26.61412ms)
  Aug 24 12:58:33.046: INFO: (5) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 26.80676ms)
  Aug 24 12:58:33.063: INFO: (6) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 15.610252ms)
  Aug 24 12:58:33.064: INFO: (6) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 16.429334ms)
  Aug 24 12:58:33.064: INFO: (6) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 16.752405ms)
  Aug 24 12:58:33.064: INFO: (6) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 17.217364ms)
  Aug 24 12:58:33.064: INFO: (6) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 17.048741ms)
  Aug 24 12:58:33.068: INFO: (6) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 20.906208ms)
  Aug 24 12:58:33.068: INFO: (6) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 20.737087ms)
  Aug 24 12:58:33.069: INFO: (6) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 21.241478ms)
  Aug 24 12:58:33.069: INFO: (6) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 22.749799ms)
  Aug 24 12:58:33.069: INFO: (6) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 22.021219ms)
  Aug 24 12:58:33.069: INFO: (6) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 22.243057ms)
  Aug 24 12:58:33.069: INFO: (6) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 21.758798ms)
  Aug 24 12:58:33.069: INFO: (6) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 22.157402ms)
  Aug 24 12:58:33.070: INFO: (6) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 23.341743ms)
  Aug 24 12:58:33.070: INFO: (6) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 22.66591ms)
  Aug 24 12:58:33.071: INFO: (6) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 24.034086ms)
  Aug 24 12:58:33.082: INFO: (7) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 11.09983ms)
  Aug 24 12:58:33.085: INFO: (7) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 13.8865ms)
  Aug 24 12:58:33.087: INFO: (7) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 15.275819ms)
  Aug 24 12:58:33.089: INFO: (7) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 17.159711ms)
  Aug 24 12:58:33.091: INFO: (7) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 18.783449ms)
  Aug 24 12:58:33.092: INFO: (7) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 19.113087ms)
  Aug 24 12:58:33.092: INFO: (7) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 20.27142ms)
  Aug 24 12:58:33.092: INFO: (7) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 19.621051ms)
  Aug 24 12:58:33.092: INFO: (7) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 20.282767ms)
  Aug 24 12:58:33.093: INFO: (7) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 20.552817ms)
  Aug 24 12:58:33.093: INFO: (7) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 21.413367ms)
  Aug 24 12:58:33.093: INFO: (7) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 20.439846ms)
  Aug 24 12:58:33.094: INFO: (7) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 21.650115ms)
  Aug 24 12:58:33.094: INFO: (7) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 22.823245ms)
  Aug 24 12:58:33.094: INFO: (7) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 21.815422ms)
  Aug 24 12:58:33.095: INFO: (7) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 22.509999ms)
  Aug 24 12:58:33.107: INFO: (8) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 10.869693ms)
  Aug 24 12:58:33.113: INFO: (8) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 18.099995ms)
  Aug 24 12:58:33.115: INFO: (8) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 18.712869ms)
  Aug 24 12:58:33.118: INFO: (8) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 22.384836ms)
  Aug 24 12:58:33.118: INFO: (8) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 20.454587ms)
  Aug 24 12:58:33.118: INFO: (8) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 21.267676ms)
  Aug 24 12:58:33.118: INFO: (8) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 22.142774ms)
  Aug 24 12:58:33.120: INFO: (8) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 25.413106ms)
  Aug 24 12:58:33.120: INFO: (8) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 23.483337ms)
  Aug 24 12:58:33.120: INFO: (8) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 22.951662ms)
  Aug 24 12:58:33.120: INFO: (8) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 23.797326ms)
  Aug 24 12:58:33.121: INFO: (8) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 21.996654ms)
  Aug 24 12:58:33.121: INFO: (8) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 21.941081ms)
  Aug 24 12:58:33.121: INFO: (8) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 25.674224ms)
  Aug 24 12:58:33.122: INFO: (8) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 23.575459ms)
  Aug 24 12:58:33.122: INFO: (8) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 24.971162ms)
  Aug 24 12:58:33.139: INFO: (9) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 16.226349ms)
  Aug 24 12:58:33.140: INFO: (9) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 17.483828ms)
  Aug 24 12:58:33.140: INFO: (9) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 17.232462ms)
  Aug 24 12:58:33.140: INFO: (9) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 17.40047ms)
  Aug 24 12:58:33.145: INFO: (9) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 21.427396ms)
  Aug 24 12:58:33.145: INFO: (9) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 21.322991ms)
  Aug 24 12:58:33.145: INFO: (9) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 22.017283ms)
  Aug 24 12:58:33.146: INFO: (9) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 23.369837ms)
  Aug 24 12:58:33.147: INFO: (9) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 23.398509ms)
  Aug 24 12:58:33.148: INFO: (9) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 25.149235ms)
  Aug 24 12:58:33.148: INFO: (9) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 23.986627ms)
  Aug 24 12:58:33.148: INFO: (9) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 24.548152ms)
  Aug 24 12:58:33.148: INFO: (9) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 24.81742ms)
  Aug 24 12:58:33.149: INFO: (9) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 25.564918ms)
  Aug 24 12:58:33.163: INFO: (9) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 39.198728ms)
  Aug 24 12:58:33.163: INFO: (9) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 39.076287ms)
  Aug 24 12:58:33.179: INFO: (10) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 13.594807ms)
  Aug 24 12:58:33.183: INFO: (10) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 17.144905ms)
  Aug 24 12:58:33.186: INFO: (10) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 20.013996ms)
  Aug 24 12:58:33.187: INFO: (10) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 20.567471ms)
  Aug 24 12:58:33.193: INFO: (10) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 26.759938ms)
  Aug 24 12:58:33.194: INFO: (10) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 26.453975ms)
  Aug 24 12:58:33.195: INFO: (10) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 29.354904ms)
  Aug 24 12:58:33.196: INFO: (10) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 28.664819ms)
  Aug 24 12:58:33.196: INFO: (10) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 30.24236ms)
  Aug 24 12:58:33.197: INFO: (10) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 30.825932ms)
  Aug 24 12:58:33.197: INFO: (10) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 30.37736ms)
  Aug 24 12:58:33.197: INFO: (10) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 29.004543ms)
  Aug 24 12:58:33.197: INFO: (10) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 30.581239ms)
  Aug 24 12:58:33.195: INFO: (10) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 29.344507ms)
  Aug 24 12:58:33.199: INFO: (10) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 31.820088ms)
  Aug 24 12:58:33.201: INFO: (10) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 33.972035ms)
  Aug 24 12:58:33.217: INFO: (11) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 15.355912ms)
  Aug 24 12:58:33.235: INFO: (11) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 32.451032ms)
  Aug 24 12:58:33.246: INFO: (11) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 43.673002ms)
  Aug 24 12:58:33.247: INFO: (11) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 44.743766ms)
  Aug 24 12:58:33.248: INFO: (11) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 45.38304ms)
  Aug 24 12:58:33.248: INFO: (11) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 45.675583ms)
  Aug 24 12:58:33.254: INFO: (11) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 51.276728ms)
  Aug 24 12:58:33.258: INFO: (11) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 55.333994ms)
  Aug 24 12:58:33.258: INFO: (11) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 55.946851ms)
  Aug 24 12:58:33.258: INFO: (11) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 55.634045ms)
  Aug 24 12:58:33.259: INFO: (11) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 56.497398ms)
  Aug 24 12:58:33.259: INFO: (11) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 56.645414ms)
  Aug 24 12:58:33.260: INFO: (11) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 57.333622ms)
  Aug 24 12:58:33.260: INFO: (11) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 57.913842ms)
  Aug 24 12:58:33.261: INFO: (11) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 58.157955ms)
  Aug 24 12:58:33.263: INFO: (11) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 60.305908ms)
  Aug 24 12:58:33.271: INFO: (12) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 7.806436ms)
  Aug 24 12:58:33.277: INFO: (12) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 13.60161ms)
  Aug 24 12:58:33.281: INFO: (12) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 16.93711ms)
  Aug 24 12:58:33.281: INFO: (12) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 17.562678ms)
  Aug 24 12:58:33.281: INFO: (12) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 17.753736ms)
  Aug 24 12:58:33.282: INFO: (12) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 17.87486ms)
  Aug 24 12:58:33.284: INFO: (12) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 19.679808ms)
  Aug 24 12:58:33.284: INFO: (12) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 19.683646ms)
  Aug 24 12:58:33.285: INFO: (12) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 20.709434ms)
  Aug 24 12:58:33.286: INFO: (12) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 22.238567ms)
  Aug 24 12:58:33.287: INFO: (12) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 22.391191ms)
  Aug 24 12:58:33.288: INFO: (12) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 22.822659ms)
  Aug 24 12:58:33.289: INFO: (12) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 25.309483ms)
  Aug 24 12:58:33.291: INFO: (12) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 25.710054ms)
  Aug 24 12:58:33.292: INFO: (12) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 27.848983ms)
  Aug 24 12:58:33.292: INFO: (12) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 26.731165ms)
  Aug 24 12:58:33.307: INFO: (13) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 14.436803ms)
  Aug 24 12:58:33.308: INFO: (13) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 15.036223ms)
  Aug 24 12:58:33.310: INFO: (13) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 16.720857ms)
  Aug 24 12:58:33.310: INFO: (13) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 16.917457ms)
  Aug 24 12:58:33.310: INFO: (13) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 17.253472ms)
  Aug 24 12:58:33.313: INFO: (13) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 19.807786ms)
  Aug 24 12:58:33.316: INFO: (13) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 22.112077ms)
  Aug 24 12:58:33.316: INFO: (13) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 22.333813ms)
  Aug 24 12:58:33.317: INFO: (13) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 24.027787ms)
  Aug 24 12:58:33.318: INFO: (13) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 24.834132ms)
  Aug 24 12:58:33.319: INFO: (13) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 25.816383ms)
  Aug 24 12:58:33.320: INFO: (13) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 26.834718ms)
  Aug 24 12:58:33.323: INFO: (13) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 29.447892ms)
  Aug 24 12:58:33.323: INFO: (13) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 30.050403ms)
  Aug 24 12:58:33.323: INFO: (13) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 29.441107ms)
  Aug 24 12:58:33.323: INFO: (13) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 29.65332ms)
  Aug 24 12:58:33.339: INFO: (14) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 15.940258ms)
  Aug 24 12:58:33.344: INFO: (14) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 19.337351ms)
  Aug 24 12:58:33.344: INFO: (14) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 19.56544ms)
  Aug 24 12:58:33.344: INFO: (14) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 19.883882ms)
  Aug 24 12:58:33.347: INFO: (14) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 22.525039ms)
  Aug 24 12:58:33.347: INFO: (14) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 22.458082ms)
  Aug 24 12:58:33.348: INFO: (14) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 23.090596ms)
  Aug 24 12:58:33.348: INFO: (14) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 23.147961ms)
  Aug 24 12:58:33.349: INFO: (14) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 24.390189ms)
  Aug 24 12:58:33.349: INFO: (14) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 24.228123ms)
  Aug 24 12:58:33.349: INFO: (14) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 24.693576ms)
  Aug 24 12:58:33.349: INFO: (14) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 25.65965ms)
  Aug 24 12:58:33.349: INFO: (14) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 24.974168ms)
  Aug 24 12:58:33.349: INFO: (14) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 25.613494ms)
  Aug 24 12:58:33.350: INFO: (14) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 25.654581ms)
  Aug 24 12:58:33.352: INFO: (14) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 28.095608ms)
  Aug 24 12:58:33.367: INFO: (15) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 14.201843ms)
  Aug 24 12:58:33.369: INFO: (15) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 14.800465ms)
  Aug 24 12:58:33.370: INFO: (15) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 16.312621ms)
  Aug 24 12:58:33.370: INFO: (15) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 15.963988ms)
  Aug 24 12:58:33.370: INFO: (15) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 17.054231ms)
  Aug 24 12:58:33.370: INFO: (15) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 16.213616ms)
  Aug 24 12:58:33.370: INFO: (15) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 17.207664ms)
  Aug 24 12:58:33.370: INFO: (15) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 17.663925ms)
  Aug 24 12:58:33.374: INFO: (15) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 19.204741ms)
  Aug 24 12:58:33.375: INFO: (15) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 20.422589ms)
  Aug 24 12:58:33.375: INFO: (15) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 21.023702ms)
  Aug 24 12:58:33.377: INFO: (15) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 22.496612ms)
  Aug 24 12:58:33.379: INFO: (15) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 24.107268ms)
  Aug 24 12:58:33.379: INFO: (15) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 24.359602ms)
  Aug 24 12:58:33.381: INFO: (15) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 27.133554ms)
  Aug 24 12:58:33.381: INFO: (15) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 25.884048ms)
  Aug 24 12:58:33.390: INFO: (16) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 8.642144ms)
  Aug 24 12:58:33.392: INFO: (16) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 10.248039ms)
  Aug 24 12:58:33.394: INFO: (16) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 11.406878ms)
  Aug 24 12:58:33.395: INFO: (16) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 11.739239ms)
  Aug 24 12:58:33.396: INFO: (16) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 14.527212ms)
  Aug 24 12:58:33.396: INFO: (16) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 11.873562ms)
  Aug 24 12:58:33.398: INFO: (16) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 15.085551ms)
  Aug 24 12:58:33.399: INFO: (16) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 16.779486ms)
  Aug 24 12:58:33.401: INFO: (16) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 19.248923ms)
  Aug 24 12:58:33.401: INFO: (16) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 17.829725ms)
  Aug 24 12:58:33.401: INFO: (16) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 18.323859ms)
  Aug 24 12:58:33.402: INFO: (16) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 18.254624ms)
  Aug 24 12:58:33.402: INFO: (16) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 19.786847ms)
  Aug 24 12:58:33.402: INFO: (16) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 18.549131ms)
  Aug 24 12:58:33.403: INFO: (16) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 19.349951ms)
  Aug 24 12:58:33.410: INFO: (16) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 27.06368ms)
  Aug 24 12:58:33.420: INFO: (17) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 8.632771ms)
  Aug 24 12:58:33.421: INFO: (17) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 9.933457ms)
  Aug 24 12:58:33.421: INFO: (17) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 9.195485ms)
  Aug 24 12:58:33.425: INFO: (17) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 12.766934ms)
  Aug 24 12:58:33.426: INFO: (17) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 15.434412ms)
  Aug 24 12:58:33.429: INFO: (17) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 15.801103ms)
  Aug 24 12:58:33.429: INFO: (17) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 16.67012ms)
  Aug 24 12:58:33.429: INFO: (17) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 17.82334ms)
  Aug 24 12:58:33.430: INFO: (17) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 17.333896ms)
  Aug 24 12:58:33.431: INFO: (17) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 18.377081ms)
  Aug 24 12:58:33.431: INFO: (17) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 18.873134ms)
  Aug 24 12:58:33.431: INFO: (17) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 19.228069ms)
  Aug 24 12:58:33.433: INFO: (17) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 19.996399ms)
  Aug 24 12:58:33.433: INFO: (17) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 20.472121ms)
  Aug 24 12:58:33.434: INFO: (17) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 22.83256ms)
  Aug 24 12:58:33.437: INFO: (17) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 24.440825ms)
  Aug 24 12:58:33.447: INFO: (18) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 9.935142ms)
  Aug 24 12:58:33.451: INFO: (18) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 13.95654ms)
  Aug 24 12:58:33.454: INFO: (18) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 16.140695ms)
  Aug 24 12:58:33.454: INFO: (18) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 16.015467ms)
  Aug 24 12:58:33.457: INFO: (18) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 18.923268ms)
  Aug 24 12:58:33.457: INFO: (18) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 18.774545ms)
  Aug 24 12:58:33.461: INFO: (18) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 22.864124ms)
  Aug 24 12:58:33.461: INFO: (18) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 23.447419ms)
  Aug 24 12:58:33.461: INFO: (18) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 23.77505ms)
  Aug 24 12:58:33.461: INFO: (18) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 23.902924ms)
  Aug 24 12:58:33.463: INFO: (18) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 24.741706ms)
  Aug 24 12:58:33.463: INFO: (18) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 25.392321ms)
  Aug 24 12:58:33.465: INFO: (18) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 26.532385ms)
  Aug 24 12:58:33.467: INFO: (18) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 28.21764ms)
  Aug 24 12:58:33.471: INFO: (18) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 32.609197ms)
  Aug 24 12:58:33.475: INFO: (18) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 36.193028ms)
  Aug 24 12:58:33.487: INFO: (19) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:443/proxy/tlsrewritem... (200; 10.453749ms)
  Aug 24 12:58:33.489: INFO: (19) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk/proxy/rewriteme">test</a> (200; 11.756224ms)
  Aug 24 12:58:33.489: INFO: (19) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 13.617683ms)
  Aug 24 12:58:33.490: INFO: (19) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">test<... (200; 13.316996ms)
  Aug 24 12:58:33.493: INFO: (19) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname1/proxy/: foo (200; 18.709585ms)
  Aug 24 12:58:33.494: INFO: (19) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 18.379026ms)
  Aug 24 12:58:33.498: INFO: (19) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname2/proxy/: tls qux (200; 22.597516ms)
  Aug 24 12:58:33.500: INFO: (19) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:460/proxy/: tls baz (200; 24.199714ms)
  Aug 24 12:58:33.500: INFO: (19) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/: <a href="/api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:1080/proxy/rewriteme">... (200; 23.675529ms)
  Aug 24 12:58:33.505: INFO: (19) /api/v1/namespaces/proxy-6819/pods/https:proxy-service-5nn74-kj6pk:462/proxy/: tls qux (200; 28.405161ms)
  Aug 24 12:58:33.510: INFO: (19) /api/v1/namespaces/proxy-6819/services/proxy-service-5nn74:portname2/proxy/: bar (200; 34.624933ms)
  Aug 24 12:58:33.513: INFO: (19) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname1/proxy/: foo (200; 36.814378ms)
  Aug 24 12:58:33.513: INFO: (19) /api/v1/namespaces/proxy-6819/services/http:proxy-service-5nn74:portname2/proxy/: bar (200; 37.492919ms)
  Aug 24 12:58:33.513: INFO: (19) /api/v1/namespaces/proxy-6819/pods/http:proxy-service-5nn74-kj6pk:162/proxy/: bar (200; 38.090304ms)
  Aug 24 12:58:33.514: INFO: (19) /api/v1/namespaces/proxy-6819/pods/proxy-service-5nn74-kj6pk:160/proxy/: foo (200; 38.153673ms)
  Aug 24 12:58:33.514: INFO: (19) /api/v1/namespaces/proxy-6819/services/https:proxy-service-5nn74:tlsportname1/proxy/: tls baz (200; 38.440633ms)
  Aug 24 12:58:33.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-5nn74 in namespace proxy-6819, will wait for the garbage collector to delete the pods @ 08/24/23 12:58:33.527
  E0824 12:58:33.562917      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:58:33.601: INFO: Deleting ReplicationController proxy-service-5nn74 took: 16.29149ms
  Aug 24 12:58:33.701: INFO: Terminating ReplicationController proxy-service-5nn74 pods took: 100.269283ms
  E0824 12:58:34.563342      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:35.563803      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-6819" for this suite. @ 08/24/23 12:58:36.504
• [5.843 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 08/24/23 12:58:36.53
  Aug 24 12:58:36.530: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 12:58:36.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:58:36.561
  E0824 12:58:36.563937      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:58:36.565
  STEP: Creating configMap with name cm-test-opt-del-ae016386-cee6-4149-8a72-966fe7be400b @ 08/24/23 12:58:36.578
  STEP: Creating configMap with name cm-test-opt-upd-ee4c6dad-6137-4ce9-9f4b-b0566c53fb54 @ 08/24/23 12:58:36.585
  STEP: Creating the pod @ 08/24/23 12:58:36.595
  E0824 12:58:37.564332      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:38.564863      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-ae016386-cee6-4149-8a72-966fe7be400b @ 08/24/23 12:58:38.667
  STEP: Updating configmap cm-test-opt-upd-ee4c6dad-6137-4ce9-9f4b-b0566c53fb54 @ 08/24/23 12:58:38.679
  STEP: Creating configMap with name cm-test-opt-create-4cf42ce3-c24e-4445-a01a-a8c26ac20ca2 @ 08/24/23 12:58:38.687
  STEP: waiting to observe update in volume @ 08/24/23 12:58:38.695
  E0824 12:58:39.565956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:40.566205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:41.566614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:42.566760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:43.567588      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:44.567768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:45.568027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:46.568832      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:47.569689      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:48.570549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:49.570697      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:50.570975      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:51.571152      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:52.571368      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:53.571791      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:54.571936      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:55.572468      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:56.572799      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:57.573364      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:58.573586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:58:59.574244      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:00.574576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:01.575053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:02.575560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:03.576421      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:04.580665      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:05.577925      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:06.577946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:07.578995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:08.579905      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:09.580588      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:10.580849      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:11.581721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:12.582122      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:13.583056      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:14.583535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:15.583614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:16.583939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:17.584843      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:18.584997      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:19.585256      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:20.586579      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:21.587318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:22.587290      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:23.588430      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:24.589156      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:25.592858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:26.590644      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:27.591692      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:28.592390      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:29.592193      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:30.592480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:31.593253      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:32.594143      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:33.594834      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:34.594733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:35.595051      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:36.595845      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:37.596279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:38.596896      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:39.596984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:40.598417      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:41.599062      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:42.599001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:43.600437      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:44.600548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:45.601515      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:46.602469      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:47.603770      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:48.603801      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:49.603867      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:50.604301      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 12:59:51.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8057" for this suite. @ 08/24/23 12:59:51.488
• [74.972 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 08/24/23 12:59:51.508
  Aug 24 12:59:51.508: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename prestop @ 08/24/23 12:59:51.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 12:59:51.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 12:59:51.597
  STEP: Creating server pod server in namespace prestop-205 @ 08/24/23 12:59:51.603
  E0824 12:59:51.604520      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pods to come up. @ 08/24/23 12:59:51.622
  E0824 12:59:52.604589      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:53.605828      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-205 @ 08/24/23 12:59:53.654
  E0824 12:59:54.606580      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:55.607574      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 08/24/23 12:59:55.684
  E0824 12:59:56.608318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:57.609127      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:58.608881      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 12:59:59.609292      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:00.609322      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:00:00.721: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Aug 24 13:00:00.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 08/24/23 13:00:00.739
  STEP: Destroying namespace "prestop-205" for this suite. @ 08/24/23 13:00:00.781
• [9.326 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 08/24/23 13:00:00.836
  Aug 24 13:00:00.836: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 13:00:00.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:00:00.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:00:00.891
  STEP: Creating pod liveness-c13b6442-a290-4000-996a-0742016102a8 in namespace container-probe-8490 @ 08/24/23 13:00:00.896
  E0824 13:00:01.609695      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:02.610263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:00:02.947: INFO: Started pod liveness-c13b6442-a290-4000-996a-0742016102a8 in namespace container-probe-8490
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 13:00:02.947
  Aug 24 13:00:02.953: INFO: Initial restart count of pod liveness-c13b6442-a290-4000-996a-0742016102a8 is 0
  E0824 13:00:03.610093      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:04.610742      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:05.611365      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:06.612510      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:07.612692      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:08.612879      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:09.613033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:10.613581      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:11.613773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:12.614354      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:13.614630      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:14.615216      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:15.616114      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:16.616655      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:17.616812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:18.616953      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:19.617141      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:20.617232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:21.617588      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:22.617773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:23.617863      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:24.618156      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:25.618283      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:26.618520      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:27.618795      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:28.619778      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:29.620617      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:30.621963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:31.622861      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:32.623070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:33.624078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:34.624984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:35.625836      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:36.626081      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:37.626264      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:38.627341      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:39.628533      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:40.628835      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:41.629287      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:42.630142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:43.630808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:44.630799      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:45.632670      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:46.632773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:47.633984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:48.634244      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:49.634147      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:50.634357      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:51.634613      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:52.634816      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:53.635396      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:54.636230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:55.637638      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:56.637343      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:57.637696      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:58.638858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:00:59.639710      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:00.640840      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:01.641914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:02.641920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:03.642432      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:04.643064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:05.643764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:06.644374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:07.644802      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:08.645788      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:09.646068      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:10.646686      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:11.647192      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:12.647914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:13.648501      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:14.648451      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:15.649183      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:16.650106      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:17.650572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:18.651587      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:19.656230      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:20.653215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:21.654179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:22.654517      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:23.654910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:24.668282      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:25.662010      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:26.662497      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:27.663472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:28.664526      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:29.665424      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:30.665643      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:31.666115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:32.666830      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:33.667866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:34.668133      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:35.668540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:36.669393      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:37.669803      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:38.670604      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:39.671668      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:40.671744      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:41.672495      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:42.673646      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:43.674713      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:44.675769      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:45.676418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:46.676574      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:47.676726      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:48.677805      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:49.678180      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:50.678462      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:51.679050      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:52.679858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:53.680344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:54.680902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:55.681365      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:56.682078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:57.683080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:58.683702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:01:59.684535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:00.685184      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:01.685916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:02.686774      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:03.687443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:04.687812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:05.688611      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:06.688980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:07.689110      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:08.695289      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:09.689281      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:10.689476      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:11.690150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:12.690600      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:13.690460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:14.690811      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:15.691648      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:16.692784      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:17.693675      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:18.694305      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:19.694535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:20.694778      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:21.695827      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:22.695980      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:23.696569      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:24.697199      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:25.698095      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:26.698636      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:27.699103      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:28.699630      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:29.699701      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:30.699915      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:31.700963      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:32.701244      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:33.702492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:34.702823      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:35.703533      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:36.703780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:37.704621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:38.705629      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:39.705974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:40.706226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:41.706633      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:42.706841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:43.707649      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:44.707889      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:45.708785      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:46.709615      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:47.709796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:48.709987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:49.711098      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:50.710783      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:51.710853      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:52.711000      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:53.711902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:54.712902      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:55.713597      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:56.713859      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:57.714058      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:58.714979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:02:59.715158      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:00.715757      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:01.716132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:02.717422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:03.717381      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:04.717519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:05.718305      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:06.718463      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:07.719523      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:08.720062      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:09.719900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:10.720996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:11.721242      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:12.721418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:13.721560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:14.722224      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:15.722551      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:16.722774      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:17.723050      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:18.723094      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:19.723100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:20.723472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:21.723587      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:22.724156      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:23.724625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:24.724964      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:25.725724      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:26.726186      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:27.726509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:28.726841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:29.729859      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:30.728424      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:31.728548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:32.729560      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:33.730773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:34.730916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:35.731256      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:36.731340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:37.731465      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:38.732286      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:39.732583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:40.733602      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:41.733999      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:42.734987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:43.736045      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:44.736184      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:45.737259      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:46.737441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:47.737556      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:48.737687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:49.738792      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:50.739090      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:51.739796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:52.739978      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:53.740207      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:54.740491      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:55.742543      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:56.740892      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:57.741574      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:58.741599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:03:59.741673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:00.742587      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:01.743442      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:02.765642      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:03.751044      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:04.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:04:04.192
  STEP: Destroying namespace "container-probe-8490" for this suite. @ 08/24/23 13:04:04.242
• [243.443 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 08/24/23 13:04:04.288
  Aug 24 13:04:04.288: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:04:04.3
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:04:04.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:04:04.36
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:04:04.366
  E0824 13:04:04.751900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:05.754141      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:06.753860      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:07.754913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:04:08.428
  Aug 24 13:04:08.436: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-f414cde1-5881-4de3-ad71-62a69b932a6f container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:04:08.477
  Aug 24 13:04:08.509: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3189" for this suite. @ 08/24/23 13:04:08.522
• [4.250 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 08/24/23 13:04:08.546
  Aug 24 13:04:08.546: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 13:04:08.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:04:08.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:04:08.581
  STEP: fetching services @ 08/24/23 13:04:08.587
  Aug 24 13:04:08.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8293" for this suite. @ 08/24/23 13:04:08.599
• [0.067 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 08/24/23 13:04:08.621
  Aug 24 13:04:08.621: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 13:04:08.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:04:08.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:04:08.657
  STEP: Counting existing ResourceQuota @ 08/24/23 13:04:08.662
  E0824 13:04:08.755166      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:09.755813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:10.756577      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:11.756901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:12.757997      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 13:04:13.672
  STEP: Ensuring resource quota status is calculated @ 08/24/23 13:04:13.682
  E0824 13:04:13.761954      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:14.761389      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 08/24/23 13:04:15.694
  STEP: Ensuring ResourceQuota status captures the pod usage @ 08/24/23 13:04:15.723
  E0824 13:04:15.762460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:16.763189      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 08/24/23 13:04:17.734
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 08/24/23 13:04:17.741
  STEP: Ensuring a pod cannot update its resource requirements @ 08/24/23 13:04:17.75
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 08/24/23 13:04:17.763
  E0824 13:04:17.763407      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:18.764320      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:19.764628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/24/23 13:04:19.787
  STEP: Ensuring resource quota status released the pod usage @ 08/24/23 13:04:19.825
  E0824 13:04:20.766242      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:21.766185      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:21.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2773" for this suite. @ 08/24/23 13:04:21.844
• [13.237 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 08/24/23 13:04:21.859
  Aug 24 13:04:21.859: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:04:21.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:04:21.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:04:21.903
  STEP: Creating projection with secret that has name projected-secret-test-map-c7b3a21a-2c7a-4c56-9e46-b335d79b0495 @ 08/24/23 13:04:21.91
  STEP: Creating a pod to test consume secrets @ 08/24/23 13:04:21.922
  E0824 13:04:22.767363      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:23.768544      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:24.768521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:25.768722      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:04:25.966
  Aug 24 13:04:25.972: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-secrets-f82665d4-91d0-4870-a2db-ddceac4c94e6 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 13:04:25.983
  Aug 24 13:04:26.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7934" for this suite. @ 08/24/23 13:04:26.018
• [4.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 08/24/23 13:04:26.032
  Aug 24 13:04:26.032: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 13:04:26.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:04:26.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:04:26.064
  STEP: Creating pod busybox-19b1b4d4-fd1c-4121-bfba-d6f36df0aeba in namespace container-probe-405 @ 08/24/23 13:04:26.068
  E0824 13:04:26.770385      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:27.770737      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:04:28.098: INFO: Started pod busybox-19b1b4d4-fd1c-4121-bfba-d6f36df0aeba in namespace container-probe-405
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 13:04:28.099
  Aug 24 13:04:28.108: INFO: Initial restart count of pod busybox-19b1b4d4-fd1c-4121-bfba-d6f36df0aeba is 0
  E0824 13:04:28.771527      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:29.771845      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:30.771867      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:31.772442      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:32.773515      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:33.773607      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:34.774372      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:35.775238      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:36.775429      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:37.776176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:38.777025      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:39.776831      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:40.777232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:41.777664      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:42.778150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:43.778576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:44.779681      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:45.779790      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:46.780727      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:47.780777      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:48.782884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:49.782806      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:50.782995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:51.783982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:52.784075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:53.784605      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:54.785697      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:55.786565      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:56.786743      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:57.787657      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:58.788975      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:04:59.789390      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:00.790406      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:01.791178      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:02.791600      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:03.792095      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:04.792782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:05.793416      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:06.793486      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:07.793844      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:08.794028      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:09.794500      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:10.794554      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:11.795644      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:12.796465      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:13.797288      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:14.798432      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:15.799071      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:16.798949      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:17.799431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:18.799559      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:19.799791      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:20.800774      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:21.801626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:22.801858      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:23.801884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:24.802785      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:25.803249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:26.804177      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:27.804291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:28.804402      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:29.804768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:30.805071      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:31.805710      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:32.805910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:33.806188      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:34.806869      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:35.806808      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:36.806991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:37.807459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:38.808326      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:39.815263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:40.815788      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:41.816128      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:42.817021      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:43.817958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:44.819011      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:45.823636      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:46.820273      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:47.820555      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:48.820730      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:49.820875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:50.821346      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:51.822213      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:52.822515      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:53.823063      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:54.824232      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:55.824162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:56.824571      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:57.825162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:58.826254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:05:59.827015      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:00.827296      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:01.828013      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:02.828090      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:03.828614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:04.828796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:05.829588      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:06.829507      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:07.829778      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:08.830240      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:09.830977      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:10.831251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:11.831500      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:12.831674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:13.832235      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:14.833098      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:15.833017      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:16.833296      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:17.833498      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:18.834440      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:19.834878      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:20.834962      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:21.835579      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:22.835557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:23.835576      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:24.835900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:25.836197      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:26.837087      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:27.837199      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:28.837448      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:29.837832      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:30.837780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:31.838149      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:32.839100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:33.839393      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:34.840006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:35.840907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:36.841419      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:37.842383      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:38.843326      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:39.844116      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:40.845826      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:41.845471      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:42.845862      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:43.846873      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:44.847542      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:45.847861      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:46.848711      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:47.849005      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:48.849535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:49.852111      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:50.852864      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:51.853659      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:52.853986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:53.854583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:54.857098      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:55.856592      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:56.857782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:57.857860      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:58.858635      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:06:59.859106      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:00.859421      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:01.859378      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:02.859472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:03.860332      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:04.860813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:05.861146      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:06.861315      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:07.861522      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:08.862081      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:09.862419      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:10.862583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:11.863062      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:12.865712      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:13.866022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:14.866580      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:15.867054      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:16.867417      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:17.867723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:18.869006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:19.868868      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:20.869011      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:21.869276      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:22.869783      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:23.870889      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:24.870920      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:25.871711      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:26.871992      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:27.872286      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:28.873173      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:29.873318      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:30.874170      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:31.874805      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:32.874948      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:33.875186      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:34.875987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:35.876169      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:36.876552      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:37.876714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:38.877786      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:39.878072      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:40.878273      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:41.878502      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:42.878763      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:43.878991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:44.879218      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:45.879378      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:46.879511      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:47.879725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:48.880489      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:49.880835      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:50.881347      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:51.882285      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:52.882718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:53.883725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:54.883790      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:55.884101      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:56.884673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:57.885586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:58.886692      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:07:59.886864      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:00.886965      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:01.887212      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:02.887303      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:03.887486      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:04.887631      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:05.888610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:06.889229      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:07.892300      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:08.890671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:09.891663      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:10.891987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:11.892535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:12.892701      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:13.893546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:14.893820      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:15.894886      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:16.895197      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:17.895293      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:18.895358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:19.895507      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:20.895697      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:21.896744      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:22.897706      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:23.898595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:24.898870      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:25.898965      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:26.899730      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:27.899822      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:28.900881      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:29.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:08:29.39
  STEP: Destroying namespace "container-probe-405" for this suite. @ 08/24/23 13:08:29.427
• [243.416 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 08/24/23 13:08:29.453
  Aug 24 13:08:29.453: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 13:08:29.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:29.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:29.504
  STEP: Creating configMap that has name configmap-test-emptyKey-7d3c1978-272a-4a19-a551-59a41520098e @ 08/24/23 13:08:29.513
  Aug 24 13:08:29.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1637" for this suite. @ 08/24/23 13:08:29.53
• [0.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 08/24/23 13:08:29.558
  Aug 24 13:08:29.558: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:08:29.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:29.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:29.603
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/24/23 13:08:29.613
  Aug 24 13:08:29.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-2104 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Aug 24 13:08:29.848: INFO: stderr: ""
  Aug 24 13:08:29.848: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/24/23 13:08:29.848
  Aug 24 13:08:29.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-2104 delete pods e2e-test-httpd-pod'
  E0824 13:08:29.901687      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:30.901848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:31.902075      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:32.671: INFO: stderr: ""
  Aug 24 13:08:32.671: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug 24 13:08:32.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2104" for this suite. @ 08/24/23 13:08:32.681
• [3.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 08/24/23 13:08:32.712
  Aug 24 13:08:32.712: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 13:08:32.714
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:32.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:32.749
  Aug 24 13:08:32.756: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: creating the pod @ 08/24/23 13:08:32.76
  STEP: submitting the pod to kubernetes @ 08/24/23 13:08:32.761
  E0824 13:08:32.903276      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:33.904188      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:34.904115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:34.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-910" for this suite. @ 08/24/23 13:08:34.977
• [2.290 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 08/24/23 13:08:35.013
  Aug 24 13:08:35.014: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename svcaccounts @ 08/24/23 13:08:35.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:35.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:35.119
  STEP: Creating ServiceAccount "e2e-sa-z5lcn"  @ 08/24/23 13:08:35.13
  Aug 24 13:08:35.141: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-z5lcn"  @ 08/24/23 13:08:35.141
  Aug 24 13:08:35.163: INFO: AutomountServiceAccountToken: true
  Aug 24 13:08:35.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3895" for this suite. @ 08/24/23 13:08:35.174
• [0.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 08/24/23 13:08:35.21
  Aug 24 13:08:35.211: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 13:08:35.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:35.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:35.245
  STEP: Given a ReplicationController is created @ 08/24/23 13:08:35.25
  STEP: When the matched label of one of its pods change @ 08/24/23 13:08:35.262
  Aug 24 13:08:35.269: INFO: Pod name pod-release: Found 0 pods out of 1
  E0824 13:08:35.906959      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:36.907341      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:37.908358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:38.908616      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:39.909017      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:40.287: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/24/23 13:08:40.319
  Aug 24 13:08:40.359: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6651" for this suite. @ 08/24/23 13:08:40.389
• [5.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 08/24/23 13:08:40.43
  Aug 24 13:08:40.430: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename security-context @ 08/24/23 13:08:40.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:40.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:40.511
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/24/23 13:08:40.525
  E0824 13:08:40.909085      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:41.909626      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:42.910236      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:43.910706      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:08:44.603
  Aug 24 13:08:44.613: INFO: Trying to get logs from node quohp9aeph3i-3 pod security-context-0d3b6228-bf6c-4db6-8ed8-1e06aaa5c002 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 13:08:44.644
  Aug 24 13:08:44.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-366" for this suite. @ 08/24/23 13:08:44.695
• [4.284 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 08/24/23 13:08:44.715
  Aug 24 13:08:44.715: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/24/23 13:08:44.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:08:44.757
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:08:44.762
  STEP: Creating 50 configmaps @ 08/24/23 13:08:44.771
  E0824 13:08:44.911597      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 08/24/23 13:08:45.386
  Aug 24 13:08:45.421: INFO: Pod name wrapped-volume-race-40d47a92-87f7-43c1-9433-bb14c304c0cf: Found 0 pods out of 5
  E0824 13:08:45.912057      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:46.914747      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:47.914598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:48.914855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:49.915484      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:50.440: INFO: Pod name wrapped-volume-race-40d47a92-87f7-43c1-9433-bb14c304c0cf: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/24/23 13:08:50.44
  STEP: Creating RC which spawns configmap-volume pods @ 08/24/23 13:08:50.514
  Aug 24 13:08:50.551: INFO: Pod name wrapped-volume-race-f12fbe60-fa4f-4702-a666-dd60b7abbc86: Found 0 pods out of 5
  E0824 13:08:50.915968      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:51.916161      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:52.916733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:53.917456      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:54.917750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:08:55.579: INFO: Pod name wrapped-volume-race-f12fbe60-fa4f-4702-a666-dd60b7abbc86: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/24/23 13:08:55.579
  STEP: Creating RC which spawns configmap-volume pods @ 08/24/23 13:08:55.625
  Aug 24 13:08:55.657: INFO: Pod name wrapped-volume-race-dd18d009-51c0-40ce-aa88-85fccbffe073: Found 0 pods out of 5
  E0824 13:08:55.917822      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:56.937881      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:57.927070      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:58.928059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:08:59.929015      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:00.678: INFO: Pod name wrapped-volume-race-dd18d009-51c0-40ce-aa88-85fccbffe073: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/24/23 13:09:00.678
  Aug 24 13:09:00.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-dd18d009-51c0-40ce-aa88-85fccbffe073 in namespace emptydir-wrapper-4714, will wait for the garbage collector to delete the pods @ 08/24/23 13:09:00.728
  Aug 24 13:09:00.803: INFO: Deleting ReplicationController wrapped-volume-race-dd18d009-51c0-40ce-aa88-85fccbffe073 took: 15.159238ms
  Aug 24 13:09:00.905: INFO: Terminating ReplicationController wrapped-volume-race-dd18d009-51c0-40ce-aa88-85fccbffe073 pods took: 101.8945ms
  E0824 13:09:00.928946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:01.929942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-f12fbe60-fa4f-4702-a666-dd60b7abbc86 in namespace emptydir-wrapper-4714, will wait for the garbage collector to delete the pods @ 08/24/23 13:09:02.808
  Aug 24 13:09:02.885: INFO: Deleting ReplicationController wrapped-volume-race-f12fbe60-fa4f-4702-a666-dd60b7abbc86 took: 15.283635ms
  E0824 13:09:02.929867      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:03.186: INFO: Terminating ReplicationController wrapped-volume-race-f12fbe60-fa4f-4702-a666-dd60b7abbc86 pods took: 301.179592ms
  E0824 13:09:03.930935      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:04.931067      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-40d47a92-87f7-43c1-9433-bb14c304c0cf in namespace emptydir-wrapper-4714, will wait for the garbage collector to delete the pods @ 08/24/23 13:09:05.587
  Aug 24 13:09:05.673: INFO: Deleting ReplicationController wrapped-volume-race-40d47a92-87f7-43c1-9433-bb14c304c0cf took: 26.466203ms
  Aug 24 13:09:05.774: INFO: Terminating ReplicationController wrapped-volume-race-40d47a92-87f7-43c1-9433-bb14c304c0cf pods took: 101.179148ms
  E0824 13:09:05.931161      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:06.932249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:07.932302      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 08/24/23 13:09:08.076
  STEP: Destroying namespace "emptydir-wrapper-4714" for this suite. @ 08/24/23 13:09:08.821
• [24.125 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 08/24/23 13:09:08.843
  Aug 24 13:09:08.843: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 13:09:08.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:09:08.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:09:08.889
  STEP: creating service endpoint-test2 in namespace services-4123 @ 08/24/23 13:09:08.905
  E0824 13:09:08.932624      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4123 to expose endpoints map[] @ 08/24/23 13:09:08.991
  Aug 24 13:09:09.003: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0824 13:09:09.932792      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:10.022: INFO: successfully validated that service endpoint-test2 in namespace services-4123 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-4123 @ 08/24/23 13:09:10.023
  E0824 13:09:10.933881      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:11.934777      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:12.934800      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:13.935568      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4123 to expose endpoints map[pod1:[80]] @ 08/24/23 13:09:14.077
  Aug 24 13:09:14.098: INFO: successfully validated that service endpoint-test2 in namespace services-4123 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 08/24/23 13:09:14.098
  Aug 24 13:09:14.098: INFO: Creating new exec pod
  E0824 13:09:14.936837      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:15.936937      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:16.937198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:17.938135      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:18.938933      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:19.135: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4123 exec execpod7hjk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 24 13:09:19.486: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 24 13:09:19.486: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:09:19.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4123 exec execpod7hjk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.26.149 80'
  Aug 24 13:09:19.809: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.26.149 80\nConnection to 10.233.26.149 80 port [tcp/http] succeeded!\n"
  Aug 24 13:09:19.809: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-4123 @ 08/24/23 13:09:19.809
  E0824 13:09:19.939685      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:20.940024      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:21.941340      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:22.941868      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4123 to expose endpoints map[pod1:[80] pod2:[80]] @ 08/24/23 13:09:23.874
  Aug 24 13:09:23.897: INFO: successfully validated that service endpoint-test2 in namespace services-4123 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 08/24/23 13:09:23.897
  E0824 13:09:23.942796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:24.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4123 exec execpod7hjk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0824 13:09:24.942819      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:25.208: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 24 13:09:25.208: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:09:25.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4123 exec execpod7hjk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.26.149 80'
  Aug 24 13:09:25.440: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.26.149 80\nConnection to 10.233.26.149 80 port [tcp/http] succeeded!\n"
  Aug 24 13:09:25.440: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-4123 @ 08/24/23 13:09:25.44
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4123 to expose endpoints map[pod2:[80]] @ 08/24/23 13:09:25.479
  Aug 24 13:09:25.523: INFO: successfully validated that service endpoint-test2 in namespace services-4123 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 08/24/23 13:09:25.525
  E0824 13:09:25.943677      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:26.527: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4123 exec execpod7hjk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug 24 13:09:26.778: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug 24 13:09:26.779: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:09:26.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4123 exec execpod7hjk4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.26.149 80'
  E0824 13:09:26.944725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:27.053: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.26.149 80\nConnection to 10.233.26.149 80 port [tcp/http] succeeded!\n"
  Aug 24 13:09:27.053: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-4123 @ 08/24/23 13:09:27.053
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-4123 to expose endpoints map[] @ 08/24/23 13:09:27.126
  Aug 24 13:09:27.182: INFO: successfully validated that service endpoint-test2 in namespace services-4123 exposes endpoints map[]
  Aug 24 13:09:27.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4123" for this suite. @ 08/24/23 13:09:27.26
• [18.429 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 08/24/23 13:09:27.273
  Aug 24 13:09:27.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:09:27.275
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:09:27.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:09:27.325
  STEP: validating cluster-info @ 08/24/23 13:09:27.329
  Aug 24 13:09:27.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-501 cluster-info'
  Aug 24 13:09:27.471: INFO: stderr: ""
  Aug 24 13:09:27.471: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Aug 24 13:09:27.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-501" for this suite. @ 08/24/23 13:09:27.48
• [0.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 08/24/23 13:09:27.495
  Aug 24 13:09:27.495: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:09:27.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:09:27.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:09:27.548
  STEP: Creating projection with secret that has name projected-secret-test-1bd55f61-6341-4d5a-af16-072094d316da @ 08/24/23 13:09:27.555
  STEP: Creating a pod to test consume secrets @ 08/24/23 13:09:27.566
  E0824 13:09:27.945599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:28.947810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:29.948249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:30.948519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:09:31.649
  Aug 24 13:09:31.656: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-secrets-9e1ebd8f-8b18-457e-aa3b-27d3b8c08a97 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 13:09:31.681
  Aug 24 13:09:31.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4846" for this suite. @ 08/24/23 13:09:31.729
• [4.250 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 08/24/23 13:09:31.747
  Aug 24 13:09:31.747: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-webhook @ 08/24/23 13:09:31.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:09:31.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:09:31.782
  STEP: Setting up server cert @ 08/24/23 13:09:31.785
  E0824 13:09:31.948640      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/24/23 13:09:32.22
  STEP: Deploying the custom resource conversion webhook pod @ 08/24/23 13:09:32.237
  STEP: Wait for the deployment to be ready @ 08/24/23 13:09:32.259
  Aug 24 13:09:32.275: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0824 13:09:32.949823      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:33.949699      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:34.298: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 24, 13, 9, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 9, 32, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 9, 32, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 9, 32, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 13:09:34.949946      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:35.959091      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:09:36.309
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:09:36.332
  E0824 13:09:36.954002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:37.333: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug 24 13:09:37.340: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 13:09:37.954922      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:38.954986      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:39.955999      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 08/24/23 13:09:39.973
  STEP: Create a v2 custom resource @ 08/24/23 13:09:40.008
  STEP: List CRs in v1 @ 08/24/23 13:09:40.25
  STEP: List CRs in v2 @ 08/24/23 13:09:40.263
  Aug 24 13:09:40.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-4153" for this suite. @ 08/24/23 13:09:40.906
• [9.206 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  E0824 13:09:40.955922      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a kubernetes client @ 08/24/23 13:09:40.956
  Aug 24 13:09:40.956: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 13:09:40.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:09:41.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:09:41.013
  STEP: Creating secret with name secret-test-map-d2177a35-64bf-4d9d-8cdc-ca429576dcd7 @ 08/24/23 13:09:41.027
  STEP: Creating a pod to test consume secrets @ 08/24/23 13:09:41.035
  E0824 13:09:41.957309      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:42.957556      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:43.958301      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:44.958812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:09:45.095
  Aug 24 13:09:45.103: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-secrets-61416b98-64db-4461-9fb3-6df1e057dd40 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/24/23 13:09:45.118
  Aug 24 13:09:45.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1827" for this suite. @ 08/24/23 13:09:45.17
• [4.228 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 08/24/23 13:09:45.21
  Aug 24 13:09:45.210: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 13:09:45.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:09:45.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:09:45.313
  STEP: Setting up server cert @ 08/24/23 13:09:45.364
  E0824 13:09:45.959441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 13:09:46.511
  STEP: Deploying the webhook pod @ 08/24/23 13:09:46.522
  STEP: Wait for the deployment to be ready @ 08/24/23 13:09:46.547
  Aug 24 13:09:46.561: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0824 13:09:46.959572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:47.960612      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:09:48.583
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:09:48.6
  E0824 13:09:48.961238      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:09:49.601: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/24/23 13:09:49.607
  STEP: create a pod @ 08/24/23 13:09:49.64
  E0824 13:09:49.961688      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:50.964427      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 08/24/23 13:09:51.674
  Aug 24 13:09:51.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=webhook-3695 attach --namespace=webhook-3695 to-be-attached-pod -i -c=container1'
  Aug 24 13:09:51.863: INFO: rc: 1
  Aug 24 13:09:51.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 13:09:51.962470      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-3695" for this suite. @ 08/24/23 13:09:52.099
  STEP: Destroying namespace "webhook-markers-5790" for this suite. @ 08/24/23 13:09:52.127
• [6.941 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 08/24/23 13:09:52.167
  Aug 24 13:09:52.167: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 13:09:52.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:09:52.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:09:52.206
  STEP: Counting existing ResourceQuota @ 08/24/23 13:09:52.21
  E0824 13:09:52.963713      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:53.964334      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:54.965162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:55.965271      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:56.966727      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 13:09:57.242
  STEP: Ensuring resource quota status is calculated @ 08/24/23 13:09:57.261
  E0824 13:09:57.966691      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:09:58.969557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 08/24/23 13:09:59.271
  STEP: Ensuring resource quota status captures replication controller creation @ 08/24/23 13:09:59.293
  E0824 13:09:59.970056      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:00.970225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 08/24/23 13:10:01.302
  STEP: Ensuring resource quota status released usage @ 08/24/23 13:10:01.316
  E0824 13:10:01.971136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:02.971629      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:03.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3987" for this suite. @ 08/24/23 13:10:03.343
• [11.190 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 08/24/23 13:10:03.358
  Aug 24 13:10:03.358: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename watch @ 08/24/23 13:10:03.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:03.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:03.415
  STEP: creating a new configmap @ 08/24/23 13:10:03.422
  STEP: modifying the configmap once @ 08/24/23 13:10:03.43
  STEP: modifying the configmap a second time @ 08/24/23 13:10:03.453
  STEP: deleting the configmap @ 08/24/23 13:10:03.472
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 08/24/23 13:10:03.483
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 08/24/23 13:10:03.487
  Aug 24 13:10:03.487: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8132  5854386b-15a1-4a55-815a-1254a8e6fced 35294 0 2023-08-24 13:10:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 13:10:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 13:10:03.489: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-8132  5854386b-15a1-4a55-815a-1254a8e6fced 35295 0 2023-08-24 13:10:03 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-24 13:10:03 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug 24 13:10:03.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8132" for this suite. @ 08/24/23 13:10:03.505
• [0.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 08/24/23 13:10:03.529
  Aug 24 13:10:03.530: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename deployment @ 08/24/23 13:10:03.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:03.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:03.56
  Aug 24 13:10:03.595: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0824 13:10:03.971928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:04.972278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:05.972773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:06.972956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:07.973429      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:08.608: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 13:10:08.609
  Aug 24 13:10:08.609: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0824 13:10:08.973494      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:09.974053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:10.619: INFO: Creating deployment "test-rollover-deployment"
  Aug 24 13:10:10.648: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0824 13:10:10.975113      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:11.975477      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:12.666: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Aug 24 13:10:12.684: INFO: Ensure that both replica sets have 1 created replica
  Aug 24 13:10:12.700: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Aug 24 13:10:12.723: INFO: Updating deployment test-rollover-deployment
  Aug 24 13:10:12.724: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0824 13:10:12.976990      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:13.977597      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:14.747: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Aug 24 13:10:14.771: INFO: Make sure deployment "test-rollover-deployment" is complete
  Aug 24 13:10:14.796: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 13:10:14.796: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 13:10:14.978193      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:15.978915      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:16.816: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 13:10:16.817: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 13:10:16.978876      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:17.979553      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:18.814: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 13:10:18.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 13:10:18.979985      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:19.980645      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:20.816: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 13:10:20.816: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 13:10:20.980890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:21.981662      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:22.814: INFO: all replica sets need to contain the pod-template-hash label
  Aug 24 13:10:22.815: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 24, 13, 10, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 24, 13, 10, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0824 13:10:22.982434      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:23.983124      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:24.810: INFO: 
  Aug 24 13:10:24.810: INFO: Ensure that both old replica sets have no replicas
  Aug 24 13:10:24.837: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6858  46f5e2b0-3b7c-4cd2-b890-33f91e44e860 35433 2 2023-08-24 13:10:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 13:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:10:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b35fc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-24 13:10:10 +0000 UTC,LastTransitionTime:2023-08-24 13:10:10 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-08-24 13:10:24 +0000 UTC,LastTransitionTime:2023-08-24 13:10:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug 24 13:10:24.846: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-6858  6f8cf2f9-4874-4089-888f-021786e262b7 35423 2 2023-08-24 13:10:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 46f5e2b0-3b7c-4cd2-b890-33f91e44e860 0xc005c12497 0xc005c12498}] [] [{kube-controller-manager Update apps/v1 2023-08-24 13:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46f5e2b0-3b7c-4cd2-b890-33f91e44e860\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:10:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c12548 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 13:10:24.846: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Aug 24 13:10:24.846: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6858  d3fa0d91-349d-41d5-bfa8-a74c7cbdf70f 35432 2 2023-08-24 13:10:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 46f5e2b0-3b7c-4cd2-b890-33f91e44e860 0xc005c12367 0xc005c12368}] [] [{e2e.test Update apps/v1 2023-08-24 13:10:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:10:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46f5e2b0-3b7c-4cd2-b890-33f91e44e860\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:10:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005c12428 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 13:10:24.846: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-6858  7b587836-9f12-49ed-b4f9-7c1418ef393d 35381 2 2023-08-24 13:10:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 46f5e2b0-3b7c-4cd2-b890-33f91e44e860 0xc005c125b7 0xc005c125b8}] [] [{kube-controller-manager Update apps/v1 2023-08-24 13:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"46f5e2b0-3b7c-4cd2-b890-33f91e44e860\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:10:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005c12668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 13:10:24.855: INFO: Pod "test-rollover-deployment-57777854c9-2ldxj" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-2ldxj test-rollover-deployment-57777854c9- deployment-6858  1042b07d-dce3-41db-b65c-370510c6a21e 35398 0 2023-08-24 13:10:12 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 6f8cf2f9-4874-4089-888f-021786e262b7 0xc005c12ba7 0xc005c12ba8}] [] [{kube-controller-manager Update v1 2023-08-24 13:10:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6f8cf2f9-4874-4089-888f-021786e262b7\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:10:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.225\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rr5vq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rr5vq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:10:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:10:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:10:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:10.233.66.225,StartTime:2023-08-24 13:10:12 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:10:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://c25d7fc4375243edb05b292bd97a90e88af5cc38d49125065dd99321eedb5e7f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.225,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:10:24.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6858" for this suite. @ 08/24/23 13:10:24.867
• [21.353 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 08/24/23 13:10:24.894
  Aug 24 13:10:24.895: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 13:10:24.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:24.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:24.975
  E0824 13:10:24.983303      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating service in namespace services-5371 @ 08/24/23 13:10:24.984
  STEP: creating service affinity-nodeport-transition in namespace services-5371 @ 08/24/23 13:10:24.984
  STEP: creating replication controller affinity-nodeport-transition in namespace services-5371 @ 08/24/23 13:10:25.015
  I0824 13:10:25.033706      15 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-5371, replica count: 3
  E0824 13:10:25.986897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:26.987599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:27.987753      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 13:10:28.086165      15 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 13:10:28.987913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:29.988148      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:30.988766      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 13:10:31.086631      15 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 13:10:31.115: INFO: Creating new exec pod
  E0824 13:10:31.996952      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:32.997135      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:33.997139      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:34.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5371 exec execpod-affinitybj485 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Aug 24 13:10:34.524: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Aug 24 13:10:34.524: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:10:34.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5371 exec execpod-affinitybj485 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.30.24 80'
  Aug 24 13:10:34.855: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.30.24 80\nConnection to 10.233.30.24 80 port [tcp/http] succeeded!\n"
  Aug 24 13:10:34.855: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:10:34.856: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5371 exec execpod-affinitybj485 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.64 30397'
  E0824 13:10:34.997857      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:35.153: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.64 30397\nConnection to 192.168.121.64 30397 port [tcp/*] succeeded!\n"
  Aug 24 13:10:35.153: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:10:35.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5371 exec execpod-affinitybj485 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.15 30397'
  Aug 24 13:10:35.471: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.15 30397\nConnection to 192.168.121.15 30397 port [tcp/*] succeeded!\n"
  Aug 24 13:10:35.471: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug 24 13:10:35.490: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5371 exec execpod-affinitybj485 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.64:30397/ ; done'
  E0824 13:10:35.998431      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:36.110: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n"
  Aug 24 13:10:36.110: INFO: stdout: "\naffinity-nodeport-transition-lstfv\naffinity-nodeport-transition-lstfv\naffinity-nodeport-transition-lstfv\naffinity-nodeport-transition-h7q54\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-h7q54\naffinity-nodeport-transition-lstfv\naffinity-nodeport-transition-lstfv\naffinity-nodeport-transition-lstfv\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-h7q54\naffinity-nodeport-transition-h7q54\naffinity-nodeport-transition-lstfv\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-lstfv"
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-lstfv
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-lstfv
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-lstfv
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-h7q54
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-h7q54
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-lstfv
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-lstfv
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-lstfv
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-h7q54
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-h7q54
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-lstfv
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.114: INFO: Received response from host: affinity-nodeport-transition-lstfv
  Aug 24 13:10:36.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5371 exec execpod-affinitybj485 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.64:30397/ ; done'
  Aug 24 13:10:36.792: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.64:30397/\n"
  Aug 24 13:10:36.792: INFO: stdout: "\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx\naffinity-nodeport-transition-vd4fx"
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.792: INFO: Received response from host: affinity-nodeport-transition-vd4fx
  Aug 24 13:10:36.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 13:10:36.802: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5371, will wait for the garbage collector to delete the pods @ 08/24/23 13:10:36.831
  Aug 24 13:10:36.952: INFO: Deleting ReplicationController affinity-nodeport-transition took: 55.075602ms
  E0824 13:10:36.998432      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:10:37.053: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.952324ms
  E0824 13:10:37.998842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:38.999764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5371" for this suite. @ 08/24/23 13:10:39.549
• [14.677 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 08/24/23 13:10:39.61
  Aug 24 13:10:39.611: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 13:10:39.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:10:39.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:10:39.664
  STEP: Creating secret with name s-test-opt-del-92b7a714-a551-4d76-956a-3171474fbc5e @ 08/24/23 13:10:39.685
  STEP: Creating secret with name s-test-opt-upd-02ab46ad-b097-41e5-8312-883034a7a68c @ 08/24/23 13:10:39.699
  STEP: Creating the pod @ 08/24/23 13:10:39.71
  E0824 13:10:40.000171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:41.000718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-92b7a714-a551-4d76-956a-3171474fbc5e @ 08/24/23 13:10:41.818
  STEP: Updating secret s-test-opt-upd-02ab46ad-b097-41e5-8312-883034a7a68c @ 08/24/23 13:10:41.837
  STEP: Creating secret with name s-test-opt-create-12dec19c-37f6-4bb7-a31b-35a2ea78c196 @ 08/24/23 13:10:41.857
  STEP: waiting to observe update in volume @ 08/24/23 13:10:41.87
  E0824 13:10:42.001864      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:43.002712      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:44.003326      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:45.003592      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:46.004406      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:47.004215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:48.004871      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:49.005771      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:50.006216      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:51.007908      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:52.008349      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:53.009381      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:54.009605      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:55.010173      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:56.010532      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:57.011126      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:58.011421      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:10:59.011444      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:00.011678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:01.012307      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:02.012737      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:03.013344      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:04.013577      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:05.013718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:06.014096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:07.014736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:08.014957      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:09.015098      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:10.016454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:11.016374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:12.016424      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:13.016652      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:14.017110      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:15.017328      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:16.017901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:17.019378      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:18.019919      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:19.020736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:20.020923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:21.021130      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:22.021756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:23.022254      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:24.022793      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:25.022904      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:26.023078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:27.023584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:28.023762      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:29.023939      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:30.024309      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:31.026296      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:32.025032      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:33.025114      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:34.025729      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:35.026151      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:36.026168      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:37.026698      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:38.027189      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:39.027860      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:40.028345      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:41.029446      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:42.031399      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:43.031129      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:44.031783      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:45.031756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:46.031852      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:47.032641      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:48.033718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:49.034175      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:50.034268      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:51.034584      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:52.034949      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:53.035633      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:54.036225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:55.036309      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:56.036841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:57.037288      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:58.037751      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:11:59.037938      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:00.039096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:01.038998      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:02.039798      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:03.039929      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:04.040115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:05.041534      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:06.041704      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:07.042516      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:08.042619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:09.043263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:10.043492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:12:10.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3491" for this suite. @ 08/24/23 13:12:10.892
• [91.297 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 08/24/23 13:12:10.908
  Aug 24 13:12:10.908: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename endpointslice @ 08/24/23 13:12:10.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:12:10.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:12:10.956
  E0824 13:12:11.047953      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:12.050850      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:13.052756      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:14.052313      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:15.052927      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:16.053825      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 08/24/23 13:12:16.118
  E0824 13:12:17.058265      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:18.055488      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:19.054865      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:20.055033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:21.055270      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 08/24/23 13:12:21.136
  E0824 13:12:22.055489      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:23.056215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:24.056485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:25.056916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:26.056866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 08/24/23 13:12:26.151
  E0824 13:12:27.057185      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:28.057678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:29.058054      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:30.058854      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:31.058936      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 08/24/23 13:12:31.165
  Aug 24 13:12:31.211: INFO: EndpointSlice for Service endpointslice-8423/example-named-port not found
  E0824 13:12:32.059258      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:33.059465      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:34.059503      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:35.072336      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:36.072545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:37.073150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:38.072935      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:39.074118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:40.074035      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:41.074216      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:12:41.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8423" for this suite. @ 08/24/23 13:12:41.255
• [30.363 seconds]
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 08/24/23 13:12:41.272
  Aug 24 13:12:41.272: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename endpointslicemirroring @ 08/24/23 13:12:41.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:12:41.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:12:41.317
  STEP: mirroring a new custom Endpoint @ 08/24/23 13:12:41.354
  Aug 24 13:12:41.371: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0824 13:12:42.075263      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:43.075386      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 08/24/23 13:12:43.379
  Aug 24 13:12:43.394: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0824 13:12:44.075720      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:45.075868      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 08/24/23 13:12:45.404
  Aug 24 13:12:45.424: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0824 13:12:46.076047      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:47.076706      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:12:47.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-9730" for this suite. @ 08/24/23 13:12:47.442
• [6.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 08/24/23 13:12:47.467
  Aug 24 13:12:47.467: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 13:12:47.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:12:47.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:12:47.504
  STEP: creating the pod @ 08/24/23 13:12:47.508
  STEP: submitting the pod to kubernetes @ 08/24/23 13:12:47.509
  W0824 13:12:47.525161      15 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0824 13:12:48.077053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:49.076904      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 08/24/23 13:12:49.558
  STEP: updating the pod @ 08/24/23 13:12:49.567
  E0824 13:12:50.077536      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:12:50.089: INFO: Successfully updated pod "pod-update-activedeadlineseconds-4b5b4f20-6af8-4946-9c68-bf2e12aaf0d1"
  E0824 13:12:51.078639      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:52.080521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:53.083154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:54.083414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:12:54.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4549" for this suite. @ 08/24/23 13:12:54.156
• [6.701 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 08/24/23 13:12:54.175
  Aug 24 13:12:54.175: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 13:12:54.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:12:54.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:12:54.217
  STEP: creating a Service @ 08/24/23 13:12:54.231
  STEP: watching for the Service to be added @ 08/24/23 13:12:54.253
  Aug 24 13:12:54.258: INFO: Found Service test-service-hppj2 in namespace services-6351 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Aug 24 13:12:54.259: INFO: Service test-service-hppj2 created
  STEP: Getting /status @ 08/24/23 13:12:54.26
  Aug 24 13:12:54.280: INFO: Service test-service-hppj2 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 08/24/23 13:12:54.28
  STEP: watching for the Service to be patched @ 08/24/23 13:12:54.291
  Aug 24 13:12:54.296: INFO: observed Service test-service-hppj2 in namespace services-6351 with annotations: map[] & LoadBalancer: {[]}
  Aug 24 13:12:54.296: INFO: Found Service test-service-hppj2 in namespace services-6351 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Aug 24 13:12:54.297: INFO: Service test-service-hppj2 has service status patched
  STEP: updating the ServiceStatus @ 08/24/23 13:12:54.297
  Aug 24 13:12:54.322: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 08/24/23 13:12:54.323
  Aug 24 13:12:54.326: INFO: Observed Service test-service-hppj2 in namespace services-6351 with annotations: map[] & Conditions: {[]}
  Aug 24 13:12:54.327: INFO: Observed event: &Service{ObjectMeta:{test-service-hppj2  services-6351  3014451e-d413-4e28-af44-9f46b264fa9d 36116 0 2023-08-24 13:12:54 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-24 13:12:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-24 13:12:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.52.215,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.52.215],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Aug 24 13:12:54.327: INFO: Found Service test-service-hppj2 in namespace services-6351 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug 24 13:12:54.328: INFO: Service test-service-hppj2 has service status updated
  STEP: patching the service @ 08/24/23 13:12:54.328
  STEP: watching for the Service to be patched @ 08/24/23 13:12:54.351
  Aug 24 13:12:54.355: INFO: observed Service test-service-hppj2 in namespace services-6351 with labels: map[test-service-static:true]
  Aug 24 13:12:54.355: INFO: observed Service test-service-hppj2 in namespace services-6351 with labels: map[test-service-static:true]
  Aug 24 13:12:54.355: INFO: observed Service test-service-hppj2 in namespace services-6351 with labels: map[test-service-static:true]
  Aug 24 13:12:54.355: INFO: Found Service test-service-hppj2 in namespace services-6351 with labels: map[test-service:patched test-service-static:true]
  Aug 24 13:12:54.355: INFO: Service test-service-hppj2 patched
  STEP: deleting the service @ 08/24/23 13:12:54.356
  STEP: watching for the Service to be deleted @ 08/24/23 13:12:54.384
  Aug 24 13:12:54.389: INFO: Observed event: ADDED
  Aug 24 13:12:54.389: INFO: Observed event: MODIFIED
  Aug 24 13:12:54.390: INFO: Observed event: MODIFIED
  Aug 24 13:12:54.390: INFO: Observed event: MODIFIED
  Aug 24 13:12:54.391: INFO: Found Service test-service-hppj2 in namespace services-6351 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Aug 24 13:12:54.391: INFO: Service test-service-hppj2 deleted
  Aug 24 13:12:54.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6351" for this suite. @ 08/24/23 13:12:54.403
• [0.243 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 08/24/23 13:12:54.421
  Aug 24 13:12:54.421: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 13:12:54.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:12:54.453
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:12:54.457
  STEP: creating a Pod with a static label @ 08/24/23 13:12:54.473
  STEP: watching for Pod to be ready @ 08/24/23 13:12:54.49
  Aug 24 13:12:54.494: INFO: observed Pod pod-test in namespace pods-2138 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Aug 24 13:12:54.502: INFO: observed Pod pod-test in namespace pods-2138 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 13:12:54 +0000 UTC  }]
  Aug 24 13:12:54.527: INFO: observed Pod pod-test in namespace pods-2138 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 13:12:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 13:12:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-24 13:12:54 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 13:12:54 +0000 UTC  }]
  E0824 13:12:55.084764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:56.084750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:12:56.108: INFO: Found Pod pod-test in namespace pods-2138 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 13:12:54 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 13:12:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 13:12:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-24 13:12:54 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 08/24/23 13:12:56.119
  STEP: getting the Pod and ensuring that it's patched @ 08/24/23 13:12:56.147
  STEP: replacing the Pod's status Ready condition to False @ 08/24/23 13:12:56.171
  STEP: check the Pod again to ensure its Ready conditions are False @ 08/24/23 13:12:56.2
  STEP: deleting the Pod via a Collection with a LabelSelector @ 08/24/23 13:12:56.2
  STEP: watching for the Pod to be deleted @ 08/24/23 13:12:56.218
  Aug 24 13:12:56.221: INFO: observed event type MODIFIED
  E0824 13:12:57.084705      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:12:58.084967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:12:58.126: INFO: observed event type MODIFIED
  Aug 24 13:12:58.257: INFO: observed event type MODIFIED
  E0824 13:12:59.085984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:12:59.130: INFO: observed event type MODIFIED
  Aug 24 13:12:59.162: INFO: observed event type MODIFIED
  Aug 24 13:12:59.182: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2138" for this suite. @ 08/24/23 13:12:59.192
• [4.788 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 08/24/23 13:12:59.211
  Aug 24 13:12:59.212: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:12:59.214
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:12:59.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:12:59.246
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:12:59.251
  E0824 13:13:00.090472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:01.088052      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:02.088342      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:03.088821      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:13:03.294
  Aug 24 13:13:03.299: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-1bce6d3b-bf08-422b-9191-c0548720aaac container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:13:03.311
  Aug 24 13:13:03.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6337" for this suite. @ 08/24/23 13:13:03.356
• [4.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 08/24/23 13:13:03.377
  Aug 24 13:13:03.377: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:13:03.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:13:03.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:13:03.415
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:13:03.419
  E0824 13:13:04.094720      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:05.094729      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:06.094907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:07.095119      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:13:07.46
  Aug 24 13:13:07.468: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-4683cdae-606a-4026-82c1-3154abe5f508 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:13:07.483
  Aug 24 13:13:07.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2078" for this suite. @ 08/24/23 13:13:07.526
• [4.166 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 08/24/23 13:13:07.548
  Aug 24 13:13:07.548: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubelet-test @ 08/24/23 13:13:07.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:13:07.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:13:07.583
  STEP: Waiting for pod completion @ 08/24/23 13:13:07.604
  E0824 13:13:08.095761      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:09.096594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:10.096747      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:11.097840      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:13:11.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4099" for this suite. @ 08/24/23 13:13:11.666
• [4.130 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 08/24/23 13:13:11.68
  Aug 24 13:13:11.680: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 13:13:11.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:13:11.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:13:11.714
  STEP: Creating service test in namespace statefulset-9153 @ 08/24/23 13:13:11.72
  STEP: Creating a new StatefulSet @ 08/24/23 13:13:11.73
  Aug 24 13:13:11.755: INFO: Found 0 stateful pods, waiting for 3
  E0824 13:13:12.098250      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:13.098711      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:14.099675      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:15.100334      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:16.100428      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:17.100545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:18.100714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:19.100884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:20.101208      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:21.101338      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:13:21.766: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 13:13:21.766: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 13:13:21.767: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Aug 24 13:13:21.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-9153 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0824 13:13:22.101741      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:13:22.173: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 13:13:22.173: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 13:13:22.173: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0824 13:13:23.102732      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:24.102850      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:25.103582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:26.104027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:27.104278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:28.105351      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:29.104668      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:30.108564      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:31.107790      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:32.106548      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/24/23 13:13:32.212
  Aug 24 13:13:32.250: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/24/23 13:13:32.251
  E0824 13:13:33.106860      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:34.107913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:35.109419      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:36.108912      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:37.109308      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:38.109556      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:39.110539      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:40.111134      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:41.111597      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:42.111951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 08/24/23 13:13:42.297
  Aug 24 13:13:42.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-9153 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:13:42.709: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 13:13:42.710: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 13:13:42.710: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0824 13:13:43.113324      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:44.114870      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:45.115454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:46.114257      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:47.114555      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:48.116273      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:49.115872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:50.116671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:51.117441      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:52.142740      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 08/24/23 13:13:52.789
  Aug 24 13:13:52.790: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-9153 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug 24 13:13:53.114: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug 24 13:13:53.115: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug 24 13:13:53.115: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0824 13:13:53.143029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:54.143173      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:55.143394      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:56.143621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:57.143847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:58.143937      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:13:59.144286      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:00.144464      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:01.145374      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:02.145679      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:03.145595      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:03.185: INFO: Updating stateful set ss2
  E0824 13:14:04.146140      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:05.146154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:06.147017      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:07.148439      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:08.148108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:09.148838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:10.149848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:11.150890      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:12.151130      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:13.151319      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 08/24/23 13:14:13.265
  Aug 24 13:14:13.279: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=statefulset-9153 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug 24 13:14:13.647: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug 24 13:14:13.647: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug 24 13:14:13.647: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0824 13:14:14.151945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:15.152103      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:16.152422      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:17.152621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:18.153064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:19.153182      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:20.153535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:21.154179      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:22.154994      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:23.154957      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:23.697: INFO: Deleting all statefulset in ns statefulset-9153
  Aug 24 13:14:23.703: INFO: Scaling statefulset ss2 to 0
  E0824 13:14:24.155020      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:25.155915      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:26.156657      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:27.157468      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:28.158022      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:29.159223      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:30.159536      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:31.159655      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:32.160167      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:33.160279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:33.743: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 13:14:33.749: INFO: Deleting statefulset ss2
  Aug 24 13:14:33.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9153" for this suite. @ 08/24/23 13:14:33.805
• [82.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 08/24/23 13:14:33.852
  Aug 24 13:14:33.853: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/24/23 13:14:33.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:14:33.898
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:14:33.904
  Aug 24 13:14:33.912: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 13:14:34.160984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:35.161789      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/24/23 13:14:36.019
  Aug 24 13:14:36.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7026 --namespace=crd-publish-openapi-7026 create -f -'
  E0824 13:14:36.162093      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:37.162619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:37.572: INFO: stderr: ""
  Aug 24 13:14:37.572: INFO: stdout: "e2e-test-crd-publish-openapi-5407-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug 24 13:14:37.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7026 --namespace=crd-publish-openapi-7026 delete e2e-test-crd-publish-openapi-5407-crds test-cr'
  Aug 24 13:14:37.875: INFO: stderr: ""
  Aug 24 13:14:37.875: INFO: stdout: "e2e-test-crd-publish-openapi-5407-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Aug 24 13:14:37.876: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7026 --namespace=crd-publish-openapi-7026 apply -f -'
  E0824 13:14:38.163307      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:39.039: INFO: stderr: ""
  Aug 24 13:14:39.039: INFO: stdout: "e2e-test-crd-publish-openapi-5407-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug 24 13:14:39.039: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7026 --namespace=crd-publish-openapi-7026 delete e2e-test-crd-publish-openapi-5407-crds test-cr'
  E0824 13:14:39.163701      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:39.233: INFO: stderr: ""
  Aug 24 13:14:39.233: INFO: stdout: "e2e-test-crd-publish-openapi-5407-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/24/23 13:14:39.233
  Aug 24 13:14:39.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=crd-publish-openapi-7026 explain e2e-test-crd-publish-openapi-5407-crds'
  Aug 24 13:14:39.838: INFO: stderr: ""
  Aug 24 13:14:39.838: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-5407-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0824 13:14:40.163847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:41.170041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:42.171150      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:42.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7026" for this suite. @ 08/24/23 13:14:42.731
• [8.896 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 08/24/23 13:14:42.749
  Aug 24 13:14:42.749: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 13:14:42.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:14:42.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:14:42.81
  STEP: creating the pod @ 08/24/23 13:14:42.818
  STEP: setting up watch @ 08/24/23 13:14:42.819
  STEP: submitting the pod to kubernetes @ 08/24/23 13:14:42.928
  STEP: verifying the pod is in kubernetes @ 08/24/23 13:14:42.951
  STEP: verifying pod creation was observed @ 08/24/23 13:14:42.962
  E0824 13:14:43.171228      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:44.171901      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 08/24/23 13:14:44.992
  STEP: verifying pod deletion was observed @ 08/24/23 13:14:45.01
  E0824 13:14:45.172458      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:46.173168      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:46.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6445" for this suite. @ 08/24/23 13:14:46.688
• [3.948 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 08/24/23 13:14:46.698
  Aug 24 13:14:46.698: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename pods @ 08/24/23 13:14:46.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:14:46.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:14:46.738
  E0824 13:14:47.173782      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:48.178984      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:49.176194      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:50.176602      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:51.176821      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:52.177499      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:14:52.883
  Aug 24 13:14:52.893: INFO: Trying to get logs from node quohp9aeph3i-3 pod client-envvars-66659251-5108-4f62-914f-6d3ccb923d3c container env3cont: <nil>
  STEP: delete the pod @ 08/24/23 13:14:52.946
  Aug 24 13:14:52.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1090" for this suite. @ 08/24/23 13:14:52.984
• [6.305 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 08/24/23 13:14:53.009
  Aug 24 13:14:53.010: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 13:14:53.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:14:53.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:14:53.057
  STEP: creating a collection of services @ 08/24/23 13:14:53.062
  Aug 24 13:14:53.062: INFO: Creating e2e-svc-a-b4j62
  Aug 24 13:14:53.081: INFO: Creating e2e-svc-b-dnrgd
  Aug 24 13:14:53.105: INFO: Creating e2e-svc-c-hbnns
  STEP: deleting service collection @ 08/24/23 13:14:53.147
  E0824 13:14:53.178302      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:53.271: INFO: Collection of services has been deleted
  Aug 24 13:14:53.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6168" for this suite. @ 08/24/23 13:14:53.28
• [0.285 seconds]
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 08/24/23 13:14:53.295
  Aug 24 13:14:53.295: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename init-container @ 08/24/23 13:14:53.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:14:53.325
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:14:53.33
  STEP: creating the pod @ 08/24/23 13:14:53.334
  Aug 24 13:14:53.334: INFO: PodSpec: initContainers in spec.initContainers
  E0824 13:14:54.178900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:55.178950      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:56.179076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:57.179610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:14:57.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-8590" for this suite. @ 08/24/23 13:14:57.822
• [4.545 seconds]
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 08/24/23 13:14:57.84
  Aug 24 13:14:57.840: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename containers @ 08/24/23 13:14:57.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:14:57.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:14:57.879
  STEP: Creating a pod to test override all @ 08/24/23 13:14:57.884
  E0824 13:14:58.180834      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:14:59.180755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:00.181639      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:01.182701      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:15:01.937
  Aug 24 13:15:01.944: INFO: Trying to get logs from node quohp9aeph3i-3 pod client-containers-7c95547c-5ab7-44da-959f-0f6e3c05dcc3 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 13:15:01.957
  Aug 24 13:15:01.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2342" for this suite. @ 08/24/23 13:15:02.006
• [4.180 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 08/24/23 13:15:02.026
  Aug 24 13:15:02.027: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename dns @ 08/24/23 13:15:02.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:15:02.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:15:02.07
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 08/24/23 13:15:02.076
  Aug 24 13:15:02.093: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-7029  4adecc8f-c2a2-4346-8563-f4c50ae31112 37129 0 2023-08-24 13:15:02 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-24 13:15:02 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2vq97,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2vq97,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0824 13:15:02.183334      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:03.183708      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 08/24/23 13:15:04.111
  Aug 24 13:15:04.111: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-7029 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:15:04.111: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 13:15:04.113: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:15:04.113: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-7029/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0824 13:15:04.184358      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS server is configured on pod... @ 08/24/23 13:15:04.293
  Aug 24 13:15:04.294: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-7029 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:15:04.294: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 13:15:04.296: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:15:04.296: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-7029/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug 24 13:15:04.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 13:15:04.490: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-7029" for this suite. @ 08/24/23 13:15:04.527
• [2.513 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 08/24/23 13:15:04.543
  Aug 24 13:15:04.543: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename deployment @ 08/24/23 13:15:04.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:15:04.582
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:15:04.588
  Aug 24 13:15:04.595: INFO: Creating deployment "webserver-deployment"
  Aug 24 13:15:04.609: INFO: Waiting for observed generation 1
  E0824 13:15:05.184572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:06.187088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:06.636: INFO: Waiting for all required pods to come up
  Aug 24 13:15:06.671: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 08/24/23 13:15:06.671
  E0824 13:15:07.190855      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:08.192573      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:09.196952      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:10.196200      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:10.788: INFO: Waiting for deployment "webserver-deployment" to complete
  Aug 24 13:15:10.806: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Aug 24 13:15:10.830: INFO: Updating deployment webserver-deployment
  Aug 24 13:15:10.830: INFO: Waiting for observed generation 2
  E0824 13:15:11.196940      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:12.197201      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:12.858: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Aug 24 13:15:12.865: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Aug 24 13:15:12.871: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug 24 13:15:12.931: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Aug 24 13:15:12.934: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Aug 24 13:15:12.942: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug 24 13:15:12.954: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Aug 24 13:15:12.955: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Aug 24 13:15:12.980: INFO: Updating deployment webserver-deployment
  Aug 24 13:15:12.980: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Aug 24 13:15:13.011: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  E0824 13:15:13.197810      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:14.274619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:15.173: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  E0824 13:15:15.209429      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:15.325: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-5974  e959e409-b7f1-44b7-a692-8e40d46763d2 37472 3 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-24 13:15:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0003a0668 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-24 13:15:13 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-08-24 13:15:13 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Aug 24 13:15:15.344: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-5974  a8150e96-759a-4dc2-9f74-68e4a7d3b17f 37461 3 2023-08-24 13:15:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e959e409-b7f1-44b7-a692-8e40d46763d2 0xc008594627 0xc008594628}] [] [{kube-controller-manager Update apps/v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e959e409-b7f1-44b7-a692-8e40d46763d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0085946c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 13:15:15.345: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Aug 24 13:15:15.347: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-5974  d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 37469 3 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e959e409-b7f1-44b7-a692-8e40d46763d2 0xc008594537 0xc008594538}] [] [{kube-controller-manager Update apps/v1 2023-08-24 13:15:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e959e409-b7f1-44b7-a692-8e40d46763d2\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0085945c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 13:15:15.777: INFO: Pod "webserver-deployment-67bd4bf6dc-468h5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-468h5 webserver-deployment-67bd4bf6dc- deployment-5974  b9e4bb5e-8c74-4695-aa63-58c87e2dec85 37426 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a0a77 0xc0003a0a78}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dn8bk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dn8bk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.785: INFO: Pod "webserver-deployment-67bd4bf6dc-554g6" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-554g6 webserver-deployment-67bd4bf6dc- deployment-5974  267211a2-5e43-4422-885f-72826e64e674 37315 0 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a0c47 0xc0003a0c48}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tr4cf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tr4cf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:10.233.65.133,StartTime:2023-08-24 13:15:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:15:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://80c6f1eccb7daea1cb88decf38f76d394a00373da58dbb1c92a0bac1970ad725,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.133,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.785: INFO: Pod "webserver-deployment-67bd4bf6dc-5v9sq" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5v9sq webserver-deployment-67bd4bf6dc- deployment-5974  4aea2913-274f-4dd7-b88e-e198f8de0a21 37269 0 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a0e37 0xc0003a0e38}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hfpkz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hfpkz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:10.233.64.190,StartTime:2023-08-24 13:15:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:15:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9897df59baec17f2271879ea7c2fa527fb9f299561abeb288d7187c8ccb238c2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.190,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.786: INFO: Pod "webserver-deployment-67bd4bf6dc-86wnq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-86wnq webserver-deployment-67bd4bf6dc- deployment-5974  3ad7ef7f-bdb5-4892-9797-3dc2cb22b438 37485 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a1037 0xc0003a1038}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jtp4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtp4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.787: INFO: Pod "webserver-deployment-67bd4bf6dc-8hjlg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8hjlg webserver-deployment-67bd4bf6dc- deployment-5974  72864e58-361a-4188-9c16-6ed34f813784 37460 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a1217 0xc0003a1218}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cvnrw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cvnrw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.788: INFO: Pod "webserver-deployment-67bd4bf6dc-cxx5d" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cxx5d webserver-deployment-67bd4bf6dc- deployment-5974  57d8011e-a653-48e3-801a-c4a0fc709f83 37489 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a1407 0xc0003a1408}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-287q7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-287q7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.789: INFO: Pod "webserver-deployment-67bd4bf6dc-h2b8p" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h2b8p webserver-deployment-67bd4bf6dc- deployment-5974  e1d3d24b-7988-4d94-a0b2-2c031dc9c10e 37298 0 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a15d7 0xc0003a15d8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.110\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-psz9s,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-psz9s,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:10.233.64.110,StartTime:2023-08-24 13:15:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:15:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://b6cf9d6f25b739ced94967d4baed47a28464251f6df5534c16292eae530a3b0d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.110,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.789: INFO: Pod "webserver-deployment-67bd4bf6dc-hnxqj" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hnxqj webserver-deployment-67bd4bf6dc- deployment-5974  10ed0506-cb3b-4483-9b5b-72770d20c282 37284 0 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a17e7 0xc0003a17e8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mfdnk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mfdnk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:10.233.66.130,StartTime:2023-08-24 13:15:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:15:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c7016a6284c12067918a91ac730d884128ac8b1da0d286bcc61e743fed817e35,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.130,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.792: INFO: Pod "webserver-deployment-67bd4bf6dc-jf9sd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jf9sd webserver-deployment-67bd4bf6dc- deployment-5974  8300e449-0591-44b8-8d1d-d197116b6e28 37487 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a19d7 0xc0003a19d8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrn6n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrn6n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.793: INFO: Pod "webserver-deployment-67bd4bf6dc-k5fl4" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-k5fl4 webserver-deployment-67bd4bf6dc- deployment-5974  4cc1462c-3fd9-4014-ac61-52e63d9639b3 37274 0 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a1bc7 0xc0003a1bc8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.111\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqdxs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqdxs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:10.233.64.111,StartTime:2023-08-24 13:15:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:15:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://bc3e01e03534b4fcfd67fddc2d20c0e6bc350c293766c8bc917b1a9e81449a62,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.111,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.794: INFO: Pod "webserver-deployment-67bd4bf6dc-ktd99" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ktd99 webserver-deployment-67bd4bf6dc- deployment-5974  924a9d8a-9131-420b-a0f1-f520a2885cae 37313 0 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0003a1ec7 0xc0003a1ec8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.6\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tvwj4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tvwj4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:10.233.65.6,StartTime:2023-08-24 13:15:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:15:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://fcebe34a9e85621fc22770581bbeafa0c67520df173d02e52050ee6f64b708d2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.6,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.806: INFO: Pod "webserver-deployment-67bd4bf6dc-m27xd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-m27xd webserver-deployment-67bd4bf6dc- deployment-5974  f6c697fd-52f9-4dfb-9e22-8524773fa742 37433 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0052b60b0 0xc0052b60b1}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v95g2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v95g2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.808: INFO: Pod "webserver-deployment-67bd4bf6dc-n4p56" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-n4p56 webserver-deployment-67bd4bf6dc- deployment-5974  ff4214b1-d6bd-4d98-a637-bfe38b046d4a 37418 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0052b6277 0xc0052b6278}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s2ml9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s2ml9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.809: INFO: Pod "webserver-deployment-67bd4bf6dc-pctb2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pctb2 webserver-deployment-67bd4bf6dc- deployment-5974  1b7cc230-a945-4761-b7f3-2531f4f5555a 37482 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0052b6447 0xc0052b6448}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5jrp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5jrp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.810: INFO: Pod "webserver-deployment-67bd4bf6dc-psw4h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-psw4h webserver-deployment-67bd4bf6dc- deployment-5974  f373a225-c69a-4e34-b772-8cdb332430fc 37449 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0052b6617 0xc0052b6618}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lw7dw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lw7dw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.811: INFO: Pod "webserver-deployment-67bd4bf6dc-rjs9h" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rjs9h webserver-deployment-67bd4bf6dc- deployment-5974  b4099641-a29e-4226-8eef-fa27c93355ce 37258 0 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0052b67e7 0xc0052b67e8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-flbg8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-flbg8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:10.233.66.18,StartTime:2023-08-24 13:15:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:15:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c5e5a12cc5aa6e224f08413127b720301fffa0af4229ed8c86d276b031850090,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.18,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.812: INFO: Pod "webserver-deployment-67bd4bf6dc-rsw76" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rsw76 webserver-deployment-67bd4bf6dc- deployment-5974  51c0a697-887a-45fc-886e-ef49284a46e8 37319 0 2023-08-24 13:15:04 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0052b69d7 0xc0052b69d8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.33\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4cphr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4cphr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:10.233.65.33,StartTime:2023-08-24 13:15:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:15:08 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3c50caccfc9ac9fa4dae89c3b3d2149fced7e03f5a0b1a03cdb8e3b97a13b43f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.33,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.812: INFO: Pod "webserver-deployment-67bd4bf6dc-sq9fx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sq9fx webserver-deployment-67bd4bf6dc- deployment-5974  9a63cfd1-287a-46a6-bd70-47f341d7bc08 37466 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0052b6bc7 0xc0052b6bc8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bxfjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bxfjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.876: INFO: Pod "webserver-deployment-67bd4bf6dc-vc4xq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vc4xq webserver-deployment-67bd4bf6dc- deployment-5974  3edc6423-0987-45ed-8ec8-dc16b9f6aea5 37483 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0052b6d97 0xc0052b6d98}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s827v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s827v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.876: INFO: Pod "webserver-deployment-67bd4bf6dc-zwxsg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-zwxsg webserver-deployment-67bd4bf6dc- deployment-5974  28cb91ea-d67d-44ba-abad-a830cc6f9103 37471 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be 0xc0052b6f67 0xc0052b6f68}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d38bbab7-08f8-48ac-9f2c-fbe0dab1f0be\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rp5sm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rp5sm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.882: INFO: Pod "webserver-deployment-7b75d79cf5-7gq2q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7gq2q webserver-deployment-7b75d79cf5- deployment-5974  28361430-d28b-4e35-bc20-feda61a365af 37352 0 2023-08-24 13:15:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc0052b7137 0xc0052b7138}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-frw69,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-frw69,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.897: INFO: Pod "webserver-deployment-7b75d79cf5-8r7x5" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8r7x5 webserver-deployment-7b75d79cf5- deployment-5974  ea72dc6b-0339-4b03-a749-154647e296bc 37361 0 2023-08-24 13:15:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc0052b7327 0xc0052b7328}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ht75q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ht75q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:,StartTime:2023-08-24 13:15:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.900: INFO: Pod "webserver-deployment-7b75d79cf5-9zw8k" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9zw8k webserver-deployment-7b75d79cf5- deployment-5974  3cd136ed-00bf-463a-a364-b7e77569eb69 37490 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc0052b7517 0xc0052b7518}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7mck,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7mck,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.901: INFO: Pod "webserver-deployment-7b75d79cf5-bqdr9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-bqdr9 webserver-deployment-7b75d79cf5- deployment-5974  1126062a-7aef-4577-9adc-eb92e4c04764 37380 0 2023-08-24 13:15:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc0052b7717 0xc0052b7718}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zsdmf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zsdmf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.901: INFO: Pod "webserver-deployment-7b75d79cf5-hm6p4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hm6p4 webserver-deployment-7b75d79cf5- deployment-5974  2db59904-adb4-4a16-a971-3c2c070e01ef 37434 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc0052b7907 0xc0052b7908}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5hhlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5hhlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.907: INFO: Pod "webserver-deployment-7b75d79cf5-lgh7k" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lgh7k webserver-deployment-7b75d79cf5- deployment-5974  92db8c01-b2ac-4ee7-ab8d-02853aaa6950 37420 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc0052b7ea7 0xc0052b7ea8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tx9p8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tx9p8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.908: INFO: Pod "webserver-deployment-7b75d79cf5-m2hkl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-m2hkl webserver-deployment-7b75d79cf5- deployment-5974  496aac49-2d2e-43b0-b2ee-7c2d7764469d 37383 0 2023-08-24 13:15:11 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc00545a0b7 0xc00545a0b8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4fgv6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4fgv6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:11 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:,StartTime:2023-08-24 13:15:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.908: INFO: Pod "webserver-deployment-7b75d79cf5-nnp5j" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nnp5j webserver-deployment-7b75d79cf5- deployment-5974  8a84add6-78da-4b9d-96f7-a7850d68ad54 37359 0 2023-08-24 13:15:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc00545a2a7 0xc00545a2a8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-msqhr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-msqhr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:,StartTime:2023-08-24 13:15:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.910: INFO: Pod "webserver-deployment-7b75d79cf5-tsxpm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tsxpm webserver-deployment-7b75d79cf5- deployment-5974  49e9611f-39a4-42ea-bb47-b9cd71bb34df 37492 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc00545a4b7 0xc00545a4b8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m6dls,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m6dls,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.911: INFO: Pod "webserver-deployment-7b75d79cf5-w7n6q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-w7n6q webserver-deployment-7b75d79cf5- deployment-5974  5edefe5c-678a-4b3c-9532-cc4a5c1f03fb 37479 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc00545a6b7 0xc00545a6b8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6j9rt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6j9rt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.911: INFO: Pod "webserver-deployment-7b75d79cf5-xdt2q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xdt2q webserver-deployment-7b75d79cf5- deployment-5974  df5e3955-6d7a-4e78-a642-2d382f028fb6 37477 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc00545a8f7 0xc00545a8f8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6xqjl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6xqjl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.19,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.911: INFO: Pod "webserver-deployment-7b75d79cf5-zbfcl" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zbfcl webserver-deployment-7b75d79cf5- deployment-5974  6ea12dad-6895-4497-b314-06807e9fe495 37463 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc00545aae7 0xc00545aae8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7dt8n,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7dt8n,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.912: INFO: Pod "webserver-deployment-7b75d79cf5-zxzz4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zxzz4 webserver-deployment-7b75d79cf5- deployment-5974  e84c9e6a-03fa-44ff-8246-1851bfe0422d 37476 0 2023-08-24 13:15:13 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 a8150e96-759a-4dc2-9f74-68e4a7d3b17f 0xc00545acd7 0xc00545acd8}] [] [{kube-controller-manager Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a8150e96-759a-4dc2-9f74-68e4a7d3b17f\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:15:13 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wxvqf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wxvqf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:15:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.64,PodIP:,StartTime:2023-08-24 13:15:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:15:15.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5974" for this suite. @ 08/24/23 13:15:15.948
• [11.625 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 08/24/23 13:15:16.169
  Aug 24 13:15:16.169: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 13:15:16.303526      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename security-context-test @ 08/24/23 13:15:16.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:15:16.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:15:16.766
  E0824 13:15:17.303535      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:18.303619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:19.304593      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:20.305103      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:21.035: INFO: Got logs for pod "busybox-privileged-false-40bc6510-f431-4261-9c15-32dde0daa650": "ip: RTNETLINK answers: Operation not permitted\n"
  Aug 24 13:15:21.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3205" for this suite. @ 08/24/23 13:15:21.046
• [4.892 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 08/24/23 13:15:21.076
  Aug 24 13:15:21.076: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename dns @ 08/24/23 13:15:21.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:15:21.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:15:21.155
  STEP: Creating a test headless service @ 08/24/23 13:15:21.164
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7950.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7950.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local;sleep 1; done
   @ 08/24/23 13:15:21.188
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7950.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7950.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local;sleep 1; done
   @ 08/24/23 13:15:21.188
  STEP: creating a pod to probe DNS @ 08/24/23 13:15:21.188
  STEP: submitting the pod to kubernetes @ 08/24/23 13:15:21.189
  E0824 13:15:21.308520      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:22.348069      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:23.348503      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:24.349278      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 13:15:25.28
  STEP: looking for the results for each expected name from probers @ 08/24/23 13:15:25.291
  Aug 24 13:15:25.303: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:25.313: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:25.325: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:25.332: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:25.341: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:25.348: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  E0824 13:15:25.349360      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:25.355: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:25.367: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:25.367: INFO: Lookups using dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local]

  E0824 13:15:26.349817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:27.349932      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:28.350573      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:29.350780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:30.350938      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:30.379: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:30.388: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:30.419: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:30.429: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:30.437: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:30.446: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:30.455: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:30.464: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:30.464: INFO: Lookups using dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local]

  E0824 13:15:31.352498      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:32.353078      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:33.353100      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:34.353245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:35.353494      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:35.377: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:35.386: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:35.398: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:35.408: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:35.415: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:35.424: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:35.430: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:35.438: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:35.438: INFO: Lookups using dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local]

  E0824 13:15:36.353885      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:37.356405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:38.356989      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:39.357892      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:40.358551      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:40.380: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:40.394: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:40.404: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:40.411: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:40.419: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:40.425: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:40.433: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:40.443: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:40.443: INFO: Lookups using dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local]

  E0824 13:15:41.359045      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:42.359168      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:43.360036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:44.360191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:45.360401      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:45.378: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:45.388: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:45.397: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:45.407: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:45.417: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:45.427: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:45.438: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:45.447: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:45.447: INFO: Lookups using dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local]

  E0824 13:15:46.362628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:47.362888      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:48.363961      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:49.364585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:50.364743      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:50.377: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:50.385: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:50.392: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:50.400: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:50.407: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:50.417: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:50.426: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:50.438: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local from pod dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf: the server could not find the requested resource (get pods dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf)
  Aug 24 13:15:50.438: INFO: Lookups using dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local wheezy_udp@dns-test-service-2.dns-7950.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7950.svc.cluster.local jessie_udp@dns-test-service-2.dns-7950.svc.cluster.local jessie_tcp@dns-test-service-2.dns-7950.svc.cluster.local]

  E0824 13:15:51.365088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:52.365997      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:53.365621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:54.374029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:55.371388      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:15:55.458: INFO: DNS probes using dns-7950/dns-test-1e36f340-1f1d-49ac-9169-96985788ffaf succeeded

  Aug 24 13:15:55.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:15:55.477
  STEP: deleting the test headless service @ 08/24/23 13:15:55.534
  STEP: Destroying namespace "dns-7950" for this suite. @ 08/24/23 13:15:55.617
• [34.558 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 08/24/23 13:15:55.639
  Aug 24 13:15:55.639: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 13:15:55.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:15:55.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:15:55.697
  STEP: Setting up server cert @ 08/24/23 13:15:55.76
  E0824 13:15:56.372478      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:57.372670      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 13:15:57.496
  STEP: Deploying the webhook pod @ 08/24/23 13:15:57.511
  STEP: Wait for the deployment to be ready @ 08/24/23 13:15:57.535
  Aug 24 13:15:57.557: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 13:15:58.373049      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:15:59.373283      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:15:59.585
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:15:59.613
  E0824 13:16:00.373542      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:00.613: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/24/23 13:16:00.628
  STEP: create a pod that should be denied by the webhook @ 08/24/23 13:16:00.665
  STEP: create a pod that causes the webhook to hang @ 08/24/23 13:16:00.701
  E0824 13:16:01.374609      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:02.375209      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:03.375362      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:04.375564      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:05.375809      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:06.375971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:07.376186      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:08.376967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:09.377121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:10.377591      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 08/24/23 13:16:10.717
  STEP: create a configmap that should be admitted by the webhook @ 08/24/23 13:16:10.771
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/24/23 13:16:10.788
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/24/23 13:16:10.808
  STEP: create a namespace that bypass the webhook @ 08/24/23 13:16:10.82
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 08/24/23 13:16:10.858
  Aug 24 13:16:10.883: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5589" for this suite. @ 08/24/23 13:16:11.078
  STEP: Destroying namespace "webhook-markers-6861" for this suite. @ 08/24/23 13:16:11.13
  STEP: Destroying namespace "exempted-namespace-7712" for this suite. @ 08/24/23 13:16:11.143
• [15.518 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 08/24/23 13:16:11.168
  Aug 24 13:16:11.168: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 13:16:11.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:11.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:11.212
  STEP: Creating namespace "e2e-ns-2sgvp" @ 08/24/23 13:16:11.218
  Aug 24 13:16:11.251: INFO: Namespace "e2e-ns-2sgvp-1519" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-2sgvp-1519" @ 08/24/23 13:16:11.252
  Aug 24 13:16:11.269: INFO: Namespace "e2e-ns-2sgvp-1519" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-2sgvp-1519" @ 08/24/23 13:16:11.269
  Aug 24 13:16:11.295: INFO: Namespace "e2e-ns-2sgvp-1519" has []v1.FinalizerName{"kubernetes"}
  Aug 24 13:16:11.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-335" for this suite. @ 08/24/23 13:16:11.311
  STEP: Destroying namespace "e2e-ns-2sgvp-1519" for this suite. @ 08/24/23 13:16:11.326
• [0.174 seconds]
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 08/24/23 13:16:11.346
  Aug 24 13:16:11.346: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 13:16:11.353
  E0824 13:16:11.377922      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:11.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:11.397
  STEP: Creating ReplicationController "e2e-rc-zjbhk" @ 08/24/23 13:16:11.406
  Aug 24 13:16:11.418: INFO: Get Replication Controller "e2e-rc-zjbhk" to confirm replicas
  E0824 13:16:12.378640      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:12.427: INFO: Get Replication Controller "e2e-rc-zjbhk" to confirm replicas
  Aug 24 13:16:12.445: INFO: Found 1 replicas for "e2e-rc-zjbhk" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-zjbhk" @ 08/24/23 13:16:12.445
  STEP: Updating a scale subresource @ 08/24/23 13:16:12.452
  STEP: Verifying replicas where modified for replication controller "e2e-rc-zjbhk" @ 08/24/23 13:16:12.471
  Aug 24 13:16:12.471: INFO: Get Replication Controller "e2e-rc-zjbhk" to confirm replicas
  E0824 13:16:13.379276      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:13.485: INFO: Get Replication Controller "e2e-rc-zjbhk" to confirm replicas
  Aug 24 13:16:13.492: INFO: Found 2 replicas for "e2e-rc-zjbhk" replication controller
  Aug 24 13:16:13.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5142" for this suite. @ 08/24/23 13:16:13.503
• [2.174 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 08/24/23 13:16:13.535
  Aug 24 13:16:13.536: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:16:13.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:13.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:13.602
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:16:13.613
  E0824 13:16:14.379982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:15.380025      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:16.381470      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:17.380462      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:16:17.659
  Aug 24 13:16:17.667: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-b9834c0a-6e95-4783-bbb6-72da9622ddb3 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:16:17.684
  Aug 24 13:16:17.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-84" for this suite. @ 08/24/23 13:16:17.722
• [4.201 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 08/24/23 13:16:17.739
  Aug 24 13:16:17.739: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename events @ 08/24/23 13:16:17.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:17.773
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:17.778
  STEP: Create set of events @ 08/24/23 13:16:17.783
  STEP: get a list of Events with a label in the current namespace @ 08/24/23 13:16:17.823
  STEP: delete a list of events @ 08/24/23 13:16:17.832
  Aug 24 13:16:17.832: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/24/23 13:16:17.878
  Aug 24 13:16:17.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-2767" for this suite. @ 08/24/23 13:16:17.892
• [0.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 08/24/23 13:16:17.906
  Aug 24 13:16:17.906: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename statefulset @ 08/24/23 13:16:17.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:17.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:17.947
  STEP: Creating service test in namespace statefulset-6284 @ 08/24/23 13:16:17.954
  STEP: Looking for a node to schedule stateful set and pod @ 08/24/23 13:16:17.968
  STEP: Creating pod with conflicting port in namespace statefulset-6284 @ 08/24/23 13:16:17.985
  STEP: Waiting until pod test-pod will start running in namespace statefulset-6284 @ 08/24/23 13:16:18
  E0824 13:16:18.380550      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:19.380673      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-6284 @ 08/24/23 13:16:20.047
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-6284 @ 08/24/23 13:16:20.061
  Aug 24 13:16:20.103: INFO: Observed stateful pod in namespace: statefulset-6284, name: ss-0, uid: 7ce8dbfe-b850-4d24-a611-959f2578424b, status phase: Pending. Waiting for statefulset controller to delete.
  Aug 24 13:16:20.143: INFO: Observed stateful pod in namespace: statefulset-6284, name: ss-0, uid: 7ce8dbfe-b850-4d24-a611-959f2578424b, status phase: Failed. Waiting for statefulset controller to delete.
  Aug 24 13:16:20.166: INFO: Observed stateful pod in namespace: statefulset-6284, name: ss-0, uid: 7ce8dbfe-b850-4d24-a611-959f2578424b, status phase: Failed. Waiting for statefulset controller to delete.
  Aug 24 13:16:20.174: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-6284
  STEP: Removing pod with conflicting port in namespace statefulset-6284 @ 08/24/23 13:16:20.175
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-6284 and will be in running state @ 08/24/23 13:16:20.206
  E0824 13:16:20.381411      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:21.381709      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:22.382505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:23.383322      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:24.384094      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:25.384651      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:26.385087      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:27.385547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:28.385907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:29.386242      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:30.387162      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:31.387721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:32.387841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:33.388606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:34.291: INFO: Deleting all statefulset in ns statefulset-6284
  Aug 24 13:16:34.299: INFO: Scaling statefulset ss to 0
  E0824 13:16:34.389462      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:35.389738      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:36.389721      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:37.389977      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:38.390922      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:39.390967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:40.391282      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:41.391380      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:42.391606      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:43.392288      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:44.337: INFO: Waiting for statefulset status.replicas updated to 0
  Aug 24 13:16:44.342: INFO: Deleting statefulset ss
  Aug 24 13:16:44.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6284" for this suite. @ 08/24/23 13:16:44.379
  E0824 13:16:44.393459      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [26.488 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 08/24/23 13:16:44.397
  Aug 24 13:16:44.397: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 13:16:44.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:44.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:44.437
  STEP: Creating configMap with name configmap-test-upd-762acfaa-f15e-4e93-ad10-c47abab8b0a6 @ 08/24/23 13:16:44.45
  STEP: Creating the pod @ 08/24/23 13:16:44.458
  E0824 13:16:45.393168      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:46.393521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 08/24/23 13:16:46.494
  STEP: Waiting for pod with binary data @ 08/24/23 13:16:46.507
  Aug 24 13:16:46.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8585" for this suite. @ 08/24/23 13:16:46.528
• [2.144 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 08/24/23 13:16:46.545
  Aug 24 13:16:46.545: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename replication-controller @ 08/24/23 13:16:46.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:46.576
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:46.581
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 08/24/23 13:16:46.585
  E0824 13:16:47.394846      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:48.395684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 08/24/23 13:16:48.621
  STEP: Then the orphan pod is adopted @ 08/24/23 13:16:48.628
  E0824 13:16:49.395684      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:16:49.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3624" for this suite. @ 08/24/23 13:16:49.654
• [3.125 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 08/24/23 13:16:49.671
  Aug 24 13:16:49.671: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:16:49.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:49.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:49.705
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:16:49.711
  E0824 13:16:50.396257      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:51.396676      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:52.397909      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:53.398580      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:16:53.748
  Aug 24 13:16:53.753: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-0cb37550-0b50-4983-bdcb-dc819b2d4dbe container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:16:53.764
  Aug 24 13:16:53.791: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2631" for this suite. @ 08/24/23 13:16:53.806
• [4.150 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 08/24/23 13:16:53.824
  Aug 24 13:16:53.824: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename resourcequota @ 08/24/23 13:16:53.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:16:53.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:16:53.87
  STEP: Discovering how many secrets are in namespace by default @ 08/24/23 13:16:53.878
  E0824 13:16:54.398839      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:55.399690      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:56.399929      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:57.400244      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:16:58.401198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 08/24/23 13:16:58.884
  E0824 13:16:59.401402      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:00.401917      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:01.402913      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:02.402851      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:03.402966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/24/23 13:17:03.902
  STEP: Ensuring resource quota status is calculated @ 08/24/23 13:17:03.912
  E0824 13:17:04.403146      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:05.403485      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 08/24/23 13:17:05.921
  STEP: Ensuring resource quota status captures secret creation @ 08/24/23 13:17:05.941
  E0824 13:17:06.403788      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:07.403960      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 08/24/23 13:17:07.948
  STEP: Ensuring resource quota status released usage @ 08/24/23 13:17:07.958
  E0824 13:17:08.404817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:09.404860      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:09.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4668" for this suite. @ 08/24/23 13:17:09.976
• [16.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 08/24/23 13:17:09.993
  Aug 24 13:17:09.993: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl-logs @ 08/24/23 13:17:09.997
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:17:10.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:17:10.034
  STEP: creating an pod @ 08/24/23 13:17:10.045
  Aug 24 13:17:10.046: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-logs-4122 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Aug 24 13:17:10.222: INFO: stderr: ""
  Aug 24 13:17:10.223: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 08/24/23 13:17:10.223
  Aug 24 13:17:10.223: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0824 13:17:10.405929      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:11.408806      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:12.243: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 08/24/23 13:17:12.243
  Aug 24 13:17:12.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-logs-4122 logs logs-generator logs-generator'
  Aug 24 13:17:12.389: INFO: stderr: ""
  Aug 24 13:17:12.389: INFO: stdout: "I0824 13:17:11.221904       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/kwl 575\nI0824 13:17:11.422083       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/sm6 231\nI0824 13:17:11.623096       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vlbh 223\nI0824 13:17:11.822432       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/wzh 547\nI0824 13:17:12.022798       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/hwb 270\nI0824 13:17:12.222072       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/qj4l 347\n"
  STEP: limiting log lines @ 08/24/23 13:17:12.389
  Aug 24 13:17:12.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-logs-4122 logs logs-generator logs-generator --tail=1'
  E0824 13:17:12.408021      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:12.538: INFO: stderr: ""
  Aug 24 13:17:12.538: INFO: stdout: "I0824 13:17:12.422301       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/qwz 479\n"
  Aug 24 13:17:12.538: INFO: got output "I0824 13:17:12.422301       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/qwz 479\n"
  STEP: limiting log bytes @ 08/24/23 13:17:12.538
  Aug 24 13:17:12.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-logs-4122 logs logs-generator logs-generator --limit-bytes=1'
  Aug 24 13:17:12.672: INFO: stderr: ""
  Aug 24 13:17:12.672: INFO: stdout: "I"
  Aug 24 13:17:12.672: INFO: got output "I"
  STEP: exposing timestamps @ 08/24/23 13:17:12.672
  Aug 24 13:17:12.672: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-logs-4122 logs logs-generator logs-generator --tail=1 --timestamps'
  Aug 24 13:17:12.812: INFO: stderr: ""
  Aug 24 13:17:12.812: INFO: stdout: "2023-08-24T13:17:12.622969374Z I0824 13:17:12.622909       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/kvg6 517\n"
  Aug 24 13:17:12.812: INFO: got output "2023-08-24T13:17:12.622969374Z I0824 13:17:12.622909       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/kvg6 517\n"
  STEP: restricting to a time range @ 08/24/23 13:17:12.812
  E0824 13:17:13.408713      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:14.409186      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:15.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-logs-4122 logs logs-generator logs-generator --since=1s'
  E0824 13:17:15.409215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:15.528: INFO: stderr: ""
  Aug 24 13:17:15.528: INFO: stdout: "I0824 13:17:14.623043       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/r77 487\nI0824 13:17:14.822611       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/zhf 475\nI0824 13:17:15.022909       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/bw5 548\nI0824 13:17:15.222460       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/nc76 575\nI0824 13:17:15.422189       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/vxcz 474\n"
  Aug 24 13:17:15.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-logs-4122 logs logs-generator logs-generator --since=24h'
  Aug 24 13:17:15.681: INFO: stderr: ""
  Aug 24 13:17:15.681: INFO: stdout: "I0824 13:17:11.221904       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/kwl 575\nI0824 13:17:11.422083       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/sm6 231\nI0824 13:17:11.623096       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/vlbh 223\nI0824 13:17:11.822432       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/wzh 547\nI0824 13:17:12.022798       1 logs_generator.go:76] 4 GET /api/v1/namespaces/default/pods/hwb 270\nI0824 13:17:12.222072       1 logs_generator.go:76] 5 POST /api/v1/namespaces/ns/pods/qj4l 347\nI0824 13:17:12.422301       1 logs_generator.go:76] 6 GET /api/v1/namespaces/default/pods/qwz 479\nI0824 13:17:12.622909       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/kvg6 517\nI0824 13:17:12.822618       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/rgdg 211\nI0824 13:17:13.023028       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/qb4 590\nI0824 13:17:13.222598       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/fd4 359\nI0824 13:17:13.422020       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/xnql 417\nI0824 13:17:13.622504       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/n8qr 579\nI0824 13:17:13.822960       1 logs_generator.go:76] 13 GET /api/v1/namespaces/kube-system/pods/5q6w 356\nI0824 13:17:14.022466       1 logs_generator.go:76] 14 GET /api/v1/namespaces/ns/pods/ppzn 508\nI0824 13:17:14.222955       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/v2f5 221\nI0824 13:17:14.422461       1 logs_generator.go:76] 16 POST /api/v1/namespaces/default/pods/wgn 314\nI0824 13:17:14.623043       1 logs_generator.go:76] 17 POST /api/v1/namespaces/kube-system/pods/r77 487\nI0824 13:17:14.822611       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/default/pods/zhf 475\nI0824 13:17:15.022909       1 logs_generator.go:76] 19 PUT /api/v1/namespaces/default/pods/bw5 548\nI0824 13:17:15.222460       1 logs_generator.go:76] 20 GET /api/v1/namespaces/default/pods/nc76 575\nI0824 13:17:15.422189       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/vxcz 474\nI0824 13:17:15.622683       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/ns/pods/7hf 597\n"
  Aug 24 13:17:15.682: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-logs-4122 delete pod logs-generator'
  E0824 13:17:16.409516      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:16.587: INFO: stderr: ""
  Aug 24 13:17:16.587: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Aug 24 13:17:16.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-4122" for this suite. @ 08/24/23 13:17:16.597
• [6.618 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 08/24/23 13:17:16.622
  Aug 24 13:17:16.622: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename subpath @ 08/24/23 13:17:16.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:17:16.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:17:16.674
  STEP: Setting up data @ 08/24/23 13:17:16.682
  STEP: Creating pod pod-subpath-test-downwardapi-c9mh @ 08/24/23 13:17:16.698
  STEP: Creating a pod to test atomic-volume-subpath @ 08/24/23 13:17:16.698
  E0824 13:17:17.409834      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:18.411059      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:19.412357      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:20.411814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:21.411967      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:22.412716      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:23.413635      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:24.413773      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:25.414635      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:26.414866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:27.415675      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:28.415910      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:29.416390      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:30.416377      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:31.416647      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:32.417377      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:33.418147      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:34.419033      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:35.420528      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:36.420714      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:37.421492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:38.422225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:39.422634      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:40.423079      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:17:40.871
  Aug 24 13:17:40.876: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-subpath-test-downwardapi-c9mh container test-container-subpath-downwardapi-c9mh: <nil>
  STEP: delete the pod @ 08/24/23 13:17:40.923
  STEP: Deleting pod pod-subpath-test-downwardapi-c9mh @ 08/24/23 13:17:40.946
  Aug 24 13:17:40.946: INFO: Deleting pod "pod-subpath-test-downwardapi-c9mh" in namespace "subpath-875"
  Aug 24 13:17:40.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-875" for this suite. @ 08/24/23 13:17:40.958
• [24.348 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 08/24/23 13:17:40.977
  Aug 24 13:17:40.977: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 13:17:40.98
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:17:41.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:17:41.019
  STEP: creating service nodeport-test with type=NodePort in namespace services-4166 @ 08/24/23 13:17:41.022
  STEP: creating replication controller nodeport-test in namespace services-4166 @ 08/24/23 13:17:41.075
  I0824 13:17:41.084266      15 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-4166, replica count: 2
  E0824 13:17:41.423941      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:42.425216      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:43.426123      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 13:17:44.136670      15 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 13:17:44.136: INFO: Creating new exec pod
  E0824 13:17:44.426545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:45.427042      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:46.427718      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:47.181: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4166 exec execpod5442q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  E0824 13:17:47.428598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:47.496: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Aug 24 13:17:47.496: INFO: stdout: "nodeport-test-dkzcd"
  Aug 24 13:17:47.496: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4166 exec execpod5442q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.255 80'
  Aug 24 13:17:47.768: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.255 80\nConnection to 10.233.48.255 80 port [tcp/http] succeeded!\n"
  Aug 24 13:17:47.768: INFO: stdout: ""
  E0824 13:17:48.429586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:48.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4166 exec execpod5442q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.255 80'
  Aug 24 13:17:49.072: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.255 80\nConnection to 10.233.48.255 80 port [tcp/http] succeeded!\n"
  Aug 24 13:17:49.073: INFO: stdout: ""
  E0824 13:17:49.430656      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:49.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4166 exec execpod5442q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.255 80'
  Aug 24 13:17:50.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.255 80\nConnection to 10.233.48.255 80 port [tcp/http] succeeded!\n"
  Aug 24 13:17:50.034: INFO: stdout: ""
  E0824 13:17:50.431291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:50.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4166 exec execpod5442q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.255 80'
  Aug 24 13:17:51.021: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.255 80\nConnection to 10.233.48.255 80 port [tcp/http] succeeded!\n"
  Aug 24 13:17:51.021: INFO: stdout: "nodeport-test-lj464"
  Aug 24 13:17:51.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4166 exec execpod5442q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.15 30746'
  Aug 24 13:17:51.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.15 30746\nConnection to 192.168.121.15 30746 port [tcp/*] succeeded!\n"
  Aug 24 13:17:51.282: INFO: stdout: "nodeport-test-dkzcd"
  Aug 24 13:17:51.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-4166 exec execpod5442q -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.64 30746'
  E0824 13:17:51.432413      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:17:51.553: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.64 30746\nConnection to 192.168.121.64 30746 port [tcp/*] succeeded!\n"
  Aug 24 13:17:51.553: INFO: stdout: "nodeport-test-dkzcd"
  Aug 24 13:17:51.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4166" for this suite. @ 08/24/23 13:17:51.563
• [10.600 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 08/24/23 13:17:51.59
  Aug 24 13:17:51.590: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename gc @ 08/24/23 13:17:51.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:17:51.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:17:51.634
  STEP: create the rc @ 08/24/23 13:17:51.646
  W0824 13:17:51.656341      15 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0824 13:17:52.433135      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:53.435375      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:54.491227      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:55.480008      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:56.486645      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:57.486812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/24/23 13:17:57.815
  STEP: wait for the rc to be deleted @ 08/24/23 13:17:58.009
  E0824 13:17:58.488029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:17:59.489546      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:00.496023      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:01.524359      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:02.518734      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 08/24/23 13:18:03.078
  E0824 13:18:03.501346      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:04.505964      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:05.507279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:06.519532      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:07.519797      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:08.520188      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:09.520341      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:10.520460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:11.520924      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:12.520955      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:13.521865      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:14.522134      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:15.522297      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:16.522518      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:17.523157      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:18.523527      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:19.523830      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:20.523995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:21.524201      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:22.524336      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:23.526767      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:24.526119      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:25.526222      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:26.526648      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:27.526674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:28.527837      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:29.527907      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:30.528252      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:31.528509      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:32.528625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/24/23 13:18:33.112
  Aug 24 13:18:33.370: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug 24 13:18:33.374: INFO: Deleting pod "simpletest.rc-287hg" in namespace "gc-5106"
  Aug 24 13:18:33.401: INFO: Deleting pod "simpletest.rc-28c8b" in namespace "gc-5106"
  Aug 24 13:18:33.424: INFO: Deleting pod "simpletest.rc-2jw6l" in namespace "gc-5106"
  Aug 24 13:18:33.488: INFO: Deleting pod "simpletest.rc-2wxcz" in namespace "gc-5106"
  Aug 24 13:18:33.521: INFO: Deleting pod "simpletest.rc-4996w" in namespace "gc-5106"
  E0824 13:18:33.528928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:33.574: INFO: Deleting pod "simpletest.rc-4glzc" in namespace "gc-5106"
  Aug 24 13:18:33.599: INFO: Deleting pod "simpletest.rc-4jfz7" in namespace "gc-5106"
  Aug 24 13:18:33.665: INFO: Deleting pod "simpletest.rc-4qfwt" in namespace "gc-5106"
  Aug 24 13:18:33.738: INFO: Deleting pod "simpletest.rc-5lc22" in namespace "gc-5106"
  Aug 24 13:18:33.818: INFO: Deleting pod "simpletest.rc-5q2xg" in namespace "gc-5106"
  Aug 24 13:18:33.882: INFO: Deleting pod "simpletest.rc-5tqlh" in namespace "gc-5106"
  Aug 24 13:18:33.976: INFO: Deleting pod "simpletest.rc-6dh5j" in namespace "gc-5106"
  Aug 24 13:18:34.120: INFO: Deleting pod "simpletest.rc-6lsgf" in namespace "gc-5106"
  Aug 24 13:18:34.170: INFO: Deleting pod "simpletest.rc-6nhxk" in namespace "gc-5106"
  Aug 24 13:18:34.215: INFO: Deleting pod "simpletest.rc-6s2hd" in namespace "gc-5106"
  Aug 24 13:18:34.268: INFO: Deleting pod "simpletest.rc-75xnr" in namespace "gc-5106"
  Aug 24 13:18:34.334: INFO: Deleting pod "simpletest.rc-7hpvz" in namespace "gc-5106"
  Aug 24 13:18:34.387: INFO: Deleting pod "simpletest.rc-7kmbx" in namespace "gc-5106"
  Aug 24 13:18:34.448: INFO: Deleting pod "simpletest.rc-7xb6h" in namespace "gc-5106"
  Aug 24 13:18:34.508: INFO: Deleting pod "simpletest.rc-88p4x" in namespace "gc-5106"
  E0824 13:18:34.532698      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:34.562: INFO: Deleting pod "simpletest.rc-8cdpm" in namespace "gc-5106"
  Aug 24 13:18:34.645: INFO: Deleting pod "simpletest.rc-8t8wj" in namespace "gc-5106"
  Aug 24 13:18:34.699: INFO: Deleting pod "simpletest.rc-94l9f" in namespace "gc-5106"
  Aug 24 13:18:34.764: INFO: Deleting pod "simpletest.rc-95dw9" in namespace "gc-5106"
  Aug 24 13:18:34.837: INFO: Deleting pod "simpletest.rc-97rnn" in namespace "gc-5106"
  Aug 24 13:18:34.930: INFO: Deleting pod "simpletest.rc-9nvkc" in namespace "gc-5106"
  Aug 24 13:18:35.007: INFO: Deleting pod "simpletest.rc-9pj2g" in namespace "gc-5106"
  Aug 24 13:18:35.094: INFO: Deleting pod "simpletest.rc-9xvtz" in namespace "gc-5106"
  Aug 24 13:18:35.173: INFO: Deleting pod "simpletest.rc-bfw26" in namespace "gc-5106"
  Aug 24 13:18:35.206: INFO: Deleting pod "simpletest.rc-bmq6g" in namespace "gc-5106"
  Aug 24 13:18:35.262: INFO: Deleting pod "simpletest.rc-cmvxz" in namespace "gc-5106"
  Aug 24 13:18:35.345: INFO: Deleting pod "simpletest.rc-dcqbp" in namespace "gc-5106"
  Aug 24 13:18:35.451: INFO: Deleting pod "simpletest.rc-dvjng" in namespace "gc-5106"
  E0824 13:18:35.535707      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:35.536: INFO: Deleting pod "simpletest.rc-dwmvj" in namespace "gc-5106"
  Aug 24 13:18:35.579: INFO: Deleting pod "simpletest.rc-f8z4t" in namespace "gc-5106"
  Aug 24 13:18:35.767: INFO: Deleting pod "simpletest.rc-fb79d" in namespace "gc-5106"
  Aug 24 13:18:35.852: INFO: Deleting pod "simpletest.rc-fd7jr" in namespace "gc-5106"
  Aug 24 13:18:35.964: INFO: Deleting pod "simpletest.rc-fgncl" in namespace "gc-5106"
  Aug 24 13:18:36.044: INFO: Deleting pod "simpletest.rc-fhmpn" in namespace "gc-5106"
  Aug 24 13:18:36.187: INFO: Deleting pod "simpletest.rc-fnsbz" in namespace "gc-5106"
  Aug 24 13:18:36.242: INFO: Deleting pod "simpletest.rc-fqdsc" in namespace "gc-5106"
  Aug 24 13:18:36.322: INFO: Deleting pod "simpletest.rc-fsr27" in namespace "gc-5106"
  Aug 24 13:18:36.403: INFO: Deleting pod "simpletest.rc-fwt9k" in namespace "gc-5106"
  Aug 24 13:18:36.492: INFO: Deleting pod "simpletest.rc-g8srj" in namespace "gc-5106"
  E0824 13:18:36.538630      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:36.579: INFO: Deleting pod "simpletest.rc-gbq76" in namespace "gc-5106"
  Aug 24 13:18:36.656: INFO: Deleting pod "simpletest.rc-gkb4r" in namespace "gc-5106"
  Aug 24 13:18:36.730: INFO: Deleting pod "simpletest.rc-gm7w4" in namespace "gc-5106"
  Aug 24 13:18:36.762: INFO: Deleting pod "simpletest.rc-gwcgp" in namespace "gc-5106"
  Aug 24 13:18:36.839: INFO: Deleting pod "simpletest.rc-gzkkk" in namespace "gc-5106"
  Aug 24 13:18:36.959: INFO: Deleting pod "simpletest.rc-hbjqs" in namespace "gc-5106"
  Aug 24 13:18:37.116: INFO: Deleting pod "simpletest.rc-hkn8c" in namespace "gc-5106"
  Aug 24 13:18:37.285: INFO: Deleting pod "simpletest.rc-hqmxn" in namespace "gc-5106"
  Aug 24 13:18:37.403: INFO: Deleting pod "simpletest.rc-hvkgd" in namespace "gc-5106"
  Aug 24 13:18:37.525: INFO: Deleting pod "simpletest.rc-jjrpw" in namespace "gc-5106"
  E0824 13:18:37.539412      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:37.576: INFO: Deleting pod "simpletest.rc-jk58w" in namespace "gc-5106"
  Aug 24 13:18:37.665: INFO: Deleting pod "simpletest.rc-jqrdl" in namespace "gc-5106"
  Aug 24 13:18:37.760: INFO: Deleting pod "simpletest.rc-jt2bj" in namespace "gc-5106"
  Aug 24 13:18:37.844: INFO: Deleting pod "simpletest.rc-jx5fz" in namespace "gc-5106"
  Aug 24 13:18:37.954: INFO: Deleting pod "simpletest.rc-kfghc" in namespace "gc-5106"
  Aug 24 13:18:38.035: INFO: Deleting pod "simpletest.rc-kfzb6" in namespace "gc-5106"
  Aug 24 13:18:38.088: INFO: Deleting pod "simpletest.rc-kpk8z" in namespace "gc-5106"
  Aug 24 13:18:38.198: INFO: Deleting pod "simpletest.rc-ktqk9" in namespace "gc-5106"
  Aug 24 13:18:38.280: INFO: Deleting pod "simpletest.rc-l6rjq" in namespace "gc-5106"
  Aug 24 13:18:38.334: INFO: Deleting pod "simpletest.rc-lvpdf" in namespace "gc-5106"
  Aug 24 13:18:38.385: INFO: Deleting pod "simpletest.rc-m22pm" in namespace "gc-5106"
  Aug 24 13:18:38.448: INFO: Deleting pod "simpletest.rc-m76x5" in namespace "gc-5106"
  E0824 13:18:38.539956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:38.553: INFO: Deleting pod "simpletest.rc-mbbk8" in namespace "gc-5106"
  Aug 24 13:18:38.625: INFO: Deleting pod "simpletest.rc-mjhdr" in namespace "gc-5106"
  Aug 24 13:18:38.713: INFO: Deleting pod "simpletest.rc-mkwvx" in namespace "gc-5106"
  Aug 24 13:18:38.775: INFO: Deleting pod "simpletest.rc-mtjnw" in namespace "gc-5106"
  Aug 24 13:18:38.873: INFO: Deleting pod "simpletest.rc-npkgl" in namespace "gc-5106"
  Aug 24 13:18:38.989: INFO: Deleting pod "simpletest.rc-pr98t" in namespace "gc-5106"
  Aug 24 13:18:39.022: INFO: Deleting pod "simpletest.rc-q9j9f" in namespace "gc-5106"
  Aug 24 13:18:39.114: INFO: Deleting pod "simpletest.rc-qv85m" in namespace "gc-5106"
  Aug 24 13:18:39.208: INFO: Deleting pod "simpletest.rc-r5pcc" in namespace "gc-5106"
  Aug 24 13:18:39.304: INFO: Deleting pod "simpletest.rc-r8td4" in namespace "gc-5106"
  Aug 24 13:18:39.336: INFO: Deleting pod "simpletest.rc-sfmlv" in namespace "gc-5106"
  Aug 24 13:18:39.366: INFO: Deleting pod "simpletest.rc-smglp" in namespace "gc-5106"
  Aug 24 13:18:39.453: INFO: Deleting pod "simpletest.rc-svz6f" in namespace "gc-5106"
  E0824 13:18:39.540360      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:39.542: INFO: Deleting pod "simpletest.rc-sxf6r" in namespace "gc-5106"
  Aug 24 13:18:39.625: INFO: Deleting pod "simpletest.rc-sz82x" in namespace "gc-5106"
  Aug 24 13:18:39.668: INFO: Deleting pod "simpletest.rc-t587v" in namespace "gc-5106"
  Aug 24 13:18:39.821: INFO: Deleting pod "simpletest.rc-tbbzg" in namespace "gc-5106"
  Aug 24 13:18:39.971: INFO: Deleting pod "simpletest.rc-tczsh" in namespace "gc-5106"
  Aug 24 13:18:40.133: INFO: Deleting pod "simpletest.rc-tr5n9" in namespace "gc-5106"
  Aug 24 13:18:40.242: INFO: Deleting pod "simpletest.rc-ttmlc" in namespace "gc-5106"
  Aug 24 13:18:40.311: INFO: Deleting pod "simpletest.rc-vlldf" in namespace "gc-5106"
  Aug 24 13:18:40.396: INFO: Deleting pod "simpletest.rc-vndhx" in namespace "gc-5106"
  Aug 24 13:18:40.485: INFO: Deleting pod "simpletest.rc-w5c76" in namespace "gc-5106"
  E0824 13:18:40.540790      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:40.541: INFO: Deleting pod "simpletest.rc-w9cfl" in namespace "gc-5106"
  Aug 24 13:18:40.572: INFO: Deleting pod "simpletest.rc-wff2l" in namespace "gc-5106"
  Aug 24 13:18:40.660: INFO: Deleting pod "simpletest.rc-wtnnb" in namespace "gc-5106"
  Aug 24 13:18:40.715: INFO: Deleting pod "simpletest.rc-x8wws" in namespace "gc-5106"
  Aug 24 13:18:40.794: INFO: Deleting pod "simpletest.rc-xrvxw" in namespace "gc-5106"
  Aug 24 13:18:40.873: INFO: Deleting pod "simpletest.rc-xt2dh" in namespace "gc-5106"
  Aug 24 13:18:41.050: INFO: Deleting pod "simpletest.rc-xzb2h" in namespace "gc-5106"
  Aug 24 13:18:41.134: INFO: Deleting pod "simpletest.rc-z4c27" in namespace "gc-5106"
  Aug 24 13:18:41.202: INFO: Deleting pod "simpletest.rc-z82g6" in namespace "gc-5106"
  Aug 24 13:18:41.379: INFO: Deleting pod "simpletest.rc-z8rzg" in namespace "gc-5106"
  E0824 13:18:41.542203      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:41.556: INFO: Deleting pod "simpletest.rc-zd4kd" in namespace "gc-5106"
  Aug 24 13:18:41.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5106" for this suite. @ 08/24/23 13:18:41.677
• [50.129 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 08/24/23 13:18:41.723
  Aug 24 13:18:41.723: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename services @ 08/24/23 13:18:41.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:18:41.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:18:41.878
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5776 @ 08/24/23 13:18:41.885
  STEP: changing the ExternalName service to type=NodePort @ 08/24/23 13:18:41.919
  STEP: creating replication controller externalname-service in namespace services-5776 @ 08/24/23 13:18:42.036
  I0824 13:18:42.054900      15 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5776, replica count: 2
  E0824 13:18:42.542583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:43.543309      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:44.543442      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 13:18:45.106657      15 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0824 13:18:45.544327      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:46.544949      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:47.544966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0824 13:18:48.107151      15 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug 24 13:18:48.107: INFO: Creating new exec pod
  E0824 13:18:48.545046      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:49.545405      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:50.545571      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:51.147: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5776 exec execpodktmrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug 24 13:18:51.506: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug 24 13:18:51.507: INFO: stdout: "externalname-service-nj4qn"
  Aug 24 13:18:51.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5776 exec execpodktmrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.47.19 80'
  E0824 13:18:51.545976      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:51.817: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.47.19 80\nConnection to 10.233.47.19 80 port [tcp/http] succeeded!\n"
  Aug 24 13:18:51.817: INFO: stdout: ""
  E0824 13:18:52.545962      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:52.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5776 exec execpodktmrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.47.19 80'
  Aug 24 13:18:53.124: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.47.19 80\nConnection to 10.233.47.19 80 port [tcp/http] succeeded!\n"
  Aug 24 13:18:53.124: INFO: stdout: "externalname-service-nj4qn"
  Aug 24 13:18:53.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5776 exec execpodktmrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.19 31007'
  Aug 24 13:18:53.444: INFO: stderr: "+ + nc -v -t -w 2 192.168.121.19 31007\necho hostName\nConnection to 192.168.121.19 31007 port [tcp/*] succeeded!\n"
  Aug 24 13:18:53.444: INFO: stdout: "externalname-service-nj4qn"
  Aug 24 13:18:53.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=services-5776 exec execpodktmrs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.15 31007'
  E0824 13:18:53.547077      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:53.716: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.15 31007\nConnection to 192.168.121.15 31007 port [tcp/*] succeeded!\n"
  Aug 24 13:18:53.716: INFO: stdout: "externalname-service-nj4qn"
  Aug 24 13:18:53.716: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug 24 13:18:53.726: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-5776" for this suite. @ 08/24/23 13:18:53.761
• [12.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 08/24/23 13:18:53.777
  Aug 24 13:18:53.777: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename watch @ 08/24/23 13:18:53.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:18:53.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:18:53.833
  STEP: getting a starting resourceVersion @ 08/24/23 13:18:53.837
  STEP: starting a background goroutine to produce watch events @ 08/24/23 13:18:53.844
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 08/24/23 13:18:53.845
  E0824 13:18:54.547003      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:55.547448      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:56.547157      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:18:56.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9260" for this suite. @ 08/24/23 13:18:56.644
• [2.917 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 08/24/23 13:18:56.696
  Aug 24 13:18:56.697: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 13:18:56.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:18:56.735
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:18:56.741
  STEP: Setting up server cert @ 08/24/23 13:18:56.784
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 13:18:57.367
  STEP: Deploying the webhook pod @ 08/24/23 13:18:57.382
  STEP: Wait for the deployment to be ready @ 08/24/23 13:18:57.401
  Aug 24 13:18:57.421: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0824 13:18:57.547996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:18:58.548438      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:18:59.446
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:18:59.467
  E0824 13:18:59.549568      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:19:00.467: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 08/24/23 13:19:00.474
  STEP: Creating a custom resource definition that should be denied by the webhook @ 08/24/23 13:19:00.523
  Aug 24 13:19:00.523: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 13:19:00.550237      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:19:00.553: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3259" for this suite. @ 08/24/23 13:19:00.688
  STEP: Destroying namespace "webhook-markers-1308" for this suite. @ 08/24/23 13:19:00.706
• [4.021 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 08/24/23 13:19:00.719
  Aug 24 13:19:00.719: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename cronjob @ 08/24/23 13:19:00.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:19:00.75
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:19:00.754
  STEP: Creating a ReplaceConcurrent cronjob @ 08/24/23 13:19:00.761
  STEP: Ensuring a job is scheduled @ 08/24/23 13:19:00.769
  E0824 13:19:01.550586      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:02.551076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:03.551463      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:04.552395      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:05.553064      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:06.553779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:07.553875      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:08.553982      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:09.555118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:10.556249      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:11.556710      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:12.557152      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:13.557338      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:14.557419      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:15.557741      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:16.557914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:17.558143      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:18.558473      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:19.560287      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:20.561562      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:21.562121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:22.562410      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:23.562472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:24.563191      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:25.563380      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:26.564002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:27.564177      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:28.565186      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:29.565544      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:30.565894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:31.566748      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:32.566955      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:33.567094      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:34.567726      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:35.568601      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:36.568799      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:37.569118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:38.569585      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:39.569817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:40.570552      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:41.570702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:42.571334      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:43.572257      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:44.572550      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:45.573654      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:46.573979      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:47.574728      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:48.575312      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:49.576768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:50.576549      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:51.577555      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:52.578293      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:53.578928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:54.579326      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:55.579665      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:56.580205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:57.580456      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:58.581613      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:19:59.581814      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:00.582403      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 08/24/23 13:20:00.778
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/24/23 13:20:00.785
  STEP: Ensuring the job is replaced with a new one @ 08/24/23 13:20:00.791
  E0824 13:20:01.583479      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:02.584372      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:03.585097      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:04.585711      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:05.586303      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:06.586882      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:07.587429      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:08.587954      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:09.588136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:10.588671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:11.588799      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:12.589519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:13.590720      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:14.590726      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:15.590968      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:16.591478      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:17.591636      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:18.592557      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:19.592905      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:20.593592      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:21.594581      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:22.595026      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:23.595702      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:24.596142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:25.596349      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:26.596434      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:27.596972      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:28.597852      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:29.598483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:30.598508      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:31.598842      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:32.599551      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:33.600621      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:34.600498      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:35.600729      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:36.601406      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:37.601627      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:38.602599      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:39.603477      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:40.603774      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:41.604040      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:42.603958      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:43.604735      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:44.604971      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:45.605195      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:46.605439      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:47.605565      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:48.605708      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:49.606989      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:50.607829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:51.608004      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:52.608610      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:53.610679      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:54.611015      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:55.611619      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:56.611832      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:57.611988      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:58.612755      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:20:59.612900      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:00.613053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 08/24/23 13:21:00.801
  Aug 24 13:21:00.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-6050" for this suite. @ 08/24/23 13:21:00.826
• [120.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 08/24/23 13:21:00.877
  Aug 24 13:21:00.877: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename dns @ 08/24/23 13:21:00.879
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:00.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:00.93
  STEP: Creating a test externalName service @ 08/24/23 13:21:00.937
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1894.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local; sleep 1; done
   @ 08/24/23 13:21:00.952
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1894.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local; sleep 1; done
   @ 08/24/23 13:21:00.953
  STEP: creating a pod to probe DNS @ 08/24/23 13:21:00.954
  STEP: submitting the pod to kubernetes @ 08/24/23 13:21:00.955
  E0824 13:21:01.618779      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:02.615603      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:03.615923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:04.616147      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 13:21:05.011
  STEP: looking for the results for each expected name from probers @ 08/24/23 13:21:05.017
  Aug 24 13:21:05.036: INFO: DNS probes using dns-test-683e25f9-fe10-446f-8035-e189383444dc succeeded

  STEP: changing the externalName to bar.example.com @ 08/24/23 13:21:05.036
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1894.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local; sleep 1; done
   @ 08/24/23 13:21:05.054
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1894.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local; sleep 1; done
   @ 08/24/23 13:21:05.055
  STEP: creating a second pod to probe DNS @ 08/24/23 13:21:05.055
  STEP: submitting the pod to kubernetes @ 08/24/23 13:21:05.056
  E0824 13:21:05.634176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:06.634641      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:07.634832      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:08.635661      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 13:21:09.09
  STEP: looking for the results for each expected name from probers @ 08/24/23 13:21:09.096
  Aug 24 13:21:09.107: INFO: File wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:09.117: INFO: File jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:09.118: INFO: Lookups using dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 failed for: [wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local]

  E0824 13:21:09.636395      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:10.636354      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:11.636663      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:12.636928      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:13.637093      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:14.128: INFO: File wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:14.136: INFO: File jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:14.137: INFO: Lookups using dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 failed for: [wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local]

  E0824 13:21:14.637841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:15.638155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:16.638371      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:17.638614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:18.639793      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:19.131: INFO: File wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:19.141: INFO: File jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:19.141: INFO: Lookups using dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 failed for: [wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local]

  E0824 13:21:19.640492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:20.640943      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:21.641246      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:22.641949      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:23.642158      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:24.133: INFO: File wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:24.144: INFO: File jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:24.144: INFO: Lookups using dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 failed for: [wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local]

  E0824 13:21:24.642884      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:25.643149      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:26.643496      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:27.644109      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:28.644751      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:29.125: INFO: File wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:29.136: INFO: File jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local from pod  dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug 24 13:21:29.136: INFO: Lookups using dns-1894/dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 failed for: [wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local]

  E0824 13:21:29.646603      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:30.646922      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:31.647099      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:32.647559      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:33.647495      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:34.136: INFO: DNS probes using dns-test-79019eb9-f27c-45ba-b4bb-523d1e533864 succeeded

  STEP: changing the service to type=ClusterIP @ 08/24/23 13:21:34.136
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1894.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1894.svc.cluster.local; sleep 1; done
   @ 08/24/23 13:21:34.168
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1894.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1894.svc.cluster.local; sleep 1; done
   @ 08/24/23 13:21:34.168
  STEP: creating a third pod to probe DNS @ 08/24/23 13:21:34.169
  STEP: submitting the pod to kubernetes @ 08/24/23 13:21:34.179
  E0824 13:21:34.648171      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:35.648000      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:36.648453      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:37.648709      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 13:21:38.229
  STEP: looking for the results for each expected name from probers @ 08/24/23 13:21:38.237
  Aug 24 13:21:38.257: INFO: DNS probes using dns-test-a125edc2-9bfb-4ef1-9779-207c5b9c00fa succeeded

  Aug 24 13:21:38.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:21:38.275
  STEP: deleting the pod @ 08/24/23 13:21:38.303
  STEP: deleting the pod @ 08/24/23 13:21:38.333
  STEP: deleting the test externalName service @ 08/24/23 13:21:38.39
  STEP: Destroying namespace "dns-1894" for this suite. @ 08/24/23 13:21:38.444
• [37.595 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 08/24/23 13:21:38.475
  Aug 24 13:21:38.475: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:21:38.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:38.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:38.55
  STEP: Creating the pod @ 08/24/23 13:21:38.556
  E0824 13:21:38.650175      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:39.650505      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:40.651594      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:41.651923      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:42.651945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:43.172: INFO: Successfully updated pod "labelsupdate048b9868-b128-44b2-932d-dc48f613a934"
  E0824 13:21:43.653711      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:44.653187      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:21:45.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8589" for this suite. @ 08/24/23 13:21:45.259
• [6.798 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 08/24/23 13:21:45.277
  Aug 24 13:21:45.277: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename taint-multiple-pods @ 08/24/23 13:21:45.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:21:45.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:21:45.317
  Aug 24 13:21:45.323: INFO: Waiting up to 1m0s for all nodes to be ready
  E0824 13:21:45.653275      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:46.653688      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:47.654151      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:48.655153      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:49.655736      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:50.656483      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:51.657251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:52.657862      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:53.659088      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:54.659689      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:55.660565      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:56.660800      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:57.661600      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:58.662547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:21:59.663514      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:00.663873      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:01.664215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:02.664815      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:03.665624      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:04.665841      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:05.666540      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:06.666866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:07.667868      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:08.668768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:09.669502      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:10.670166      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:11.670750      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:12.670914      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:13.671882      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:14.672321      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:15.672836      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:16.673057      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:17.673293      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:18.673311      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:19.673607      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:20.674418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:21.675205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:22.675780      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:23.676336      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:24.676848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:25.677472      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:26.677651      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:27.678640      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:28.678717      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:29.678942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:30.679111      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:31.679575      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:32.679976      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:33.681006      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:34.681723      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:35.681956      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:36.682632      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:37.682764      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:38.683897      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:39.684036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:40.684806      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:41.685513      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:42.685661      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:43.686572      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:44.686653      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:45.375: INFO: Waiting for terminating namespaces to be deleted...
  Aug 24 13:22:45.382: INFO: Starting informer...
  STEP: Starting pods... @ 08/24/23 13:22:45.383
  Aug 24 13:22:45.622: INFO: Pod1 is running on quohp9aeph3i-3. Tainting Node
  E0824 13:22:45.687541      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:46.687600      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:47.687816      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:47.865: INFO: Pod2 is running on quohp9aeph3i-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/24/23 13:22:47.865
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/24/23 13:22:47.908
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 08/24/23 13:22:47.925
  E0824 13:22:48.688553      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:49.688931      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:50.689330      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:51.689929      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:52.690154      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:53.691083      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:22:54.067: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0824 13:22:54.691466      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:55.692356      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:56.693030      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:57.693569      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:58.693614      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:22:59.693796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:00.694395      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:01.694517      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:02.695056      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:03.695279      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:04.695508      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:05.695639      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:06.695839      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:07.696039      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:08.696813      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:09.697068      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:10.697226      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:11.697379      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:12.697602      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:13.698725      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:14.135: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Aug 24 13:23:14.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/24/23 13:23:14.172
  STEP: Destroying namespace "taint-multiple-pods-9389" for this suite. @ 08/24/23 13:23:14.181
• [88.916 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 08/24/23 13:23:14.196
  Aug 24 13:23:14.197: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename configmap @ 08/24/23 13:23:14.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:14.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:14.239
  STEP: Creating configMap with name configmap-test-volume-4e60c1e9-c5de-45f4-a367-e51b012fb95d @ 08/24/23 13:23:14.246
  STEP: Creating a pod to test consume configMaps @ 08/24/23 13:23:14.255
  E0824 13:23:14.698582      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:15.699142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:16.699681      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:17.700046      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:23:18.299
  Aug 24 13:23:18.305: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-configmaps-91317fc5-3804-4a25-9468-2536910c6b9c container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 13:23:18.336
  Aug 24 13:23:18.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9531" for this suite. @ 08/24/23 13:23:18.374
• [4.193 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 08/24/23 13:23:18.395
  Aug 24 13:23:18.395: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:23:18.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:18.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:18.439
  STEP: Creating configMap with name projected-configmap-test-volume-f73ab962-3be2-4e10-b5a2-54c364577d9c @ 08/24/23 13:23:18.445
  STEP: Creating a pod to test consume configMaps @ 08/24/23 13:23:18.455
  E0824 13:23:18.700225      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:19.700251      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:20.701356      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:21.701827      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:23:22.491
  Aug 24 13:23:22.499: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-configmaps-ff8f2dc4-d6ac-48aa-88e2-82b44c41ba33 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 13:23:22.513
  Aug 24 13:23:22.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9229" for this suite. @ 08/24/23 13:23:22.551
• [4.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 08/24/23 13:23:22.575
  Aug 24 13:23:22.575: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename lease-test @ 08/24/23 13:23:22.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:22.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:22.633
  E0824 13:23:22.702525      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:22.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-5232" for this suite. @ 08/24/23 13:23:22.766
• [0.205 seconds]
------------------------------
SSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 08/24/23 13:23:22.784
  Aug 24 13:23:22.785: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename hostport @ 08/24/23 13:23:22.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:22.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:22.837
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 08/24/23 13:23:22.852
  E0824 13:23:23.703393      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:24.703911      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.19 on the node which pod1 resides and expect scheduled @ 08/24/23 13:23:24.944
  E0824 13:23:25.704118      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:26.704096      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.19 but use UDP protocol on the node which pod2 resides @ 08/24/23 13:23:26.99
  E0824 13:23:27.704455      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:28.704674      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:29.704916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:30.705632      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:31.706760      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:32.706993      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 08/24/23 13:23:33.084
  Aug 24 13:23:33.084: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.19 http://127.0.0.1:54323/hostname] Namespace:hostport-5051 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:23:33.084: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 13:23:33.087: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:23:33.087: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5051/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.19+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.19, port: 54323 @ 08/24/23 13:23:33.261
  Aug 24 13:23:33.261: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.19:54323/hostname] Namespace:hostport-5051 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:23:33.261: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 13:23:33.263: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:23:33.263: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5051/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.19%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.19, port: 54323 UDP @ 08/24/23 13:23:33.404
  Aug 24 13:23:33.404: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.19 54323] Namespace:hostport-5051 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug 24 13:23:33.404: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  Aug 24 13:23:33.405: INFO: ExecWithOptions: Clientset creation
  Aug 24 13:23:33.406: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5051/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.19+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0824 13:23:33.708029      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:34.708343      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:35.708329      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:36.708519      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:37.708801      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:38.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-5051" for this suite. @ 08/24/23 13:23:38.55
• [15.781 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 08/24/23 13:23:38.567
  Aug 24 13:23:38.568: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:23:38.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:38.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:38.615
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:23:38.62
  E0824 13:23:38.708942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:39.709181      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:40.709888      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:41.710205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:23:42.682
  Aug 24 13:23:42.689: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-1dadf486-274c-41ae-8e8e-e5a6bb10724c container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:23:42.702
  E0824 13:23:42.710848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:42.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5610" for this suite. @ 08/24/23 13:23:42.747
• [4.191 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 08/24/23 13:23:42.761
  Aug 24 13:23:42.761: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 13:23:42.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:42.801
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:42.807
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/24/23 13:23:42.811
  E0824 13:23:43.711234      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:44.711707      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:45.711741      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:46.711972      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:23:46.849
  Aug 24 13:23:46.857: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-9326f963-a92b-48af-9826-8b8e0c3e69a0 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 13:23:46.875
  Aug 24 13:23:46.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8144" for this suite. @ 08/24/23 13:23:46.923
• [4.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 08/24/23 13:23:46.943
  Aug 24 13:23:46.943: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename containers @ 08/24/23 13:23:46.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:46.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:46.985
  E0824 13:23:47.716838      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:48.717364      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:49.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6569" for this suite. @ 08/24/23 13:23:49.069
• [2.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 08/24/23 13:23:49.093
  Aug 24 13:23:49.093: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename container-probe @ 08/24/23 13:23:49.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:23:49.132
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:23:49.138
  STEP: Creating pod busybox-da8272bc-ecb5-471d-8769-527a296fd614 in namespace container-probe-1747 @ 08/24/23 13:23:49.145
  E0824 13:23:49.719737      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:50.719053      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:23:51.183: INFO: Started pod busybox-da8272bc-ecb5-471d-8769-527a296fd614 in namespace container-probe-1747
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/24/23 13:23:51.183
  Aug 24 13:23:51.190: INFO: Initial restart count of pod busybox-da8272bc-ecb5-471d-8769-527a296fd614 is 0
  E0824 13:23:51.719041      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:52.719418      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:53.719636      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:54.720076      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:55.720398      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:56.721001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:57.721001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:58.721851      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:23:59.722452      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:00.723142      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:01.723356      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:02.723991      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:03.724762      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:04.725357      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:05.725545      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:06.726084      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:07.726752      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:08.726807      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:09.727002      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:10.727233      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:11.728103      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:12.728881      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:13.728821      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:14.729498      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:15.729894      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:16.730390      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:17.730951      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:18.731961      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:19.732989      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:20.733164      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:21.733873      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:22.734669      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:23.735804      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:24.736460      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:25.737430      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:26.738066      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:27.738080      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:28.739284      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:29.739366      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:30.739934      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:31.740198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:32.740671      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:33.741811      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:34.741861      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:35.742108      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:36.742799      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:37.743032      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:38.743343      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:39.744155      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:40.744454      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:24:41.476: INFO: Restart count of pod container-probe-1747/busybox-da8272bc-ecb5-471d-8769-527a296fd614 is now 1 (50.285960953s elapsed)
  Aug 24 13:24:41.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:24:41.487
  STEP: Destroying namespace "container-probe-1747" for this suite. @ 08/24/23 13:24:41.513
• [52.438 seconds]
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 08/24/23 13:24:41.531
  Aug 24 13:24:41.531: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename csistoragecapacity @ 08/24/23 13:24:41.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:24:41.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:24:41.584
  STEP: getting /apis @ 08/24/23 13:24:41.594
  STEP: getting /apis/storage.k8s.io @ 08/24/23 13:24:41.607
  STEP: getting /apis/storage.k8s.io/v1 @ 08/24/23 13:24:41.61
  STEP: creating @ 08/24/23 13:24:41.613
  STEP: watching @ 08/24/23 13:24:41.656
  Aug 24 13:24:41.656: INFO: starting watch
  STEP: getting @ 08/24/23 13:24:41.677
  STEP: listing in namespace @ 08/24/23 13:24:41.684
  STEP: listing across namespaces @ 08/24/23 13:24:41.691
  STEP: patching @ 08/24/23 13:24:41.7
  STEP: updating @ 08/24/23 13:24:41.716
  Aug 24 13:24:41.727: INFO: waiting for watch events with expected annotations in namespace
  Aug 24 13:24:41.728: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 08/24/23 13:24:41.729
  E0824 13:24:41.745036      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting a collection @ 08/24/23 13:24:41.764
  Aug 24 13:24:41.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-3389" for this suite. @ 08/24/23 13:24:41.82
• [0.308 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 08/24/23 13:24:41.85
  Aug 24 13:24:41.850: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename events @ 08/24/23 13:24:41.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:24:41.9
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:24:41.906
  STEP: creating a test event @ 08/24/23 13:24:41.911
  STEP: listing all events in all namespaces @ 08/24/23 13:24:41.922
  STEP: patching the test event @ 08/24/23 13:24:41.93
  STEP: fetching the test event @ 08/24/23 13:24:41.944
  STEP: updating the test event @ 08/24/23 13:24:41.951
  STEP: getting the test event @ 08/24/23 13:24:41.975
  STEP: deleting the test event @ 08/24/23 13:24:41.987
  STEP: listing all events in all namespaces @ 08/24/23 13:24:42.006
  Aug 24 13:24:42.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1977" for this suite. @ 08/24/23 13:24:42.03
• [0.202 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 08/24/23 13:24:42.056
  Aug 24 13:24:42.056: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename secrets @ 08/24/23 13:24:42.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:24:42.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:24:42.108
  STEP: Creating secret with name secret-test-7262596c-7837-4f86-9c46-8500abd46741 @ 08/24/23 13:24:42.113
  STEP: Creating a pod to test consume secrets @ 08/24/23 13:24:42.124
  E0824 13:24:42.748832      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:43.746768      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:44.747682      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:45.748312      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:24:46.167
  Aug 24 13:24:46.174: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-secrets-258caefa-c9bd-4808-bfe9-c0a324ee7327 container secret-env-test: <nil>
  STEP: delete the pod @ 08/24/23 13:24:46.188
  Aug 24 13:24:46.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7743" for this suite. @ 08/24/23 13:24:46.238
• [4.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 08/24/23 13:24:46.257
  Aug 24 13:24:46.257: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename conformance-tests @ 08/24/23 13:24:46.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:24:46.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:24:46.3
  STEP: Getting node addresses @ 08/24/23 13:24:46.306
  Aug 24 13:24:46.306: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Aug 24 13:24:46.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-8484" for this suite. @ 08/24/23 13:24:46.344
• [0.103 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 08/24/23 13:24:46.368
  Aug 24 13:24:46.368: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename emptydir @ 08/24/23 13:24:46.372
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:24:46.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:24:46.414
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/24/23 13:24:46.422
  E0824 13:24:46.749107      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:47.749480      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:48.750054      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:49.750132      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:24:50.469
  Aug 24 13:24:50.476: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-95fa5c78-f537-40d1-9f92-4244cf7e0930 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 13:24:50.498
  Aug 24 13:24:50.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-203" for this suite. @ 08/24/23 13:24:50.54
• [4.187 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 08/24/23 13:24:50.562
  Aug 24 13:24:50.562: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename certificates @ 08/24/23 13:24:50.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:24:50.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:24:50.606
  E0824 13:24:50.750851      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 08/24/23 13:24:51.538
  STEP: getting /apis/certificates.k8s.io @ 08/24/23 13:24:51.546
  STEP: getting /apis/certificates.k8s.io/v1 @ 08/24/23 13:24:51.549
  STEP: creating @ 08/24/23 13:24:51.551
  STEP: getting @ 08/24/23 13:24:51.589
  STEP: listing @ 08/24/23 13:24:51.597
  STEP: watching @ 08/24/23 13:24:51.608
  Aug 24 13:24:51.609: INFO: starting watch
  STEP: patching @ 08/24/23 13:24:51.612
  STEP: updating @ 08/24/23 13:24:51.628
  Aug 24 13:24:51.644: INFO: waiting for watch events with expected annotations
  Aug 24 13:24:51.645: INFO: saw patched and updated annotations
  STEP: getting /approval @ 08/24/23 13:24:51.646
  STEP: patching /approval @ 08/24/23 13:24:51.652
  STEP: updating /approval @ 08/24/23 13:24:51.668
  STEP: getting /status @ 08/24/23 13:24:51.687
  STEP: patching /status @ 08/24/23 13:24:51.695
  STEP: updating /status @ 08/24/23 13:24:51.711
  STEP: deleting @ 08/24/23 13:24:51.731
  E0824 13:24:51.751197      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting a collection @ 08/24/23 13:24:51.761
  Aug 24 13:24:51.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-1432" for this suite. @ 08/24/23 13:24:51.81
• [1.265 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 08/24/23 13:24:51.827
  Aug 24 13:24:51.827: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename namespaces @ 08/24/23 13:24:51.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:24:51.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:24:51.878
  STEP: Creating a test namespace @ 08/24/23 13:24:51.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:24:51.933
  STEP: Creating a service in the namespace @ 08/24/23 13:24:51.938
  STEP: Deleting the namespace @ 08/24/23 13:24:51.955
  STEP: Waiting for the namespace to be removed. @ 08/24/23 13:24:51.984
  E0824 13:24:52.751843      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:53.753120      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:54.753247      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:55.753385      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:56.753547      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:57.753954      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 08/24/23 13:24:57.99
  STEP: Verifying there is no service in the namespace @ 08/24/23 13:24:58.019
  Aug 24 13:24:58.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4056" for this suite. @ 08/24/23 13:24:58.038
  STEP: Destroying namespace "nsdeletetest-9566" for this suite. @ 08/24/23 13:24:58.05
  Aug 24 13:24:58.056: INFO: Namespace nsdeletetest-9566 was already deleted
  STEP: Destroying namespace "nsdeletetest-8142" for this suite. @ 08/24/23 13:24:58.056
• [6.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 08/24/23 13:24:58.079
  Aug 24 13:24:58.079: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename sched-preemption @ 08/24/23 13:24:58.081
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:24:58.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:24:58.131
  Aug 24 13:24:58.177: INFO: Waiting up to 1m0s for all nodes to be ready
  E0824 13:24:58.754608      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:24:59.754812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:00.755121      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:01.755872      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:02.756922      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:03.757189      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:04.758212      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:05.758406      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:06.758690      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:07.758974      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:08.759966      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:09.760850      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:10.761215      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:11.761663      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:12.762891      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:13.762997      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:14.763136      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:15.763812      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:16.764176      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:17.764305      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:18.764500      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:19.765001      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:20.765306      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:21.765961      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:22.766211      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:23.766443      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:24.766559      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:25.766817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:26.767294      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:27.767830      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:28.767733      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:29.768291      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:30.768776      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:31.768941      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:32.769205      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:33.769504      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:34.770577      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:35.771301      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:36.771514      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:37.772115      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:38.772316      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:39.772817      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:40.773180      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:41.773675      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:42.774861      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:43.775525      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:44.775866      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:45.776601      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:46.776778      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:47.777272      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:48.777667      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:49.777867      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:50.777995      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:51.778877      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:52.779824      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:53.780173      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:54.781151      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:55.781396      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:56.782466      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:57.782628      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:25:58.240: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/24/23 13:25:58.247
  Aug 24 13:25:58.286: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug 24 13:25:58.299: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug 24 13:25:58.332: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug 24 13:25:58.347: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Aug 24 13:25:58.401: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Aug 24 13:25:58.424: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/24/23 13:25:58.424
  E0824 13:25:58.783942      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:25:59.784085      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:00.784250      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:01.784583      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 08/24/23 13:26:02.485
  E0824 13:26:02.784770      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:03.785828      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:04.785796      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:05.786285      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:06.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 13:26:06.786945      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "sched-preemption-549" for this suite. @ 08/24/23 13:26:06.798
• [68.742 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 08/24/23 13:26:06.834
  Aug 24 13:26:06.834: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:26:06.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:06.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:06.884
  STEP: Creating configMap with name projected-configmap-test-volume-map-84d3df78-6018-4814-aa61-9d404c455e68 @ 08/24/23 13:26:06.896
  STEP: Creating a pod to test consume configMaps @ 08/24/23 13:26:06.909
  E0824 13:26:07.787512      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:08.788588      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:09.788598      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:10.788793      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:26:10.976
  Aug 24 13:26:10.984: INFO: Trying to get logs from node quohp9aeph3i-3 pod pod-projected-configmaps-a2d040de-4c60-42b9-815b-6df8abae81a9 container agnhost-container: <nil>
  STEP: delete the pod @ 08/24/23 13:26:10.995
  Aug 24 13:26:11.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8059" for this suite. @ 08/24/23 13:26:11.036
• [4.218 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 08/24/23 13:26:11.058
  Aug 24 13:26:11.059: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename deployment @ 08/24/23 13:26:11.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:11.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:11.104
  Aug 24 13:26:11.132: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0824 13:26:11.791024      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:12.791130      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:13.792440      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:14.792533      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:15.792829      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:16.145: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/24/23 13:26:16.145
  Aug 24 13:26:16.146: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 08/24/23 13:26:16.169
  Aug 24 13:26:16.193: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9824  706130ce-0a05-4baf-863b-355231612215 43063 1 2023-08-24 13:26:16 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-24 13:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0054c1498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Aug 24 13:26:16.203: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Aug 24 13:26:16.203: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Aug 24 13:26:16.204: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9824  eaca9b94-d385-4c86-bfd6-d689a18a51f6 43065 1 2023-08-24 13:26:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 706130ce-0a05-4baf-863b-355231612215 0xc004632127 0xc004632128}] [] [{e2e.test Update apps/v1 2023-08-24 13:26:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-24 13:26:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-24 13:26:16 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"706130ce-0a05-4baf-863b-355231612215\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0046321f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug 24 13:26:16.219: INFO: Pod "test-cleanup-controller-gx5gg" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-gx5gg test-cleanup-controller- deployment-9824  de02eb44-5414-4310-b263-10c2a8332bea 43014 0 2023-08-24 13:26:11 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller eaca9b94-d385-4c86-bfd6-d689a18a51f6 0xc0046324f7 0xc0046324f8}] [] [{kube-controller-manager Update v1 2023-08-24 13:26:11 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"eaca9b94-d385-4c86-bfd6-d689a18a51f6\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-24 13:26:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.164\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j8hsz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j8hsz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:quohp9aeph3i-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:26:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:26:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:26:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-24 13:26:11 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.15,PodIP:10.233.66.164,StartTime:2023-08-24 13:26:11 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-24 13:26:12 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://eefabb2403244003a3e5938f90092b28094cf32214c71bdff34072f4eecb20d9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.164,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug 24 13:26:16.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9824" for this suite. @ 08/24/23 13:26:16.246
• [5.213 seconds]
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 08/24/23 13:26:16.273
  Aug 24 13:26:16.274: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename disruption @ 08/24/23 13:26:16.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:16.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:16.341
  STEP: Creating a pdb that targets all three pods in a test replica set @ 08/24/23 13:26:16.347
  STEP: Waiting for the pdb to be processed @ 08/24/23 13:26:16.359
  E0824 13:26:16.793126      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:17.793161      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 08/24/23 13:26:18.385
  STEP: Waiting for all pods to be running @ 08/24/23 13:26:18.386
  Aug 24 13:26:18.400: INFO: pods: 1 < 3
  E0824 13:26:18.793996      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:19.794550      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:20.413: INFO: running pods: 2 < 3
  E0824 13:26:20.794653      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:21.795889      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 08/24/23 13:26:22.41
  STEP: Updating the pdb to allow a pod to be evicted @ 08/24/23 13:26:22.432
  STEP: Waiting for the pdb to be processed @ 08/24/23 13:26:22.451
  E0824 13:26:22.795847      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:23.796831      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/24/23 13:26:24.465
  STEP: Waiting for all pods to be running @ 08/24/23 13:26:24.465
  STEP: Waiting for the pdb to observed all healthy pods @ 08/24/23 13:26:24.474
  STEP: Patching the pdb to disallow a pod to be evicted @ 08/24/23 13:26:24.53
  STEP: Waiting for the pdb to be processed @ 08/24/23 13:26:24.628
  E0824 13:26:24.797848      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:25.797987      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 08/24/23 13:26:26.65
  STEP: locating a running pod @ 08/24/23 13:26:26.66
  STEP: Deleting the pdb to allow a pod to be evicted @ 08/24/23 13:26:26.68
  STEP: Waiting for the pdb to be deleted @ 08/24/23 13:26:26.697
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/24/23 13:26:26.703
  STEP: Waiting for all pods to be running @ 08/24/23 13:26:26.703
  Aug 24 13:26:26.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7610" for this suite. @ 08/24/23 13:26:26.775
  E0824 13:26:26.799094      15 retrywatcher.go:130] "Watch failed" err="context canceled"
• [10.530 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 08/24/23 13:26:26.813
  Aug 24 13:26:26.813: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename security-context @ 08/24/23 13:26:26.817
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:26.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:26.859
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/24/23 13:26:26.865
  E0824 13:26:27.800027      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:28.801082      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:29.801198      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:30.801274      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:26:30.924
  Aug 24 13:26:30.936: INFO: Trying to get logs from node quohp9aeph3i-3 pod security-context-6510a277-7902-46ec-807d-7d51d58d4ba1 container test-container: <nil>
  STEP: delete the pod @ 08/24/23 13:26:30.952
  Aug 24 13:26:30.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4129" for this suite. @ 08/24/23 13:26:30.992
• [4.196 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 08/24/23 13:26:31.009
  Aug 24 13:26:31.009: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename projected @ 08/24/23 13:26:31.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:31.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:31.048
  STEP: Creating configMap with name cm-test-opt-del-25c2e9df-63e6-4592-80ab-b81009f7e4e1 @ 08/24/23 13:26:31.06
  STEP: Creating configMap with name cm-test-opt-upd-dbbe7b46-5ef0-49c7-9e28-e2e733fc3317 @ 08/24/23 13:26:31.07
  STEP: Creating the pod @ 08/24/23 13:26:31.079
  E0824 13:26:31.805327      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:32.805521      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-25c2e9df-63e6-4592-80ab-b81009f7e4e1 @ 08/24/23 13:26:33.178
  STEP: Updating configmap cm-test-opt-upd-dbbe7b46-5ef0-49c7-9e28-e2e733fc3317 @ 08/24/23 13:26:33.218
  STEP: Creating configMap with name cm-test-opt-create-756a80bc-bd13-4a18-b2d7-34ff8c75f32e @ 08/24/23 13:26:33.316
  STEP: waiting to observe update in volume @ 08/24/23 13:26:33.369
  E0824 13:26:33.805737      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:34.806055      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:35.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7909" for this suite. @ 08/24/23 13:26:35.445
• [4.458 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 08/24/23 13:26:35.468
  Aug 24 13:26:35.468: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename dns @ 08/24/23 13:26:35.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:35.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:35.518
  STEP: Creating a test headless service @ 08/24/23 13:26:35.524
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1440.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1440.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 08/24/23 13:26:35.536
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1440.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1440.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 08/24/23 13:26:35.536
  STEP: creating a pod to probe DNS @ 08/24/23 13:26:35.536
  STEP: submitting the pod to kubernetes @ 08/24/23 13:26:35.536
  E0824 13:26:35.806633      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:36.806972      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:37.807379      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:38.808039      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/24/23 13:26:39.587
  STEP: looking for the results for each expected name from probers @ 08/24/23 13:26:39.594
  Aug 24 13:26:39.631: INFO: DNS probes using dns-1440/dns-test-54d2d09f-6e46-4a84-a17a-4187d50ade42 succeeded

  Aug 24 13:26:39.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/24/23 13:26:39.643
  STEP: deleting the test headless service @ 08/24/23 13:26:39.679
  STEP: Destroying namespace "dns-1440" for this suite. @ 08/24/23 13:26:39.716
• [4.274 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 08/24/23 13:26:39.753
  Aug 24 13:26:39.753: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename kubectl @ 08/24/23 13:26:39.756
  E0824 13:26:39.808625      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:39.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:39.863
  STEP: Starting the proxy @ 08/24/23 13:26:39.869
  Aug 24 13:26:39.871: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4116876534 --namespace=kubectl-6654 proxy --unix-socket=/tmp/kubectl-proxy-unix1052502255/test'
  STEP: retrieving proxy /api/ output @ 08/24/23 13:26:40.042
  Aug 24 13:26:40.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6654" for this suite. @ 08/24/23 13:26:40.079
• [0.346 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 08/24/23 13:26:40.104
  Aug 24 13:26:40.104: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename var-expansion @ 08/24/23 13:26:40.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:40.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:40.147
  STEP: Creating a pod to test substitution in container's command @ 08/24/23 13:26:40.152
  E0824 13:26:40.810492      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:41.808981      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:42.810130      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:43.810962      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:26:44.189
  Aug 24 13:26:44.196: INFO: Trying to get logs from node quohp9aeph3i-3 pod var-expansion-a3bd46da-4718-422e-a957-ae0935635c19 container dapi-container: <nil>
  STEP: delete the pod @ 08/24/23 13:26:44.22
  Aug 24 13:26:44.255: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6611" for this suite. @ 08/24/23 13:26:44.269
• [4.179 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 08/24/23 13:26:44.283
  Aug 24 13:26:44.283: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename webhook @ 08/24/23 13:26:44.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:44.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:44.321
  STEP: Setting up server cert @ 08/24/23 13:26:44.368
  E0824 13:26:44.811245      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/24/23 13:26:45.486
  STEP: Deploying the webhook pod @ 08/24/23 13:26:45.505
  STEP: Wait for the deployment to be ready @ 08/24/23 13:26:45.54
  Aug 24 13:26:45.559: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0824 13:26:45.811938      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:46.812678      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/24/23 13:26:47.582
  STEP: Verifying the service has paired with the endpoint @ 08/24/23 13:26:47.601
  E0824 13:26:47.813494      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:48.602: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug 24 13:26:48.610: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  E0824 13:26:48.813612      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-124-crds.webhook.example.com via the AdmissionRegistration API @ 08/24/23 13:26:49.139
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/24/23 13:26:49.175
  E0824 13:26:49.814833      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:50.814916      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug 24 13:26:51.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0824 13:26:51.815214      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-3983" for this suite. @ 08/24/23 13:26:52.224
  STEP: Destroying namespace "webhook-markers-1928" for this suite. @ 08/24/23 13:26:52.241
• [7.971 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 08/24/23 13:26:52.279
  Aug 24 13:26:52.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4116876534
  STEP: Building a namespace api object, basename downward-api @ 08/24/23 13:26:52.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/24/23 13:26:52.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/24/23 13:26:52.334
  STEP: Creating a pod to test downward API volume plugin @ 08/24/23 13:26:52.341
  E0824 13:26:52.819262      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:53.819144      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:54.819414      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0824 13:26:55.819794      15 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/24/23 13:26:56.394
  Aug 24 13:26:56.400: INFO: Trying to get logs from node quohp9aeph3i-3 pod downwardapi-volume-3c9aa9ff-82ca-40c0-8616-bacea20c8217 container client-container: <nil>
  STEP: delete the pod @ 08/24/23 13:26:56.413
  Aug 24 13:26:56.443: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-313" for this suite. @ 08/24/23 13:26:56.451
• [4.183 seconds]
------------------------------
SSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Aug 24 13:26:56.467: INFO: Running AfterSuite actions on node 1
  Aug 24 13:26:56.467: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.112 seconds]
------------------------------

Ran 378 of 7207 Specs in 6462.720 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h47m43.764912112s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

